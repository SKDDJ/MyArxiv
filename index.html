<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-07T00:00:00Z">2024-11-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">127</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit <span class="highlight-title">Diffusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have been proven highly effective at generating high-quality
images. However, as these models grow larger, they require significantly more
memory and suffer from higher latency, posing substantial challenges for
deployment. In this work, we aim to accelerate diffusion models by quantizing
their weights and activations to 4 bits. At such an aggressive level, both
weights and activations are highly sensitive, where conventional post-training
quantization methods for large language models like smoothing become
insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit
quantization paradigm. Different from smoothing which redistributes outliers
between weights and activations, our approach absorbs these outliers using a
low-rank branch. We first consolidate the outliers by shifting them from
activations to weights, then employ a high-precision low-rank branch to take in
the weight outliers with Singular Value Decomposition (SVD). This process eases
the quantization on both sides. However, na\"{\i}vely running the low-rank
branch independently incurs significant overhead due to extra data movement of
activations, negating the quantization speedup. To address this, we co-design
an inference engine Nunchaku that fuses the kernels of the low-rank branch into
those of the low-bit branch to cut off redundant memory access. It can also
seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for
re-quantization. Extensive experiments on SDXL, PixArt-$\Sigma$, and FLUX.1
validate the effectiveness of SVDQuant in preserving image quality. We reduce
the memory usage for the 12B FLUX.1 models by 3.5$\times$, achieving
3.0$\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB
laptop 4090 GPU, paving the way for more interactive applications on PCs. Our
quantization library and inference engine are open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Quantization Library: https://github.com/mit-han-lab/deepcompressor
  Inference Engine: https://github.com/mit-han-lab/nunchaku Website:
  https://hanlab.mit.edu/projects/svdquant Demo: https://svdquant.mit.edu Blog:
  https://hanlab.mit.edu/blog/svdquant</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProEdit: Simple Progression is All You Need for High-Quality 3D Scene
  Editing <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Kun Chen, Yu-Xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes ProEdit - a simple yet effective framework for
high-quality 3D scene editing guided by diffusion distillation in a novel
progressive manner. Inspired by the crucial observation that multi-view
inconsistency in scene editing is rooted in the diffusion model's large
feasible output space (FOS), our framework controls the size of FOS and reduces
inconsistency by decomposing the overall editing task into several subtasks,
which are then executed progressively on the scene. Within this framework, we
design a difficulty-aware subtask decomposition scheduler and an adaptive 3D
Gaussian splatting (3DGS) training strategy, ensuring high quality and
efficiency in performing each subtask. Extensive evaluation shows that our
ProEdit achieves state-of-the-art results in various scenes and challenging
editing tasks, all through a simple framework without any expensive or
sophisticated add-ons like distillation losses, components, or training
procedures. Notably, ProEdit also provides a new way to control, preview, and
select the "aggressivity" of editing operation during the editing process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Project Page: https://immortalco.github.io/ProEdit/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diff-2-in-1: Bridging Generation and Dense Perception with <span class="highlight-title">Diffusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhong Zheng, Zhipeng Bao, Ruoyu Zhao, Martial Hebert, Yu-Xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beyond high-fidelity image synthesis, diffusion models have recently
exhibited promising results in dense visual perception tasks. However, most
existing work treats diffusion models as a standalone component for perception
tasks, employing them either solely for off-the-shelf data augmentation or as
mere feature extractors. In contrast to these isolated and thus sub-optimal
efforts, we introduce a unified, versatile, diffusion-based framework,
Diff-2-in-1, that can simultaneously handle both multi-modal data generation
and dense visual perception, through a unique exploitation of the
diffusion-denoising process. Within this framework, we further enhance
discriminative visual perception via multi-modal generation, by utilizing the
denoising network to create multi-modal data that mirror the distribution of
the original training set. Importantly, Diff-2-in-1 optimizes the utilization
of the created diverse and faithful data by leveraging a novel self-improving
learning mechanism. Comprehensive experimental evaluations validate the
effectiveness of our framework, showcasing consistent performance improvements
across various discriminative backbones and high-quality multi-modal data
generation characterized by both realism and usefulness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReCapture: Generative Video Camera Controls for User-Provided Videos
  using Masked Video Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David E. Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, Nataniel Ruiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, breakthroughs in video modeling have allowed for controllable
camera trajectories in generated videos. However, these methods cannot be
directly applied to user-provided videos that are not generated by a video
model. In this paper, we present ReCapture, a method for generating new videos
with novel camera trajectories from a single user-provided video. Our method
allows us to re-generate the reference video, with all its existing scene
motion, from vastly different angles and with cinematic camera motion. Notably,
using our method we can also plausibly hallucinate parts of the scene that were
not observable in the reference video. Our method works by (1) generating a
noisy anchor video with a new camera trajectory using multiview diffusion
models or depth-based point cloud rendering and then (2) regenerating the
anchor video into a clean and temporally consistent reangled video using our
proposed masked video fine-tuning technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://generative-video-camera-controls.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing The Language of Visual Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David M. Chan, Rodolfo Corona, Joonyong Park, Cheol Jun Cho, Yutong Bai, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the introduction of transformer-based models for vision and language
tasks, such as LLaVA and Chameleon, there has been renewed interest in the
discrete tokenized representation of images. These models often treat image
patches as discrete tokens, analogous to words in natural language, learning
joint alignments between visual and human languages. However, little is known
about the statistical behavior of these visual languages - whether they follow
similar frequency distributions, grammatical structures, or topologies as
natural languages. In this paper, we take a natural-language-centric approach
to analyzing discrete visual languages and uncover striking similarities and
fundamental differences. We demonstrate that, although visual languages adhere
to Zipfian distributions, higher token innovation drives greater entropy and
lower compression, with tokens predominantly representing object parts,
indicating intermediate granularity. We also show that visual languages lack
cohesive grammatical structures, leading to higher perplexity and weaker
hierarchical organization compared to natural languages. Finally, we
demonstrate that, while vision models align more closely with natural languages
than other models, this alignment remains significantly weaker than the
cohesion found within natural languages. Through these experiments, we
demonstrate how understanding the statistical properties of discrete visual
languages can inform the design of more effective computer vision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP is one of the most important multimodal foundational models today. What
powers CLIP's capabilities? The rich supervision signals provided by natural
language, the carrier of human knowledge, shape a powerful cross-modal
representation space. However, with the rapid advancements in large language
models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and
generation are continually being pushed. This raises an intriguing question:
can the capabilities of LLMs be harnessed to further improve multimodal
representation learning? The potential benefits of incorporating LLMs into CLIP
are clear. LLMs' strong textual understanding can fundamentally improve CLIP's
ability to handle image captions, drastically enhancing its ability to process
long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs
are trained on a vast corpus of text, possessing open-world knowledge. This
allows them to expand on caption information during training, increasing the
efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel
approach that embraces the power of LLMs to unlock CLIP's potential. By
fine-tuning the LLM in the caption space with contrastive learning, we extract
its textual capabilities into the output embeddings, significantly improving
the output layer's textual discriminability. We then design an efficient
training process where the fine-tuned LLM acts as a powerful teacher for CLIP's
visual encoder. Thanks to the LLM's presence, we can now incorporate longer and
more complex captions without being restricted by vanilla CLIP's text encoder's
context window and ability limitations. Our experiments demonstrate that this
approach brings substantial improvements in cross-modal tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HourVideo: 1-Hour Video-Language Understanding <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Cristóbal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, Li Fei-Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HourVideo, a benchmark dataset for hour-long video-language
understanding. Our dataset consists of a novel task suite comprising
summarization, perception (recall, tracking), visual reasoning (spatial,
temporal, predictive, causal, counterfactual), and navigation (room-to-room,
object retrieval) tasks. HourVideo includes 500 manually curated egocentric
videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and
features 12,976 high-quality, five-way multiple-choice questions. Benchmarking
results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve
marginal improvements over random chance. In stark contrast, human experts
significantly outperform the state-of-the-art long-context multimodal model,
Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal
capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are
available at https://hourvideo.stanford.edu
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track; 28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoFi: Scalable Local Image Reconstruction with Implicit Neural
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        AmirEhsan Khorashadizadeh, Tobías I. Liaudat, Tianlin Liu, Jason D. McEwen, Ivan Dokmanić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural fields or implicit neural representations (INRs) have attracted
significant attention in machine learning and signal processing due to their
efficient continuous representation of images and 3D volumes. In this work, we
build on INRs and introduce a coordinate-based local processing framework for
solving imaging inverse problems, termed LoFi (Local Field). Unlike
conventional methods for image reconstruction, LoFi processes local information
at each coordinate \textit{separately} by multi-layer perceptrons (MLPs),
recovering the object at that specific coordinate. Similar to INRs, LoFi can
recover images at any continuous coordinate, enabling image reconstruction at
multiple resolutions. With comparable or better performance than standard CNNs
for image reconstruction, LoFi achieves excellent generalization to
out-of-distribution data and memory usage almost independent of image
resolution. Remarkably, training on $1024 \times 1024$ images requires just 3GB
of memory -- over 20 times less than the memory typically needed by standard
CNNs. Additionally, LoFi's local design allows it to train on extremely small
datasets with less than 10 samples, without overfitting or the need for
regularization or early stopping. Finally, we use LoFi as a denoising prior in
a plug-and-play framework for solving general inverse problems to benefit from
its continuous image representation and strong generalization. Although trained
on low-resolution images, LoFi can be used as a low-dimensional prior to solve
inverse problems at any resolution. We validate our framework across a variety
of imaging modalities, from low-dose computed tomography to radio
interferometric imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, David B. Lindell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods for image-to-video generation have achieved impressive,
photo-realistic quality. However, adjusting specific elements in generated
videos, such as object motion or camera movement, is often a tedious process of
trial and error, e.g., involving re-generating videos with different random
seeds. Recent techniques address this issue by fine-tuning a pre-trained model
to follow conditioning signals, such as bounding boxes or point trajectories.
Yet, this fine-tuning procedure can be computationally expensive, and it
requires datasets with annotated object motion, which can be difficult to
procure. In this work, we introduce SG-I2V, a framework for controllable
image-to-video generation that is self-guided$\unicode{x2013}$offering
zero-shot control by relying solely on the knowledge present in a pre-trained
image-to-video diffusion model without the need for fine-tuning or external
knowledge. Our zero-shot method outperforms unsupervised baselines while being
competitive with supervised models in terms of visual quality and motion
fidelity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://kmcode1.github.io/Projects/SG-I2V/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planar Reflection-Aware Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Gao, Yipeng Wang, Changil Kim, Jia-Bin Huang, Johannes Kopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) have demonstrated exceptional capabilities in
reconstructing complex scenes with high fidelity. However, NeRF's view
dependency can only handle low-frequency reflections. It falls short when
handling complex planar reflections, often interpreting them as erroneous scene
geometries and leading to duplicated and inaccurate scene representations. To
address this challenge, we introduce a reflection-aware NeRF that jointly
models planar reflectors, such as windows, and explicitly casts reflected rays
to capture the source of the high-frequency reflections. We query a single
radiance field to render the primary color and the source of the reflection. We
propose a sparse edge regularization to help utilize the true sources of
reflections for rendering planar reflections rather than creating a duplicate
along the primary ray at the same depth. As a result, we obtain accurate scene
geometry. Rendering along the primary ray results in a clean, reflection-free
view, while explicitly rendering along the reflected ray allows us to
reconstruct highly detailed reflections. Our extensive quantitative and
qualitative evaluations of real-world datasets demonstrate our method's
enhanced performance in accurately handling reflections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AsCAN: Asymmetric Convolution-Attention Networks for Efficient
  Recognition and Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Kag, Huseyin Coskun, Jierun Chen, Junli Cao, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, Jian Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network architecture design requires making many crucial decisions.
The common desiderata is that similar decisions, with little modifications, can
be reused in a variety of tasks and applications. To satisfy that,
architectures must provide promising latency and performance trade-offs,
support a variety of tasks, scale efficiently with respect to the amounts of
data and compute, leverage available data from other tasks, and efficiently
support various hardware. To this end, we introduce AsCAN -- a hybrid
architecture, combining both convolutional and transformer blocks. We revisit
the key design principles of hybrid architectures and propose a simple and
effective \emph{asymmetric} architecture, where the distribution of
convolutional and transformer blocks is \emph{asymmetric}, containing more
convolutional blocks in the earlier stages, followed by more transformer blocks
in later stages. AsCAN supports a variety of tasks: recognition, segmentation,
class-conditional image generation, and features a superior trade-off between
performance and latency. We then scale the same architecture to solve a
large-scale text-to-image task and show state-of-the-art performance compared
to the most recent public and commercial models. Notably, even without any
computation optimization for transformer blocks, our models still yield faster
inference speed than existing works featuring efficient attention mechanisms,
highlighting the advantages and the value of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Project Page:
  https://snap-research.github.io/snap_image/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VAIR: Visuo-Acoustic Implicit Representations for Low-Cost, Multi-Modal
  Transparent Surface Reconstruction in Indoor Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Advaith V. Sethuraman, Onur Bagoren, Harikrishnan Seetharaman, Dalton Richardson, Joseph Taylor, Katherine A. Skinner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile robots operating indoors must be prepared to navigate challenging
scenes that contain transparent surfaces. This paper proposes a novel method
for the fusion of acoustic and visual sensing modalities through implicit
neural representations to enable dense reconstruction of transparent surfaces
in indoor scenes. We propose a novel model that leverages generative latent
optimization to learn an implicit representation of indoor scenes consisting of
transparent surfaces. We demonstrate that we can query the implicit
representation to enable volumetric rendering in image space or 3D geometry
reconstruction (point clouds or mesh) with transparent surface prediction. We
evaluate our method's effectiveness qualitatively and quantitatively on a new
dataset collected using a custom, low-cost sensing platform featuring RGB-D
cameras and ultrasonic sensors. Our method exhibits significant improvement
over state-of-the-art for transparent surface reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://umfieldrobotics.github.io/VAIR_site/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering Hidden Subspaces in Video <span class="highlight-title">Diffusion</span> Models Using
  Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mischa Dombrowski, Hadrien Reynaud, Bernhard Kainz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent Video Diffusion Models can easily deceive casual observers and domain
experts alike thanks to the produced image quality and temporal consistency.
Beyond entertainment, this creates opportunities around safe data sharing of
fully synthetic datasets, which are crucial in healthcare, as well as other
domains relying on sensitive personal information. However, privacy concerns
with this approach have not fully been addressed yet, and models trained on
synthetic data for specific downstream tasks still perform worse than those
trained on real data. This discrepancy may be partly due to the sampling space
being a subspace of the training videos, effectively reducing the training data
size for downstream models. Additionally, the reduced temporal consistency when
generating long videos could be a contributing factor.
  In this paper, we first show that training privacy-preserving models in
latent space is computationally more efficient and generalize better.
Furthermore, to investigate downstream degradation factors, we propose to use a
re-identification model, previously employed as a privacy preservation filter.
We demonstrate that it is sufficient to train this model on the latent space of
the video generator. Subsequently, we use these models to evaluate the subspace
covered by synthetic video datasets and thus introduce a new way to measure the
faithfulness of generative machine learning models. We focus on a specific
application in healthcare echocardiography to illustrate the effectiveness of
our novel methods. Our findings indicate that only up to 30.8% of the training
videos are learned in latent video diffusion models, which could explain the
lack of performance when training downstream tasks on synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 tables, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Xu, Chenyu Wang, Zibo Zhao, Wen Liu, Yi Ma, Shenghua Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to design a unified Computer-Aided Design (CAD) generation
system that can easily generate CAD models based on the user's inputs in the
form of textual description, images, point clouds, or even a combination of
them. Towards this goal, we introduce the CAD-MLLM, the first system capable of
generating parametric CAD models conditioned on the multimodal input.
Specifically, within the CAD-MLLM framework, we leverage the command sequences
of CAD models and then employ advanced large language models (LLMs) to align
the feature space across these diverse multi-modalities data and CAD models'
vectorized representations. To facilitate the model training, we design a
comprehensive data construction and annotation pipeline that equips each CAD
model with corresponding multimodal data. Our resulting dataset, named
Omni-CAD, is the first multimodal CAD dataset that contains textual
description, multi-view images, points, and command sequence for each CAD
model. It contains approximately 450K instances and their CAD construction
sequences. To thoroughly evaluate the quality of our generated CAD models, we
go beyond current evaluation metrics that focus on reconstruction quality by
introducing additional metrics that assess topology quality and surface
enclosure extent. Extensive experimental results demonstrate that CAD-MLLM
significantly outperforms existing conditional generative methods and remains
highly robust to noises and missing points. The project page and more
visualizations can be found at: https://cad-mllm.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://cad-mllm.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page
  Multi-document Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document visual question answering (DocVQA) pipelines that answer questions
from documents have broad applications. Existing methods focus on handling
single-page documents with multi-modal language models (MLMs), or rely on
text-based retrieval-augmented generation (RAG) that uses text extraction tools
such as optical character recognition (OCR). However, there are difficulties in
applying these methods in real-world scenarios: (a) questions often require
information across different pages or documents, where MLMs cannot handle many
long documents; (b) documents often have important information in visual
elements such as figures, but text extraction tools ignore them. We introduce
M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various
document contexts (closed-domain and open-domain), question hops (single-hop
and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG
finds relevant documents and answers questions using a multi-modal retriever
and an MLM, so that it can efficiently handle single or many documents while
preserving visual information. Since previous DocVQA datasets ask questions in
the context of a specific document, we also present M3DocVQA, a new benchmark
for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.
In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results
show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance
than many strong baselines, including state-of-the-art performance in
MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and
retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully
handle various scenarios, such as when relevant information exists across
multiple pages and when answer evidence only exists in images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://m3docrag.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Reinforcement Learning-Based Automatic Video Editing Method Using
  Pre-trained Vision-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panwen Hu, Nan Xiao, Feifei Li, Yongquan Chen, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this era of videos, automatic video editing techniques attract more and
more attention from industry and academia since they can reduce workloads and
lower the requirements for human editors. Existing automatic editing systems
are mainly scene- or event-specific, e.g., soccer game broadcasting, yet the
automatic systems for general editing, e.g., movie or vlog editing which covers
various scenes and events, were rarely studied before, and converting the
event-driven editing method to a general scene is nontrivial. In this paper, we
propose a two-stage scheme for general editing. Firstly, unlike previous works
that extract scene-specific features, we leverage the pre-trained
Vision-Language Model (VLM) to extract the editing-relevant representations as
editing context. Moreover, to close the gap between the professional-looking
videos and the automatic productions generated with simple guidelines, we
propose a Reinforcement Learning (RL)-based editing framework to formulate the
editing problem and train the virtual editor to make better sequential editing
decisions. Finally, we evaluate the proposed method on a more general editing
task with a real movie dataset. Experimental results demonstrate the
effectiveness and benefits of the proposed context representation and the
learning ability of our RL-based editing framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SaSR-Net: Source-Aware Semantic Representation Network for Enhancing
  Audio-Visual Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ianyu Yang, Yiyang Nan, Lisen Dai, Zhenwen Liang, Yapeng Tian, Xiangliang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-Visual Question Answering (AVQA) is a challenging task that involves
answering questions based on both auditory and visual information in videos. A
significant challenge is interpreting complex multi-modal scenes, which include
both visual objects and sound sources, and connecting them to the given
question. In this paper, we introduce the Source-aware Semantic Representation
Network (SaSR-Net), a novel model designed for AVQA. SaSR-Net utilizes
source-wise learnable tokens to efficiently capture and align audio-visual
elements with the corresponding question. It streamlines the fusion of audio
and visual information using spatial and temporal attention mechanisms to
identify answers in multi-modal scenes. Extensive experiments on the Music-AVQA
and AVQA-Yang datasets show that SaSR-Net outperforms state-of-the-art AVQA
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DimensionX: Create Any 3D and 4D Scenes from a Single Image with
  Controllable Video <span class="highlight-title">Diffusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, Yikai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce \textbf{DimensionX}, a framework designed to
generate photorealistic 3D and 4D scenes from just a single image with video
diffusion. Our approach begins with the insight that both the spatial structure
of a 3D scene and the temporal evolution of a 4D scene can be effectively
represented through sequences of video frames. While recent video diffusion
models have shown remarkable success in producing vivid visuals, they face
limitations in directly recovering 3D/4D scenes due to limited spatial and
temporal controllability during generation. To overcome this, we propose
ST-Director, which decouples spatial and temporal factors in video diffusion by
learning dimension-aware LoRAs from dimension-variant data. This controllable
video diffusion approach enables precise manipulation of spatial structure and
temporal dynamics, allowing us to reconstruct both 3D and 4D representations
from sequential frames with the combination of spatial and temporal dimensions.
Additionally, to bridge the gap between generated videos and real-world scenes,
we introduce a trajectory-aware mechanism for 3D generation and an
identity-preserving denoising strategy for 4D generation. Extensive experiments
on various real-world and synthetic datasets demonstrate that DimensionX
achieves superior results in controllable video generation, as well as in 3D
and 4D scene generation, compared with previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://chenshuo20.github.io/DimensionX/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StoryAgent: Customized Storytelling Video Generation via Multi-Agent
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panwen Hu, Jin Jiang, Jianqi Chen, Mingfei Han, Shengcai Liao, Xiaojun Chang, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of AI-Generated Content (AIGC) has spurred research into automated
video generation to streamline conventional processes. However, automating
storytelling video production, particularly for customized narratives, remains
challenging due to the complexity of maintaining subject consistency across
shots. While existing approaches like Mora and AesopAgent integrate multiple
agents for Story-to-Video (S2V) generation, they fall short in preserving
protagonist consistency and supporting Customized Storytelling Video Generation
(CSVG). To address these limitations, we propose StoryAgent, a multi-agent
framework designed for CSVG. StoryAgent decomposes CSVG into distinct subtasks
assigned to specialized agents, mirroring the professional production process.
Notably, our framework includes agents for story design, storyboard generation,
video creation, agent coordination, and result evaluation. Leveraging the
strengths of different models, StoryAgent enhances control over the generation
process, significantly improving character consistency. Specifically, we
introduce a customized Image-to-Video (I2V) method, LoRA-BE, to enhance
intra-shot temporal consistency, while a novel storyboard generation pipeline
is proposed to maintain subject consistency across shots. Extensive experiments
demonstrate the effectiveness of our approach in synthesizing highly consistent
storytelling videos, outperforming state-of-the-art methods. Our contributions
include the introduction of StoryAgent, a versatile framework for video
generation tasks, and novel techniques for preserving protagonist consistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MVSplat360: Feed-Forward 360 Scene Synthesis from Sparse Views <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, Jianfei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MVSplat360, a feed-forward approach for 360{\deg} novel view
synthesis (NVS) of diverse real-world scenes, using only sparse observations.
This setting is inherently ill-posed due to minimal overlap among input views
and insufficient visual information provided, making it challenging for
conventional methods to achieve high-quality results. Our MVSplat360 addresses
this by effectively combining geometry-aware 3D reconstruction with temporally
consistent video generation. Specifically, it refactors a feed-forward 3D
Gaussian Splatting (3DGS) model to render features directly into the latent
space of a pre-trained Stable Video Diffusion (SVD) model, where these features
then act as pose and visual cues to guide the denoising process and produce
photorealistic 3D-consistent views. Our model is end-to-end trainable and
supports rendering arbitrary views with as few as 5 sparse input views. To
evaluate MVSplat360's performance, we introduce a new benchmark using the
challenging DL3DV-10K dataset, where MVSplat360 achieves superior visual
quality compared to state-of-the-art methods on wide-sweeping or even 360{\deg}
NVS tasks. Experiments on the existing benchmark RealEstate10K also confirm the
effectiveness of our model. The video results are available on our project
page: https://donydchen.github.io/mvsplat360.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, Project page: https://donydchen.github.io/mvsplat360,
  Code: https://github.com/donydchen/mvsplat360</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shehan Munasinghe, Hanan Gani, Wenqi Zhu, Jiale Cao, Eric Xing, Fahad Shahbaz Khan, Salman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-grained alignment between videos and text is challenging due to complex
spatial and temporal dynamics in videos. Existing video-based Large Multimodal
Models (LMMs) handle basic conversations but struggle with precise pixel-level
grounding in videos. To address this, we introduce VideoGLaMM, a LMM designed
for fine-grained pixel-level grounding in videos based on user-provided textual
inputs. Our design seamlessly connects three key components: a Large Language
Model, a dual vision encoder that emphasizes both spatial and temporal details,
and a spatio-temporal decoder for accurate mask generation. This connection is
facilitated via tunable V-L and L-V adapters that enable close Vision-Language
(VL) alignment. The architecture is trained to synchronize both spatial and
temporal elements of video content with textual instructions. To enable
fine-grained grounding, we curate a multimodal dataset featuring detailed
visually-grounded conversations using a semiautomatic annotation pipeline,
resulting in a diverse set of 38k video-QA triplets along with 83k objects and
671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded
Conversation Generation, Visual Grounding, and Referring Video Segmentation.
Experimental results show that our model consistently outperforms existing
approaches across all three tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report of VideoGLaMM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stem-OB: Generalizable Visual Imitation Learning with Stem-Like
  Convergent Observation through <span class="highlight-title">Diffusion</span> Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizhe Hu, Zihang Rui, Yao He, Yuyao Liu, Pu Hua, Huazhe Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual imitation learning methods demonstrate strong performance, yet they
lack generalization when faced with visual input perturbations, including
variations in lighting and textures, impeding their real-world application. We
propose Stem-OB that utilizes pretrained image diffusion models to suppress
low-level visual differences while maintaining high-level scene structures.
This image inversion process is akin to transforming the observation into a
shared representation, from which other observations stem, with extraneous
details removed. Stem-OB contrasts with data-augmentation approaches as it is
robust to various unspecified appearance changes without the need for
additional training. Our method is a simple yet highly effective plug-and-play
solution. Empirical results confirm the effectiveness of our approach in
simulated tasks and show an exceptionally significant improvement in real-world
applications, with an average increase of 22.2% in success rates compared to
the best baseline. See https://hukz18.github.io/Stem-Ob/ for more info.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Arxiv preprint version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Iris Centre Localisation for Assistive Eye-Gaze Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nipun Sandamal Ranasekara Pathiranage, Stefania Cristina, Kenneth P. Camilleri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this research work, we address the problem of robust iris centre
localisation in unconstrained conditions as a core component of our eye-gaze
tracking platform. We investigate the application of U-Net variants for
segmentation-based and regression-based approaches to improve our iris centre
localisation, which was previously based on Bayes' classification. The achieved
results are comparable to or better than the state-of-the-art, offering a
drastic improvement over those achieved by the Bayes' classifier, and without
sacrificing the real-time performance of our eye-gaze tracking platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In the Era of Prompt Learning with Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankit Jha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale foundation models like CLIP have shown strong zero-shot
generalization but struggle with domain shifts, limiting their adaptability. In
our work, we introduce \textsc{StyLIP}, a novel domain-agnostic prompt learning
strategy for Domain Generalization (DG). StyLIP disentangles visual style and
content in CLIP`s vision encoder by using style projectors to learn
domain-specific prompt tokens and combining them with content features. Trained
contrastively, this approach enables seamless adaptation across domains,
outperforming state-of-the-art methods on multiple DG benchmarks. Additionally,
we propose AD-CLIP for unsupervised domain adaptation (DA), leveraging CLIP`s
frozen vision backbone to learn domain-invariant prompts through image style
and content features. By aligning domains in embedding space with entropy
minimization, AD-CLIP effectively handles domain shifts, even when only target
domain samples are available. Lastly, we outline future work on class discovery
using prompt learning for semantic segmentation in remote sensing, focusing on
identifying novel or rare classes in unstructured environments. This paves the
way for more adaptive and generalizable models in complex, real-world
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICVGIP 2024, Young Faculty Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZAHA: Introducing the Level of Facade Generalization and the Large-Scale
  Point Cloud Facade Semantic Segmentation Benchmark <span class="highlight-title">Dataset</span> <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olaf Wysocki, Yue Tan, Thomas Froech, Yan Xia, Magdalena Wysocki, Ludwig Hoegner, Daniel Cremers, Christoph Holst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facade semantic segmentation is a long-standing challenge in photogrammetry
and computer vision. Although the last decades have witnessed the influx of
facade segmentation methods, there is a lack of comprehensive facade classes
and data covering the architectural variability. In ZAHA, we introduce Level of
Facade Generalization (LoFG), novel hierarchical facade classes designed based
on international urban modeling standards, ensuring compatibility with
real-world challenging classes and uniform methods' comparison. Realizing the
LoFG, we present to date the largest semantic 3D facade segmentation dataset,
providing 601 million annotated points at five and 15 classes of LoFG2 and
LoFG3, respectively. Moreover, we analyze the performance of baseline semantic
segmentation methods on our introduced LoFG classes and data, complementing it
with a discussion on the unresolved challenges for facade segmentation. We
firmly believe that ZAHA shall facilitate further development of 3D facade
semantic segmentation methods, enabling robust segmentation indispensable in
creating urban digital twins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025 (IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV))</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A multi-purpose automatic editing system based on lecture semantics for
  remote education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panwen Hu, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote teaching has become popular recently due to its convenience and
safety, especially under extreme circumstances like a pandemic. However, online
students usually have a poor experience since the information acquired from the
views provided by the broadcast platforms is limited. One potential solution is
to show more camera views simultaneously, but it is technically challenging and
distracting for the viewers. Therefore, an automatic multi-camera
directing/editing system, which aims at selecting the most concerned view at
each time instance to guide the attention of online students, is in urgent
demand. However, existing systems mostly make simple assumptions and focus on
tracking the position of the speaker instead of the real lecture semantics, and
therefore have limited capacities to deliver optimal information flow. To this
end, this paper proposes an automatic multi-purpose editing system based on the
lecture semantics, which can both direct the multiple video streams for
real-time broadcasting and edit the optimal video offline for review purposes.
Our system directs the views by semantically analyzing the class events while
following the professional directing rules, mimicking a human director to
capture the regions of interest from the viewpoint of the onsite students. We
conduct both qualitative and quantitative analyses to verify the effectiveness
of the proposed system and its components.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Gaussian Representation for Incomplete CT Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaokai Wu, Yuxiang Lu, Wei Ji, Suizhi Huang, Fengyu Yang, Shalayiding Sirejiding, Qichen He, Jing Tong, Yanbiao Ji, Yue Ding, Hongtao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incomplete Computed Tomography (CT) benefits patients by reducing radiation
exposure. However, reconstructing high-fidelity images from limited views or
angles remains challenging due to the ill-posed nature of the problem. Deep
Learning Reconstruction (DLR) methods have shown promise in enhancing image
quality, but the paradox between training data diversity and high
generalization ability remains unsolved. In this paper, we propose a novel
Gaussian Representation for Incomplete CT Reconstruction (GRCT) without the
usage of any neural networks or full-dose CT data. Specifically, we model the
3D volume as a set of learnable Gaussians, which are optimized directly from
the incomplete sinogram. Our method can be applied to multiple views and angles
without changing the architecture. Additionally, we propose a differentiable
Fast CT Reconstruction method for efficient clinical usage. Extensive
experiments on multiple datasets and settings demonstrate significant
improvements in reconstruction quality metrics and high efficiency. We plan to
release our code as open-source.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D$^3$epth: Self-Supervised Depth Estimation with Dynamic Mask in Dynamic
  Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Chen, Hong Liu, Wenhao Li, Ying Zhu, Guoquan Wang, Jianbing Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation is a crucial technology in robotics. Recently,
self-supervised depth estimation methods have demonstrated great potential as
they can efficiently leverage large amounts of unlabelled real-world data.
However, most existing methods are designed under the assumption of static
scenes, which hinders their adaptability in dynamic environments. To address
this issue, we present D$^3$epth, a novel method for self-supervised depth
estimation in dynamic scenes. It tackles the challenge of dynamic objects from
two key perspectives. First, within the self-supervised framework, we design a
reprojection constraint to identify regions likely to contain dynamic objects,
allowing the construction of a dynamic mask that mitigates their impact at the
loss level. Second, for multi-frame depth estimation, we introduce a cost
volume auto-masking strategy that leverages adjacent frames to identify regions
associated with dynamic objects and generate corresponding masks. This provides
guidance for subsequent processes. Furthermore, we propose a spectral entropy
uncertainty module that incorporates spectral entropy to guide uncertainty
estimation during depth fusion, effectively addressing issues arising from cost
volume computation in dynamic environments. Extensive experiments on KITTI and
Cityscapes datasets demonstrate that the proposed method consistently
outperforms existing self-supervised monocular depth estimation baselines. Code
is available at \url{https://github.com/Csyunling/D3epth}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Open sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-end Inception-Unet based Generative Adversarial Networks for Snow
  and Rain Removals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Kajo, Mohamed Kas, Yassine Ruichek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The superior performance introduced by deep learning approaches in removing
atmospheric particles such as snow and rain from a single image; favors their
usage over classical ones. However, deep learning-based approaches still suffer
from challenges related to the particle appearance characteristics such as
size, type, and transparency. Furthermore, due to the unique characteristics of
rain and snow particles, single network based deep learning approaches struggle
in handling both degradation scenarios simultaneously. In this paper, a global
framework that consists of two Generative Adversarial Networks (GANs) is
proposed where each handles the removal of each particle individually. The
architectures of both desnowing and deraining GANs introduce the integration of
a feature extraction phase with the classical U-net generator network which in
turn enhances the removal performance in the presence of severe variations in
size and appearance. Furthermore, a realistic dataset that contains pairs of
snowy images next to their groundtruth images estimated using a low-rank
approximation approach; is presented. The experiments show that the proposed
desnowing and deraining approaches achieve significant improvements in
comparison to the state-of-the-art approaches when tested on both synthetic and
realistic datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GANESH: Generalizable NeRF for Lensless Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rakesh Raj Madavan, Akshat Kaimal, Badhrinarayanan K V, Vinayak Gupta, Rohit Choudhary, Chandrakala Shanmuganathan, Kaushik Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lensless imaging offers a significant opportunity to develop ultra-compact
cameras by removing the conventional bulky lens system. However, without a
focusing element, the sensor's output is no longer a direct image but a complex
multiplexed scene representation. Traditional methods have attempted to address
this challenge by employing learnable inversions and refinement models, but
these methods are primarily designed for 2D reconstruction and do not
generalize well to 3D reconstruction. We introduce GANESH, a novel framework
designed to enable simultaneous refinement and novel view synthesis from
multi-view lensless images. Unlike existing methods that require scene-specific
training, our approach supports on-the-fly inference without retraining on each
scene. Moreover, our framework allows us to tune our model to specific scenes,
enhancing the rendering and refinement quality. To facilitate research in this
area, we also present the first multi-view lensless dataset, LenslessScenes.
Extensive experiments demonstrate that our method outperforms current
approaches in reconstruction accuracy and refinement quality. Code and video
results are available at https://rakesh-123-cryp.github.io/Rakesh.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPVO: Motion-Prior based Visual Odometry for PointGoal Navigation <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayan Paul, Ruddra dev Roychoudhury, Brojeshwar Bhowmick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual odometry (VO) is essential for enabling accurate point-goal navigation
of embodied agents in indoor environments where GPS and compass sensors are
unreliable and inaccurate. However, traditional VO methods face challenges in
wide-baseline scenarios, where fast robot motions and low frames per second
(FPS) during inference hinder their performance, leading to drift and
catastrophic failures in point-goal navigation. Recent deep-learned VO methods
show robust performance but suffer from sample inefficiency during training;
hence, they require huge datasets and compute resources. So, we propose a
robust and sample-efficient VO pipeline based on motion priors available while
an agent is navigating an environment. It consists of a training-free
action-prior based geometric VO module that estimates a coarse relative pose
which is further consumed as a motion prior by a deep-learned VO model, which
finally produces a fine relative pose to be used by the navigation policy. This
strategy helps our pipeline achieve up to 2x sample efficiency during training
and demonstrates superior accuracy and robustness in point-goal navigation
tasks compared to state-of-the-art VO method(s). Realistic indoor environments
of the Gibson dataset is used in the AI-Habitat simulator to evaluate the
proposed approach using navigation metrics (like success/SPL) and pose metrics
(like RPE/ATE). We hope this method further opens a direction of work where
motion priors from various sources can be utilized to improve VO estimates and
achieve better results in embodied navigation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 50SFM Workshop of the 18th European Conference on
  Computer Vision (ECCV) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Effective Pipeline for Whole-Slide Image Glomerulus Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Huu Cap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole-slide images (WSI) glomerulus segmentation is essential for accurately
diagnosing kidney diseases. In this work, we propose a practical pipeline for
glomerulus segmentation that effectively enhances both patch-level and
WSI-level segmentation tasks. Our approach leverages stitching on overlapping
patches, increasing the detection coverage, especially when glomeruli are
located near patch image borders. In addition, we conduct comprehensive
evaluations from different segmentation models across two large and diverse
datasets with over 30K glomerulus annotations. Experimental results demonstrate
that models using our pipeline outperform the previous state-of-the-art method,
achieving superior results across both datasets and setting a new benchmark for
glomerulus segmentation in WSIs. The code and pre-trained models are available
at https://github.com/huuquan1994/wsi_glomerulus_seg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Rectified Flow for Inversion and Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rectified-flow-based diffusion transformers, such as FLUX and OpenSora, have
demonstrated exceptional performance in the field of image and video
generation. Despite their robust generative capabilities, these models often
suffer from inaccurate inversion, which could further limit their effectiveness
in downstream tasks such as image and video editing. To address this issue, we
propose RF-Solver, a novel training-free sampler that enhances inversion
precision by reducing errors in the process of solving rectified flow ODEs.
Specifically, we derive the exact formulation of the rectified flow ODE and
perform a high-order Taylor expansion to estimate its nonlinear components,
significantly decreasing the approximation error at each timestep. Building
upon RF-Solver, we further design RF-Edit, which comprises specialized
sub-modules for image and video editing. By sharing self-attention layer
features during the editing process, RF-Edit effectively preserves the
structural information of the source image or video while achieving
high-quality editing results. Our approach is compatible with any pre-trained
rectified-flow-based models for image and video tasks, requiring no additional
training or optimization. Extensive experiments on text-to-image generation,
image & video inversion, and image & video editing demonstrate the robust
performance and adaptability of our methods. Code is available at
https://github.com/wangjiangshan0725/RF-Solver-Edit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Differentiable Logic Gate Networks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Petersen, Hilde Kuehne, Christian Borgelt, Julian Welzel, Stefano Ermon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing inference cost of machine learning models, there is a
growing interest in models with fast and efficient inference. Recently, an
approach for learning logic gate networks directly via a differentiable
relaxation was proposed. Logic gate networks are faster than conventional
neural network approaches because their inference only requires logic gate
operators such as NAND, OR, and XOR, which are the underlying building blocks
of current hardware and can be efficiently executed. We build on this idea,
extending it by deep logic gate tree convolutions, logical OR pooling, and
residual initializations. This allows scaling logic gate networks up by over
one order of magnitude and utilizing the paradigm of convolution. On CIFAR-10,
we achieve an accuracy of 86.29% using only 61 million logic gates, which
improves over the SOTA while being 29x smaller.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controlling Human Shape and Pose in Text-to-Image <span class="highlight-title">Diffusion</span> Models via
  Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benito Buchheim, Max Reimann, Jürgen Döllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a methodology for conditional control of human shape and pose in
pretrained text-to-image diffusion models using a 3D human parametric model
(SMPL). Fine-tuning these diffusion models to adhere to new conditions requires
large datasets and high-quality annotations, which can be more cost-effectively
acquired through synthetic data generation rather than real-world data.
However, the domain gap and low scene diversity of synthetic data can
compromise the pretrained model's visual fidelity. We propose a
domain-adaptation technique that maintains image quality by isolating
synthetically trained conditional information in the classifier-free guidance
vector and composing it with another control network to adapt the generated
images to the input domain. To achieve SMPL control, we fine-tune a
ControlNet-based architecture on the synthetic SURREAL dataset of rendered
humans and apply our domain adaptation at generation time. Experiments
demonstrate that our model achieves greater shape and pose diversity than the
2d pose-based ControlNet, while maintaining the visual fidelity and improving
stability, proving its usefulness for downstream tasks such as human animation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subspace-Constrained Quadratic Matrix Factorization: Algorithm and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Zhai, Xiaohui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matrix Factorization has emerged as a widely adopted framework for modeling
data exhibiting low-rank structures. To address challenges in manifold
learning, this paper presents a subspace-constrained quadratic matrix
factorization model. The model is designed to jointly learn key low-dimensional
structures, including the tangent space, the normal subspace, and the quadratic
form that links the tangent space to a low-dimensional representation. We solve
the proposed factorization model using an alternating minimization method,
involving an in-depth investigation of nonlinear regression and projection
subproblems. Theoretical properties of the quadratic projection problem and
convergence characteristics of the alternating strategy are also investigated.
To validate our approach, we conduct numerical experiments on synthetic and
real-world datasets. Results demonstrate that our model outperforms existing
methods, highlighting its robustness and efficacy in capturing core
low-dimensional structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuroFly: A framework for whole-brain single neuron reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rubin Zhao, Yang Liu, Shiqi Zhang, Zijian Yi, Yanyang Xiao, Fang Xu, Yi Yang, Pencheng Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neurons, with their elongated, tree-like dendritic and axonal structures,
enable efficient signal integration and long-range communication across brain
regions. By reconstructing individual neurons' morphology, we can gain valuable
insights into brain connectivity, revealing the structure basis of cognition,
movement, and perception. Despite the accumulation of extensive 3D microscopic
imaging data, progress has been considerably hindered by the absence of
automated tools to streamline this process. Here we introduce NeuroFly, a
validated framework for large-scale automatic single neuron reconstruction.
This framework breaks down the process into three distinct stages:
segmentation, connection, and proofreading. In the segmentation stage, we
perform automatic segmentation followed by skeletonization to generate
over-segmented neuronal fragments without branches. During the connection
stage, we use a 3D image-based path following approach to extend each fragment
and connect it with other fragments of the same neuron. Finally, human
annotators are required only to proofread the few unresolved positions. The
first two stages of our process are clearly defined computer vision problems,
and we have trained robust baseline models to solve them. We validated
NeuroFly's efficiency using in-house datasets that include a variety of
challenging scenarios, such as dense arborizations, weak axons, images with
contamination. We will release the datasets along with a suite of visualization
and annotation tools for better reproducibility. Our goal is to foster
collaboration among researchers to address the neuron reconstruction challenge,
ultimately accelerating advancements in neuroscience research. The dataset and
code are available at https://github.com/beanli161514/neurofly
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Multi-Level Alignments for Semi-Supervised Domain Adaptation
  SAR Target Recognition Using Simulated Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzheng Zhang, Hui Zhu, Hongqian Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, an intriguing research trend for automatic target recognition (ATR)
from synthetic aperture radar (SAR) imagery has arisen: using simulated data to
train ATR models is a feasible solution to the issue of inadequate measured
data. To close the domain gap that exists between the real and simulated data,
the unsupervised domain adaptation (UDA) techniques are frequently exploited to
construct ATR models. However, for UDA, the target domain lacks labeled data to
direct the model training, posing a great challenge to ATR performance. To
address the above problem, a semi-supervised domain adaptation (SSDA) framework
has been proposed adopting progressive multi-level alignments for simulated
data-aided SAR ATR. First, a progressive wavelet transform data augmentation
(PWTDA) is presented by analyzing the discrepancies of wavelet decomposition
sub-bands of two domain images, obtaining the domain-level alignment.
Specifically, the domain gap is narrowed by mixing the wavelet transform
high-frequency sub-band components. Second, we develop an asymptotic
instance-prototype alignment (AIPA) strategy to push the source domain
instances close to the corresponding target prototypes, aiming to achieve
category-level alignment. Moreover, the consistency alignment is implemented by
excavating the strong-weak augmentation consistency of both individual samples
and the multi-sample relationship, enhancing the generalization capability of
the model. Extensive experiments on the Synthetic and Measured Paired Labeled
Experiment (SAMPLE) dataset, indicate that our approach obtains recognition
accuracies of 99.63% and 98.91% in two common experimental settings with only
one labeled sample per class of the target domain, outperforming the most
advanced SSDA techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From CNN to ConvRNN: Adapting Visualization Techniques for Time-Series
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabien Poirier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, neural networks are commonly used to solve various problems.
Unfortunately, despite their effectiveness, they are often perceived as black
boxes capable of providing answers without explaining their decisions, which
raises numerous ethical and legal concerns. Fortunately, the field of
explainability helps users understand these results. This aspect of machine
learning allows users to grasp the decision-making process of a model and
verify the relevance of its outcomes. In this article, we focus on the learning
process carried out by a ``time distributed`` convRNN, which performs anomaly
detection from video data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ESC-MISR: Enhancing Spatial Correlations for Multi-Image
  Super-Resolution in Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihui Zhang, Jinhui Pang, Jianan Li, Xiaoshuai Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Image Super-Resolution (MISR) is a crucial yet challenging research
task in the remote sensing community. In this paper, we address the challenging
task of Multi-Image Super-Resolution in Remote Sensing (MISR-RS), aiming to
generate a High-Resolution (HR) image from multiple Low-Resolution (LR) images
obtained by satellites. Recently, the weak temporal correlations among LR
images have attracted increasing attention in the MISR-RS task. However,
existing MISR methods treat the LR images as sequences with strong temporal
correlations, overlooking spatial correlations and imposing temporal
dependencies. To address this problem, we propose a novel end-to-end framework
named Enhancing Spatial Correlations in MISR (ESC-MISR), which fully exploits
the spatial-temporal relations of multiple images for HR image reconstruction.
Specifically, we first introduce a novel fusion module named Multi-Image
Spatial Transformer (MIST), which emphasizes parts with clearer global spatial
features and enhances the spatial correlations between LR images. Besides, we
perform a random shuffle strategy for the sequential inputs of LR images to
attenuate temporal dependencies and capture weak temporal correlations in the
training stage. Compared with the state-of-the-art methods, our ESC-MISR
achieves 0.70dB and 0.76dB cPSNR improvements on the two bands of the PROBA-V
dataset respectively, demonstrating the superiority of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Brightness Adaptation for Robust Multi-modal Image Fusion <span class="chip">IJCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Sun, Bing Cao, Pengfei Zhu, Qinghua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared and visible image fusion aim to integrate modality strengths for
visually enhanced, informative images. Visible imaging in real-world scenarios
is susceptible to dynamic environmental brightness fluctuations, leading to
texture degradation. Existing fusion methods lack robustness against such
brightness perturbations, significantly compromising the visual fidelity of the
fused imagery. To address this challenge, we propose the Brightness Adaptive
multimodal dynamic fusion framework (BA-Fusion), which achieves robust image
fusion despite dynamic brightness fluctuations. Specifically, we introduce a
Brightness Adaptive Gate (BAG) module, which is designed to dynamically select
features from brightness-related channels for normalization, while preserving
brightness-independent structural information within the source images.
Furthermore, we propose a brightness consistency loss function to optimize the
BAG module. The entire framework is tuned via alternating training strategies.
Extensive experiments validate that our method surpasses state-of-the-art
methods in preserving multi-modal image information and visual fidelity, while
exhibiting remarkable robustness across varying brightness levels. Our code is
available: https://github.com/SunYM2020/BA-Fusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reciprocal Point Learning Network with Large Electromagnetic Kernel for
  SAR Open-Set Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiayang Xiao, Zhuoxuan Li, Ruyi Zhang, Jiacheng Chen, Haipeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The limitations of existing Synthetic Aperture Radar (SAR) Automatic Target
Recognition (ATR) methods lie in their confinement by the closed-environment
assumption, hindering their effective and robust handling of unknown target
categories in open environments. Open Set Recognition (OSR), a pivotal facet
for algorithmic practicality, intends to categorize known classes while
denoting unknown ones as "unknown." The chief challenge in OSR involves
concurrently mitigating risks associated with generalizing features from a
restricted set of known classes to numerous unknown samples and the open space
exposure to potential unknown data. To enhance open-set SAR classification, a
method called scattering kernel with reciprocal learning network is proposed.
Initially, a feature learning framework is constructed based on reciprocal
point learning (RPL), establishing a bounded space for potential unknown
classes. This approach indirectly introduces unknown information into a learner
confined to known classes, thereby acquiring more concise and discriminative
representations. Subsequently, considering the variability in the imaging of
targets at different angles and the discreteness of components in SAR images, a
proposal is made to design convolutional kernels based on large-sized attribute
scattering center models. This enhances the ability to extract intrinsic
non-linear features and specific scattering characteristics in SAR images,
thereby improving the discriminative features of the model and mitigating the
impact of imaging variations on classification performance. Experiments on the
MSTAR datasets substantiate the superior performance of the proposed approach
called ASC-RPL over mainstream methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Federated Learning for Cross-view Geo-localization <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Anagnostopoulos, Alexandros Gkillas, Nikos Piperigkos, Aris S. Lalos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose a methodology combining Federated Learning (FL) with
Cross-view Image Geo-localization (CVGL) techniques. We address the challenges
of data privacy and heterogeneity in autonomous vehicle environments by
proposing a personalized Federated Learning scenario that allows selective
sharing of model parameters. Our method implements a coarse-to-fine approach,
where clients share only the coarse feature extractors while keeping
fine-grained features specific to local environments. We evaluate our approach
against traditional centralized and single-client training schemes using the
KITTI dataset combined with satellite imagery. Results demonstrate that our
federated CVGL method achieves performance close to centralized training while
maintaining data privacy. The proposed partial model sharing strategy shows
comparable or slightly better performance than classical FL, offering
significant reduced communication overhead without sacrificing accuracy. Our
work contributes to more robust and privacy-preserving localization systems for
autonomous vehicles operating in diverse environments
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, Preprint submitted to the IEEE 26th International
  Workshop on Multimedia Signal Processing (MMSP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DNN-based 3D Cloud Retrieval for Variable Solar Illumination and
  Multiview Spaceborne Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamar Klein, Tom Aizenberg, Roi Ronen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climate studies often rely on remotely sensed images to retrieve
two-dimensional maps of cloud properties. To advance volumetric analysis, we
focus on recovering the three-dimensional (3D) heterogeneous extinction
coefficient field of shallow clouds using multiview remote sensing data.
Climate research requires large-scale worldwide statistics. To enable scalable
data processing, previous deep neural networks (DNNs) can infer at spaceborne
remote sensing downlink rates. However, prior methods are limited to a fixed
solar illumination direction. In this work, we introduce the first scalable
DNN-based system for 3D cloud retrieval that accommodates varying camera poses
and solar directions. By integrating multiview cloud intensity images with
camera poses and solar direction data, we achieve greater flexibility in
recovery. Training of the DNN is performed by a novel two-stage scheme to
address the high number of degrees of freedom in this problem. Our approach
shows substantial improvements over previous state-of-the-art, particularly in
handling variations in the sun's zenith angle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent
  Cooperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Liu, Pan Zhou, Yingjun Du, Ah-Hwee Tan, Cees G. M. Snoek, Jan-Jakob Sonke, Efstratios Gavves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the cooperation problem among large language model
(LLM) based embodied agents, where agents must cooperate to achieve a common
goal. Previous methods often execute actions extemporaneously and incoherently,
without long-term strategic and cooperative planning, leading to redundant
steps, failures, and even serious repercussions in complex tasks like
search-and-rescue missions where discussion and cooperative plan are crucial.
To solve this issue, we propose Cooperative Plan Optimization (CaPo) to enhance
the cooperation efficiency of LLM-based embodied agents. Inspired by human
cooperation schemes, CaPo improves cooperation efficiency with two phases: 1)
meta-plan generation, and 2) progress-adaptive meta-plan and execution. In the
first phase, all agents analyze the task, discuss, and cooperatively create a
meta-plan that decomposes the task into subtasks with detailed steps, ensuring
a long-term strategic and coherent plan for efficient coordination. In the
second phase, agents execute tasks according to the meta-plan and dynamically
adjust it based on their latest progress (e.g., discovering a target object)
through multi-turn discussions. This progress-based adaptation eliminates
redundant actions, improving the overall cooperation efficiency of agents.
Experimental results on the ThreeDworld Multi-Agent Transport and Communicative
Watch-And-Help tasks demonstrate that CaPo achieves much higher task completion
rate and efficiency compared with state-of-the-arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Search and Discovery of Visual Cultural Heritage Collections
  with Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taylor Arnold, Lauren Tilton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many cultural institutions have made large digitized visual collections
available online, often under permissible re-use licences. Creating interfaces
for exploring and searching these collections is difficult, particularly in the
absence of granular metadata. In this paper, we introduce a method for using
state-of-the-art multimodal large language models (LLMs) to enable an
open-ended, explainable search and discovery interface for visual collections.
We show how our approach can create novel clustering and recommendation systems
that avoid common pitfalls of methods based directly on visual embeddings. Of
particular interest is the ability to offer concrete textual explanations of
each recommendation without the need to preselect the features of interest.
Together, these features can create a digital interface that is more open-ended
and flexible while also being better suited to addressing privacy and ethical
concerns. Through a case study using a collection of documentary photographs,
we provide several metrics showing the efficacy and possibilities of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, CHR 2024: Computational Humanities Research Conference,
  December 4 - 6, 2024, Aarhus University, Denmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Image Color Mapping for a Historic Photographic Collection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taylor Arnold, Lauren Tilton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the 1970s, the United States Environmental Protection Agency sponsored
Documerica, a large-scale photography initiative to document environmental
subjects nation-wide. While over 15,000 digitized public-domain photographs
from the collection are available online, most of the images were scanned from
damaged copies of the original prints. We present and evaluate a modified
histogram matching technique based on the underlying chemistry of the prints
for correcting the damaged images by using training data collected from a small
set of undamaged prints. The entire set of color-adjusted Documerica images is
made available in an open repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, CHR 2024: Computational Humanities Research Conference,
  December 4 - 6, 2024, Aarhus University, Denmark</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICH-SCNet: Intracerebral Hemorrhage Segmentation and Prognosis
  Classification Network Using CLIP-guided SAM mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinlei Yu, Ahmed Elazab, Ruiquan Ge, Hui Jin, Xinchen Jiang, Gangyong Jia, Qing Wu, Qinglei Shi, Changmiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intracerebral hemorrhage (ICH) is the most fatal subtype of stroke and is
characterized by a high incidence of disability. Accurate segmentation of the
ICH region and prognosis prediction are critically important for developing and
refining treatment plans for post-ICH patients. However, existing approaches
address these two tasks independently and predominantly focus on imaging data
alone, thereby neglecting the intrinsic correlation between the tasks and
modalities. This paper introduces a multi-task network, ICH-SCNet, designed for
both ICH segmentation and prognosis classification. Specifically, we integrate
a SAM-CLIP cross-modal interaction mechanism that combines medical text and
segmentation auxiliary information with neuroimaging data to enhance
cross-modal feature recognition. Additionally, we develop an effective feature
fusion module and a multi-task loss function to improve performance further.
Extensive experiments on an ICH dataset reveal that our approach surpasses
other state-of-the-art methods. It excels in the overall performance of
classification tasks and outperforms competing models in all segmentation task
metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 3 tables, published to BIBM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DanceFusion: A Spatio-Temporal Skeleton <span class="highlight-title">Diffusion</span> <span class="highlight-title">Transformer</span> for
  Audio-Driven Dance Motion Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhao, Zhengmin Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces DanceFusion, a novel framework for reconstructing and
generating dance movements synchronized to music, utilizing a Spatio-Temporal
Skeleton Diffusion Transformer. The framework adeptly handles incomplete and
noisy skeletal data common in short-form dance videos on social media platforms
like TikTok. DanceFusion incorporates a hierarchical Transformer-based
Variational Autoencoder (VAE) integrated with a diffusion model, significantly
enhancing motion realism and accuracy. Our approach introduces sophisticated
masking techniques and a unique iterative diffusion process that refines the
motion sequences, ensuring high fidelity in both motion generation and
synchronization with accompanying audio cues. Comprehensive evaluations
demonstrate that DanceFusion surpasses existing methods, providing
state-of-the-art performance in generating dynamic, realistic, and
stylistically diverse dance motions. Potential applications of this framework
extend to content creation, virtual reality, and interactive entertainment,
promising substantial advancements in automated dance generation. Visit our
project page at https://th-mlab.github.io/DanceFusion/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAP-VL: Text Layout-Aware Pre-training for Enriched Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Fhima, Elad Ben Avraham, Oren Nuriel, Yair Kittenplon, Roy Ganz, Aviad Aberdam, Ron Litman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language (VL) models have garnered considerable research interest;
however, they still face challenges in effectively handling text within images.
To address this limitation, researchers have developed two approaches. The
first method involves utilizing external Optical Character Recognition (OCR)
tools to extract textual information from images, which is then prepended to
other textual inputs. The second strategy focuses on employing extremely
high-resolution images to improve text recognition capabilities. In this paper,
we focus on enhancing the first strategy by introducing a novel method, named
TAP-VL, which treats OCR information as a distinct modality and seamlessly
integrates it into any VL model. TAP-VL employs a lightweight transformer-based
OCR module to receive OCR with layout information, compressing it into a short
fixed-length sequence for input into the LLM. Initially, we conduct
model-agnostic pretraining of the OCR module on unlabeled documents, followed
by its integration into any VL architecture through brief fine-tuning.
Extensive experiments demonstrate consistent performance improvements when
applying TAP-VL to top-performing VL models, across scene-text and
document-based VL benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Multi-Task Brain Tumour Segmentation with Synthetic Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Ferreira, Tiago Jesus, Behrus Puladi, Jens Kleesiek, Victor Alves, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the winning solution of task 1 and the third-placed
solution of task 3 of the BraTS challenge. The use of automated tools in
clinical practice has increased due to the development of more and more
sophisticated and reliable algorithms. However, achieving clinical standards
and developing tools for real-life scenarios is a major challenge. To this end,
BraTS has organised tasks to find the most advanced solutions for specific
purposes. In this paper, we propose the use of synthetic data to train
state-of-the-art frameworks in order to improve the segmentation of adult
gliomas in a post-treatment scenario, and the segmentation of meningioma for
radiotherapy planning. Our results suggest that the use of synthetic data leads
to more robust algorithms, although the synthetic data generation pipeline is
not directly suited to the meningioma task. The code for these tasks is
available at https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain Tumour Removing and Missing Modality Generation using 3D WDM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Ferreira, Gijs Luijten, Behrus Puladi, Jens Kleesiek, Victor Alves, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the second-placed solution for task 8 and the
participation solution for task 7 of BraTS 2024. The adoption of automated
brain analysis algorithms to support clinical practice is increasing. However,
many of these algorithms struggle with the presence of brain lesions or the
absence of certain MRI modalities. The alterations in the brain's morphology
leads to high variability and thus poor performance of predictive models that
were trained only on healthy brains. The lack of information that is usually
provided by some of the missing MRI modalities also reduces the reliability of
the prediction models trained with all modalities. In order to improve the
performance of these models, we propose the use of conditional 3D wavelet
diffusion models. The wavelet transform enabled full-resolution image training
and prediction on a GPU with 48 GB VRAM, without patching or downsampling,
preserving all information for prediction. For the inpainting task of BraTS
2024, the use of a large and variable number of healthy masks and the stability
and efficiency of the 3D wavelet diffusion model resulted in 0.007, 22.61 and
0.842 in the validation set and 0.07 , 22.8 and 0.91 in the testing set (MSE,
PSNR and SSIM respectively). The code for these tasks is available at
https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-temporal crack segmentation in concrete structure using deep
  learning approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Said Harb, Pedro Achanccaray, Mehdi Maboudi, Markus Gerke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cracks are among the earliest indicators of deterioration in concrete
structures. Early automatic detection of these cracks can significantly extend
the lifespan of critical infrastructures, such as bridges, buildings, and
tunnels, while simultaneously reducing maintenance costs and facilitating
efficient structural health monitoring. This study investigates whether
leveraging multi-temporal data for crack segmentation can enhance segmentation
quality. Therefore, we compare a Swin UNETR trained on multi-temporal data with
a U-Net trained on mono-temporal data to assess the effect of temporal
information compared with conventional single-epoch approaches. To this end, a
multi-temporal dataset comprising 1356 images, each with 32 sequential crack
propagation images, was created. After training the models, experiments were
conducted to analyze their generalization ability, temporal consistency, and
segmentation quality. The multi-temporal approach consistently outperformed its
mono-temporal counterpart, achieving an IoU of $82.72\%$ and a F1-score of
$90.54\%$, representing a significant improvement over the mono-temporal
model's IoU of $76.69\%$ and F1-score of $86.18\%$, despite requiring only half
of the trainable parameters. The multi-temporal model also displayed a more
consistent segmentation quality, with reduced noise and fewer errors. These
results suggest that temporal information significantly enhances the
performance of segmentation models, offering a promising solution for improved
crack detection and the long-term monitoring of concrete structures, even with
limited sequential data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Population estimation using 3D city modelling and Carto2S <span class="highlight-title">dataset</span>s -- A
  case study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jai G Singla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the launch of Carto2S series of satellites, high resolution images
(0.6-1.0 meters) are acquired and available for use. High resolution Digital
Elevation Model (DEM) with better accuracies can be generated using C2S
multi-view and multi date datasets. DEMs are further used as an input to derive
Digital terrain models (DTMs) and to extract accurate heights of the objects
(building and tree) over the surface of the Earth. Extracted building heights
are validated with ground control points and can be used for generation of city
modelling and resource estimation like population estimation, health planning,
water and transport resource estimations. In this study, an attempt is made to
assess the population of a township using high-resolution Indian remote sensing
satellite datasets. We used Carto 2S multi-view data and generated a precise
DEM and DTM over a city area. Using DEM and DTM datasets, accurate heights of
the buildings are extracted which are further validated with ground data.
Accurate building heights and high resolution imagery are used for generating
accurate virtual 3D city model and assessing the number of floor and carpet
area of the houses/ flats/ apartments. Population estimation of the area is
made using derived information of no of houses/ flats/ apartments from the
satellite datasets. Further, information about number of hospital and schools
around the residential area is extracted from open street maps (OSM).
Population estimation using satellite data and derived information from OSM
datasets can prove to be very good tool for local administrator and decision
makers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solar potential analysis over Indian cities using high-resolution
  satellite imagery and DEM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jai Singla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the research work in the solar potential analysis is performed
utilizing aerial imagery, LiDAR data, and satellite imagery. However, in the
existing studies using satellite data, parameters such as trees/ vegetation
shadow, adjacent higher architectural structures, and eccentric roof structures
in urban areas were not considered, and relatively coarser-resolution datasets
were used for analysis. In this work, we have implemented a novel approach to
estimate rooftop solar potential using inputs of high-resolution satellite
imagery (0.5 cm), a digital elevation model (1m), along with ground station
radiation data. Solar radiation analysis is performed using the diffusion
proportion and transmissivity ratio derived from the ground station data hosted
by IMD. It was observed that due to seasonal variations, environmental effects
and technical reasons such as solar panel structure etc., there can be a
significant loss of electricity generation up to 50%. Based on the results, it
is also understood that using 1m DEM and 50cm satellite imagery, more authentic
results are produced over the urban areas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross- and Intra-image Prototypical Learning for Multi-label Disease
  Diagnosis and Interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Wang, Fengbei Liu, Yuanhong Chen, Helen Frazer, Gustavo Carneiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in prototypical learning have shown remarkable potential to
provide useful decision interpretations associating activation maps and
predictions with class-specific training prototypes. Such prototypical learning
has been well-studied for various single-label diseases, but for quite relevant
and more challenging multi-label diagnosis, where multiple diseases are often
concurrent within an image, existing prototypical learning models struggle to
obtain meaningful activation maps and effective class prototypes due to the
entanglement of the multiple diseases. In this paper, we present a novel Cross-
and Intra-image Prototypical Learning (CIPL) framework, for accurate
multi-label disease diagnosis and interpretation from medical images. CIPL
takes advantage of common cross-image semantics to disentangle the multiple
diseases when learning the prototypes, allowing a comprehensive understanding
of complicated pathological lesions. Furthermore, we propose a new two-level
alignment-based regularisation strategy that effectively leverages consistent
intra-image information to enhance interpretation robustness and predictive
performance. Extensive experiments show that our CIPL attains the
state-of-the-art (SOTA) classification accuracy in two public multi-label
benchmarks of disease diagnosis: thoracic radiography and fundus images.
Quantitative interpretability results show that CIPL also has superiority in
weakly-supervised thoracic disease localisation over other leading saliency-
and prototype-based explanation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Social EgoMesh Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Scofano, Alessio Sampieri, Edoardo De Matteis, Indro Spinelli, Fabio Galasso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the 3D pose of the camera wearer in egocentric video
sequences is crucial to modeling human behavior in virtual and augmented
reality applications. The task presents unique challenges due to the limited
visibility of the user's body caused by the front-facing camera mounted on
their head. Recent research has explored the utilization of the scene and
ego-motion, but it has overlooked humans' interactive nature. We propose a
novel framework for Social Egocentric Estimation of body MEshes (SEE-ME). Our
approach is the first to estimate the wearer's mesh using only a latent
probabilistic diffusion model, which we condition on the scene and, for the
first time, on the social wearer-interactee interactions. Our in-depth study
sheds light on when social interaction matters most for ego-mesh estimation; it
quantifies the impact of interpersonal distance and gaze direction. Overall,
SEE-ME surpasses the current best technique, reducing the pose estimation error
(MPJPE) by 53%. The code is available at https://github.com/L-Scofano/SEEME.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Semi-Supervised Learning on Line Segment Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johanna Engman, Karl Åström, Magnus Oskarsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a method for line segment detection in images, based
on a semi-supervised framework. Leveraging the use of a consistency loss based
on differently augmented and perturbed unlabeled images with a small amount of
labeled data, we show comparable results to fully supervised methods. This
opens up application scenarios where annotation is difficult or expensive, and
for domain specific adaptation of models. We are specifically interested in
real-time and online applications, and investigate small and efficient learning
backbones. Our method is to our knowledge the first to target line detection
using modern state-of-the-art methodologies for semi-supervised learning. We
test the method on both standard benchmarks and domain specific scenarios for
forestry applications, showing the tractability of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TexLiverNet: Leveraging Medical Knowledge and Spatial-Frequency
  Perception for Enhanced Liver Tumor Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyan Jiang, Zhi Zhou, Hailing Wang, Guozhong Wang, Zhijun Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating textual data with imaging in liver tumor segmentation is
essential for enhancing diagnostic accuracy. However, current multi-modal
medical datasets offer only general text annotations, lacking lesion-specific
details critical for extracting nuanced features, especially for fine-grained
segmentation of tumor boundaries and small lesions. To address these
limitations, we developed datasets with lesion-specific text annotations for
liver tumors and introduced the TexLiverNet model. TexLiverNet employs an
agent-based cross-attention module that integrates text features efficiently
with visual features, significantly reducing computational costs. Additionally,
enhanced spatial and adaptive frequency domain perception is proposed to
precisely delineate lesion boundaries, reduce background interference, and
recover fine details in small lesions. Comprehensive evaluations on public and
private datasets demonstrate that TexLiverNet achieves superior performance
compared to current state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verification of Neural Networks against Convolutional Perturbations via
  Parameterised Kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Brückner, Alessio Lomuscio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a method for the efficient verification of neural networks against
convolutional perturbations such as blurring or sharpening. To define input
perturbations we use well-known camera shake, box blur and sharpen kernels. We
demonstrate that these kernels can be linearly parameterised in a way that
allows for a variation of the perturbation strength while preserving desired
kernel properties. To facilitate their use in neural network verification, we
develop an efficient way of convolving a given input with these parameterised
kernels. The result of this convolution can be used to encode the perturbation
in a verification setting by prepending a linear layer to a given network. This
leads to tight bounds and a high effectiveness in the resulting verification
step. We add further precision by employing input splitting as a branch and
bound strategy. We demonstrate that we are able to verify robustness on a
number of standard benchmarks where the baseline is unable to provide any
safety certificates. To the best of our knowledge, this is the first solution
for verifying robustness against specific convolutional perturbations such as
camera shake.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Inherent Robustness of One-Stage Object Detection against
  Out-of-Distribution Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aitor Martinez-Seras, Javier Del Ser, Alain Andres, Pablo Garcia-Bringas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robustness is a fundamental aspect for developing safe and trustworthy
models, particularly when they are deployed in the open world. In this work we
analyze the inherent capability of one-stage object detectors to robustly
operate in the presence of out-of-distribution (OoD) data. Specifically, we
propose a novel detection algorithm for detecting unknown objects in image
data, which leverages the features extracted by the model from each sample.
Differently from other recent approaches in the literature, our proposal does
not require retraining the object detector, thereby allowing for the use of
pretrained models. Our proposed OoD detector exploits the application of
supervised dimensionality reduction techniques to mitigate the effects of the
curse of dimensionality on the features extracted by the model. Furthermore, it
utilizes high-resolution feature maps to identify potential unknown objects in
an unsupervised fashion. Our experiments analyze the Pareto trade-off between
the performance detecting known and unknown objects resulting from different
algorithmic configurations and inference confidence thresholds. We also compare
the performance of our proposed algorithm to that of logits-based post-hoc OoD
methods, as well as possible fusion strategies. Finally, we discuss on the
competitiveness of all tested methods against state-of-the-art OoD approaches
for object detection models over the recently published Unknown Object
Detection benchmark. The obtained results verify that the performance of
avant-garde post-hoc OoD detectors can be further improved when combined with
our proposed algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 figures, 4 tables, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PASSION for Dermatology: Bridging the Diversity Gap with Pigmented Skin
  Images from Sub-Saharan Africa <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philippe Gottfrois, Fabian Gröger, Faly Herizo Andriambololoniaina, Ludovic Amruthalingam, Alvaro Gonzalez-Jimenez, Christophe Hsu, Agnes Kessy, Simone Lionetti, Daudi Mavura, Wingston Ng'ambi, Dingase Faith Ngongonda, Marc Pouly, Mendrika Fifaliana Rakotoarisaona, Fahafahantsoa Rapelanoro Rabenja, Ibrahima Traoré, Alexander A. Navarini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Africa faces a huge shortage of dermatologists, with less than one per
million people. This is in stark contrast to the high demand for dermatologic
care, with 80% of the paediatric population suffering from largely untreated
skin conditions. The integration of AI into healthcare sparks significant hope
for treatment accessibility, especially through the development of AI-supported
teledermatology. Current AI models are predominantly trained on white-skinned
patients and do not generalize well enough to pigmented patients. The PASSION
project aims to address this issue by collecting images of skin diseases in
Sub-Saharan countries with the aim of open-sourcing this data. This dataset is
the first of its kind, consisting of 1,653 patients for a total of 4,901
images. The images are representative of telemedicine settings and encompass
the most common paediatric conditions: eczema, fungals, scabies, and impetigo.
We also provide a baseline machine learning model trained on the dataset and a
detailed performance analysis for the subpopulations represented in the
dataset. The project website can be found at https://passionderm.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DomainGallery: Few-shot Domain-driven Image Generation by
  Attribute-centric Finetuning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Duan, Yan Hong, Bo Zhang, Jun Lan, Huijia Zhu, Weiqiang Wang, Jianfu Zhang, Li Niu, Liqing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent progress in text-to-image models pretrained on large-scale
datasets has enabled us to generate various images as long as we provide a text
prompt describing what we want. Nevertheless, the availability of these models
is still limited when we expect to generate images that fall into a specific
domain either hard to describe or just unseen to the models. In this work, we
propose DomainGallery, a few-shot domain-driven image generation method which
aims at finetuning pretrained Stable Diffusion on few-shot target datasets in
an attribute-centric manner. Specifically, DomainGallery features prior
attribute erasure, attribute disentanglement, regularization and enhancement.
These techniques are tailored to few-shot domain-driven generation in order to
solve key issues that previous works have failed to settle. Extensive
experiments are given to validate the superior performance of DomainGallery on
a variety of domain-driven generation scenarios. Codes are available at
https://github.com/Ldhlwh/DomainGallery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Fingerprints for Adversarial Attack Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haim Fisher, Moni Shahar, Yehezkel S. Resheff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models for image classification have become standard tools in
recent years. A well known vulnerability of these models is their
susceptibility to adversarial examples. These are generated by slightly
altering an image of a certain class in a way that is imperceptible to humans
but causes the model to classify it wrongly as another class. Many algorithms
have been proposed to address this problem, falling generally into one of two
categories: (i) building robust classifiers (ii) directly detecting attacked
images. Despite the good performance of these detectors, we argue that in a
white-box setting, where the attacker knows the configuration and weights of
the network and the detector, they can overcome the detector by running many
examples on a local copy, and sending only those that were not detected to the
actual model. This problem is common in security applications where even a very
good model is not sufficient to ensure safety. In this paper we propose to
overcome this inherent limitation of any static defence with randomization. To
do so, one must generate a very large family of detectors with consistent
performance, and select one or more of them randomly for each input. For the
individual detectors, we suggest the method of neural fingerprints. In the
training phase, for each class we repeatedly sample a tiny random subset of
neurons from certain layers of the network, and if their average is
sufficiently different between clean and attacked images of the focal class
they are considered a fingerprint and added to the detector bank. During test
time, we sample fingerprints from the bank associated with the label predicted
by the model, and detect attacks using a likelihood ratio test. We evaluate our
detectors on ImageNet with different attack methods and model architectures,
and show near-perfect detection with low rates of false detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ l0-Regularized Sparse Coding-based Interpretable Network for Multi-Modal
  Image Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gargi Panda, Soumitra Kundu, Saumik Bhattacharya, Aurobinda Routray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal image fusion (MMIF) enhances the information content of the fused
image by combining the unique as well as common features obtained from
different modality sensor images, improving visualization, object detection,
and many more tasks. In this work, we introduce an interpretable network for
the MMIF task, named FNet, based on an l0-regularized multi-modal convolutional
sparse coding (MCSC) model. Specifically, for solving the l0-regularized CSC
problem, we develop an algorithm unrolling-based l0-regularized sparse coding
(LZSC) block. Given different modality source images, FNet first separates the
unique and common features from them using the LZSC block and then these
features are combined to generate the final fused image. Additionally, we
propose an l0-regularized MCSC model for the inverse fusion process. Based on
this model, we introduce an interpretable inverse fusion network named IFNet,
which is utilized during FNet's training. Extensive experiments show that FNet
achieves high-quality fusion results across five different MMIF tasks.
Furthermore, we show that FNet enhances downstream object detection in
visible-thermal image pairs. We have also visualized the intermediate results
of FNet, which demonstrates the good interpretability of our network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Sign Language Recognition System using Deep Learning with
  MediaPipe Holistic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharvani Srivastava, Sudhakar Singh,  Pooja, Shiv Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign languages are the language of hearing-impaired people who use visuals
like the hand, facial, and body movements for communication. There are
different signs and gestures representing alphabets, words, and phrases.
Nowadays approximately 300 sign languages are being practiced worldwide such as
American Sign Language (ASL), Chinese Sign Language (CSL), Indian Sign Language
(ISL), and many more. Sign languages are dependent on the vocal language of a
place. Unlike vocal or spoken languages, there are no helping words in sign
language like is, am, are, was, were, will, be, etc. As only a limited
population is well-versed in sign language, this lack of familiarity of sign
language hinders hearing-impaired people from communicating freely and easily
with everyone. This issue can be addressed by a sign language recognition (SLR)
system which has the capability to translate the sign language into vocal
language. In this paper, a continuous SLR system is proposed using a deep
learning model employing Long Short-Term Memory (LSTM), trained and tested on
an ISL primary dataset. This dataset is created using MediaPipe Holistic
pipeline for tracking face, hand, and body movements and collecting landmarks.
The system recognizes the signs and gestures in real-time with 88.23% accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, Wireless Pers Commun</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedDP: Privacy-preserving method based on federated learning for
  histopathology image segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangrui Pan, Mao Huang, Lian Wang, Pinle Qin, Shaoliang Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hematoxylin and Eosin (H&E) staining of whole slide images (WSIs) is
considered the gold standard for pathologists and medical practitioners for
tumor diagnosis, surgical planning, and post-operative assessment. With the
rapid advancement of deep learning technologies, the development of numerous
models based on convolutional neural networks and transformer-based models has
been applied to the precise segmentation of WSIs. However, due to privacy
regulations and the need to protect patient confidentiality, centralized
storage and processing of image data are impractical. Training a centralized
model directly is challenging to implement in medical settings due to these
privacy concerns.This paper addresses the dispersed nature and privacy
sensitivity of medical image data by employing a federated learning framework,
allowing medical institutions to collaboratively learn while protecting patient
privacy. Additionally, to address the issue of original data reconstruction
through gradient inversion during the federated learning training process,
differential privacy introduces noise into the model updates, preventing
attackers from inferring the contributions of individual samples, thereby
protecting the privacy of the training data.Experimental results show that the
proposed method, FedDP, minimally impacts model accuracy while effectively
safeguarding the privacy of cancer pathology image data, with only a slight
decrease in Dice, Jaccard, and Acc indices by 0.55%, 0.63%, and 0.42%,
respectively. This approach facilitates cross-institutional collaboration and
knowledge sharing while protecting sensitive data privacy, providing a viable
solution for further research and application in the medical field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in BIBM2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pose2Trajectory: Using <span class="highlight-title">Transformer</span>s on Body Pose to Predict Tennis
  Player's Trajectory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali K. AlShami, Terrance Boult, Jugal Kalita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking the trajectory of tennis players can help camera operators in
production. Predicting future movement enables cameras to automatically track
and predict a player's future trajectory without human intervention. Predicting
future human movement in the context of complex physical tasks is also
intellectually satisfying. Swift advancements in sports analytics and the wide
availability of videos for tennis have inspired us to propose a novel method
called Pose2Trajectory, which predicts a tennis player's future trajectory as a
sequence derived from their body joints' data and ball position. Demonstrating
impressive accuracy, our approach capitalizes on body joint information to
provide a comprehensive understanding of the human body's geometry and motion,
thereby enhancing the prediction of the player's trajectory. We use
encoder-decoder Transformer architecture trained on the joints and trajectory
information of the players with ball positions. The predicted sequence can
provide information to help close-up cameras to keep tracking the tennis
player, following centroid coordinates. We generate a high-quality dataset from
multiple videos to assist tennis player movement prediction using object
detection and human pose estimation methods. It contains bounding boxes and
joint information for tennis players and ball positions in singles tennis
games. Our method shows promising results in predicting the tennis player's
movement trajectory with different sequence prediction lengths using the joints
and trajectory information with the ball position.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synergy-Guided Regional Supervision of Pseudo Labels for Semi-Supervised
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Wang, Xinlin Zhang, Yuanbin Chen, Yuanbo Zhou, Longxuan Zhao, Tao Tan, Tong Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning has received considerable attention for its
potential to leverage abundant unlabeled data to enhance model robustness.
Pseudo labeling is a widely used strategy in semi supervised learning. However,
existing methods often suffer from noise contamination, which can undermine
model performance. To tackle this challenge, we introduce a novel
Synergy-Guided Regional Supervision of Pseudo Labels (SGRS-Net) framework.
Built upon the mean teacher network, we employ a Mix Augmentation module to
enhance the unlabeled data. By evaluating the synergy before and after
augmentation, we strategically partition the pseudo labels into distinct
regions. Additionally, we introduce a Region Loss Evaluation module to assess
the loss across each delineated area. Extensive experiments conducted on the LA
dataset have demonstrated superior performance over state-of-the-art
techniques, underscoring the efficiency and practicality of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CFPNet: Improving Lightweight ToF Depth Completion via Cross-zone
  Feature Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laiyan Ding, Hualie Jiang, Rui Xu, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth completion using lightweight time-of-flight (ToF) depth sensors is
attractive due to their low cost. However, lightweight ToF sensors usually have
a limited field of view (FOV) compared with cameras. Thus, only pixels in the
zone area of the image can be associated with depth signals. Previous methods
fail to propagate depth features from the zone area to the outside-zone area
effectively, thus suffering from degraded depth completion performance outside
the zone. To this end, this paper proposes the CFPNet to achieve cross-zone
feature propagation from the zone area to the outside-zone area with two novel
modules. The first is a direct-attention-based propagation module (DAPM), which
enforces direct cross-zone feature acquisition. The second is a
large-kernel-based propagation module (LKPM), which realizes cross-zone feature
propagation by utilizing convolution layers with kernel sizes up to 31. CFPNet
achieves state-of-the-art (SOTA) depth completion performance by combining
these two modules properly, as verified by extensive experimental results on
the ZJU-L5 dataset. The code will be made public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Models for UAV-Assisted Bridge Inspection: A YOLO
  Benchmark Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trong-Nhan Phan, Hoang-Hai Nguyen, Thi-Thu-Hien Ha, Huy-Tan Thai, Kim-Hung Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual inspections of bridges are critical to ensure their safety and
identify potential failures early. This inspection process can be rapidly and
accurately automated by using unmanned aerial vehicles (UAVs) integrated with
deep learning models. However, choosing an appropriate model that is
lightweight enough to integrate into the UAV and fulfills the strict
requirements for inference time and accuracy is challenging. Therefore, our
work contributes to the advancement of this model selection process by
conducting a benchmark of 23 models belonging to the four newest YOLO variants
(YOLOv5, YOLOv6, YOLOv7, YOLOv8) on COCO-Bridge-2021+, a dataset for bridge
details detection. Through comprehensive benchmarking, we identify YOLOv8n,
YOLOv7tiny, YOLOv6m, and YOLOv6m6 as the models offering an optimal balance
between accuracy and processing speed, with mAP@50 scores of 0.803, 0.837,
0.853, and 0.872, and inference times of 5.3ms, 7.5ms, 14.06ms, and 39.33ms,
respectively. Our findings accelerate the model selection process for UAVs,
enabling more efficient and reliable bridge inspections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreeCap: Hybrid Calibration-Free Motion Capture in Open Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoru Xue, Yiming Ren, Zining Song, Mao Ye, Xinge Zhu, Yuexin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel hybrid calibration-free method FreeCap to accurately
capture global multi-person motions in open environments. Our system combines a
single LiDAR with expandable moving cameras, allowing for flexible and precise
motion estimation in a unified world coordinate. In particular, We introduce a
local-to-global pose-aware cross-sensor human-matching module that predicts the
alignment among each sensor, even in the absence of calibration. Additionally,
our coarse-to-fine sensor-expandable pose optimizer further optimizes the 3D
human key points and the alignments, it is also capable of incorporating
additional cameras to enhance accuracy. Extensive experiments on Human-M3 and
FreeMotion datasets demonstrate that our method significantly outperforms
state-of-the-art single-modal methods, offering an expandable and efficient
solution for multi-person motion capture across various applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient single image non-uniformity correction algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yohann Tendero, Jerome Gilles, Stephane Landeau, Jean-Michel Morel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new way to correct the non-uniformity (NU) in
uncooled infrared-type images. The main defect of these uncooled images is the
lack of a column (resp. line) time-dependent cross-calibration, resulting in a
strong column (resp. line) and time dependent noise. This problem can be
considered as a 1D flicker of the columns inside each frame. Thus, classic
movie deflickering algorithms can be adapted, to equalize the columns (resp.
the lines). The proposed method therefore applies to the series formed by the
columns of an infrared image a movie deflickering algorithm. The obtained
single image method works on static images, and therefore requires no
registration, no camera motion compensation, and no closed aperture sensor
equalization. Thus, the method has only one camera dependent parameter, and is
landscape independent. This simple method will be compared to a state of the
art total variation single image correction on raw real and simulated images.
The method is real time, requiring only two operations per pixel. It involves
no test-pattern calibration and produces no "ghost artifacts".
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2411.03615</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Properties of BV-G structures + textures decomposition models.
  Application to road detection in satellite images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerome Gilles, Yves Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present some theoretical results about a structures-textures
image decomposition model which was proposed by the second author. We prove a
theorem which gives the behavior of this model in different cases. Finally, as
a consequence of the theorem we derive an algorithm for the detection of long
and thin objects applied to a road networks detection application in aerial or
satellite images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BendVLM: Test-Time Debiasing of Vision-Language Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walter Gerych, Haoran Zhang, Kimia Hamidieh, Eileen Pan, Maanas Sharma, Thomas Hartvigsen, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language model (VLM) embeddings have been shown to encode biases
present in their training data, such as societal biases that prescribe negative
characteristics to members of various racial and gender identities. VLMs are
being quickly adopted for a variety of tasks ranging from few-shot
classification to text-guided image generation, making debiasing VLM embeddings
crucial. Debiasing approaches that fine-tune the VLM often suffer from
catastrophic forgetting. On the other hand, fine-tuning-free methods typically
utilize a "one-size-fits-all" approach that assumes that correlation with the
spurious attribute can be explained using a single linear direction across all
possible inputs. In this work, we propose Bend-VLM, a nonlinear,
fine-tuning-free approach for VLM embedding debiasing that tailors the
debiasing operation to each unique input. This allows for a more flexible
debiasing approach. Additionally, we do not require knowledge of the set of
inputs a priori to inference time, making our method more appropriate for
online, open-set tasks such as retrieval and text guided image generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Understanding Makes for A Good Tokenizer for Image Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luting Wang, Yang Zhao, Zijian Zhang, Jiashi Feng, Si Liu, Bingyi Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract Modern image generation (IG) models have been shown to capture rich
semantics valuable for image understanding (IU) tasks. However, the potential
of IU models to improve IG performance remains uncharted. We address this issue
using a token-based IG framework, which relies on effective tokenizers to
project images into token sequences. Currently, pixel reconstruction (e.g.,
VQGAN) dominates the training objective for image tokenizers. In contrast, our
approach adopts the feature reconstruction objective, where tokenizers are
trained by distilling knowledge from pretrained IU encoders. Comprehensive
comparisons indicate that tokenizers with strong IU capabilities achieve
superior IG performance across a variety of metrics, datasets, tasks, and
proposal networks. Notably, VQ-KD CLIP achieves $4.10$ FID on ImageNet-1k
(IN-1k). Visualization suggests that the superiority of VQ-KD can be partly
attributed to the rich semantics within the VQ-KD codebook. We further
introduce a straightforward pipeline to directly transform IU encoders into
tokenizers, demonstrating exceptional effectiveness for IG tasks. These
discoveries may energize further exploration into image tokenizer research and
inspire the community to reassess the relationship between IU and IG. The code
is released at https://github.com/magic-research/vector_quantization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Bronchoscopy Depth Estimation through Synthetic-to-Real Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyao Tian, Huai Liao, Xinyan Huang, Lujie Li, Hongbin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation has shown promise in general imaging tasks, aiding
in localization and 3D reconstruction. While effective in various domains, its
application to bronchoscopic images is hindered by the lack of labeled data,
challenging the use of supervised learning methods. In this work, we propose a
transfer learning framework that leverages synthetic data with depth labels for
training and adapts domain knowledge for accurate depth estimation in real
bronchoscope data. Our network demonstrates improved depth prediction on real
footage using domain adaptation compared to training solely on synthetic data,
validating our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProGraph: Temporally-alignable Probability Guided Graph Topological
  Modeling for 3D Human Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongsheng Wang, Zehui Feng, Tong Xiao, Genfan Yang, Shengyu Zhang, Fei Wu, Feng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current 3D human motion reconstruction methods from monocular videos rely on
features within the current reconstruction window, leading to distortion and
deformations in the human structure under local occlusions or blurriness in
video frames. To estimate realistic 3D human mesh sequences based on incomplete
features, we propose Temporally-alignable Probability Guided Graph Topological
Modeling for 3D Human Reconstruction (ProGraph). For missing parts recovery, we
exploit the explicit topological-aware probability distribution across the
entire motion sequence. To restore the complete human, Graph Topological
Modeling (GTM) learns the underlying topological structure, focusing on the
relationships inherent in the individual parts. Next, to generate blurred
motion parts, Temporal-alignable Probability Distribution (TPDist) utilizes the
GTM to predict features based on distribution. This interactive mechanism
facilitates motion consistency, allowing the restoration of human parts.
Furthermore, Hierarchical Human Loss (HHLoss) constrains the probability
distribution errors of inter-frame features during topological structure
variation. Our Method achieves superior results than other SOTA methods in
addressing occlusions and blurriness on 3DPW.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MegaPortrait: Revisiting <span class="highlight-title">Diffusion</span> Control for High-fidelity Portrait
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yang, Sotiris Anagnostidis, Enis Simsar, Thomas Hofmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MegaPortrait. It's an innovative system for creating personalized
portrait images in computer vision. It has three modules: Identity Net, Shading
Net, and Harmonization Net. Identity Net generates learned identity using a
customized model fine-tuned with source images. Shading Net re-renders
portraits using extracted representations. Harmonization Net fuses pasted faces
and the reference image's body for coherent results. Our approach with
off-the-shelf Controlnets is better than state-of-the-art AI portrait products
in identity preservation and image fidelity. MegaPortrait has a simple but
effective design and we compare it with other methods and products to show its
superiority.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LidaRefer: Outdoor 3D Visual Grounding for Autonomous Driving with
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeong-Seung Baek, Heung-Seon Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding (VG) aims to locate relevant objects or regions within 3D
scenes based on natural language descriptions. Although recent methods for
indoor 3D VG have successfully transformer-based architectures to capture
global contextual information and enable fine-grained cross-modal fusion, they
are unsuitable for outdoor environments due to differences in the distribution
of point clouds between indoor and outdoor settings. Specifically, first,
extensive LiDAR point clouds demand unacceptable computational and memory
resources within transformers due to the high-dimensional visual features.
Second, dominant background points and empty spaces in sparse LiDAR point
clouds complicate cross-modal fusion owing to their irrelevant visual
information. To address these challenges, we propose LidaRefer, a
transformer-based 3D VG framework designed for large-scale outdoor scenes.
Moreover, during training, we introduce a simple and effective localization
method, which supervises the decoder's queries to localize not only a target
object but also ambiguous objects that might be confused as the target due to
the exhibition of similar attributes in a scene or the incorrect understanding
of a language description. This supervision enhances the model's ability to
distinguish ambiguous objects from a target by learning the differences in
their spatial relationships and attributes. LidaRefer achieves state-of-the-art
performance on Talk2Car-3D, a 3D VG dataset for autonomous driving, with
significant improvements under various evaluation settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UEVAVD: A <span class="highlight-title">Dataset</span> for Developing UAV's Eye View Active Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhua Jiang, Tianpeng Liu, Li Liu, Zhen Liu, Yongxiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Occlusion is a longstanding difficulty that challenges the UAV-based object
detection. Many works address this problem by adapting the detection model.
However, few of them exploit that the UAV could fundamentally improve detection
performance by changing its viewpoint. Active Object Detection (AOD) offers an
effective way to achieve this purpose. Through Deep Reinforcement Learning
(DRL), AOD endows the UAV with the ability of autonomous path planning to
search for the observation that is more conducive to target identification.
Unfortunately, there exists no available dataset for developing the UAV AOD
method. To fill this gap, we released a UAV's eye view active vision dataset
named UEVAVD and hope it can facilitate research on the UAV AOD problem.
Additionally, we improve the existing DRL-based AOD method by incorporating the
inductive bias when learning the state representation. First, due to the
partial observability, we use the gated recurrent unit to extract state
representations from the observation sequence instead of the single-view
observation. Second, we pre-decompose the scene with the Segment Anything Model
(SAM) and filter out the irrelevant information with the derived masks. With
these practices, the agent could learn an active viewing policy with better
generalization capability. The effectiveness of our innovations is validated by
the experiments on the UEVAVD dataset. Our dataset will soon be available at
https://github.com/Leo000ooo/UEVAVD_dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GazeGen: Gaze-Driven User Interaction for Visual Content Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He-Yen Hsieh, Ziyun Li, Sai Qian Zhang, Wei-Te Mark Ting, Kao-Den Chang, Barbara De Salvo, Chiao Liu, H. T. Kung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GazeGen, a user interaction system that generates visual content
(images and videos) for locations indicated by the user's eye gaze. GazeGen
allows intuitive manipulation of visual content by targeting regions of
interest with gaze. Using advanced techniques in object detection and
generative AI, GazeGen performs gaze-controlled image adding/deleting,
repositioning, and surface material changes of image objects, and converts
static images into videos. Central to GazeGen is the DFT Gaze (Distilled and
Fine-Tuned Gaze) agent, an ultra-lightweight model with only 281K parameters,
performing accurate real-time gaze predictions tailored to individual users'
eyes on small edge devices. GazeGen is the first system to combine visual
content generation with real-time gaze estimation, made possible exclusively by
DFT Gaze. This real-time gaze estimation enables various visual content
generation tasks, all controlled by the user's gaze. The input for DFT Gaze is
the user's eye images, while the inputs for visual content generation are the
user's view and the predicted gaze point from DFT Gaze. To achieve efficient
gaze predictions, we derive the small model from a large model (10x larger) via
novel knowledge distillation and personal adaptation techniques. We integrate
knowledge distillation with a masked autoencoder, developing a compact yet
powerful gaze estimation model. This model is further fine-tuned with Adapters,
enabling highly accurate and personalized gaze predictions with minimal user
input. DFT Gaze ensures low-latency and precise gaze tracking, supporting a
wide range of gaze-driven tasks. We validate the performance of DFT Gaze on AEA
and OpenEDS2020 benchmarks, demonstrating low angular gaze error and low
latency on the edge device (Raspberry Pi 4). Furthermore, we describe
applications of GazeGen, illustrating its versatility and effectiveness in
various usage scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HandCraft: Anatomically Correct Restoration of Malformed Hands in
  <span class="highlight-title">Diffusion</span> Generated Images <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyue Qin, Yiqun Zhang, Yang Liu, Dylan Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative text-to-image models, such as Stable Diffusion, have demonstrated
a remarkable ability to generate diverse, high-quality images. However, they
are surprisingly inept when it comes to rendering human hands, which are often
anatomically incorrect or reside in the "uncanny valley". In this paper, we
propose a method HandCraft for restoring such malformed hands. This is achieved
by automatically constructing masks and depth images for hands as conditioning
signals using a parametric model, allowing a diffusion-based image editor to
fix the hand's anatomy and adjust its pose while seamlessly integrating the
changes into the original image, preserving pose, color, and style. Our
plug-and-play hand restoration solution is compatible with existing pretrained
diffusion models, and the restoration process facilitates adoption by eschewing
any fine-tuning or training requirements for the diffusion models. We also
contribute MalHand datasets that contain generated images with a wide variety
of malformed hands in several styles for hand detector training and hand
restoration benchmarking, and demonstrate through qualitative and quantitative
evaluation that HandCraft not only restores anatomical correctness but also
maintains the integrity of the overall image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Analysis of U-Net-based models for Segmentation of Cardiac
  MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09980v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09980v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ketan Suhaas Saichandran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical imaging refers to the technologies and methods utilized to view the
human body and its inside, in order to diagnose, monitor, or even treat medical
disorders. This paper aims to explore the application of deep learning
techniques in the semantic segmentation of Cardiac short-axis MRI (Magnetic
Resonance Imaging) images, aiming to enhance the diagnosis, monitoring, and
treatment of medical disorders related to the heart. The focus centers on
implementing various architectures that are derivatives of U-Net, to
effectively isolate specific parts of the heart for comprehensive anatomical
and functional analysis. Through a combination of images, graphs, and
quantitative metrics, the efficacy of the models and their predictions are
showcased. Additionally, this paper addresses encountered challenges and
outline strategies for future improvements. This abstract provides a concise
overview of the efforts in utilizing deep learning for cardiac image
segmentation, emphasizing both the accomplishments and areas for further
refinement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GD doesn't make the cut: Three ways that non-differentiability affects
  neural network training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08426v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08426v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Krishna Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper critically examines the fundamental distinctions between gradient
methods applied to non-differentiable functions (NGDMs) and classical gradient
descents (GDs) for differentiable functions, revealing significant gaps in
current deep learning optimization theory. We demonstrate that NGDMs exhibit
markedly different convergence properties compared to GDs, strongly challenging
the applicability of extensive neural network convergence literature based on
$L-smoothness$ to non-smooth neural networks. Our analysis reveals paradoxical
behavior of NDGM solutions for $L_{1}$-regularized problems, where increasing
regularization counterintuitively leads to larger $L_{1}$ norms of optimal
solutions. This finding calls into question widely adopted $L_{1}$ penalization
techniques for network pruning. We further challenge the common assumption that
optimization algorithms like RMSProp behave similarly in differentiable and
non-differentiable contexts. Expanding on the Edge of Stability phenomenon, we
demonstrate its occurrence in a broader class of functions, including Lipschitz
continuous convex differentiable functions. This finding raises important
questions about its relevance and interpretation in non-convex,
non-differentiable neural networks, particularly those using ReLU activations.
Our work identifies critical misunderstandings of NDGMs in influential
literature, stemming from an overreliance on strong smoothness assumptions.
These findings necessitate a reevaluation of optimization dynamics in deep
learning, emphasizing the crucial need for more nuanced theoretical foundations
in analyzing these complex systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring QUIC Dynamics: A Large-Scale <span class="highlight-title">Dataset</span> for Encrypted Traffic
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barak Gahtan, Robert J. Shahla, Alex M. Bronstein, Reuven Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  QUIC, a new and increasingly used transport protocol, addresses and resolves
the limitations of TCP by offering improved security, performance, and features
such as stream multiplexing and connection migration. These features, however,
also present challenges for network operators who need to monitor and analyze
web traffic. In this paper, we introduce VisQUIC, a labeled dataset comprising
over 100,000 QUIC traces from more than 44,000 websites (URLs), collected over
a four-month period. These traces provide the foundation for generating more
than seven million images, with configurable parameters of window length, pixel
resolution, normalization, and labels. These images enable an observer looking
at the interactions between a client and a server to analyze and gain insights
about QUIC encrypted connections. To illustrate the dataset's potential, we
offer a use-case example of an observer estimating the number of HTTP/3
responses/requests pairs in a given QUIC, which can reveal server behavior,
client--server interactions, and the load imposed by an observed connection. We
formulate the problem as a discrete regression problem, train a machine
learning (ML) model for it, and then evaluate it using the proposed dataset on
an example use case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The dataset and the supplementary material can be provided upon
  request</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C3T: Cross-modal Transfer Through Time for Human Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhi Kamboj, Anh Duy Nguyen, Minh Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to unlock the potential of diverse sensors, we investigate a method
to transfer knowledge between modalities using the structure of a unified
multimodal representation space for Human Action Recognition (HAR). We
formalize and explore an understudied cross-modal transfer setting we term
Unsupervised Modality Adaptation (UMA), where the modality used in testing is
not used in supervised training, i.e. zero labeled instances of the test
modality are available during training. We develop three methods to perform
UMA: Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer
Through Time (C3T). Our extensive experiments on various camera+IMU datasets
compare these methods to each other in the UMA setting, and to their empirical
upper bound in the supervised setting. The results indicate C3T is the most
robust and highest performing by at least a margin of 8%, and nears the
supervised setting performance even in the presence of temporal noise. This
method introduces a novel mechanism for aligning signals across time-varying
latent vectors, extracted from the receptive field of temporal convolutions.
Our findings suggest that C3T has significant potential for developing
generalizable models for time-series sensor data, opening new avenues for
multi-modal learning in various applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Caching for Faster Video Generation with <span class="highlight-title">Diffusion</span> <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumara Kahatapitiya, Haozhe Liu, Sen He, Ding Liu, Menglin Jia, Chenyang Zhang, Michael S. Ryoo, Tian Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating temporally-consistent high-fidelity videos can be computationally
expensive, especially over longer temporal spans. More-recent Diffusion
Transformers (DiTs) -- despite making significant headway in this context --
have only heightened such challenges as they rely on larger models and heavier
attention mechanisms, resulting in slower inference speeds. In this paper, we
introduce a training-free method to accelerate video DiTs, termed Adaptive
Caching (AdaCache), which is motivated by the fact that "not all videos are
created equal": meaning, some videos require fewer denoising steps to attain a
reasonable quality than others. Building on this, we not only cache
computations through the diffusion process, but also devise a caching schedule
tailored to each video generation, maximizing the quality-latency trade-off. We
further introduce a Motion Regularization (MoReg) scheme to utilize video
information within AdaCache, essentially controlling the compute allocation
based on motion content. Altogether, our plug-and-play contributions grant
significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video
generation) without sacrificing the generation quality, across multiple video
DiT baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project-page is available at https://adacache-dit.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CardioSpectrum: Comprehensive Myocardium Motion Analysis with 3D Deep
  Learning and Geometric Insights <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03794v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03794v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahar Zuler, Shai Tejman-Yarden, Dan Raviv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to map left ventricle (LV) myocardial motion using computed
tomography angiography (CTA) is essential to diagnosing cardiovascular
conditions and guiding interventional procedures. Due to their inherent
locality, conventional neural networks typically have difficulty predicting
subtle tangential movements, which considerably lessens the level of precision
at which myocardium three-dimensional (3D) mapping can be performed. Using 3D
optical flow techniques and Functional Maps (FMs), we present a comprehensive
approach to address this problem. FMs are known for their capacity to capture
global geometric features, thus providing a fuller understanding of 3D
geometry. As an alternative to traditional segmentation-based priors, we employ
surface-based two-dimensional (2D) constraints derived from spectral
correspondence methods. Our 3D deep learning architecture, based on the ARFlow
model, is optimized to handle complex 3D motion analysis tasks. By
incorporating FMs, we can capture the subtle tangential movements of the
myocardium surface precisely, hence significantly improving the accuracy of 3D
mapping of the myocardium. The experimental results confirm the effectiveness
of this method in enhancing myocardium motion analysis. This approach can
contribute to improving cardiovascular diagnosis and treatment. Our code and
additional resources are available at:
https://shaharzuler.github.io/CardioSpectrumPage
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been early accepted to MICCAI 2024, LNCS 15005,
  Springer, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pediatric Wrist Fracture Detection Using Feature Context Excitation
  Modules in X-ray Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui-Yang Ju, Chun-Tse Chien, Enkaer Xieerke, Jen-Shiun Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Children often suffer wrist trauma in daily life, while they usually need
radiologists to analyze and interpret X-ray images before surgical treatment by
surgeons. The development of deep learning has enabled neural networks to serve
as computer-assisted diagnosis (CAD) tools to help doctors and experts in
medical image diagnostics. Since YOLOv8 model has obtained the satisfactory
success in object detection tasks, it has been applied to various fracture
detection. This work introduces four variants of Feature Contexts
Excitation-YOLOv8 (FCE-YOLOv8) model, each incorporating a different FCE module
(i.e., modules of Squeeze-and-Excitation (SE), Global Context (GC),
Gather-Excite (GE), and Gaussian Context Transformer (GCT)) to enhance the
model performance. Experimental results on GRAZPEDWRI-DX dataset demonstrate
that our proposed YOLOv8+GC-M3 model improves the mAP@50 value from 65.78% to
66.32%, outperforming the state-of-the-art (SOTA) model while reducing
inference time. Furthermore, our proposed YOLOv8+SE-M3 model achieves the
highest mAP@50 value of 67.07%, exceeding the SOTA performance. The
implementation of this work is available at
https://github.com/RuiyangJu/FCE-YOLOv8.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2407.03163</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities
  of Neurosymbolic AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruwan Wickramarachchi, Cory Henson, Amit Sheth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of Generative AI, Neurosymbolic AI is emerging as a powerful
approach for tasks spanning from perception to cognition. The use of
Neurosymbolic AI has been shown to achieve enhanced capabilities, including
improved grounding, alignment, explainability, and reliability. However, due to
its nascent stage, there is a lack of widely available real-world benchmark
datasets tailored to Neurosymbolic AI tasks. To address this gap and support
the evaluation of current and future methods, we introduce DSceneKG -- a suite
of knowledge graphs of driving scenes built from real-world, high-quality
scenes from multiple open autonomous driving datasets. In this article, we
detail the construction process of DSceneKG and highlight its application in
seven different tasks. DSceneKG is publicly accessible at:
https://github.com/ruwantw/DSceneKG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting CLIP: Insights on the Robustness to ImageNet Distribution
  Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Crabbé, Pau Rodríguez, Vaishaal Shankar, Luca Zappella, Arno Blaas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What distinguishes robust models from non-robust ones? While for ImageNet
distribution shifts it has been shown that such differences in robustness can
be traced back predominantly to differences in training data, so far it is not
known what that translates to in terms of what the model has learned. In this
work, we bridge this gap by probing the representation spaces of 16 robust
zero-shot CLIP vision encoders with various backbones (ResNets and ViTs) and
pretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and {DataComp}),
and comparing them to the representation spaces of less robust models with
identical backbones, but different (pre)training sets or objectives (CLIP
pretraining on ImageNet-Captions, and supervised training or finetuning on
ImageNet).Through this analysis, we generate three novel insights. Firstly, we
detect the presence of outlier features in robust zero-shot CLIP vision
encoders, which to the best of our knowledge is the first time these are
observed in non-language and non-transformer models. Secondly, we find the
existence of outlier features to be an indication of ImageNet shift robustness
in models, since we only find them in robust models in our analysis. Lastly, we
also investigate the number of unique encoded concepts in the representation
space and find zero-shot CLIP models to encode a higher number of unique
concepts in their representation space. However, we do not find this to be an
indicator of ImageNet shift robustness and hypothesize that it is rather
related to the language supervision. Since the presence of outlier features can
be detected without access to any data from shifted datasets, we believe that
they could be a useful tool for practitioners to get a feeling for the
distribution shift robustness of a pretrained model during deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5%
  Parameters and 90% Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16261v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16261v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangwei Gao, Zhe Chen, Erfei Cui, Yiming Ren, Weiyun Wang, Jinguo Zhu, Hao Tian, Shenglong Ye, Junjun He, Xizhou Zhu, Lewei Lu, Tong Lu, Yu Qiao, Jifeng Dai, Wenhai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have demonstrated impressive
performance in vision-language tasks across a broad spectrum of domains.
However, the large model scale and associated high computational costs pose
significant challenges for training and deploying MLLMs on consumer-grade GPUs
or edge devices, thereby hindering their widespread application. In this work,
we introduce Mini-InternVL, a series of MLLMs with parameters ranging from 1B
to 4B, which achieves 90% of the performance with only 5% of the parameters.
This significant improvement in efficiency and effectiveness makes our models
more accessible and applicable in various real-world scenarios. To further
promote the adoption of our models, we develop a unified adaptation framework
for Mini-InternVL, which enables our models to transfer and outperform
specialized models in downstream tasks, including autonomous driving, medical
images, and remote sensing. We believe that our study can provide valuable
insights and resources to advance the development of efficient and effective
MLLMs. Code is available at https://github.com/OpenGVLab/InternVL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BrainSegFounder: Towards 3D Foundation Models for Neuroimage
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10395v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10395v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Cox, Peng Liu, Skylar E. Stolte, Yunchao Yang, Kang Liu, Kyle B. See, Huiwen Ju, Ruogu Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning field of brain health research increasingly leverages
artificial intelligence (AI) to interpret and analyze neurological data. This
study introduces a novel approach towards the creation of medical foundation
models by integrating a large-scale multi-modal magnetic resonance imaging
(MRI) dataset derived from 41,400 participants in its own. Our method involves
a novel two-stage pretraining approach using vision transformers. The first
stage is dedicated to encoding anatomical structures in generally healthy
brains, identifying key features such as shapes and sizes of different brain
regions. The second stage concentrates on spatial information, encompassing
aspects like location and the relative positioning of brain structures. We
rigorously evaluate our model, BrainFounder, using the Brain Tumor Segmentation
(BraTS) challenge and Anatomical Tracings of Lesions After Stroke v2.0 (ATLAS
v2.0) datasets. BrainFounder demonstrates a significant performance gain,
surpassing the achievements of the previous winning solutions using fully
supervised learning. Our findings underscore the impact of scaling up both the
complexity of the model and the volume of unlabeled training data derived from
generally healthy brains, which enhances the accuracy and predictive
capabilities of the model in complex neuroimaging tasks with MRI. The
implications of this research provide transformative insights and practical
applications in healthcare and make substantial steps towards the creation of
foundation models for Medical AI. Our pretrained models and training code can
be found at https://github.com/lab-smile/GatorBrain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures, to be published in Medical Image Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiT4Edit: <span class="highlight-title">Diffusion</span> <span class="highlight-title">Transformer</span> for Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, Zeyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in UNet-based image editing, methods for shape-aware
object editing in high-resolution images are still lacking. Compared to UNet,
Diffusion Transformers (DiT) demonstrate superior capabilities to effectively
capture the long-range dependencies among patches, leading to higher-quality
image generation. In this paper, we propose DiT4Edit, the first Diffusion
Transformer-based image editing framework. Specifically, DiT4Edit uses the
DPM-Solver inversion algorithm to obtain the inverted latents, reducing the
number of steps compared to the DDIM inversion algorithm commonly used in
UNet-based frameworks. Additionally, we design unified attention control and
patches merging, tailored for transformer computation streams. This integration
allows our framework to generate higher-quality edited images faster. Our
design leverages the advantages of DiT, enabling it to surpass UNet structures
in image editing, especially in high-resolution and arbitrary-size images.
Extensive experiments demonstrate the strong performance of DiT4Edit across
various editing scenarios, highlighting the potential of Diffusion Transformers
in supporting image editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpikeBottleNet: Spike-Driven Feature Compression Architecture for
  Edge-Cloud Co-Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08673v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08673v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maruf Hassan, Steven Davy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge-cloud co-inference enables efficient deep neural network (DNN)
deployment by splitting the architecture between an edge device and cloud
server, crucial for resource-constraint edge devices. This approach requires
balancing on-device computations and communication costs, often achieved
through compressed intermediate feature transmission. Conventional DNN
architectures require continuous data processing and floating point
activations, leading to considerable energy consumption and increased feature
sizes, thus raising transmission costs. This challenge motivates exploring
binary, event-driven activations using spiking neural networks (SNNs), known
for their extreme energy efficiency. In this research, we propose
SpikeBottleNet, a novel architecture for edge-cloud co-inference systems that
integrates a spiking neuron model to significantly reduce energy consumption on
edge devices. A key innovation of our study is an intermediate feature
compression technique tailored for SNNs for efficient feature transmission.
This technique leverages a split computing approach to strategically place
encoder-decoder bottleneck units within complex deep architectures like ResNet
and MobileNet. Experimental results demonstrate that SpikeBottleNet achieves up
to 256x bit compression in the final convolutional layer of ResNet, with
minimal accuracy loss (0.16%). Additionally, our approach enhances edge device
energy efficiency by up to 144x compared to the baseline BottleNet, making it
ideal for resource-limited edge devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Padding in Patch-Based GANs for Seamless Infinite-Sized Texture
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02340v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02340v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alhasan Abdellatif, Ahmed H. Elsheikh, Hannah P. Menke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Texture models based on Generative Adversarial Networks (GANs) use
zero-padding to implicitly encode positional information of the image features.
However, when extending the spatial input to generate images at large sizes,
zero-padding can often lead to degradation in image quality due to the
incorrect positional information at the center of the image. Moreover,
zero-padding can limit the diversity within the generated large images. In this
paper, we propose a novel approach for generating stochastic texture images at
large arbitrary sizes using GANs based on patch-by-patch generation. Instead of
zero-padding, the model uses \textit{local padding} in the generator that
shares border features between the generated patches; providing positional
context and ensuring consistency at the boundaries. The proposed models are
trainable on a single texture image and have a constant GPU scalability with
respect to the output image size, and hence can generate images of infinite
sizes. We show in the experiments that our method has a significant advancement
beyond existing GANs-based texture models in terms of the quality and diversity
of the generated textures. Furthermore, the implementation of local padding in
the state-of-the-art super-resolution models effectively eliminates tiling
artifacts enabling large-scale super-resolution. Our code is available at
\url{https://github.com/ai4netzero/Infinite_Texture_GANs}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17822v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17822v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, Juho Kannala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-fidelity 3D reconstruction of common indoor scenes is crucial for VR and
AR applications. 3D Gaussian splatting, a novel differentiable rendering
technique, has achieved state-of-the-art novel view synthesis results with high
rendering speeds and relatively low training times. However, its performance on
scenes commonly seen in indoor datasets is poor due to the lack of geometric
constraints during optimization. In this work, we explore the use of readily
accessible geometric cues to enhance Gaussian splatting optimization in
challenging, ill-posed, and textureless scenes. We extend 3D Gaussian splatting
with depth and normal cues to tackle challenging indoor datasets and showcase
techniques for efficient mesh extraction. Specifically, we regularize the
optimization procedure with depth information, enforce local smoothness of
nearby Gaussians, and use off-the-shelf monocular networks to achieve better
alignment with the true scene geometry. We propose an adaptive depth loss based
on the gradient of color images, improving depth estimation and novel view
synthesis results over various baselines. Our simple yet effective
regularization technique enables direct mesh extraction from the Gaussian
representation, yielding more physically accurate reconstructions of indoor
scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in 2025 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representing Domain-Mixing Optical Degradation for Real-World
  Computational Aberration Correction via Vector Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Jiang, Zhonghua Yi, Shaohua Gao, Yao Gao, Xiaolong Qian, Hao Shi, Lei Sun, JinXing Niu, Kaiwei Wang, Kailun Yang, Jian Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relying on paired synthetic data, existing learning-based Computational
Aberration Correction (CAC) methods are confronted with the intricate and
multifaceted synthetic-to-real domain gap, which leads to suboptimal
performance in real-world applications. In this paper, in contrast to improving
the simulation pipeline, we deliver a novel insight into real-world CAC from
the perspective of Unsupervised Domain Adaptation (UDA). By incorporating
readily accessible unpaired real-world data into training, we formalize the
Domain Adaptive CAC (DACAC) task, and then introduce a comprehensive Real-world
aberrated images (Realab) dataset to benchmark it. The setup task presents a
formidable challenge due to the intricacy of understanding the target optical
degradation domain. To this intent, we propose a novel Quantized Domain-Mixing
Representation (QDMR) framework as a potent solution to the issue. Centering
around representing and quantizing the optical degradation which is consistent
across different images, QDMR adapts the CAC model to the target domain from
three key aspects: (1) reconstructing aberrated images of both domains by a
VQGAN to learn a Domain-Mixing Codebook (DMC) characterizing the optical
degradation; (2) modulating the deep features in CAC model with DMC to transfer
the target domain knowledge; and (3) leveraging the trained VQGAN to generate
pseudo target aberrated images from the source ones for convincing target
domain supervision. Extensive experiments on both synthetic and real-world
benchmarks reveal that the models with QDMR consistently surpass the
competitive methods in mitigating the synthetic-to-real gap, which produces
visually pleasant real-world CAC results with fewer artifacts. Codes and
datasets are made publicly available at https://github.com/zju-jiangqi/QDMR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Optics & Laser Technology. Codes and datasets are made
  publicly available at https://github.com/zju-jiangqi/QDMR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating alignment between humans and neural network representations
  in image-based learning tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09377v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09377v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Demircan, Tankred Saanum, Leonardo Pettini, Marcel Binz, Blazej M Baczkowski, Christian F Doeller, Mona M Garvert, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans represent scenes and objects in rich feature spaces, carrying
information that allows us to generalise about category memberships and
abstract functions with few examples. What determines whether a neural network
model generalises like a human? We tested how well the representations of $86$
pretrained neural network models mapped to human learning trajectories across
two tasks where humans had to learn continuous relationships and categories of
natural images. In these tasks, both human participants and neural networks
successfully identified the relevant stimulus features within a few trials,
demonstrating effective generalisation. We found that while training dataset
size was a core determinant of alignment with human choices, contrastive
training with multi-modal data (text and imagery) was a common feature of
currently publicly available models that predicted human generalisation.
Intrinsic dimensionality of representations had different effects on alignment
for different model types. Lastly, we tested three sets of human-aligned
representations and found no consistent improvements in predictive accuracy
compared to the baselines. In conclusion, pretrained neural networks can serve
to extract representations for cognitive models, as they appear to capture some
fundamental aspects of cognition that are transferable across tasks. Both our
paradigms and modelling approach offer a novel way to quantify alignment
between neural networks and humans and extend cognitive science into more
naturalistic domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Integrated Decision Gradients (IDG-DP) for
  Radar-based Human Activity Recognition <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idris Zakariyya, Linda Tran, Kaushik Bhargav Sivangi, Paul Henderson, Fani Deligianni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion analysis offers significant potential for healthcare monitoring
and early detection of diseases. The advent of radar-based sensing systems has
captured the spotlight for they are able to operate without physical contact
and they can integrate with pre-existing Wi-Fi networks. They are also seen as
less privacy-invasive compared to camera-based systems. However, recent
research has shown high accuracy in recognizing subjects or gender from radar
gait patterns, raising privacy concerns. This study addresses these issues by
investigating privacy vulnerabilities in radar-based Human Activity Recognition
(HAR) systems and proposing a novel method for privacy preservation using
Differential Privacy (DP) driven by attributions derived with Integrated
Decision Gradient (IDG) algorithm. We investigate Black-box Membership
Inference Attack (MIA) Models in HAR settings across various levels of
attacker-accessible information. We extensively evaluated the effectiveness of
the proposed IDG-DP method by designing a CNN-based HAR model and rigorously
assessing its resilience against MIAs. Experimental results demonstrate the
potential of IDG-DP in mitigating privacy attacks while maintaining utility
across all settings, particularly excelling against label-only and shadow model
black-box MIA attacks. This work represents a crucial step towards balancing
the need for effective radar-based HAR with robust privacy protection in
healthcare environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025. 12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ICAL: Implicit Character-Aided Learning for Enhanced Handwritten
  Mathematical Expression Recognition <span class="chip">ICDAR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09032v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09032v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhua Zhu, Liangcai Gao, Wenqi Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant progress has been made in the field of handwritten mathematical
expression recognition, while existing encoder-decoder methods are usually
difficult to model global information in $LaTeX$. Therefore, this paper
introduces a novel approach, Implicit Character-Aided Learning (ICAL), to mine
the global expression information and enhance handwritten mathematical
expression recognition. Specifically, we propose the Implicit Character
Construction Module (ICCM) to predict implicit character sequences and use a
Fusion Module to merge the outputs of the ICCM and the decoder, thereby
producing corrected predictions. By modeling and utilizing implicit character
information, ICAL achieves a more accurate and context-aware interpretation of
handwritten mathematical expressions. Experimental results demonstrate that
ICAL notably surpasses the state-of-the-art(SOTA) models, improving the
expression recognition rate (ExpRate) by 2.25\%/1.81\%/1.39\% on the CROHME
2014/2016/2019 datasets respectively, and achieves a remarkable 69.06\% on the
challenging HME100k test set. We make our code available on the GitHub:
https://github.com/qingzhenduyu/ICAL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICDAR 2024 Oral Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16591v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16591v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijie Wang, Guandu Liu, Bin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in vision-language foundational models, such as CLIP, have
demonstrated significant strides in zero-shot classification. However, the
extensive parameterization of models like CLIP necessitates a
resource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have
introduced training-free methods aimed at bolstering the efficacy of downstream
tasks. While these approaches incorporate support sets to maintain data
distribution consistency between knowledge cache and test sets, they often fall
short in terms of generalization on the test set, particularly when faced with
test data exhibiting substantial distributional variations. In this work, we
present CapS-Adapter, an innovative method that employs a caption-based support
set, effectively harnessing both image and caption features to exceed existing
state-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly
constructs support sets that closely mirror target distributions, utilizing
instance-level distribution features extracted from multimodal large models. By
leveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances
predictive accuracy through the use of multimodal support sets. Our method
achieves outstanding zero-shot classification results across 19 benchmark
datasets, improving accuracy by 2.19\% over the previous leading method. Our
contributions are substantiated through extensive validation on multiple
benchmark datasets, demonstrating superior performance and robust
generalization capabilities. Our code is made publicly available at
https://github.com/WLuLi/CapS-Adapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Multimedia 2024 Poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Classification by Coupling Data Mollification with Label
  Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Heinonen, Ba-Hien Tran, Michael Kampffmeyer, Maurizio Filippone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Introducing training-time augmentations is a key technique to enhance
generalization and prepare deep neural networks against test-time corruptions.
Inspired by the success of generative diffusion models, we propose a novel
approach of coupling data mollification, in the form of image noising and
blurring, with label smoothing to align predicted label confidences with image
degradation. The method is simple to implement, introduces negligible
overheads, and can be combined with existing augmentations. We demonstrate
improved robustness and uncertainty quantification on the corrupted image
benchmarks of the CIFAR and TinyImageNet datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOVA3: Learning to Visual Question Answering, Asking and Assessment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Hengyuan Zhao, Pan Zhou, Difei Gao, Zechen Bai, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering, asking, and assessment are three innate human traits
crucial for understanding the world and acquiring knowledge. By enhancing these
capabilities, humans can more effectively utilize data, leading to better
comprehension and learning outcomes. Current Multimodal Large Language Models
(MLLMs) primarily focus on question answering, often neglecting the full
potential of questioning and assessment skills. Inspired by the human learning
mechanism, we introduce LOVA3, an innovative framework named "Learning tO
Visual question Answering, Asking and Assessment," designed to equip MLLMs with
these additional capabilities. Our approach involves the creation of two
supplementary training tasks GenQA and EvalQA, aiming at fostering the skills
of asking and assessing questions in the context of images. To develop the
questioning ability, we compile a comprehensive set of multimodal foundational
tasks. For assessment, we introduce a new benchmark called EvalQABench,
comprising 64,000 training samples (split evenly between positive and negative
samples) and 5,000 validation and testing samples. We posit that enhancing
MLLMs with the capabilities to answer, ask, and assess questions will enhance
their multimodal comprehension, ultimately improving overall performance. To
validate this hypothesis, we train MLLMs using the LOVA3 framework and evaluate
them on a range of multimodal datasets and benchmarks. Our results demonstrate
consistent performance gains, underscoring the critical role of these
additional tasks in fostering comprehensive intelligence in MLLMs. The code is
available at https://github.com/showlab/LOVA3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. The code is available at
  https://github.com/showlab/LOVA3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Pattern Completion: Self-supervised Controllable
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Chen, Guofan Fan, Jinying Gao, Lei Ma, Bo Lei, Tiejun Huang, Shan Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human brain exhibits a strong ability to spontaneously associate
different visual attributes of the same or similar visual scene, such as
associating sketches and graffiti with real-world visual objects, usually
without supervising information. In contrast, in the field of artificial
intelligence, controllable generation methods like ControlNet heavily rely on
annotated training datasets such as depth maps, semantic segmentation maps, and
poses, which limits the method's scalability. Inspired by the neural mechanisms
that may contribute to the brain's associative power, specifically the cortical
modularization and hippocampal pattern completion, here we propose a
self-supervised controllable generation (SCG) framework. Firstly, we introduce
an equivariant constraint to promote inter-module independence and intra-module
correlation in a modular autoencoder network, thereby achieving functional
specialization. Subsequently, based on these specialized modules, we employ a
self-supervised pattern completion approach for controllable generation
training. Experimental results demonstrate that the proposed modular
autoencoder effectively achieves functional specialization, including the
modular processing of color, brightness, and edge detection, and exhibits
brain-like features including orientation selectivity, color antagonism, and
center-surround receptive fields. Through self-supervised training, associative
generation capabilities spontaneously emerge in SCG, demonstrating excellent
generalization ability to various tasks such as associative generation on
painting, sketches, and ancient graffiti. Compared to the previous
representative method ControlNet, our proposed approach not only demonstrates
superior robustness in more challenging high-noise scenarios but also possesses
more promising scalability potential due to its self-supervised manner.Codes
are released on Github and Gitee.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An efficient dual-branch framework via implicit self-texture enhancement
  for arbitrary-scale histopathology image super-resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15613v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15613v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghong Duan, Linhao Qu, Zhiwei Yang, Manning Wang, Chenxi Zhang, Zhijian Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality whole-slide scanning is expensive, complex, and time-consuming,
thus limiting the acquisition and utilization of high-resolution histopathology
images in daily clinical work. Deep learning-based single-image
super-resolution (SISR) techniques provide an effective way to solve this
problem. However, the existing SISR models applied in histopathology images can
only work in fixed integer scaling factors, decreasing their applicability.
Though methods based on implicit neural representation (INR) have shown
promising results in arbitrary-scale super-resolution (SR) of natural images,
applying them directly to histopathology images is inadequate because they have
unique fine-grained image textures different from natural images. Thus, we
propose an Implicit Self-Texture Enhancement-based dual-branch framework (ISTE)
for arbitrary-scale SR of histopathology images to address this challenge. The
proposed ISTE contains a feature aggregation branch and a texture learning
branch. We employ the feature aggregation branch to enhance the learning of the
local details for SR images while utilizing the texture learning branch to
enhance the learning of high-frequency texture details. Then, we design a
two-stage texture enhancement strategy to fuse the features from the two
branches to obtain the SR images. Experiments on publicly available datasets,
including TMA, HistoSR, and the TCGA lung cancer datasets, demonstrate that
ISTE outperforms existing fixed-scale and arbitrary-scale SR algorithms across
various scaling factors. Additionally, extensive experiments have shown that
the histopathology images reconstructed by the proposed ISTE are applicable to
downstream pathology image analysis tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Surgical Instrument Recognition and Segmentation in
  Robotic-Assisted Surgeries: A Systematic Review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatimaelzahraa Ali Ahmed, Mahmoud Yousef, Mariam Ali Ahmed, Hasan Omar Ali, Anns Mahboob, Hazrat Ali, Zubair Shah, Omar Aboumarzouk, Abdulla Al Ansari, Shidin Balakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying deep learning (DL) for annotating surgical instruments in
robot-assisted minimally invasive surgeries (MIS) represents a significant
advancement in surgical technology. This systematic review examines 48 studies
that and advanced DL methods and architectures. These sophisticated DL models
have shown notable improvements in the precision and efficiency of detecting
and segmenting surgical tools. The enhanced capabilities of these models
support various clinical applications, including real-time intraoperative
guidance, comprehensive postoperative evaluations, and objective assessments of
surgical skills. By accurately identifying and segmenting surgical instruments
in video data, DL models provide detailed feedback to surgeons, thereby
improving surgical outcomes and reducing complication risks. Furthermore, the
application of DL in surgical education is transformative. The review
underscores the significant impact of DL on improving the accuracy of skill
assessments and the overall quality of surgical training programs. However,
implementing DL in surgical tool detection and segmentation faces challenges,
such as the need for large, accurately annotated datasets to train these models
effectively. The manual annotation process is labor-intensive and
time-consuming, posing a significant bottleneck. Future research should focus
on automating the detection and segmentation process and enhancing the
robustness of DL models against environmental variations. Expanding the
application of DL models across various surgical specialties will be essential
to fully realize this technology's potential. Integrating DL with other
emerging technologies, such as augmented reality (AR), also offers promising
opportunities to further enhance the precision and efficacy of surgical
procedures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages, 9 figures, Published in Artificial Intelligence Reviews
  journal <https://link.springer.com/journal/10462></span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TaE: Task-aware Expandable Representation for Long Tail Class
  Incremental Learning <span class="chip">ACCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linjie Li, Zhenyu Wu, Jiaming Liu, Yang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning is dedicated to the development of deep learning
models that are capable of acquiring new knowledge while retaining previously
learned information. Most methods focus on balanced data distribution for each
task, overlooking real-world long-tailed distributions. Therefore, Long-Tailed
Class-Incremental Learning has been introduced, which trains on data where head
classes have more samples than tail classes. Existing methods mainly focus on
preserving representative samples from previous classes to combat catastrophic
forgetting. Recently, dynamic network algorithms freeze old network structures
and expand new ones, achieving significant performance. However, with the
introduction of the long-tail problem, merely extending Determined blocks can
lead to miscalibrated predictions, while expanding the entire backbone results
in an explosion of memory size. To address these issues, we introduce a novel
Task-aware Expandable (TaE) framework, dynamically allocating and updating
task-specific trainable parameters to learn diverse representations from each
incremental task while resisting forgetting through the majority of frozen
model parameters. To further encourage the class-specific feature
representation, we develop a Centroid-Enhanced (CEd) method to guide the update
of these task-aware parameters. This approach is designed to adaptively
allocate feature space for every class by adjusting the distance between intra-
and inter-class features, which can extend to all "training from sketch"
algorithms. Extensive experiments demonstrate that TaE achieves
state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GS2Pose: Two-stage 6D Object Pose Estimation Guided by Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jilan Mei, Junbo Li, Cai Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new method for accurate and robust 6D pose estimation
of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose
can utilize the reconstruction results without requiring a high-quality CAD
model, which means it only requires segmented RGBD images as input.
Specifically, GS2Pose employs a two-stage structure consisting of coarse
estimation followed by refined estimation. In the coarse stage, a lightweight
U-Net network with a polarization attention mechanism, called Pose-Net, is
designed. By using the 3DGS model for supervised training, Pose-Net can
generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose
formulates a pose regression algorithm following the idea of reprojection or
Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to
extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that
refines the coarse pose by comparing the input images with the rendered images.
GS-Refiner also selectively updates parameters in the 3DGS model to achieve
environmental adaptation, thereby enhancing the algorithm's robustness and
flexibility to illuminative variation, occlusion, and other challenging
disruptive factors. GS2Pose was evaluated through experiments conducted on the
LineMod dataset, where it was compared with similar algorithms, yielding highly
competitive results. The code for GS2Pose will soon be released on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training-free Zero-shot Composed Image Retrieval via Weighted Modality
  Fusion and Similarity <span class="chip">TAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04918v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04918v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ren-Di Wu, Yu-Yen Lin, Huei-Fang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed image retrieval (CIR), which formulates the query as a combination
of a reference image and modified text, has emerged as a new form of image
search due to its enhanced ability to capture user intent. However, training a
CIR model in a supervised manner typically requires labor-intensive collection
of (reference image, text modifier, target image) triplets. While existing
zero-shot CIR (ZS-CIR) methods eliminate the need for training on specific
downstream datasets, they still require additional pretraining on large-scale
image datasets. In this paper, we introduce a training-free approach for
ZS-CIR. Our approach, Weighted Modality fusion and similarity for CIR
(WeiMoCIR), operates under the assumption that image and text modalities can be
effectively combined using a simple weighted average. This allows the query
representation to be constructed directly from the reference image and text
modifier. To further enhance retrieval performance, we employ multimodal large
language models (MLLMs) to generate image captions for the database images and
incorporate these textual captions into the similarity computation by combining
them with image information using a weighted average. Our approach is simple,
easy to implement, and its effectiveness is validated through experiments on
the FashionIQ and CIRR datasets. Code is available at
https://github.com/whats2000/WeiMoCIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures, International Conference on Technologies and
  Applications of Artificial Intelligence (TAAI) Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Calibrated Robust Fine-Tuning of Vision-Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01723v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01723v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changdae Oh, Hyesu Lim, Mijoo Kim, Dongyoon Han, Sangdoo Yun, Jaegul Choo, Alexander Hauptmann, Zhi-Qi Cheng, Kyungwoo Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving out-of-distribution (OOD) generalization during in-distribution
(ID) adaptation is a primary goal of robust fine-tuning of zero-shot models
beyond naive fine-tuning. However, despite decent OOD generalization
performance from recent robust fine-tuning methods, confidence calibration for
reliable model output has not been fully addressed. This work proposes a robust
fine-tuning method that improves both OOD accuracy and confidence calibration
simultaneously in vision language models. Firstly, we show that both OOD
classification and OOD calibration errors have a shared upper bound consisting
of two terms of ID data: 1) ID calibration error and 2) the smallest singular
value of the ID input covariance matrix. Based on this insight, we design a
novel framework that conducts fine-tuning with a constrained multimodal
contrastive loss enforcing a larger smallest singular value, which is further
guided by the self-distillation of a moving-averaged model to achieve
calibrated prediction as well. Starting from empirical evidence supporting our
theoretical statements, we provide extensive experimental results on ImageNet
distribution shift benchmarks that demonstrate the effectiveness of our theorem
and its practical implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (a short version was presented at the NeurIPS 2023
  Workshop on Distribution Shifts); Major modification of (v7): Fixing the
  x-axis of Figure 3 and Pearson correlation, accordingly</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field <span class="chip">ECCV'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Wang, Feitong Tan, Ziqian Bai, Yinda Zhang, Shichen Liu, Qiangeng Xu, Menglei Chai, Anish Prabhu, Rohit Pandey, Sean Fanello, Zeng Huang, Yun Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that neural radiance fields (NeRFs) on top of
parametric models have reached SOTA quality to build photorealistic head
avatars from a monocular video. However, one major limitation of the NeRF-based
avatars is the slow rendering speed due to the dense point sampling of NeRF,
preventing them from broader utility on resource-constrained devices. We
introduce LightAvatar, the first head avatar model based on neural light fields
(NeLFs). LightAvatar renders an image from 3DMM parameters and a camera pose
via a single network forward pass, without using mesh or volume rendering. The
proposed approach, while being conceptually appealing, poses a significant
challenge towards real-time efficiency and training stability. To resolve them,
we introduce dedicated network designs to obtain proper representations for the
NeLF model and maintain a low FLOPs budget. Meanwhile, we tap into a
distillation-based training strategy that uses a pretrained avatar model as
teacher to synthesize abundant pseudo data for training. A warping field
network is introduced to correct the fitting error in the real data so that the
model can learn better. Extensive experiments suggest that our method can
achieve new SOTA image quality quantitatively or qualitatively, while being
significantly faster than the counterparts, reporting 174.1 FPS (512x512
resolution) on a consumer-grade GPU (RTX3090) with no customized optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV'24 CADL Workshop. Code:
  https://github.com/MingSun-Tse/LightAvatar-TensorFlow. V2: Corrected speed
  benchmark with GaussianAvatar</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeNIe: Generative Hard Negative Images Through <span class="highlight-title">Diffusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroush Abbasi Koohpayegani, Anuj Singh, K L Navaneet, Hamed Pirsiavash, Hadi Jamali-Rad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is crucial in training deep models, preventing them from
overfitting to limited data. Recent advances in generative AI, e.g., diffusion
models, have enabled more sophisticated augmentation techniques that produce
data resembling natural images. We introduce GeNIe a novel augmentation method
which leverages a latent diffusion model conditioned on a text prompt to
combine two contrasting data points (an image from the source category and a
text prompt from the target category) to generate challenging augmentations. To
achieve this, we adjust the noise level (equivalently, number of diffusion
iterations) to ensure the generated image retains low-level and background
features from the source image while representing the target category,
resulting in a hard negative sample for the source category. We further
automate and enhance GeNIe by adaptively adjusting the noise level selection on
a per image basis (coined as GeNIe-Ada), leading to further performance
improvements. Our extensive experiments, in both few-shot and long-tail
distribution settings, demonstrate the effectiveness of our novel augmentation
method and its superior performance over the prior art. Our code is available
at: https://github.com/UCDvision/GeNIe
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available https://github.com/UCDvision/GeNIe</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeafloorAI: A Large-scale Vision-Language <span class="highlight-title">Dataset</span> for Seafloor
  Geological <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00172v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00172v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kien X. Nguyen, Fengchun Qiao, Arthur Trembanis, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major obstacle to the advancements of machine learning models in marine
science, particularly in sonar imagery analysis, is the scarcity of AI-ready
datasets. While there have been efforts to make AI-ready sonar image dataset
publicly available, they suffer from limitations in terms of environment
setting and scale. To bridge this gap, we introduce SeafloorAI, the first
extensive AI-ready datasets for seafloor mapping across 5 geological layers
that is curated in collaboration with marine scientists. We further extend the
dataset to SeafloorGenAI by incorporating the language component in order to
facilitate the development of both vision- and language-capable machine
learning models for sonar imagery. The dataset consists of 62 geo-distributed
data surveys spanning 17,300 square kilometers, with 696K sonar images, 827K
annotated segmentation masks, 696K detailed language descriptions and
approximately 7M question-answer pairs. By making our data processing source
code publicly available, we aim to engage the marine science community to
enrich the data pool and inspire the machine learning community to develop more
robust models. This collaborative approach will enhance the capabilities and
applications of our datasets within both fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Typicalness-Aware Learning for Failure Detection <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Liu, Jiequan Cui, Zhuotao Tian, Senqiao Yang, Qingdong He, Xiaoling Wang, Jingyong Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) often suffer from the overconfidence issue, where
incorrect predictions are made with high confidence scores, hindering the
applications in critical systems. In this paper, we propose a novel approach
called Typicalness-Aware Learning (TAL) to address this issue and improve
failure detection performance. We observe that, with the cross-entropy loss,
model predictions are optimized to align with the corresponding labels via
increasing logit magnitude or refining logit direction. However, regarding
atypical samples, the image content and their labels may exhibit disparities.
This discrepancy can lead to overfitting on atypical samples, ultimately
resulting in the overconfidence issue that we aim to address. To tackle the
problem, we have devised a metric that quantifies the typicalness of each
sample, enabling the dynamic adjustment of the logit magnitude during the
training process. By allowing atypical samples to be adequately fitted while
preserving reliable logit direction, the problem of overconfidence can be
mitigated. TAL has been extensively evaluated on benchmark datasets, and the
results demonstrate its superiority over existing failure detection methods.
Specifically, TAL achieves a more than 5% improvement on CIFAR100 in terms of
the Area Under the Risk-Coverage Curve (AURC) compared to the state-of-the-art.
Code is available at https://github.com/liuyijungoon/TAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Undermining Image and Text Classification Algorithms Using Adversarial
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Langalibalele Lunga, Suhas Sreehari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are prone to adversarial attacks, where inputs can be
manipulated in order to cause misclassifications. While previous research has
focused on techniques like Generative Adversarial Networks (GANs), there's
limited exploration of GANs and Synthetic Minority Oversampling Technique
(SMOTE) in text and image classification models to perform adversarial attacks.
Our study addresses this gap by training various machine learning models and
using GANs and SMOTE to generate additional data points aimed at attacking text
classification models. Furthermore, we extend our investigation to face
recognition models, training a Convolutional Neural Network(CNN) and subjecting
it to adversarial attacks with fast gradient sign perturbations on key features
identified by GradCAM, a technique used to highlight key image characteristics
CNNs use in classification. Our experiments reveal a significant vulnerability
in classification models. Specifically, we observe a 20 % decrease in accuracy
for the top-performing text classification models post-attack, along with a 30
% decrease in facial recognition accuracy. This highlights the susceptibility
of these models to manipulation of input data. Adversarial attacks not only
compromise the security but also undermine the reliability of machine learning
systems. By showcasing the impact of adversarial attacks on both text
classification and face recognition models, our study underscores the urgent
need for develop robust defenses against such vulnerabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at Electronic Imaging Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Text-to-Image <span class="highlight-title">Diffusion</span> Models with Reward Backpropagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03739v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03739v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have recently emerged at the forefront of
image generation, powered by very large-scale unsupervised or weakly supervised
text-to-image training datasets. Due to their unsupervised training,
controlling their behavior in downstream tasks, such as maximizing
human-perceived image quality, image-text alignment, or ethical image
generation, is difficult. Recent works finetune diffusion models to downstream
reward functions using vanilla reinforcement learning, notorious for the high
variance of the gradient estimators. In this paper, we propose AlignProp, a
method that aligns diffusion models to downstream reward functions using
end-to-end backpropagation of the reward gradient through the denoising
process. While naive implementation of such backpropagation would require
prohibitive memory resources for storing the partial derivatives of modern
text-to-image models, AlignProp finetunes low-rank adapter weight modules and
uses gradient checkpointing, to render its memory usage viable. We test
AlignProp in finetuning diffusion models to various objectives, such as
image-text semantic alignment, aesthetics, compressibility and controllability
of the number of objects present, as well as their combinations. We show
AlignProp achieves higher rewards in fewer training steps than alternatives,
while being conceptually simpler, making it a straightforward choice for
optimizing diffusion models for differentiable reward functions of interest.
Code and Visualization results are available at https://align-prop.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is subsumed by a later paper of ours: arXiv:2407.08737</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tang Li, Mengmeng Ma, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pretrained foundation models demonstrate exceptional performance and,
in some high-stakes applications, even surpass human experts. However, most of
these models are currently evaluated primarily on prediction accuracy,
overlooking the validity of the rationales behind their accurate predictions.
For the safe deployment of foundation models, there is a pressing need to
ensure double-correct predictions, i.e., correct prediction backed by correct
rationales. To achieve this, we propose a two-phase scheme: First, we curate a
new dataset that offers structured rationales for visual recognition tasks.
Second, we propose a rationale-informed optimization method to guide the model
in disentangling and localizing visual evidence for each rationale, without
requiring manual annotations. Extensive experiments and ablation studies
demonstrate that our model outperforms state-of-the-art models by up to 10.1%
in prediction accuracy across a wide range of tasks. Furthermore, our method
significantly improves the model's rationale correctness, improving
localization by 7.5% and disentanglement by 36.5%. Our dataset, source code,
and pretrained weights: https://github.com/deep-real/DCP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 38th Conference on Neural Information
  Processing Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ bit2bit: 1-bit quanta video reconstruction via self-supervised photon
  prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23247v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23247v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehe Liu, Alexander Krull, Hector Basevi, Ales Leonardis, Michael W. Jenkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quanta image sensors, such as SPAD arrays, are an emerging sensor technology,
producing 1-bit arrays representing photon detection events over exposures as
short as a few nanoseconds. In practice, raw data are post-processed using
heavy spatiotemporal binning to create more useful and interpretable images at
the cost of degrading spatiotemporal resolution. In this work, we propose
bit2bit, a new method for reconstructing high-quality image stacks at the
original spatiotemporal resolution from sparse binary quanta image data.
Inspired by recent work on Poisson denoising, we developed an algorithm that
creates a dense image sequence from sparse binary photon data by predicting the
photon arrival location probability distribution. However, due to the binary
nature of the data, we show that the assumption of a Poisson distribution is
inadequate. Instead, we model the process with a Bernoulli lattice process from
the truncated Poisson. This leads to the proposal of a novel self-supervised
solution based on a masked loss function. We evaluate our method using both
simulated and real data. On simulated data from a conventional video, we
achieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06
photons per pixel per frame). We also present a novel dataset containing a wide
range of real SPAD high-speed videos under various challenging imaging
conditions. The scenes cover strong/weak ambient light, strong motion,
ultra-fast events, etc., which will be made available to the community, on
which we demonstrate the promise of our approach. Both reconstruction quality
and throughput substantially surpass the state-of-the-art methods (e.g., Quanta
Burst Photography (QBP)). Our approach significantly enhances the visualization
and usability of the data, enabling the application of existing analysis
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Surgical Instrument Segmentation Without Human Intervention:
  A Graph Partitioning View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14789v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14789v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Sheng, Jianan Fan, Dongnan Liu, Ron Kikinis, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical instrument segmentation (SIS) on endoscopic images stands as a
long-standing and essential task in the context of computer-assisted
interventions for boosting minimally invasive surgery. Given the recent surge
of deep learning methodologies and their data-hungry nature, training a neural
predictive model based on massive expert-curated annotations has been
dominating and served as an off-the-shelf approach in the field, which could,
however, impose prohibitive burden to clinicians for preparing fine-grained
pixel-wise labels corresponding to the collected surgical video frames. In this
work, we propose an unsupervised method by reframing the video frame
segmentation as a graph partitioning problem and regarding image pixels as
graph nodes, which is significantly different from the previous efforts. A
self-supervised pre-trained model is firstly leveraged as a feature extractor
to capture high-level semantic features. Then, Laplacian matrixs are computed
from the features and are eigendecomposed for graph partitioning. On the "deep"
eigenvectors, a surgical video frame is meaningfully segmented into different
modules such as tools and tissues, providing distinguishable semantic
information like locations, classes, and relations. The segmentation problem
can then be naturally tackled by applying clustering or threshold on the
eigenvectors. Extensive experiments are conducted on various datasets (e.g.,
EndoVis2017, EndoVis2018, UCL, etc.) for different clinical endpoints. Across
all the challenging scenarios, our method demonstrates outstanding performance
and robustness higher than unsupervised state-of-the-art (SOTA) methods. The
code is released at https://github.com/MingyuShengSMY/GraphClusteringSIS.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The 32nd ACM International Conference on Multimedia (ACM
  MM 2024) Workshop on Multimedia Computing for Health and Medicine (MCHM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMNCutter: Affinity-Attention-Guided Multi-View Normalized Cutter for
  Unsupervised Surgical Instrument Segmentation <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Sheng, Jianan Fan, Dongnan Liu, Ron Kikinis, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical instrument segmentation (SIS) is pivotal for robotic-assisted
minimally invasive surgery, assisting surgeons by identifying surgical
instruments in endoscopic video frames. Recent unsupervised surgical instrument
segmentation (USIS) methods primarily rely on pseudo-labels derived from
low-level features such as color and optical flow, but these methods show
limited effectiveness and generalizability in complex and unseen endoscopic
scenarios. In this work, we propose a label-free unsupervised model featuring a
novel module named Multi-View Normalized Cutter (m-NCutter). Different from
previous USIS works, our model is trained using a graph-cutting loss function
that leverages patch affinities for supervision, eliminating the need for
pseudo-labels. The framework adaptively determines which affinities from which
levels should be prioritized. Therefore, the low- and high-level features and
their affinities are effectively integrated to train a label-free unsupervised
model, showing superior effectiveness and generalization ability. We conduct
comprehensive experiments across multiple SIS datasets to validate our
approach's state-of-the-art (SOTA) performance, robustness, and exceptional
potential as a pre-trained model. Our code is released at
https://github.com/MingyuShengSMY/AMNCutter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 2025 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controllable Talking Face Generation by Implicit Facial Keypoints
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Zhao, Jiaying Shi, Wenjun Li, Shudong Wang, Shenghui Xu, Zhaoming Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking face generation has garnered significant interest within
the domain of digital human research. Existing methods are encumbered by
intricate model architectures that are intricately dependent on each other,
complicating the process of re-editing image or video inputs. In this work, we
present ControlTalk, a talking face generation method to control face
expression deformation based on driven audio, which can construct the head pose
and facial expression including lip motion for both single image or sequential
video inputs in a unified manner. By utilizing a pre-trained video synthesis
renderer and proposing the lightweight adaptation, ControlTalk achieves precise
and naturalistic lip synchronization while enabling quantitative control over
mouth opening shape. Our experiments show that our method is superior to
state-of-the-art performance on widely used benchmarks, including HDTF and
MEAD. The parameterized adaptation demonstrates remarkable generalization
capabilities, effectively handling expression deformation across same-ID and
cross-ID scenarios, and extending its utility to out-of-domain portraits,
regardless of languages. Code is available at
https://github.com/NetEase-Media/ControlTalk.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Principled Probabilistic Imaging using <span class="highlight-title">Diffusion</span> Models as Plug-and-Play
  Priors <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihui Wu, Yu Sun, Yifan Chen, Bingliang Zhang, Yisong Yue, Katherine L. Bouman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have recently shown outstanding capabilities in
modeling complex image distributions, making them expressive image priors for
solving Bayesian inverse problems. However, most existing DM-based methods rely
on approximations in the generative process to be generic to different inverse
problems, leading to inaccurate sample distributions that deviate from the
target posterior defined within the Bayesian framework. To harness the
generative power of DMs while avoiding such approximations, we propose a Markov
chain Monte Carlo algorithm that performs posterior sampling for general
inverse problems by reducing it to sampling the posterior of a Gaussian
denoising problem. Crucially, we leverage a general DM formulation as a unified
interface that allows for rigorously solving the denoising problem with a range
of state-of-the-art DMs. We demonstrate the effectiveness of the proposed
method on six inverse problems (three linear and three nonlinear), including a
real-world black hole imaging problem. Experimental results indicate that our
proposed method offers more accurate reconstructions and posterior estimation
compared to existing DM-based imaging inverse methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TLCM: Training-efficient Latent Consistency Model for Image Generation
  with 2-8 Steps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05768v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05768v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingsong Xie, Zhenyi Liao, Zhijie Deng, Chen chen, Haonan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distilling latent diffusion models (LDMs) into ones that are fast to sample
from is attracting growing research interest. However, the majority of existing
methods face two critical challenges: (1) They hinge on long training using a
huge volume of real data. (2) They routinely lead to quality degradation for
generation, especially in text-image alignment. This paper proposes a novel
training-efficient Latent Consistency Model (TLCM) to overcome these
challenges. Our method first accelerates LDMs via data-free multistep latent
consistency distillation (MLCD), and then data-free latent consistency
distillation is proposed to efficiently guarantee the inter-segment consistency
in MLCD. Furthermore, we introduce bags of techniques, e.g., distribution
matching, adversarial learning, and preference learning, to enhance TLCM's
performance at few-step inference without any real data. TLCM demonstrates a
high level of flexibility by enabling adjustment of sampling steps within the
range of 2 to 8 while still producing competitive outputs compared to full-step
approaches. Notably, TLCM enjoys the data-free merit by employing synthetic
data from the teacher for distillation. With just 70 training hours on an A100
GPU, a 3-step TLCM distilled from SDXL achieves an impressive CLIP Score of
33.68 and an Aesthetic Score of 5.97 on the MSCOCO-2017 5K benchmark,
surpassing various accelerated models and even outperforming the teacher model
in human preference metrics. We also demonstrate the versatility of TLCMs in
applications including image style transfer, controllable generation, and
Chinese-to-image generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Zero-shot Multispectral Pansharpening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06633v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06633v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Rui, Xiangyong Cao, Yining Li, Deyu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pansharpening aims to generate a high spatial resolution multispectral image
(HRMS) by fusing a low spatial resolution multispectral image (LRMS) and a
panchromatic image (PAN). The most challenging issue for this task is that only
the to-be-fused LRMS and PAN are available, and the existing deep
learning-based methods are unsuitable since they rely on many training pairs.
Traditional variational optimization (VO) based methods are well-suited for
addressing such a problem. They focus on carefully designing explicit fusion
rules as well as regularizations for an optimization problem, which are based
on the researcher's discovery of the image relationships and image structures.
Unlike previous VO-based methods, in this work, we explore such complex
relationships by a parameterized term rather than a manually designed one.
Specifically, we propose a zero-shot pansharpening method by introducing a
neural network into the optimization objective. This network estimates a
representation component of HRMS, which mainly describes the relationship
between HRMS and PAN. In this way, the network achieves a similar goal to the
so-called deep image prior because it implicitly regulates the relationship
between the HRMS and PAN images through its inherent structure. We directly
minimize this optimization objective via network parameters and the expected
HRMS image through iterative updating. Extensive experiments on various
benchmark datasets demonstrate that our proposed method can achieve better
performance compared with other state-of-the-art methods. The codes are
available at https://github.com/xyrui/PSDip.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SS3DM: Benchmarking Street-View Surface Reconstruction with a Synthetic
  3D Mesh <span class="highlight-title">Dataset</span> <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubin Hu, Kairui Wen, Heng Zhou, Xiaoyang Guo, Yong-Jin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing accurate 3D surfaces for street-view scenarios is crucial for
applications such as digital entertainment and autonomous driving simulation.
However, existing street-view datasets, including KITTI, Waymo, and nuScenes,
only offer noisy LiDAR points as ground-truth data for geometric evaluation of
reconstructed surfaces. These geometric ground-truths often lack the necessary
precision to evaluate surface positions and do not provide data for assessing
surface normals. To overcome these challenges, we introduce the SS3DM dataset,
comprising precise \textbf{S}ynthetic \textbf{S}treet-view \textbf{3D}
\textbf{M}esh models exported from the CARLA simulator. These mesh models
facilitate accurate position evaluation and include normal vectors for
evaluating surface normal. To simulate the input data in realistic driving
scenarios for 3D reconstruction, we virtually drive a vehicle equipped with six
RGB cameras and five LiDAR sensors in diverse outdoor scenes. Leveraging this
dataset, we establish a benchmark for state-of-the-art surface reconstruction
methods, providing a comprehensive evaluation of the associated challenges.
  For more information, visit our homepage at https://ss3dm.top.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, Track on Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">186</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit <span class="highlight-title">Diffusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have been proven highly effective at generating high-quality
images. However, as these models grow larger, they require significantly more
memory and suffer from higher latency, posing substantial challenges for
deployment. In this work, we aim to accelerate diffusion models by quantizing
their weights and activations to 4 bits. At such an aggressive level, both
weights and activations are highly sensitive, where conventional post-training
quantization methods for large language models like smoothing become
insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit
quantization paradigm. Different from smoothing which redistributes outliers
between weights and activations, our approach absorbs these outliers using a
low-rank branch. We first consolidate the outliers by shifting them from
activations to weights, then employ a high-precision low-rank branch to take in
the weight outliers with Singular Value Decomposition (SVD). This process eases
the quantization on both sides. However, na\"{\i}vely running the low-rank
branch independently incurs significant overhead due to extra data movement of
activations, negating the quantization speedup. To address this, we co-design
an inference engine Nunchaku that fuses the kernels of the low-rank branch into
those of the low-bit branch to cut off redundant memory access. It can also
seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for
re-quantization. Extensive experiments on SDXL, PixArt-$\Sigma$, and FLUX.1
validate the effectiveness of SVDQuant in preserving image quality. We reduce
the memory usage for the 12B FLUX.1 models by 3.5$\times$, achieving
3.0$\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB
laptop 4090 GPU, paving the way for more interactive applications on PCs. Our
quantization library and inference engine are open-sourced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Quantization Library: https://github.com/mit-han-lab/deepcompressor
  Inference Engine: https://github.com/mit-han-lab/nunchaku Website:
  https://hanlab.mit.edu/projects/svdquant Demo: https://svdquant.mit.edu Blog:
  https://hanlab.mit.edu/blog/svdquant</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diff-2-in-1: Bridging Generation and Dense Perception with <span class="highlight-title">Diffusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhong Zheng, Zhipeng Bao, Ruoyu Zhao, Martial Hebert, Yu-Xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beyond high-fidelity image synthesis, diffusion models have recently
exhibited promising results in dense visual perception tasks. However, most
existing work treats diffusion models as a standalone component for perception
tasks, employing them either solely for off-the-shelf data augmentation or as
mere feature extractors. In contrast to these isolated and thus sub-optimal
efforts, we introduce a unified, versatile, diffusion-based framework,
Diff-2-in-1, that can simultaneously handle both multi-modal data generation
and dense visual perception, through a unique exploitation of the
diffusion-denoising process. Within this framework, we further enhance
discriminative visual perception via multi-modal generation, by utilizing the
denoising network to create multi-modal data that mirror the distribution of
the original training set. Importantly, Diff-2-in-1 optimizes the utilization
of the created diverse and faithful data by leveraging a novel self-improving
learning mechanism. Comprehensive experimental evaluations validate the
effectiveness of our framework, showcasing consistent performance improvements
across various discriminative backbones and high-quality multi-modal data
generation characterized by both realism and usefulness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReCapture: Generative Video Camera Controls for User-Provided Videos
  using Masked Video Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Junhao Zhang, Roni Paiss, Shiran Zada, Nikhil Karnad, David E. Jacobs, Yael Pritch, Inbar Mosseri, Mike Zheng Shou, Neal Wadhwa, Nataniel Ruiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, breakthroughs in video modeling have allowed for controllable
camera trajectories in generated videos. However, these methods cannot be
directly applied to user-provided videos that are not generated by a video
model. In this paper, we present ReCapture, a method for generating new videos
with novel camera trajectories from a single user-provided video. Our method
allows us to re-generate the reference video, with all its existing scene
motion, from vastly different angles and with cinematic camera motion. Notably,
using our method we can also plausibly hallucinate parts of the scene that were
not observable in the reference video. Our method works by (1) generating a
noisy anchor video with a new camera trajectory using multiview diffusion
models or depth-based point cloud rendering and then (2) regenerating the
anchor video into a clean and temporally consistent reangled video using our
proposed masked video fine-tuning technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://generative-video-camera-controls.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing The Language of Visual Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David M. Chan, Rodolfo Corona, Joonyong Park, Cheol Jun Cho, Yutong Bai, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the introduction of transformer-based models for vision and language
tasks, such as LLaVA and Chameleon, there has been renewed interest in the
discrete tokenized representation of images. These models often treat image
patches as discrete tokens, analogous to words in natural language, learning
joint alignments between visual and human languages. However, little is known
about the statistical behavior of these visual languages - whether they follow
similar frequency distributions, grammatical structures, or topologies as
natural languages. In this paper, we take a natural-language-centric approach
to analyzing discrete visual languages and uncover striking similarities and
fundamental differences. We demonstrate that, although visual languages adhere
to Zipfian distributions, higher token innovation drives greater entropy and
lower compression, with tokens predominantly representing object parts,
indicating intermediate granularity. We also show that visual languages lack
cohesive grammatical structures, leading to higher perplexity and weaker
hierarchical organization compared to natural languages. Finally, we
demonstrate that, while vision models align more closely with natural languages
than other models, this alignment remains significantly weaker than the
cohesion found within natural languages. Through these experiments, we
demonstrate how understanding the statistical properties of discrete visual
languages can inform the design of more effective computer vision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiqi Liu, Zhanqiu Guo, Mohit Warke, Soumith Chintala, Chris Paxton, Nur Muhammad Mahi Shafiullah, Lerrel Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant progress has been made in open-vocabulary mobile manipulation,
where the goal is for a robot to perform tasks in any environment given a
natural language description. However, most current systems assume a static
environment, which limits the system's applicability in real-world scenarios
where environments frequently change due to human intervention or the robot's
own actions. In this work, we present DynaMem, a new approach to open-world
mobile manipulation that uses a dynamic spatio-semantic memory to represent a
robot's environment. DynaMem constructs a 3D data structure to maintain a
dynamic memory of point clouds, and answers open-vocabulary object localization
queries using multimodal LLMs or open-vocabulary features generated by
state-of-the-art vision-language models. Powered by DynaMem, our robots can
explore novel environments, search for objects not found in memory, and
continuously update the memory as objects move, appear, or disappear in the
scene. We run extensive experiments on the Stretch SE3 robots in three real and
nine offline scenes, and achieve an average pick-and-drop success rate of 70%
on non-stationary objects, which is more than a 2x improvement over
state-of-the-art static systems. Our code as well as our experiment and
deployment videos are open sourced and can be found on our project website:
https://dynamem.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://dynamem.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HourVideo: 1-Hour Video-Language Understanding <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keshigeyan Chandrasegaran, Agrim Gupta, Lea M. Hadzic, Taran Kota, Jimming He, Cristóbal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, Li Fei-Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HourVideo, a benchmark dataset for hour-long video-language
understanding. Our dataset consists of a novel task suite comprising
summarization, perception (recall, tracking), visual reasoning (spatial,
temporal, predictive, causal, counterfactual), and navigation (room-to-room,
object retrieval) tasks. HourVideo includes 500 manually curated egocentric
videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and
features 12,976 high-quality, five-way multiple-choice questions. Benchmarking
results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve
marginal improvements over random chance. In stark contrast, human experts
significantly outperform the state-of-the-art long-context multimodal model,
Gemini Pro 1.5 (85.0% vs. 37.3%), highlighting a substantial gap in multimodal
capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are
available at https://hourvideo.stanford.edu
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track; 28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoFi: Scalable Local Image Reconstruction with Implicit Neural
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        AmirEhsan Khorashadizadeh, Tobías I. Liaudat, Tianlin Liu, Jason D. McEwen, Ivan Dokmanić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural fields or implicit neural representations (INRs) have attracted
significant attention in machine learning and signal processing due to their
efficient continuous representation of images and 3D volumes. In this work, we
build on INRs and introduce a coordinate-based local processing framework for
solving imaging inverse problems, termed LoFi (Local Field). Unlike
conventional methods for image reconstruction, LoFi processes local information
at each coordinate \textit{separately} by multi-layer perceptrons (MLPs),
recovering the object at that specific coordinate. Similar to INRs, LoFi can
recover images at any continuous coordinate, enabling image reconstruction at
multiple resolutions. With comparable or better performance than standard CNNs
for image reconstruction, LoFi achieves excellent generalization to
out-of-distribution data and memory usage almost independent of image
resolution. Remarkably, training on $1024 \times 1024$ images requires just 3GB
of memory -- over 20 times less than the memory typically needed by standard
CNNs. Additionally, LoFi's local design allows it to train on extremely small
datasets with less than 10 samples, without overfitting or the need for
regularization or early stopping. Finally, we use LoFi as a denoising prior in
a plug-and-play framework for solving general inverse problems to benefit from
its continuous image representation and strong generalization. Although trained
on low-resolution images, LoFi can be used as a low-dimensional prior to solve
inverse problems at any resolution. We validate our framework across a variety
of imaging modalities, from low-dose computed tomography to radio
interferometric imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Which bits went where? Past and future transfer entropy decomposition
  with the information bottleneck <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kieran A. Murphy, Zhuowen Yin, Dani S. Bassett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whether the system under study is a shoal of fish, a collection of neurons,
or a set of interacting atmospheric and oceanic processes, transfer entropy
measures the flow of information between time series and can detect possible
causal relationships. Much like mutual information, transfer entropy is
generally reported as a single value summarizing an amount of shared variation,
yet a more fine-grained accounting might illuminate much about the processes
under study. Here we propose to decompose transfer entropy and localize the
bits of variation on both sides of information flow: that of the originating
process's past and that of the receiving process's future. We employ the
information bottleneck (IB) to compress the time series and identify the
transferred entropy. We apply our method to decompose the transfer entropy in
several synthetic recurrent processes and an experimental mouse dataset of
concurrent behavioral and neural activity. Our approach highlights the nuanced
dynamics within information flow, laying a foundation for future explorations
into the intricate interplay of temporal processes in complex systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 workshop "Machine learning and the physical sciences"
  Camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering in Causal Attention Masking <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Karagodin, Yury Polyanskiy, Philippe Rigollet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a modification of the self-attention dynamics proposed by
Geshkovski et al. (arXiv:2312.10794) to better reflect the practically
relevant, causally masked attention used in transformer architectures for
generative AI. This modification translates into an interacting particle system
that cannot be interpreted as a mean-field gradient flow. Despite this loss of
structure, we significantly strengthen the results of Geshkovski et al.
(arXiv:2312.10794) in this context: While previous rigorous results focused on
cases where all three matrices (Key, Query, and Value) were scaled identities,
we prove asymptotic convergence to a single cluster for arbitrary key-query
matrices and a value matrix equal to the identity. Additionally, we establish a
connection to the classical R\'enyi parking problem from combinatorial geometry
to make initial theoretical steps towards demonstrating the existence of
meta-stable states.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024), 22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, David B. Lindell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods for image-to-video generation have achieved impressive,
photo-realistic quality. However, adjusting specific elements in generated
videos, such as object motion or camera movement, is often a tedious process of
trial and error, e.g., involving re-generating videos with different random
seeds. Recent techniques address this issue by fine-tuning a pre-trained model
to follow conditioning signals, such as bounding boxes or point trajectories.
Yet, this fine-tuning procedure can be computationally expensive, and it
requires datasets with annotated object motion, which can be difficult to
procure. In this work, we introduce SG-I2V, a framework for controllable
image-to-video generation that is self-guided$\unicode{x2013}$offering
zero-shot control by relying solely on the knowledge present in a pre-trained
image-to-video diffusion model without the need for fine-tuning or external
knowledge. Our zero-shot method outperforms unsupervised baselines while being
competitive with supervised models in terms of visual quality and motion
fidelity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://kmcode1.github.io/Projects/SG-I2V/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Task Learning through Inverse Generative Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aviv Netanyahu, Yilun Du, Antonia Bronars, Jyothish Pari, Joshua Tenenbaum, Tianmin Shu, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning the intents of an agent, defined by its goals or motion style, is
often extremely challenging from just a few examples. We refer to this problem
as task concept learning and present our approach, Few-Shot Task Learning
through Inverse Generative Modeling (FTL-IGM), which learns new task concepts
by leveraging invertible neural generative models. The core idea is to pretrain
a generative model on a set of basic concepts and their demonstrations. Then,
given a few demonstrations of a new concept (such as a new goal or a new
action), our method learns the underlying concepts through backpropagation
without updating the model weights, thanks to the invertibility of the
generative model. We evaluate our method in five domains -- object
rearrangement, goal-oriented navigation, motion caption of human actions,
autonomous driving, and real-world table-top manipulation. Our experimental
results demonstrate that via the pretrained generative model, we successfully
learn novel concepts and generate agent plans or motion corresponding to these
concepts in (1) unseen environments and (2) in composition with training
concepts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noisy Zero-Shot Coordination: Breaking The Common Knowledge Assumption
  In Zero-Shot Coordination Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usman Anwar, Ashish Pandian, Jia Wan, David Krueger, Jakob Foerster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot coordination (ZSC) is a popular setting for studying the ability of
reinforcement learning (RL) agents to coordinate with novel partners. Prior ZSC
formulations assume the $\textit{problem setting}$ is common knowledge: each
agent knows the underlying Dec-POMDP, knows others have this knowledge, and so
on ad infinitum. However, this assumption rarely holds in complex real-world
settings, which are often difficult to fully and correctly specify. Hence, in
settings where this common knowledge assumption is invalid, agents trained
using ZSC methods may not be able to coordinate well. To address this
limitation, we formulate the $\textit{noisy zero-shot coordination}$ (NZSC)
problem. In NZSC, agents observe different noisy versions of the ground truth
Dec-POMDP, which are assumed to be distributed according to a fixed noise
model. Only the distribution of ground truth Dec-POMDPs and the noise model are
common knowledge. We show that a NZSC problem can be reduced to a ZSC problem
by designing a meta-Dec-POMDP with an augmented state space consisting of all
the ground-truth Dec-POMDPs. For solving NZSC problems, we propose a simple and
flexible meta-learning method called NZSC training, in which the agents are
trained across a distribution of coordination problems - which they only get to
observe noisy versions of. We show that with NZSC training, RL agents can be
trained to coordinate well with novel partners even when the (exact) problem
setting of the coordination is not common knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuffixDecoding: A Model-Free Approach to Speeding Up Large Language
  Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Oliaro, Zhihao Jia, Daniel Campos, Aurick Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SuffixDecoding, a novel model-free approach to accelerating large
language model (LLM) inference through speculative decoding. Unlike existing
methods that rely on draft models or specialized decoding heads, SuffixDecoding
leverages suffix trees built from previously generated outputs to efficiently
predict candidate token sequences. Our approach enables flexible
tree-structured speculation without the overhead of maintaining and
orchestrating additional models. SuffixDecoding builds and dynamically updates
suffix trees to capture patterns in the generated text, using them to construct
speculation trees through a principled scoring mechanism based on empirical
token frequencies. SuffixDecoding requires only CPU memory which is plentiful
and underutilized on typical LLM serving nodes. We demonstrate that
SuffixDecoding achieves competitive speedups compared to model-based approaches
across diverse workloads including open-domain chat, code generation, and
text-to-SQL tasks. For open-ended chat and code generation tasks,
SuffixDecoding achieves up to $1.4\times$ higher output throughput than
SpecInfer and up to $1.1\times$ lower time-per-token (TPOT) latency. For a
proprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to
$2.9\times$ higher output throughput and $3\times$ lower latency than
speculative decoding. Our evaluation shows that SuffixDecoding maintains high
acceptance rates even with small reference corpora of 256 examples, while
continuing to improve performance as more historical outputs are incorporated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AsCAN: Asymmetric Convolution-Attention Networks for Efficient
  Recognition and Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Kag, Huseyin Coskun, Jierun Chen, Junli Cao, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, Jian Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network architecture design requires making many crucial decisions.
The common desiderata is that similar decisions, with little modifications, can
be reused in a variety of tasks and applications. To satisfy that,
architectures must provide promising latency and performance trade-offs,
support a variety of tasks, scale efficiently with respect to the amounts of
data and compute, leverage available data from other tasks, and efficiently
support various hardware. To this end, we introduce AsCAN -- a hybrid
architecture, combining both convolutional and transformer blocks. We revisit
the key design principles of hybrid architectures and propose a simple and
effective \emph{asymmetric} architecture, where the distribution of
convolutional and transformer blocks is \emph{asymmetric}, containing more
convolutional blocks in the earlier stages, followed by more transformer blocks
in later stages. AsCAN supports a variety of tasks: recognition, segmentation,
class-conditional image generation, and features a superior trade-off between
performance and latency. We then scale the same architecture to solve a
large-scale text-to-image task and show state-of-the-art performance compared
to the most recent public and commercial models. Notably, even without any
computation optimization for transformer blocks, our models still yield faster
inference speed than existing works featuring efficient attention mechanisms,
highlighting the advantages and the value of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Project Page:
  https://snap-research.github.io/snap_image/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BitNet a4.8: 4-bit Activations for 1-bit LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Wang, Shuming Ma, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on the 1-bit Large Language Models (LLMs), such as BitNet
b1.58, presents a promising direction for reducing the inference cost of LLMs
while maintaining their performance. In this work, we introduce BitNet a4.8,
enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid
quantization and sparsification strategy to mitigate the quantization errors
introduced by the outlier channels. Specifically, we utilize 4-bit activations
for inputs to the attention and feed-forward network layers, while sparsifying
intermediate states followed with 8-bit quantization. Extensive experiments
demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58
with equivalent training costs, while being faster in inference with enabling
4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of
parameters and supports 3-bit KV cache, further enhancing the efficiency of
large-scale LLM deployment and inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPGD: Steepest Perturbed Gradient Descent Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir M. Vahedi, Horea T. Ilies
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization algorithms are pivotal in advancing various scientific and
industrial fields but often encounter obstacles such as trapping in local
minima, saddle points, and plateaus (flat regions), which makes the convergence
to reasonable or near-optimal solutions particularly challenging. This paper
presents the Steepest Perturbed Gradient Descent (SPGD), a novel algorithm that
innovatively combines the principles of the gradient descent method with
periodic uniform perturbation sampling to effectively circumvent these
impediments and lead to better solutions whenever possible. SPGD is
distinctively designed to generate a set of candidate solutions and select the
one exhibiting the steepest loss difference relative to the current solution.
It enhances the traditional gradient descent approach by integrating a
strategic exploration mechanism that significantly increases the likelihood of
escaping sub-optimal local minima and navigating complex optimization
landscapes effectively. Our approach not only retains the directed efficiency
of gradient descent but also leverages the exploratory benefits of stochastic
perturbations, thus enabling a more comprehensive search for global optima
across diverse problem spaces. We demonstrate the efficacy of SPGD in solving
the 3D component packing problem, an NP-hard challenge. Preliminary results
show a substantial improvement over four established methods, particularly on
response surfaces with complex topographies and in multidimensional non-convex
continuous optimization problems. Comparative analyses with established 2D
benchmark functions highlight SPGD's superior performance, showcasing its
ability to navigate complex optimization landscapes. These results emphasize
SPGD's potential as a versatile tool for a wide range of optimization problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 26 figures, submitted to Journal of Mechanical Design</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pareto Set Identification With Posterior Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cyrille Kone, Marc Jourdan, Emilie Kaufmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of identifying the best answer among a collection of items having
real-valued distribution is well-understood.
  Despite its practical relevance for many applications, fewer works have
studied its extension when multiple and potentially conflicting metrics are
available to assess an item's quality.
  Pareto set identification (PSI) aims to identify the set of answers whose
means are not uniformly worse than another.
  This paper studies PSI in the transductive linear setting with potentially
correlated objectives.
  Building on posterior sampling in both the stopping and the sampling rules,
we propose the PSIPS algorithm that deals simultaneously with structure and
correlation without paying the computational cost of existing oracle-based
algorithms.
  Both from a frequentist and Bayesian perspective, PSIPS is asymptotically
optimal.
  We demonstrate its good empirical performance in real-world and synthetic
instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fed-LDR: Federated Local Data-infused Graph Creation with Node-centric
  Model Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiechao Gao, Yuangang Li, Syeda Faiza Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid acceleration of global urbanization has introduced novel challenges
in enhancing urban infrastructure and services. Spatio-temporal data,
integrating spatial and temporal dimensions, has emerged as a critical tool for
understanding urban phenomena and promoting sustainability. In this context,
Federated Learning (FL) has gained prominence as a distributed learning
paradigm aligned with the privacy requirements of urban IoT environments.
However, integrating traditional and deep learning models into the FL framework
poses significant challenges, particularly in capturing complex spatio-temporal
dependencies and adapting to diverse urban conditions. To address these
challenges, we propose the Federated Local Data-Infused Graph Creation with
Node-centric Model Refinement (Fed-LDR) algorithm. Fed-LDR leverages FL and
Graph Convolutional Networks (GCN) to enhance spatio-temporal data analysis in
urban environments. The algorithm comprises two key modules: (1) the Local
Data-Infused Graph Creation (LDIGC) module, which dynamically reconfigures
adjacency matrices to reflect evolving spatial relationships within urban
environments, and (2) the Node-centric Model Refinement (NoMoR) module, which
customizes model parameters for individual urban nodes to accommodate
heterogeneity. Evaluations on the PeMSD4 and PeMSD8 datasets demonstrate
Fed-LDR's superior performance over six baseline methods. Fed-LDR achieved the
lowest Mean Absolute Error (MAE) values of 20.15 and 17.30, and the lowest Root
Mean Square Error (RMSE) values of 32.30 and 27.15, respectively, while
maintaining a high correlation coefficient of 0.96 across both datasets.
Notably, on the PeMSD4 dataset, Fed-LDR reduced MAE and RMSE by up to 81\% and
78\%, respectively, compared to the best-performing baseline FedMedian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Robustness of Reinforcement Learning Algorithms for
  Autonomous Shipping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bavo Lesy, Ali Anwar, Siegfried Mercelis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been growing interest in autonomous shipping due to its
potential to improve maritime efficiency and safety. The use of advanced
technologies, such as artificial intelligence, can address the current
navigational and operational challenges in autonomous shipping. In particular,
inland waterway transport (IWT) presents a unique set of challenges, such as
crowded waterways and variable environmental conditions. In such dynamic
settings, the reliability and robustness of autonomous shipping solutions are
critical factors for ensuring safe operations. This paper examines the
robustness of benchmark deep reinforcement learning (RL) algorithms,
implemented for IWT within an autonomous shipping simulator, and their ability
to generate effective motion planning policies. We demonstrate that a
model-free approach can achieve an adequate policy in the simulator,
successfully navigating port environments never encountered during training. We
focus particularly on Soft-Actor Critic (SAC), which we show to be inherently
more robust to environmental disturbances compared to MuZero, a
state-of-the-art model-based RL algorithm. In this paper, we take a significant
step towards developing robust, applied RL frameworks that can be generalized
to various vessel types and navigate complex port- and inland environments and
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures. Will be presented at IEEE RAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure Matters: Dynamic Policy Gradient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Klein, Xiangyuan Zhang, Tamer Başar, Simon Weissmann, Leif Döring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study $\gamma$-discounted infinite-horizon tabular Markov
decision processes (MDPs) and introduce a framework called dynamic policy
gradient (DynPG). The framework directly integrates dynamic programming with
(any) policy gradient method, explicitly leveraging the Markovian property of
the environment. DynPG dynamically adjusts the problem horizon during training,
decomposing the original infinite-horizon MDP into a sequence of contextual
bandit problems. By iteratively solving these contextual bandits, DynPG
converges to the stationary optimal policy of the infinite-horizon MDP. To
demonstrate the power of DynPG, we establish its non-asymptotic global
convergence rate under the tabular softmax parametrization, focusing on the
dependencies on salient but essential parameters of the MDP. By combining
classical arguments from dynamic programming with more recent convergence
arguments of policy gradient schemes, we prove that softmax DynPG scales
polynomially in the effective horizon $(1-\gamma)^{-1}$. Our findings contrast
recent exponential lower bound examples for vanilla policy gradient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Missing Data Imputation through Combined Bipartite Graph and
  Complete Directed Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Zhang, Hongtu Zhu, Ziqi Chen, Yingjie Zhang, Hai Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we aim to address a significant challenge in the field of
missing data imputation: identifying and leveraging the interdependencies among
features to enhance missing data imputation for tabular data. We introduce a
novel framework named the Bipartite and Complete Directed Graph Neural Network
(BCGNN). Within BCGNN, observations and features are differentiated as two
distinct node types, and the values of observed features are converted into
attributed edges linking them. The bipartite segment of our framework
inductively learns embedding representations for nodes, efficiently utilizing
the comprehensive information encapsulated in the attributed edges. In
parallel, the complete directed graph segment adeptly outlines and communicates
the complex interdependencies among features. When compared to contemporary
leading imputation methodologies, BCGNN consistently outperforms them,
achieving a noteworthy average reduction of 15% in mean absolute error for
feature imputation tasks under different missing mechanisms. Our extensive
experimental investigation confirms that an in-depth grasp of the
interdependence structure substantially enhances the model's feature embedding
ability. We also highlight the model's superior performance in label prediction
tasks involving missing data, and its formidable ability to generalize to
unseen data points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sampling-guided Heterogeneous Graph Neural Network with Temporal
  Smoothing for Scalable Longitudinal Data Imputation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Zhang, Ziqi Chen, Qiao Liu, Jinhan Xie, Hongtu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel framework, the Sampling-guided
Heterogeneous Graph Neural Network (SHT-GNN), to effectively tackle the
challenge of missing data imputation in longitudinal studies. Unlike
traditional methods, which often require extensive preprocessing to handle
irregular or inconsistent missing data, our approach accommodates arbitrary
missing data patterns while maintaining computational efficiency. SHT-GNN
models both observations and covariates as distinct node types, connecting
observation nodes at successive time points through subject-specific
longitudinal subnetworks, while covariate-observation interactions are
represented by attributed edges within bipartite graphs. By leveraging
subject-wise mini-batch sampling and a multi-layer temporal smoothing
mechanism, SHT-GNN efficiently scales to large datasets, while effectively
learning node representations and imputing missing data. Extensive experiments
on both synthetic and real-world datasets, including the Alzheimer's Disease
Neuroimaging Initiative (ADNI) dataset, demonstrate that SHT-GNN significantly
outperforms existing imputation methods, even with high missing data rates. The
empirical results highlight SHT-GNN's robust imputation capabilities and
superior performance, particularly in the context of complex, large-scale
longitudinal data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Euclidean Mixture Model for Social Network Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roshni G. Iyer, Yewen Wang, Wei Wang, Yizhou Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is largely agreed that social network links are formed due to either
homophily or social influence. Inspired by this, we aim at understanding the
generation of links via providing a novel embedding-based graph formation
model. Different from existing graph representation learning, where link
generation probabilities are defined as a simple function of the corresponding
node embeddings, we model the link generation as a mixture model of the two
factors. In addition, we model the homophily factor in spherical space and the
influence factor in hyperbolic space to accommodate the fact that (1) homophily
results in cycles and (2) influence results in hierarchies in networks. We also
design a special projection to align these two spaces. We call this model
Non-Euclidean Mixture Model, i.e., NMM. We further integrate NMM with our
non-Euclidean graph variational autoencoder (VAE) framework, NMM-GNN. NMM-GNN
learns embeddings through a unified framework which uses non-Euclidean GNN
encoders, non-Euclidean Gaussian priors, a non-Euclidean decoder, and a novel
space unification loss component to unify distinct non-Euclidean geometric
spaces. Experiments on public datasets show NMM-GNN significantly outperforms
state-of-the-art baselines on social network generation and classification
tasks, demonstrating its ability to better explain how the social network is
formed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think Smart, Act SMARL! Analyzing Probabilistic Logic Driven Safety in
  Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satchit Chatterji, Erman Acar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important challenge for enabling the deployment of reinforcement learning
(RL) algorithms in the real world is safety. This has resulted in the recent
research field of Safe RL, which aims to learn optimal policies that are safe.
One successful approach in that direction is probabilistic logic shields (PLS),
a model-based Safe RL technique that uses formal specifications based on
probabilistic logic programming, constraining an agent's policy to comply with
those specifications in a probabilistic sense. However, safety is inherently a
multi-agent concept, since real-world environments often involve multiple
agents interacting simultaneously, leading to a complex system which is hard to
control. Moreover, safe multi-agent RL (Safe MARL) is still underexplored. In
order to address this gap, in this paper we ($i$) introduce Shielded MARL
(SMARL) by extending PLS to MARL -- in particular, we introduce Probabilistic
Logic Temporal Difference Learning (PLTD) to enable shielded independent
Q-learning (SIQL), and introduce shielded independent PPO (SIPPO) using
probabilistic logic policy gradients; ($ii$) show its positive effect and use
as an equilibrium selection mechanism in various game-theoretic environments
including two-player simultaneous games, extensive-form games, stochastic
games, and some grid-world extensions in terms of safety, cooperation, and
alignment with normative behaviors; and ($iii$) look into the asymmetric case
where only one agent is shielded, and show that the shielded agent has a
significant influence on the unshielded one, providing further evidence of
SMARL's ability to enhance safety and cooperation in diverse multi-agent
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OneProt: Towards Multi-Modal Protein Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Klemens Flöge, Srisruthi Udayakumar, Johanna Sommer, Marie Piraud, Stefan Kesselheim, Vincent Fortuin, Stephan Günneman, Karel J van der Weg, Holger Gohlke, Alina Bazarova, Erinc Merdivan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent AI advances have enabled multi-modal systems to model and translate
diverse information spaces. Extending beyond text and vision, we introduce
OneProt, a multi-modal AI for proteins that integrates structural, sequence,
alignment, and binding site data. Using the ImageBind framework, OneProt aligns
the latent spaces of modality encoders along protein sequences. It demonstrates
strong performance in retrieval tasks and surpasses state-of-the-art methods in
various downstream tasks, including metal ion binding classification,
gene-ontology annotation, and enzyme function prediction. This work expands
multi-modal capabilities in protein models, paving the way for applications in
drug discovery, biocatalytic reaction planning, and protein engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 15 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clinicians' Voice: Fundamental Considerations for XAI in Healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. E. Röber, R. Goedhart, S. İ. Birbil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable AI (XAI) holds the promise of advancing the implementation and
adoption of AI-based tools in practice, especially in high-stakes environments
like healthcare. However, most of the current research is disconnected from its
practical applications and lacks input of end users. To address this, we
conducted semi-structured interviews with clinicians to discuss their thoughts,
hopes, and concerns. We find that clinicians generally think positively about
developing AI-based tools for clinical practice, but they have concerns about
how these will fit into their workflow and how it will impact clinician-patient
relations. We further identify education of clinicians on AI as a crucial
factor for the success of AI in healthcare and highlight aspects clinicians are
looking for in (X)AI-based tools. In contrast to other studies, we take on a
holistic and exploratory perspective to identify general requirements, which is
necessary before moving on to testing specific (X)AI products for healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformalized Credal Regions for Classification with Ambiguous Ground
  Truth 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Caprio, David Stutz, Shuo Li, Arnaud Doucet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An open question in \emph{Imprecise Probabilistic Machine Learning} is how to
empirically derive a credal region (i.e., a closed and convex family of
probabilities on the output space) from the available data, without any prior
knowledge or assumption. In classification problems, credal regions are a tool
that is able to provide provable guarantees under realistic assumptions by
characterizing the uncertainty about the distribution of the labels. Building
on previous work, we show that credal regions can be directly constructed using
conformal methods. This allows us to provide a novel extension of classical
conformal prediction to problems with ambiguous ground truth, that is, when the
exact labels for given inputs are not exactly known. The resulting construction
enjoys desirable practical and theoretical properties: (i) conformal coverage
guarantees, (ii) smaller prediction sets (compared to classical conformal
prediction regions) and (iii) disentanglement of uncertainty sources
(epistemic, aleatoric). We empirically verify our findings on both synthetic
and real datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asymptotic regularity of a generalised stochastic Halpern scheme with
  applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Pischke, Thomas Powell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide abstract, general and highly uniform rates of asymptotic
regularity for a generalized stochastic Halpern-style iteration, which
incorporates a second mapping in the style of a Krasnoselskii-Mann iteration.
This iteration is general in two ways: First, it incorporates stochasticity in
a completely abstract way rather than fixing a sampling method; secondly, it
includes as special cases stochastic versions of various schemes from the
optimization literature, including Halpern's iteration as well as a
Krasnoselskii-Mann iteration with Tikhonov regularization terms in the sense of
Bo\c{t}, Csetnek and Meier. For these particular cases, we in particular obtain
linear rates of asymptotic regularity, matching (or improving) the currently
best known rates for these iterations in stochastic optimization, and quadratic
rates of asymptotic regularity are obtained in the context of inner product
spaces for the general iteration. We utilize these rates to give bounds on the
oracle complexity of such iterations under suitable variance assumptions and
batching strategies, again presented in an abstract style. Finally, we sketch
how the schemes presented here can be instantiated in the context of
reinforcement learning to yield novel methods for Q-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning in Budgeted Auctions with Spacing Objectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giannis Fikioris, Robert Kleinberg, Yoav Kolumbus, Raunak Kumar, Yishay Mansour, Éva Tardos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many repeated auction settings, participants care not only about how
frequently they win but also how their winnings are distributed over time. This
problem arises in various practical domains where avoiding congested demand is
crucial, such as online retail sales and compute services, as well as in
advertising campaigns that require sustained visibility over time. We introduce
a simple model of this phenomenon, modeling it as a budgeted auction where the
value of a win is a concave function of the time since the last win. This
implies that for a given number of wins, even spacing over time is optimal. We
also extend our model and results to the case when not all wins result in
"conversions" (realization of actual gains), and the probability of conversion
depends on a context. The goal is to maximize and evenly space conversions
rather than just wins.
  We study the optimal policies for this setting in second-price auctions and
offer learning algorithms for the bidders that achieve low regret against the
optimal bidding policy in a Bayesian online setting. Our main result is a
computationally efficient online learning algorithm that achieves $\tilde
O(\sqrt T)$ regret. We achieve this by showing that an infinite-horizon Markov
decision process (MDP) with the budget constraint in expectation is essentially
equivalent to our problem, even when limiting that MDP to a very small number
of states. The algorithm achieves low regret by learning a bidding policy that
chooses bids as a function of the context and the system's state, which will be
the time elapsed since the last win (or conversion). We show that
state-independent strategies incur linear regret even without uncertainty of
conversions. We complement this by showing that there are state-independent
strategies that, while still having linear regret, achieve a $(1-\frac 1 e)$
approximation to the optimal reward.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine learning and optimization-based approaches to duality in
  statistical physics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea E. V. Ferrari, Prateek Gupta, Nabil Iqbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The notion of duality -- that a given physical system can have two different
mathematical descriptions -- is a key idea in modern theoretical physics.
Establishing a duality in lattice statistical mechanics models requires the
construction of a dual Hamiltonian and a map from the original to the dual
observables. By using simple neural networks to parameterize these maps and
introducing a loss function that penalises the difference between correlation
functions in original and dual models, we formulate the process of duality
discovery as an optimization problem. We numerically solve this problem and
show that our framework can rediscover the celebrated Kramers-Wannier duality
for the 2d Ising model, reconstructing the known mapping of temperatures. We
also discuss an alternative approach which uses known features of the mapping
of topological lines to reduce the problem to optimizing the couplings in a
dual Hamiltonian, and explore next-to-nearest neighbour deformations of the 2d
Ising duality. We discuss future directions and prospects for discovering new
dualities within this framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages + appendices, lots of plots</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plasticity Loss in Deep Reinforcement Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Klein, Lukas Miklautz, Kevin Sidak, Claudia Plant, Sebastian Tschiatschek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Akin to neuroplasticity in human brains, the plasticity of deep neural
networks enables their quick adaption to new data. This makes plasticity
particularly crucial for deep Reinforcement Learning (RL) agents: Once
plasticity is lost, an agent's performance will inevitably plateau because it
cannot improve its policy to account for changes in the data distribution,
which are a necessary consequence of its learning process. Thus, developing
well-performing and sample-efficient agents hinges on their ability to remain
plastic during training. Furthermore, the loss of plasticity can be connected
to many other issues plaguing deep RL, such as training instabilities, scaling
failures, overestimation bias, and insufficient exploration. With this survey,
we aim to provide an overview of the emerging research on plasticity loss for
academics and practitioners of deep reinforcement learning. First, we propose a
unified definition of plasticity loss based on recent works, relate it to
definitions from the literature, and discuss metrics for measuring plasticity
loss. Then, we categorize and discuss numerous possible causes of plasticity
loss before reviewing currently employed mitigation strategies. Our taxonomy is
the first systematic overview of the current state of the field. Lastly, we
discuss prevalent issues within the literature, such as a necessity for broader
evaluation, and provide recommendations for future research, like gaining a
better understanding of an agent's neural activity and behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D$^3$epth: Self-Supervised Depth Estimation with Dynamic Mask in Dynamic
  Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Chen, Hong Liu, Wenhao Li, Ying Zhu, Guoquan Wang, Jianbing Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation is a crucial technology in robotics. Recently,
self-supervised depth estimation methods have demonstrated great potential as
they can efficiently leverage large amounts of unlabelled real-world data.
However, most existing methods are designed under the assumption of static
scenes, which hinders their adaptability in dynamic environments. To address
this issue, we present D$^3$epth, a novel method for self-supervised depth
estimation in dynamic scenes. It tackles the challenge of dynamic objects from
two key perspectives. First, within the self-supervised framework, we design a
reprojection constraint to identify regions likely to contain dynamic objects,
allowing the construction of a dynamic mask that mitigates their impact at the
loss level. Second, for multi-frame depth estimation, we introduce a cost
volume auto-masking strategy that leverages adjacent frames to identify regions
associated with dynamic objects and generate corresponding masks. This provides
guidance for subsequent processes. Furthermore, we propose a spectral entropy
uncertainty module that incorporates spectral entropy to guide uncertainty
estimation during depth fusion, effectively addressing issues arising from cost
volume computation in dynamic environments. Extensive experiments on KITTI and
Cityscapes datasets demonstrate that the proposed method consistently
outperforms existing self-supervised monocular depth estimation baselines. Code
is available at \url{https://github.com/Csyunling/D3epth}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Open sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VTechAGP: An Academic-to-General-Audience Text Paraphrase <span class="highlight-title">Dataset</span> and
  Benchmark Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Cheng, Jiaying Gong, Chenhan Yuan, William A. Ingram, Edward Fox, Hoda Eldardiry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing text simplification or paraphrase datasets mainly focus on
sentence-level text generation in a general domain. These datasets are
typically developed without using domain knowledge. In this paper, we release a
novel dataset, VTechAGP, which is the first academic-to-general-audience text
paraphrase dataset consisting of 4,938 document-level these and dissertation
academic and general-audience abstract pairs from 8 colleges authored over 25
years. We also propose a novel dynamic soft prompt generative language model,
DSPT5. For training, we leverage a contrastive-generative loss function to
learn the keyword vectors in the dynamic prompt. For inference, we adopt a
crowd-sampling decoding strategy at both semantic and structural levels to
further select the best output candidate. We evaluate DSPT5 and various
state-of-the-art large language models (LLMs) from multiple perspectives.
Results demonstrate that the SOTA LLMs does not provide satisfactory outcomes,
while the lightweight DSPT5 can achieve competitive results. To the best of our
knowledge, we are the first to build a benchmark dataset and solutions for
academic-to-general-audience text paraphrase dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Packing Algorithm for Optimized Mapping of Artificial Neural
  Networks onto Non-Volatile Memory Cross-Bar Arrays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        W. Haensch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuromorphic computing with crossbar arrays has emerged as a promising
alternative to improve computing efficiency for machine learning. Previous work
has focused on implementing crossbar arrays to perform basic mathematical
operations. However, in this paper, we explore the impact of mapping the layers
of an artificial neural network onto physical cross-bar arrays arranged in
tiles across a chip. We have developed a simplified mapping algorithm to
determine the number of physical tiles, with fixed optimal array dimensions,
and to estimate the minimum area occupied by these tiles for a given design
objective. This simplified algorithm is compared with conventional binary
linear optimization, which solves the equivalent bin-packing problem. We have
found that the optimum solution is not necessarily related to the minimum
number of tiles; rather, it is shown to be an interaction between tile array
capacity and the scaling properties of its peripheral circuits. Additionally,
we have discovered that square arrays are not always the best choice for
optimal mapping, and that performance optimization comes at the cost of total
tile area
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Hoeffding Tree: A Transparent and Differentiable Model on Data
  Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirsten Köbschall, Lisa Hartung, Stefan Kramer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose soft Hoeffding trees (SoHoT) as a new differentiable and
transparent model for possibly infinite and changing data streams. Stream
mining algorithms such as Hoeffding trees grow based on the incoming data
stream, but they currently lack the adaptability of end-to-end deep learning
systems. End-to-end learning can be desirable if a feature representation is
learned by a neural network and used in a tree, or if the outputs of trees are
further processed in a deep learning model or workflow. Different from
Hoeffding trees, soft trees can be integrated into such systems due to their
differentiability, but are neither transparent nor explainable. Our novel model
combines the extensibility and transparency of Hoeffding trees with the
differentiability of soft trees. We introduce a new gating function to regulate
the balance between univariate and multivariate splits in the tree. Experiments
are performed on 20 data streams, comparing SoHoT to standard Hoeffding trees,
Hoeffding trees with limited complexity, and soft trees applying a sparse
activation function for sample routing. The results show that soft Hoeffding
trees outperform Hoeffding trees in estimating class probabilities and, at the
same time, maintain transparency compared to soft trees, with relatively small
losses in terms of AUROC and cross-entropy. We also demonstrate how to trade
off transparency against performance using a hyperparameter, obtaining
univariate splits at one end of the spectrum and multivariate splits at the
other.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defending Deep Regression Models against Backdoor Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingyu Du, Yupei Liu, Jinyuan Jia, Guohao Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep regression models are used in a wide variety of safety-critical
applications, but are vulnerable to backdoor attacks. Although many defenses
have been proposed for classification models, they are ineffective as they do
not consider the uniqueness of regression models. First, the outputs of
regression models are continuous values instead of discretized labels. Thus,
the potential infected target of a backdoored regression model has infinite
possibilities, which makes it impossible to be determined by existing defenses.
Second, the backdoor behavior of backdoored deep regression models is triggered
by the activation values of all the neurons in the feature space, which makes
it difficult to be detected and mitigated using existing defenses. To resolve
these problems, we propose DRMGuard, the first defense to identify if a deep
regression model in the image domain is backdoored or not. DRMGuard formulates
the optimization problem for reverse engineering based on the unique
output-space and feature-space characteristics of backdoored deep regression
models. We conduct extensive evaluations on two regression tasks and four
datasets. The results show that DRMGuard can consistently defend against
various backdoor attacks. We also generalize four state-of-the-art defenses
designed for classifiers to regression models, and compare DRMGuard with them.
The results show that DRMGuard significantly outperforms all those defenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlignXIE: Improving Multilingual Information Extraction by Cross-Lingual
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zuo, Wenxuan Jiang, Wenxuan Liu, Zixuan Li, Long Bai, Hanbin Wang, Yutao Zeng, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empirical evidence suggests that LLMs exhibit spontaneous cross-lingual
alignment. Our findings suggest that although LLMs also demonstrate promising
cross-lingual alignment in Information Extraction, there remains significant
imbalance across languages, revealing an underlying deficiency in the IE
alignment. To address this issue, we propose AlignXIE, a powerful code-based
LLM that significantly enhances cross-lingual IE alignment through two
strategies. Firstly, AlignXIE formulates IE across different languages,
especially non-English ones, as code generation tasks, standardizing the
representation of various schemas using Python classes to ensure consistency of
the same ontology in different languages and align the schema. Secondly, it
incorporates an IE cross-lingual alignment phase through a translated instance
prediction task proposed in this paper to align the extraction process,
utilizing ParallelNER, an IE bilingual parallel dataset with 257,190 samples,
generated by our proposed LLM-based automatic pipeline for IE parallel data
construction, with manual annotation to ensure quality. Ultimately, we obtain
AlignXIE through multilingual IE instruction tuning. Although without training
in 9 unseen languages, AlignXIE surpasses ChatGPT by $30.17\%$ and SoTA by
$20.03\%$, thereby demonstrating superior cross-lingual IE capabilities.
Comprehensive evaluations on 63 IE benchmarks in Chinese and English under
various settings, demonstrate that AlignXIE significantly enhances
cross-lingual and multilingual IE through boosting the IE alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in
  Financial Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuewen Han, Neng Wang, Shangkun Che, Hongyang Yang, Kunpeng Zhang, Sean Xin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the application of generative artificial intelligence
(GenAI) in financial analysis and investment decision-making has gained
significant attention. However, most existing approaches rely on single-agent
systems, which fail to fully utilize the collaborative potential of multiple AI
agents. In this paper, we propose a novel multi-agent collaboration system
designed to enhance decision-making in financial investment research. The
system incorporates agent groups with both configurable group sizes and
collaboration structures to leverage the strengths of each agent group type. By
utilizing a sub-optimal combination strategy, the system dynamically adapts to
varying market conditions and investment scenarios, optimizing performance
across different tasks. We focus on three sub-tasks: fundamentals, market
sentiment, and risk analysis, by analyzing the 2023 SEC 10-K forms of 30
companies listed on the Dow Jones Index. Our findings reveal significant
performance variations based on the configurations of AI agents for different
tasks. The results demonstrate that our multi-agent collaboration system
outperforms traditional single-agent models, offering improved accuracy,
efficiency, and adaptability in complex financial environments. This study
highlights the potential of multi-agent systems in transforming financial
analysis and investment decision-making by integrating diverse analytical
perspectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating Trade-offs: Policy Summarization for Multi-Objective
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuzanna Osika, Jazmin Zatarain-Salazar, Frans A. Oliehoek, Pradeep K. Murukannaiah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective reinforcement learning (MORL) is used to solve problems
involving multiple objectives. An MORL agent must make decisions based on the
diverse signals provided by distinct reward functions. Training an MORL agent
yields a set of solutions (policies), each presenting distinct trade-offs among
the objectives (expected returns). MORL enhances explainability by enabling
fine-grained comparisons of policies in the solution set based on their
trade-offs as opposed to having a single policy. However, the solution set is
typically large and multi-dimensional, where each policy (e.g., a neural
network) is represented by its objective values.
  We propose an approach for clustering the solution set generated by MORL. By
considering both policy behavior and objective values, our clustering method
can reveal the relationship between policy behaviors and regions in the
objective space. This approach can enable decision makers (DMs) to identify
overarching trends and insights in the solution set rather than examining each
policy individually. We tested our method in four multi-objective environments
and found it outperformed traditional k-medoids clustering. Additionally, we
include a case study that demonstrates its real-world application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learn to Solve Vehicle Routing Problems ASAP: A Neural Optimization
  Approach for Time-Constrained Vehicle Routing Problems with Finite Vehicle
  Fleet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elija Deineko, Carina Kehrt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding a feasible and prompt solution to the Vehicle Routing Problem (VRP)
is a prerequisite for efficient freight transportation, seamless logistics, and
sustainable mobility. Traditional optimization methods reach their limits when
confronted with the real-world complexity of VRPs, which involve numerous
constraints and objectives. Recently, the ability of generative Artificial
Intelligence (AI) to solve combinatorial tasks, known as Neural Combinatorial
Optimization (NCO), demonstrated promising results, offering new perspectives.
In this study, we propose an NCO approach to solve a time-constrained
capacitated VRP with a finite vehicle fleet size. The approach is based on an
encoder-decoder architecture, formulated in line with the Policy Optimization
with Multiple Optima (POMO) protocol and trained via a Proximal Policy
Optimization (PPO) algorithm. We successfully trained the policy with multiple
objectives (minimizing the total distance while maximizing vehicle utilization)
and evaluated it on medium and large instances, benchmarking it against
state-of-the-art heuristics. The method is able to find adequate and
cost-efficient solutions, showing both flexibility and robust generalization.
Finally, we provide a critical analysis of the solution generated by NCO and
discuss the challenges and opportunities of this new branch of intelligent
learning algorithms emerging in optimization science, focusing on freight
transportation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Affiliation: German Aerospace Center (DLR), Institute of Transport
  Research, Rudower Chaussee 7, 12489 Berlin Correspondence:
  Elija.deineko@dlr.de</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning dynamical systems from data: Gradient-based dictionary
  optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Tabish, Neil K. Chada, Stefan Klus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Koopman operator plays a crucial role in analyzing the global behavior of
dynamical systems. Existing data-driven methods for approximating the Koopman
operator or discovering the governing equations of the underlying system
typically require a fixed set of basis functions, also called dictionary. The
optimal choice of basis functions is highly problem-dependent and often
requires domain knowledge. We present a novel gradient descent-based
optimization framework for learning suitable and interpretable basis functions
from data and show how it can be used in combination with EDMD, SINDy, and
PDE-FIND. We illustrate the efficacy of the proposed approach with the aid of
various benchmark problems such as the Ornstein-Uhlenbeck process, Chua's
circuit, a nonlinear heat equation, as well as protein-folding data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining the Minoria: Unknown, Under-represented, and Under-performing
  Minority Groups <span class="chip">VLDB 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsen Dehghankar, Abolfazl Asudeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to a variety of reasons, such as privacy, data in the wild often misses
the grouping information required for identifying minorities. On the other
hand, it is known that machine learning models are only as good as the data
they are trained on and, hence, may underperform for the under-represented
minority groups. The missing grouping information presents a dilemma for
responsible data scientists who find themselves in an unknown-unknown
situation, where not only do they not have access to the grouping attributes
but do not also know what groups to consider.
  This paper is an attempt to address this dilemma. Specifically, we propose a
minority mining problem, where we find vectors in the attribute space that
reveal potential groups that are under-represented and under-performing.
Technically speaking, we propose a geometric transformation of data into a dual
space and use notions such as the arrangement of hyperplanes to design an
efficient algorithm for the problem in lower dimensions. Generalizing our
solution to the higher dimensions is cursed by dimensionality. Therefore, we
propose a solution based on smart exploration of the search space for such
cases. We conduct comprehensive experiments using real-world and synthetic
datasets alongside the theoretical analysis. Our experiment results demonstrate
the effectiveness of our proposed solutions in mining the unknown,
under-represented, and under-performing minorities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is currently under review at VLDB 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Temporal Resolution Domain Adaptation for Spiking Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanja Karilanova, Maxime Fabre, Emre Neftci, Ayça Özçelikkale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) are biologically-inspired deep neural networks
that efficiently extract temporal information while offering promising gains in
terms of energy efficiency and latency when deployed on neuromorphic devices.
However, SNN model parameters are sensitive to temporal resolution, leading to
significant performance drops when the temporal resolution of target data at
the edge is not the same with that of the pre-deployment source data used for
training, especially when fine-tuning is not possible at the edge. To address
this challenge, we propose three novel domain adaptation methods for adapting
neuron parameters to account for the change in time resolution without
re-training on target time-resolution. The proposed methods are based on a
mapping between neuron dynamics in SNNs and State Space Models (SSMs); and are
applicable to general neuron models. We evaluate the proposed methods under
spatio-temporal data tasks, namely the audio keyword spotting datasets SHD and
MSWC as well as the image classification NMINST dataset. Our methods provide an
alternative to - and in majority of the cases significantly outperform - the
existing reference method that simply scales the time constant. Moreover, our
results show that high accuracy on high temporal resolution data can be
obtained by time efficient training on lower temporal resolution data and model
adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equivariant Graph Attention Networks with Structural Motifs for
  Predicting Cell Line-Specific Synergistic Drug Combinations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Schwehr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cancer is the second leading cause of death, with chemotherapy as one of the
primary forms of treatment. As a result, researchers are turning to drug
combination therapy to decrease drug resistance and increase efficacy. Current
methods of drug combination screening, such as in vivo and in vitro, are
inefficient due to stark time and monetary costs. In silico methods have become
increasingly important for screening drugs, but current methods are inaccurate
and generalize poorly to unseen anticancer drugs. In this paper, I employ a
geometric deep-learning model utilizing a graph attention network that is
equivariant to 3D rotations, translations, and reflections with structural
motifs. Additionally, the gene expression of cancer cell lines is utilized to
classify synergistic drug combinations specific to each cell line. I compared
the proposed geometric deep learning framework to current state-of-the-art
(SOTA) methods, and the proposed model architecture achieved greater
performance on all 12 benchmark tasks performed on the DrugComb dataset.
Specifically, the proposed framework outperformed other SOTA methods by an
accuracy difference greater than 28%. Based on these results, I believe that
the equivariant graph attention network's capability of learning geometric data
accounts for the large performance improvements. The model's ability to
generalize to foreign drugs is thought to be due to the structural motifs
providing a better representation of the molecule. Overall, I believe that the
proposed equivariant geometric deep learning framework serves as an effective
tool for virtually screening anticancer drug combinations for further
validation in a wet lab environment. The code for this work is made available
online at: https://github.com/WeToTheMoon/EGAT_DrugSynergy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure, Presented at IEEE CIBCB</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Respecting the limit:Bayesian optimization with a bound on the optimal
  value 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyang Wang, Juergen Branke, Matthias Poloczek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world optimization problems, we have prior information about
what objective function values are achievable. In this paper, we study the
scenario that we have either exact knowledge of the minimum value or a,
possibly inexact, lower bound on its value. We propose bound-aware Bayesian
optimization (BABO), a Bayesian optimization method that uses a new surrogate
model and acquisition function to utilize such prior information. We present
SlogGP, a new surrogate model that incorporates bound information and adapts
the Expected Improvement (EI) acquisition function accordingly. Empirical
results on a variety of benchmarks demonstrate the benefit of taking prior
information about the optimal value into account, and that the proposed
approach significantly outperforms existing techniques. Furthermore, we notice
that even in the absence of prior information on the bound, the proposed SlogGP
surrogate model still performs better than the standard GP model in most cases,
which we explain by its larger expressiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convolutional Differentiable Logic Gate Networks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Petersen, Hilde Kuehne, Christian Borgelt, Julian Welzel, Stefano Ermon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing inference cost of machine learning models, there is a
growing interest in models with fast and efficient inference. Recently, an
approach for learning logic gate networks directly via a differentiable
relaxation was proposed. Logic gate networks are faster than conventional
neural network approaches because their inference only requires logic gate
operators such as NAND, OR, and XOR, which are the underlying building blocks
of current hardware and can be efficiently executed. We build on this idea,
extending it by deep logic gate tree convolutions, logical OR pooling, and
residual initializations. This allows scaling logic gate networks up by over
one order of magnitude and utilizing the paradigm of convolution. On CIFAR-10,
we achieve an accuracy of 86.29% using only 61 million logic gates, which
improves over the SOTA while being 29x smaller.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuromorphic Wireless Split Computing with Multi-Level Spikes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dengyu Wu, Jiechen Chen, Bipin Rajendran, H. Vincent Poor, Osvaldo Simeone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by biological processes, neuromorphic computing utilizes spiking
neural networks (SNNs) to perform inference tasks, offering significant
efficiency gains for workloads involving sequential data. Recent advances in
hardware and software have demonstrated that embedding a few bits of payload in
each spike exchanged between the spiking neurons can further enhance inference
accuracy. In a split computing architecture, where the SNN is divided across
two separate devices, the device storing the first layers must share
information about the spikes generated by the local output neurons with the
other device. Consequently, the advantages of multi-level spikes must be
balanced against the challenges of transmitting additional bits between the two
devices.
  This paper addresses these challenges by investigating a wireless
neuromorphic split computing architecture employing multi-level SNNs. For this
system, we present the design of digital and analog modulation schemes
optimized for an orthogonal frequency division multiplexing (OFDM) radio
interface. Simulation and experimental results using software-defined radios
provide insights into the performance gains of multi-level SNN models and the
optimal payload size as a function of the quality of the connection between a
transmitter and receiver.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subspace-Constrained Quadratic Matrix Factorization: Algorithm and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Zhai, Xiaohui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matrix Factorization has emerged as a widely adopted framework for modeling
data exhibiting low-rank structures. To address challenges in manifold
learning, this paper presents a subspace-constrained quadratic matrix
factorization model. The model is designed to jointly learn key low-dimensional
structures, including the tangent space, the normal subspace, and the quadratic
form that links the tangent space to a low-dimensional representation. We solve
the proposed factorization model using an alternating minimization method,
involving an in-depth investigation of nonlinear regression and projection
subproblems. Theoretical properties of the quadratic projection problem and
convergence characteristics of the alternating strategy are also investigated.
To validate our approach, we conduct numerical experiments on synthetic and
real-world datasets. Results demonstrate that our model outperforms existing
methods, highlighting its robustness and efficacy in capturing core
low-dimensional structures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxin Hu, Hao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following the milestones in large language models (LLMs) and multimodal
models, we have seen a surge in applying LLMs to biochemical tasks. Leveraging
graph features and molecular text representations, LLMs can tackle various
tasks, such as predicting chemical reaction outcomes and describing molecular
properties. However, most current work overlooks the multi-level nature of
graph features. The impact of different feature levels on LLMs and the
importance of each level remain unexplored, and it is possible that different
chemistry tasks require different feature levels. In this work, we first
investigate the effect of feature granularity by fusing GNN-generated feature
tokens, discovering that even reducing all tokens to a single token does not
significantly impact performance. We then explore the effect of various feature
levels on performance, finding that both the quality of LLM-generated molecules
and performance on different tasks benefit from different feature levels. We
conclude with two key insights: (1) current molecular Multimodal LLMs(MLLMs)
lack a comprehensive understanding of graph features, and (2) static processing
is not sufficient for hierarchical graph feature. Our code will be publicly
available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Field Assessment of Force Torque Sensors for Planetary Rover Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Levin Gerdes, Carlos Pérez del Pulgar, Raúl Castilla Arquillo, Martin Azkarate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proprioceptive sensors on planetary rovers serve for state estimation and for
understanding terrain and locomotion performance. While inertial measurement
units (IMUs) are widely used to this effect, force-torque sensors are less
explored for planetary navigation despite their potential to directly measure
interaction forces and provide insights into traction performance. This paper
presents an evaluation of the performance and use cases of force-torque sensors
based on data collected from a six-wheeled rover during tests over varying
terrains, speeds, and slopes. We discuss challenges, such as sensor signal
reliability and terrain response accuracy, and identify opportunities regarding
the use of these sensors. The data is openly accessible and includes
force-torque measurements from each of the six-wheel assemblies as well as IMU
data from within the rover chassis. This paper aims to inform the design of
future studies and rover upgrades, particularly in sensor integration and
control algorithms, to improve navigation capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Multiple Dimensions of Spuriousness in Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel J. Bell, Skyler Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning correlations from data forms the foundation of today's machine
learning (ML) and artificial intelligence (AI) research. While such an approach
enables the automatic discovery of patterned relationships within big data
corpora, it is susceptible to failure modes when unintended correlations are
captured. This vulnerability has expanded interest in interrogating
spuriousness, often critiqued as an impediment to model performance, fairness,
and robustness. In this article, we trace deviations from the conventional
definition of statistical spuriousness-which denotes a non-causal observation
arising from either coincidence or confounding variables-to articulate how ML
researchers make sense of spuriousness in practice. Drawing on a broad survey
of ML literature, we conceptualize the "multiple dimensions of spuriousness,"
encompassing: relevance ("Models should only use correlations that are relevant
to the task."), generalizability ("Models should only use correlations that
generalize to unseen data"), human-likeness ("Models should only use
correlations that a human would use to perform the same task"), and harmfulness
("Models should only use correlations that are not harmful"). These dimensions
demonstrate that ML spuriousness goes beyond the causal/non-causal dichotomy
and that the disparate interpretative paths researchers choose could
meaningfully influence the trajectory of ML development. By underscoring how a
fundamental problem in ML is contingently negotiated in research contexts, we
contribute to ongoing debates about responsible practices in AI development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is network fragmentation a useful complexity measure? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Coenraad Mouton, Randle Rabe, Daniël G. Haasbroek, Marthinus W. Theunissen, Hermanus L. Potgieter, Marelie H. Davel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been observed that the input space of deep neural network classifiers
can exhibit `fragmentation', where the model function rapidly changes class as
the input space is traversed. The severity of this fragmentation tends to
follow the double descent curve, achieving a maximum at the interpolation
regime. We study this phenomenon in the context of image classification and ask
whether fragmentation could be predictive of generalization performance. Using
a fragmentation-based complexity measure, we show this to be possible by
achieving good performance on the PGDL (Predicting Generalization in Deep
Learning) benchmark. In addition, we report on new observations related to
fragmentation, namely (i) fragmentation is not limited to the input space but
occurs in the hidden representations as well, (ii) fragmentation follows the
trends in the validation error throughout training, and (iii) fragmentation is
not a direct result of increased weight norms. Together, this indicates that
fragmentation is a phenomenon worth investigating further when studying the
generalization ability of deep neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Continual Learning using Pre-Trained Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marlon Tobaben, Marcus Klasson, Rui Li, Arno Solin, Antti Honkela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work explores the intersection of continual learning (CL) and
differential privacy (DP). Crucially, continual learning models must retain
knowledge across tasks, but this conflicts with the differential privacy
requirement of restricting individual samples to be memorised in the model. We
propose using pre-trained models to address the trade-offs between privacy and
performance in a continual learning setting.More specifically, we present
necessary assumptions to enable privacy-preservation and propose combining
pre-trained models with parameter-free classifiers and parameter-efficient
adapters that are learned under differential privacy. Our experiments
demonstrate their effectiveness and provide insights into balancing the
competing demands of continual learning and privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures, Accepted at Scalable Continual Learning for
  Lifelong Foundation Models Workshop at 38th Conference on Neural Information
  Processing Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-Aware Resource Management for C-V2X Platooning via Multi-Agent
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Shao, Qiong Wu, Pingyi Fan, Kezhi Wang, Qiang Fan, Wen Chen, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a semantic-aware multi-modal resource allocation (SAMRA)
for multi-task using multi-agent reinforcement learning (MARL), termed
SAMRAMARL, utilizing in platoon systems where cellular vehicle-to-everything
(C-V2X) communication is employed. The proposed approach leverages the semantic
information to optimize the allocation of communication resources. By
integrating a distributed multi-agent reinforcement learning (MARL) algorithm,
SAMRAMARL enables autonomous decision-making for each vehicle, channel
assignment optimization, power allocation, and semantic symbol length based on
the contextual importance of the transmitted information. This
semantic-awareness ensures that both vehicle-to-vehicle (V2V) and
vehicle-to-infrastructure (V2I) communications prioritize data that is critical
for maintaining safe and efficient platoon operations. The framework also
introduces a tailored quality of experience (QoE) metric for semantic
communication, aiming to maximize QoE in V2V links while improving the success
rate of semantic information transmission (SRS). Extensive simulations has
demonstrated that SAMRAMARL outperforms existing methods, achieving significant
gains in QoE and communication efficiency in C-V2X platooning scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to IEEE Journal. The source code has
  been released
  at:https://github.com/qiongwu86/Semantic-Aware-Resource-Management-for-C-V2X-Platooning-via-Multi-Agent-Reinforcement-Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EffiCANet: Efficient Time Series Forecasting with Convolutional
  Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinxing Zhou, Jiaqi Ye, Shubao Zhao, Ming Jin, Chengyi Yang, Yanlong Wen, Xiaojie Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of multivariate time series data from sensor networks
in domains like industrial monitoring and smart cities requires efficient and
accurate forecasting models. Current deep learning methods often fail to
adequately capture long-range dependencies and complex inter-variable
relationships, especially under real-time processing constraints. These
limitations arise as many models are optimized for either short-term
forecasting with limited receptive fields or long-term accuracy at the cost of
efficiency. Additionally, dynamic and intricate interactions between variables
in real-world data further complicate modeling efforts. To address these
limitations, we propose EffiCANet, an Efficient Convolutional Attention Network
designed to enhance forecasting accuracy while maintaining computational
efficiency. EffiCANet integrates three key components: (1) a Temporal
Large-kernel Decomposed Convolution (TLDC) module that captures long-term
temporal dependencies while reducing computational overhead; (2) an
Inter-Variable Group Convolution (IVGC) module that captures complex and
evolving relationships among variables; and (3) a Global Temporal-Variable
Attention (GTVA) mechanism that prioritizes critical temporal and
inter-variable features. Extensive evaluations across nine benchmark datasets
show that EffiCANet achieves the maximum reduction of 10.02% in MAE over
state-of-the-art models, while cutting computational costs by 26.2% relative to
conventional large-kernel convolution methods, thanks to its efficient
decomposition strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Trust in Clinically Significant Prostate Cancer Prediction
  with Multiple Magnetic Resonance Imaging Modalities <span class="chip">ML4H</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Ng, Chi-en Amy Tai, E. Zhixuan Zeng, Alexander Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the United States, prostate cancer is the second leading cause of deaths
in males with a predicted 35,250 deaths in 2024. However, most diagnoses are
non-lethal and deemed clinically insignificant which means that the patient
will likely not be impacted by the cancer over their lifetime. As a result,
numerous research studies have explored the accuracy of predicting clinical
significance of prostate cancer based on magnetic resonance imaging (MRI)
modalities and deep neural networks. Despite their high performance, these
models are not trusted by most clinical scientists as they are trained solely
on a single modality whereas clinical scientists often use multiple magnetic
resonance imaging modalities during their diagnosis. In this paper, we
investigate combining multiple MRI modalities to train a deep learning model to
enhance trust in the models for clinically significant prostate cancer
prediction. The promising performance and proposed training pipeline showcase
the benefits of incorporating multiple MRI modalities for enhanced trust and
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings paper presented at Machine Learning for Health (ML4H)
  symposium 2024, December 15-16, 2024, Vancouver, Canada, 6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Centrality Graph Shift Operators for Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassine Abbahaddou, Fragkiskos D. Malliaros, Johannes F. Lutzeyer, Michalis Vazirgiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Shift Operators (GSOs), such as the adjacency and graph Laplacian
matrices, play a fundamental role in graph theory and graph representation
learning. Traditional GSOs are typically constructed by normalizing the
adjacency matrix by the degree matrix, a local centrality metric. In this work,
we instead propose and study Centrality GSOs (CGSOs), which normalize adjacency
matrices by global centrality metrics such as the PageRank, $k$-core or count
of fixed length walks. We study spectral properties of the CGSOs, allowing us
to get an understanding of their action on graph signals. We confirm this
understanding by defining and running the spectral clustering algorithm based
on different CGSOs on several synthetic and real-world datasets. We furthermore
outline how our CGSO can act as the message passing operator in any Graph
Neural Network and in particular demonstrate strong performance of a variant of
the Graph Convolutional Network and Graph Attention Network using our CGSOs on
several real-world benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IGDrivSim: A Benchmark for the Imitation Gap in Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clémence Grislain, Risto Vuorio, Cong Lu, Shimon Whiteson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing autonomous vehicles that can navigate complex environments with
human-level safety and efficiency is a central goal in self-driving research. A
common approach to achieving this is imitation learning, where agents are
trained to mimic human expert demonstrations collected from real-world driving
scenarios. However, discrepancies between human perception and the self-driving
car's sensors can introduce an \textit{imitation gap}, leading to imitation
learning failures. In this work, we introduce \textbf{IGDrivSim}, a benchmark
built on top of the Waymax simulator, designed to investigate the effects of
the imitation gap in learning autonomous driving policy from human expert
demonstrations. Our experiments show that this perception gap between human
experts and self-driving agents can hinder the learning of safe and effective
driving behaviors. We further show that combining imitation with reinforcement
learning, using a simple penalty reward for prohibited behaviors, effectively
mitigates these failures. Our code is open-sourced at:
https://github.com/clemgris/IGDrivSim.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DISCO: DISCovering Overfittings as Causal Rules for Text Classification
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhang, Vinay Setty, Yumeng Wang, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of neural language models, the deployment of
over-parameterized models has surged, increasing the need for interpretable
explanations comprehensible to human inspectors. Existing post-hoc
interpretability methods, which often focus on unigram features of single input
textual instances, fail to capture the models' decision-making process fully.
Additionally, many methods do not differentiate between decisions based on
spurious correlations and those based on a holistic understanding of the input.
Our paper introduces DISCO, a novel method for discovering global, rule-based
explanations by identifying causal n-gram associations with model predictions.
This method employs a scalable sequence mining technique to extract relevant
text spans from training data, associate them with model predictions, and
conduct causality checks to distill robust rules that elucidate model behavior.
These rules expose potential overfitting and provide insights into misleading
feature combinations. We validate DISCO through extensive testing,
demonstrating its superiority over existing methods in offering comprehensive
insights into complex model behaviors. Our approach successfully identifies all
shortcuts manually introduced into the training data (100% detection rate on
the MultiRC dataset), resulting in an 18.8% regression in model performance --
a capability unmatched by any other method. Furthermore, DISCO supports
interactive explanations, enabling human inspectors to distinguish spurious
causes in the rule-based output. This alleviates the burden of abundant
instance-wise explanations and helps assess the model's risk when encountering
out-of-distribution (OOD) data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ wav2sleep: A Unified Multi-Modal Approach to Sleep Stage Classification
  from Physiological Signals <span class="chip">ML4H</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan F. Carter, Lionel Tarassenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate classification of sleep stages from less obtrusive sensor
measurements such as the electrocardiogram (ECG) or photoplethysmogram (PPG)
could enable important applications in sleep medicine. Existing approaches to
this problem have typically used deep learning models designed and trained to
operate on one or more specific input signals. However, the datasets used to
develop these models often do not contain the same sets of input signals. Some
signals, particularly PPG, are much less prevalent than others, and this has
previously been addressed with techniques such as transfer learning.
Additionally, only training on one or more fixed modalities precludes
cross-modal information transfer from other sources, which has proved valuable
in other problem domains. To address this, we introduce wav2sleep, a unified
model designed to operate on variable sets of input signals during training and
inference. After jointly training on over 10,000 overnight recordings from six
publicly available polysomnography datasets, including SHHS and MESA, wav2sleep
outperforms existing sleep stage classification models across test-time input
combinations including ECG, PPG, and respiratory signals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Machine Learning for Health (ML4H) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cybercrime Prediction via Geographically Weighted Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Al-Zafar Khan, Jamal Al-Karaki, Emad Mahafzah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the success of Geographically Weighted Regression and its
accounting for spatial variations, we propose GeogGNN -- A graph neural network
model that accounts for geographical latitude and longitudinal points. Using a
synthetically generated dataset, we apply the algorithm for a 4-class
classification problem in cybersecurity with seemingly realistic geographic
coordinates centered in the Gulf Cooperation Council region. We demonstrate
that it has higher accuracy than standard neural networks and convolutional
neural networks that treat the coordinates as features. Encouraged by the
speed-up in model accuracy by the GeogGNN model, we provide a general
mathematical result that demonstrates that a geometrically weighted neural
network will, in principle, always display higher accuracy in the
classification of spatially dependent data by making use of spatial continuity
and local averaging features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, Submitted to the International Jordanian
  Cybersecurity Conference 2024 (IJCC24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Multi-Task Brain Tumour Segmentation with Synthetic Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Ferreira, Tiago Jesus, Behrus Puladi, Jens Kleesiek, Victor Alves, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the winning solution of task 1 and the third-placed
solution of task 3 of the BraTS challenge. The use of automated tools in
clinical practice has increased due to the development of more and more
sophisticated and reliable algorithms. However, achieving clinical standards
and developing tools for real-life scenarios is a major challenge. To this end,
BraTS has organised tasks to find the most advanced solutions for specific
purposes. In this paper, we propose the use of synthetic data to train
state-of-the-art frameworks in order to improve the segmentation of adult
gliomas in a post-treatment scenario, and the segmentation of meningioma for
radiotherapy planning. Our results suggest that the use of synthetic data leads
to more robust algorithms, although the synthetic data generation pipeline is
not directly suited to the meningioma task. The code for these tasks is
available at https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain Tumour Removing and Missing Modality Generation using 3D WDM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Ferreira, Gijs Luijten, Behrus Puladi, Jens Kleesiek, Victor Alves, Jan Egger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the second-placed solution for task 8 and the
participation solution for task 7 of BraTS 2024. The adoption of automated
brain analysis algorithms to support clinical practice is increasing. However,
many of these algorithms struggle with the presence of brain lesions or the
absence of certain MRI modalities. The alterations in the brain's morphology
leads to high variability and thus poor performance of predictive models that
were trained only on healthy brains. The lack of information that is usually
provided by some of the missing MRI modalities also reduces the reliability of
the prediction models trained with all modalities. In order to improve the
performance of these models, we propose the use of conditional 3D wavelet
diffusion models. The wavelet transform enabled full-resolution image training
and prediction on a GPU with 48 GB VRAM, without patching or downsampling,
preserving all information for prediction. For the inpainting task of BraTS
2024, the use of a large and variable number of healthy masks and the stability
and efficiency of the 3D wavelet diffusion model resulted in 0.007, 22.61 and
0.842 in the validation set and 0.07 , 22.8 and 0.91 in the testing set (MSE,
PSNR and SSIM respectively). The code for these tasks is available at
https://github.com/ShadowTwin41/BraTS_2023_2024_solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharp Analysis for KL-Regularized Contextual Bandits and RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heyang Zhao, Chenlu Ye, Quanquan Gu, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reverse-Kullback-Leibler (KL) regularization has emerged to be a predominant
technique used to enhance policy optimization in reinforcement learning (RL)
and reinforcement learning from human feedback (RLHF), which forces the learned
policy to stay close to a reference policy. While the effectiveness and
necessity of KL-regularization have been empirically demonstrated in various
practical scenarios, current theoretical analysis of KL-regularized RLHF still
obtains the same $\mathcal{O}(1 / \epsilon^2)$ sample complexity as problems
without KL-regularization. To understand the fundamental distinction between
policy learning objectives with KL-regularization and ones without
KL-regularization, we are the first to theoretically demonstrate the power of
KL-regularization by providing a sharp analysis for KL-regularized contextual
bandits and RLHF, revealing an $\mathcal{O}(1 / \epsilon)$ sample complexity
when $\epsilon$ is sufficiently small.
  We further explore the role of data coverage in contextual bandits and RLHF.
While the coverage assumption is commonly employed in offline RLHF to link the
samples from the reference policy to the optimal policy, often at the cost of a
multiplicative dependence on the coverage coefficient, its impact on the sample
complexity of online RLHF remains unclear. Previous theoretical analyses of
online RLHF typically require explicit exploration and additional structural
assumptions on the reward function class. In contrast, we show that with
sufficient coverage from the reference policy, a simple two-stage mixed
sampling strategy can achieve a sample complexity with only an additive
dependence on the coverage coefficient. Our results provide a comprehensive
understanding of the roles of KL-regularization and data coverage in RLHF,
shedding light on the design of more efficient RLHF algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Semi-Supervised Learning on Line Segment Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johanna Engman, Karl Åström, Magnus Oskarsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a method for line segment detection in images, based
on a semi-supervised framework. Leveraging the use of a consistency loss based
on differently augmented and perturbed unlabeled images with a small amount of
labeled data, we show comparable results to fully supervised methods. This
opens up application scenarios where annotation is difficult or expensive, and
for domain specific adaptation of models. We are specifically interested in
real-time and online applications, and investigate small and efficient learning
backbones. Our method is to our knowledge the first to target line detection
using modern state-of-the-art methodologies for semi-supervised learning. We
test the method on both standard benchmarks and domain specific scenarios for
forestry applications, showing the tractability of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verification of Neural Networks against Convolutional Perturbations via
  Parameterised Kernels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedikt Brückner, Alessio Lomuscio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a method for the efficient verification of neural networks against
convolutional perturbations such as blurring or sharpening. To define input
perturbations we use well-known camera shake, box blur and sharpen kernels. We
demonstrate that these kernels can be linearly parameterised in a way that
allows for a variation of the perturbation strength while preserving desired
kernel properties. To facilitate their use in neural network verification, we
develop an efficient way of convolving a given input with these parameterised
kernels. The result of this convolution can be used to encode the perturbation
in a verification setting by prepending a linear layer to a given network. This
leads to tight bounds and a high effectiveness in the resulting verification
step. We add further precision by employing input splitting as a branch and
bound strategy. We demonstrate that we are able to verify robustness on a
number of standard benchmarks where the baseline is unable to provide any
safety certificates. To the best of our knowledge, this is the first solution
for verifying robustness against specific convolutional perturbations such as
camera shake.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Inherent Robustness of One-Stage Object Detection against
  Out-of-Distribution Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aitor Martinez-Seras, Javier Del Ser, Alain Andres, Pablo Garcia-Bringas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robustness is a fundamental aspect for developing safe and trustworthy
models, particularly when they are deployed in the open world. In this work we
analyze the inherent capability of one-stage object detectors to robustly
operate in the presence of out-of-distribution (OoD) data. Specifically, we
propose a novel detection algorithm for detecting unknown objects in image
data, which leverages the features extracted by the model from each sample.
Differently from other recent approaches in the literature, our proposal does
not require retraining the object detector, thereby allowing for the use of
pretrained models. Our proposed OoD detector exploits the application of
supervised dimensionality reduction techniques to mitigate the effects of the
curse of dimensionality on the features extracted by the model. Furthermore, it
utilizes high-resolution feature maps to identify potential unknown objects in
an unsupervised fashion. Our experiments analyze the Pareto trade-off between
the performance detecting known and unknown objects resulting from different
algorithmic configurations and inference confidence thresholds. We also compare
the performance of our proposed algorithm to that of logits-based post-hoc OoD
methods, as well as possible fusion strategies. Finally, we discuss on the
competitiveness of all tested methods against state-of-the-art OoD approaches
for object detection models over the recently published Unknown Object
Detection benchmark. The obtained results verify that the performance of
avant-garde post-hoc OoD detectors can be further improved when combined with
our proposed algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 figures, 4 tables, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting the Learned Model in MuZero Planning <span class="chip">TAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung Guei, Yan-Ru Ju, Wei-Yu Chen, Ti-Rong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MuZero has achieved superhuman performance in various games by using a
dynamics network to predict environment dynamics for planning, without relying
on simulators. However, the latent states learned by the dynamics network make
its planning process opaque. This paper aims to demystify MuZero's model by
interpreting the learned latent states. We incorporate observation
reconstruction and state consistency into MuZero training and conduct an
in-depth analysis to evaluate latent states across two board games: 9x9 Go and
Outer-Open Gomoku, and three Atari games: Breakout, Ms. Pacman, and Pong. Our
findings reveal that while the dynamics network becomes less accurate over
longer simulations, MuZero still performs effectively by using planning to
correct errors. Our experiments also show that the dynamics network learns
better latent states in board games than in Atari games. These insights
contribute to a better understanding of MuZero and offer directions for future
research to improve the playing performance, robustness, and interpretability
of the MuZero algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 29th International Conference on Technologies and
  Applications of Artificial Intelligence (TAAI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Federated Analytics via Differentially Private
  Measurements of Statistical Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mary Scott, Graham Cormode, Carsten Maple
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statistical heterogeneity is a measure of how skewed the samples of a dataset
are. It is a common problem in the study of differential privacy that the usage
of a statistically heterogeneous dataset results in a significant loss of
accuracy. In federated scenarios, statistical heterogeneity is more likely to
happen, and so the above problem is even more pressing. We explore the three
most promising ways to measure statistical heterogeneity and give formulae for
their accuracy, while simultaneously incorporating differential privacy. We
find the optimum privacy parameters via an analytic mechanism, which
incorporates root finding methods. We validate the main theorems and related
hypotheses experimentally, and test the robustness of the analytic mechanism to
different heterogeneity levels. The analytic mechanism in a distributed setting
delivers superior accuracy to all combinations involving the classic mechanism
and/or the centralized setting. All measures of statistical heterogeneity do
not lose significant accuracy when a heterogeneous sample is used.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 tables, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Higher-Order GNNs Meet Efficiency: Sparse Sobolev Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jhony H. Giraldo, Aref Einizade, Andjela Todorovic, Jhon A. Castro-Correa, Mohsen Badiey, Thierry Bouwmans, Fragkiskos D. Malliaros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have shown great promise in modeling
relationships between nodes in a graph, but capturing higher-order
relationships remains a challenge for large-scale networks. Previous studies
have primarily attempted to utilize the information from higher-order neighbors
in the graph, involving the incorporation of powers of the shift operator, such
as the graph Laplacian or adjacency matrix. This approach comes with a
trade-off in terms of increased computational and memory demands. Relying on
graph spectral theory, we make a fundamental observation: the regular and the
Hadamard power of the Laplacian matrix behave similarly in the spectrum. This
observation has significant implications for capturing higher-order information
in GNNs for various tasks such as node classification and semi-supervised
learning. Consequently, we propose a novel graph convolutional operator based
on the sparse Sobolev norm of graph signals. Our approach, known as Sparse
Sobolev GNN (S2-GNN), employs Hadamard products between matrices to maintain
the sparsity level in graph representations. S2-GNN utilizes a cascade of
filters with increasing Hadamard powers to generate a diverse set of functions.
We theoretically analyze the stability of S2-GNN to show the robustness of the
model against possible graph perturbations. We also conduct a comprehensive
evaluation of S2-GNN across various graph mining, semi-supervised node
classification, and computer vision tasks. In particular use cases, our
algorithm demonstrates competitive performance compared to state-of-the-art
GNNs in terms of performance and running time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Label Noise on Learning Complex Features <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Vashisht, P. Krishna Kumar, Harsha Vardhan Govind, Harish G. Ramaswamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks trained with stochastic gradient descent exhibit an inductive
bias towards simpler decision boundaries, typically converging to a narrow
family of functions, and often fail to capture more complex features. This
phenomenon raises concerns about the capacity of deep models to adequately
learn and represent real-world datasets. Traditional approaches such as
explicit regularization, data augmentation, architectural modifications, etc.,
have largely proven ineffective in encouraging the models to learn diverse
features. In this work, we investigate the impact of pre-training models with
noisy labels on the dynamics of SGD across various architectures and datasets.
We show that pretraining promotes learning complex functions and diverse
features in the presence of noise. Our experiments demonstrate that
pre-training with noisy labels encourages gradient descent to find alternate
minima that do not solely depend upon simple features, rather learns more
complex and broader set of features, without hurting performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Workshop on Scientific Methods for Understanding Deep
  Learning, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Latent Action Policies for Model-Based Offline Reinforcement
  Learning <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Alles, Philip Becker-Ehmck, Patrick van der Smagt, Maximilian Karl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In offline reinforcement learning, a policy is learned using a static dataset
in the absence of costly feedback from the environment. In contrast to the
online setting, only using static datasets poses additional challenges, such as
policies generating out-of-distribution samples. Model-based offline
reinforcement learning methods try to overcome these by learning a model of the
underlying dynamics of the environment and using it to guide policy search. It
is beneficial but, with limited datasets, errors in the model and the issue of
value overestimation among out-of-distribution states can worsen performance.
Current model-based methods apply some notion of conservatism to the Bellman
update, often implemented using uncertainty estimation derived from model
ensembles. In this paper, we propose Constrained Latent Action Policies (C-LAP)
which learns a generative model of the joint distribution of observations and
actions. We cast policy learning as a constrained objective to always stay
within the support of the latent action distribution, and use the generative
capabilities of the model to impose an implicit constraint on the generated
actions. Thereby eliminating the need to use additional uncertainty penalties
on the Bellman update and significantly decreasing the number of gradient steps
required to learn a policy. We empirically evaluate C-LAP on the D4RL and
V-D4RL benchmark, and show that C-LAP is competitive to state-of-the-art
methods, especially outperforming on datasets with visual observations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pruning Literals for Highly Efficient Explainability at Word Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Kumar Yadav, Bimal Bhattarai, Abhik Jana, Lei Jiao, Seid Muhie Yimam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing an explainable model becomes crucial now for Natural Language
Processing(NLP) since most of the state-of-the-art machine learning models
provide a limited explanation for the prediction. In the spectrum of an
explainable model, Tsetlin Machine(TM) is promising because of its capability
of providing word-level explanation using proposition logic. However, concern
rises over the elaborated combination of literals (propositional logic) in the
clause that makes the model difficult for humans to comprehend, despite having
a transparent learning process. In this paper, we design a post-hoc pruning of
clauses that eliminate the randomly placed literals in the clause thereby
making the model more efficiently interpretable than the vanilla TM.
Experiments on the publicly available YELP-HAT Dataset demonstrate that the
proposed pruned TM's attention map aligns more with the human attention map
than the vanilla TM's attention map. In addition, the pairwise similarity
measure also surpasses the attention map-based neural network models. In terms
of accuracy, the proposed pruning method does not degrade the accuracy
significantly but rather enhances the performance up to 4% to 9% in some test
data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Prediction Neural Network (UpNet): Embedding Artificial
  Neural Network in Bayesian Inversion Framework to Quantify the Uncertainty of
  Remote Sensing Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dasheng Fan, Xihan Mu, Yongkang Lai, Donghui Xie, Guangjian Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the retrieval of large-scale vegetation biophysical parameters, the
inversion of radiative transfer models (RTMs) is the most commonly used
approach. In recent years, Artificial Neural Network (ANN)-based methods have
become the mainstream for inverting RTMs due to their high accuracy and
computational efficiency. It has been widely used in the retrieval of
biophysical variables (BV). However, due to the lack of the Bayesian inversion
theory interpretation, it faces challenges in quantifying the retrieval
uncertainty, a crucial metric for product quality validation and downstream
applications such as data assimilation or ecosystem carbon cycling modeling.
This study proved that the ANN trained with squared loss outputs the posterior
mean, providing a rigorous foundation for its uncertainty quantification,
regularization, and incorporation of prior information. A Bayesian theoretical
framework was subsequently proposed for ANN-based methods. Using this
framework, we derived a new algorithm called Uncertainty Prediction Neural
Network (UpNet), which enables the simultaneous training of two ANNs to
retrieve BV and provide retrieval uncertainty. To validate our method, we
compared UpNet with the standard Bayesian inference method, i.e., Markov Chain
Monte Carlo (MCMC), in the inversion of a widely used RTM called ProSAIL for
retrieving BVs and estimating uncertainty. The results demonstrated that the
BVs retrieved and the uncertainties estimated by UpNet were highly consistent
with those from MCMC, achieving over a million-fold acceleration. These results
indicated that UpNet has significant potential for fast retrieval and
uncertainty quantification of BVs or other parameters with medium and
high-resolution remote sensing data. Our Python implementation is available at:
https://github.com/Dash-RSer/UpNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, f figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Peri-midFormer: Periodic Pyramid <span class="highlight-title">Transformer</span> for Time Series Analysis <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Wu, Gechang Yao, Zhixi Feng, Shuyuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series analysis finds wide applications in fields such as weather
forecasting, anomaly detection, and behavior recognition. Previous methods
attempted to model temporal variations directly using 1D time series. However,
this has been quite challenging due to the discrete nature of data points in
time series and the complexity of periodic variation. In terms of periodicity,
taking weather and traffic data as an example, there are multi-periodic
variations such as yearly, monthly, weekly, and daily, etc. In order to break
through the limitations of the previous methods, we decouple the implied
complex periodic variations into inclusion and overlap relationships among
different level periodic components based on the observation of the
multi-periodicity therein and its inclusion relationships. This explicitly
represents the naturally occurring pyramid-like properties in time series,
where the top level is the original time series and lower levels consist of
periodic components with gradually shorter periods, which we call the periodic
pyramid. To further extract complex temporal variations, we introduce
self-attention mechanism into the periodic pyramid, capturing complex periodic
relationships by computing attention between periodic components based on their
inclusion, overlap, and adjacency relationships. Our proposed Peri-midFormer
demonstrates outstanding performance in five mainstream time series analysis
tasks, including short- and long-term forecasting, imputation, classification,
and anomaly detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38th Conference on Neural Information Processing Systems (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measure-to-measure interpolation using <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borjan Geshkovski, Philippe Rigollet, Domènec Ruiz-Balet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers are deep neural network architectures that underpin the recent
successes of large language models. Unlike more classical architectures that
can be viewed as point-to-point maps, a Transformer acts as a
measure-to-measure map implemented as specific interacting particle system on
the unit sphere: the input is the empirical measure of tokens in a prompt and
its evolution is governed by the continuity equation. In fact, Transformers are
not limited to empirical measures and can in principle process any input
measure. As the nature of data processed by Transformers is expanding rapidly,
it is important to investigate their expressive power as maps from an arbitrary
measure to another arbitrary measure. To that end, we provide an explicit
choice of parameters that allows a single Transformer to match $N$ arbitrary
input measures to $N$ arbitrary target measures, under the minimal assumption
that every pair of input-target measures can be matched by some transport map.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision Language Models are In-Context Value Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yecheng Jason Ma, Joey Hejna, Ayzaan Wahid, Chuyuan Fu, Dhruv Shah, Jacky Liang, Zhuo Xu, Sean Kirmani, Peng Xu, Danny Driess, Ted Xiao, Jonathan Tompson, Osbert Bastani, Dinesh Jayaraman, Wenhao Yu, Tingnan Zhang, Dorsa Sadigh, Fei Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting temporal progress from visual trajectories is important for
intelligent robots that can learn, adapt, and improve. However, learning such
progress estimator, or temporal value function, across different tasks and
domains requires both a large amount of diverse data and methods which can
scale and generalize. To address these challenges, we present Generative Value
Learning (\GVL), a universal value function estimator that leverages the world
knowledge embedded in vision-language models (VLMs) to predict task progress.
Naively asking a VLM to predict values for a video sequence performs poorly due
to the strong temporal correlation between successive frames. Instead, GVL
poses value estimation as a temporal ordering problem over shuffled video
frames; this seemingly more challenging task encourages VLMs to more fully
exploit their underlying semantic and temporal grounding capabilities to
differentiate frames based on their perceived task progress, consequently
producing significantly better value predictions. Without any robot or task
specific training, GVL can in-context zero-shot and few-shot predict effective
values for more than 300 distinct real-world tasks across diverse robot
platforms, including challenging bimanual manipulation tasks. Furthermore, we
demonstrate that GVL permits flexible multi-modal in-context learning via
examples from heterogeneous tasks and embodiments, such as human videos. The
generality of GVL enables various downstream applications pertinent to
visuomotor policy learning, including dataset filtering, success detection, and
advantage-weighted regression -- all without any model training or finetuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website and demo:
  https://generative-value-learning.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hypercube Policy Regularization Framework for Offline Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Shen, Hanyan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning has received extensive attention from scholars
because it avoids the interaction between the agent and the environment by
learning a policy through a static dataset. However, general reinforcement
learning methods cannot get satisfactory results in offline reinforcement
learning due to the out-of-distribution state actions that the dataset cannot
cover during training. To solve this problem, the policy regularization method
that tries to directly clone policies used in static datasets has received
numerous studies due to its simplicity and effectiveness. However, policy
constraint methods make the agent choose the corresponding actions in the
static dataset. This type of constraint is usually over-conservative, which
results in suboptimal policies, especially in low-quality static datasets. In
this paper, a hypercube policy regularization framework is proposed, this
method alleviates the constraints of policy constraint methods by allowing the
agent to explore the actions corresponding to similar states in the static
dataset, which increases the effectiveness of algorithms in low-quality
datasets. It was also theoretically demonstrated that the hypercube policy
regularization framework can effectively improve the performance of original
algorithms. In addition, the hypercube policy regularization framework is
combined with TD3-BC and Diffusion-QL for experiments on D4RL datasets which
are called TD3-BC-C and Diffusion-QL-C. The experimental results of the score
demonstrate that TD3-BC-C and Diffusion-QL-C perform better than
state-of-the-art algorithms like IQL, CQL, TD3-BC and Diffusion-QL in most D4RL
environments in approximate time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Fingerprints for Adversarial Attack Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haim Fisher, Moni Shahar, Yehezkel S. Resheff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models for image classification have become standard tools in
recent years. A well known vulnerability of these models is their
susceptibility to adversarial examples. These are generated by slightly
altering an image of a certain class in a way that is imperceptible to humans
but causes the model to classify it wrongly as another class. Many algorithms
have been proposed to address this problem, falling generally into one of two
categories: (i) building robust classifiers (ii) directly detecting attacked
images. Despite the good performance of these detectors, we argue that in a
white-box setting, where the attacker knows the configuration and weights of
the network and the detector, they can overcome the detector by running many
examples on a local copy, and sending only those that were not detected to the
actual model. This problem is common in security applications where even a very
good model is not sufficient to ensure safety. In this paper we propose to
overcome this inherent limitation of any static defence with randomization. To
do so, one must generate a very large family of detectors with consistent
performance, and select one or more of them randomly for each input. For the
individual detectors, we suggest the method of neural fingerprints. In the
training phase, for each class we repeatedly sample a tiny random subset of
neurons from certain layers of the network, and if their average is
sufficiently different between clean and attacked images of the focal class
they are considered a fingerprint and added to the detector bank. During test
time, we sample fingerprints from the bank associated with the label predicted
by the model, and detect attacks using a likelihood ratio test. We evaluate our
detectors on ImageNet with different attack methods and model architectures,
and show near-perfect detection with low rates of false detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time stress detection on social network posts using big data
  technology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai-Yen Phan Nguyen, Phi-Lan Ly, Duc-Manh Le, Trong-Hop Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of modern life, particularly in Industry 4.0 within the online
space, emotions and moods are frequently conveyed through social media posts.
The trend of sharing stories, thoughts, and feelings on these platforms
generates a vast and promising data source for Big Data. This creates both a
challenge and an opportunity for research in applying technology to develop
more automated and accurate methods for detecting stress in social media users.
In this study, we developed a real-time system for stress detection in online
posts, using the "Dreaddit: A Reddit Dataset for Stress Analysis in Social
Media," which comprises 187,444 posts across five different Reddit domains.
Each domain contains texts with both stressful and non-stressful content,
showcasing various expressions of stress. A labeled dataset of 3,553 lines was
created for training. Apache Kafka, PySpark, and AirFlow were utilized to build
and deploy the model. Logistic Regression yielded the best results for new
streaming data, achieving 69,39% for measuring accuracy and 68,97 for measuring
F1-scores.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Sign Language Recognition System using Deep Learning with
  MediaPipe Holistic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharvani Srivastava, Sudhakar Singh,  Pooja, Shiv Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign languages are the language of hearing-impaired people who use visuals
like the hand, facial, and body movements for communication. There are
different signs and gestures representing alphabets, words, and phrases.
Nowadays approximately 300 sign languages are being practiced worldwide such as
American Sign Language (ASL), Chinese Sign Language (CSL), Indian Sign Language
(ISL), and many more. Sign languages are dependent on the vocal language of a
place. Unlike vocal or spoken languages, there are no helping words in sign
language like is, am, are, was, were, will, be, etc. As only a limited
population is well-versed in sign language, this lack of familiarity of sign
language hinders hearing-impaired people from communicating freely and easily
with everyone. This issue can be addressed by a sign language recognition (SLR)
system which has the capability to translate the sign language into vocal
language. In this paper, a continuous SLR system is proposed using a deep
learning model employing Long Short-Term Memory (LSTM), trained and tested on
an ISL primary dataset. This dataset is created using MediaPipe Holistic
pipeline for tracking face, hand, and body movements and collecting landmarks.
The system recognizes the signs and gestures in real-time with 88.23% accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, Wireless Pers Commun</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Normalized Space Alignment: A Versatile Metric for Representation
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danish Ebadulla, Aditya Gulati, Ambuj Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a manifold analysis technique for neural network
representations. Normalized Space Alignment (NSA) compares pairwise distances
between two point clouds derived from the same source and having the same size,
while potentially possessing differing dimensionalities. NSA can act as both an
analytical tool and a differentiable loss function, providing a robust means of
comparing and aligning representations across different layers and models. It
satisfies the criteria necessary for both a similarity metric and a neural
network loss function. We showcase NSA's versatility by illustrating its
utility as a representation space analysis metric, a structure-preserving loss
function, and a robustness analysis tool. NSA is not only computationally
efficient but it can also approximate the global structural discrepancy during
mini-batching, facilitating its use in a wide variety of neural network
training paradigms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improve the Fitting Accuracy of Deep Learning for the Nonlinear
  Schrödinger Equation Using Linear Feature Decoupling Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Zhang, Zekun Niu, Minghui Shi, Weisheng Hu, Lilin Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We utilize the Feature Decoupling Distributed (FDD) method to enhance the
capability of deep learning to fit the Nonlinear Schrodinger Equation (NLSE),
significantly reducing the NLSE loss compared to non decoupling model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synergy-Guided Regional Supervision of Pseudo Labels for Semi-Supervised
  Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Wang, Xinlin Zhang, Yuanbin Chen, Yuanbo Zhou, Longxuan Zhao, Tao Tan, Tong Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning has received considerable attention for its
potential to leverage abundant unlabeled data to enhance model robustness.
Pseudo labeling is a widely used strategy in semi supervised learning. However,
existing methods often suffer from noise contamination, which can undermine
model performance. To tackle this challenge, we introduce a novel
Synergy-Guided Regional Supervision of Pseudo Labels (SGRS-Net) framework.
Built upon the mean teacher network, we employ a Mix Augmentation module to
enhance the unlabeled data. By evaluating the synergy before and after
augmentation, we strategically partition the pseudo labels into distinct
regions. Additionally, we introduce a Region Loss Evaluation module to assess
the loss across each delineated area. Extensive experiments conducted on the LA
dataset have demonstrated superior performance over state-of-the-art
techniques, underscoring the efficiency and practicality of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Series-to-Series <span class="highlight-title">Diffusion</span> Bridge Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Zhanbo Feng, Feng Zhou, Robert C Qiu, Zenan Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have risen to prominence in time series forecasting,
showcasing their robust capability to model complex data distributions.
However, their effectiveness in deterministic predictions is often constrained
by instability arising from their inherent stochasticity. In this paper, we
revisit time series diffusion models and present a comprehensive framework that
encompasses most existing diffusion-based methods. Building on this theoretical
foundation, we propose a novel diffusion-based time series forecasting model,
the Series-to-Series Diffusion Bridge Model ($\mathrm{S^2DBM}$), which
leverages the Brownian Bridge process to reduce randomness in reverse
estimations and improves accuracy by incorporating informative priors and
conditions derived from historical time series data. Experimental results
demonstrate that $\mathrm{S^2DBM}$ delivers superior performance in
point-to-point forecasting and competes effectively with other diffusion-based
models in probabilistic forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-R: A Framework for Domain-Adaptive Maintenance Scheme Generation
  Combining Hierarchical Agents and RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laifa Tao, Qixuan Huang, Xianjun Wu, Weiwei Zhang, Yunlong Wu, Bin Li, Chen Lu, Xingshuo Hai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of smart devices has emphasized the critical role of
maintenance in production activities. Interactive Electronic Technical Manuals
(IETMs) are vital tools that support the maintenance of smart equipment.
However, traditional IETMs face challenges such as transitioning from Graphical
User Interfaces (GUIs) to natural Language User Interfaces (LUIs) and managing
complex logical relationships. Additionally, they must meet the current demands
for higher intelligence. This paper proposes a Maintenance Scheme Generation
Method based on Large Language Models (LLM-R). The proposed method includes
several key innovations: We propose the Low Rank Adaptation-Knowledge Retention
(LORA-KR) loss technology to proportionally adjust mixed maintenance data for
fine-tuning the LLM. This method prevents knowledge conflicts caused by mixed
data, improving the model's adaptability and reasoning ability in specific
maintenance domains, Besides, Hierarchical Task-Based Agent and
Instruction-level Retrieval-Augmented Generation (RAG) technologies are adopted
to optimize the generation steps and mitigate the phenomenon of hallucination
caused by the model's Inability to access contextual information. This
enhancement improves the model's flexibility and accuracy in handling known or
unknown maintenance objects and maintenance scheme scenarios. To validate the
proposed method's effectiveness in maintenance tasks, a maintenance scheme
dataset was constructed using objects from different fields. The experimental
results show that the accuracy of the maintenance schemes generated by the
proposed method reached 91.59%, indicating which improvement enhances the
intelligence of maintenance schemes and introduces novel technical approaches
for equipment maintenance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Adaptive Agent Training in Open-Ended Simulators by Targeting
  Diversity <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robby Costales, Stefanos Nikolaidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The wider application of end-to-end learning methods to embodied
decision-making domains remains bottlenecked by their reliance on a
superabundance of training data representative of the target domain.
Meta-reinforcement learning (meta-RL) approaches abandon the aim of zero-shot
generalization--the goal of standard reinforcement learning (RL)--in favor of
few-shot adaptation, and thus hold promise for bridging larger generalization
gaps. While learning this meta-level adaptive behavior still requires
substantial data, efficient environment simulators approaching real-world
complexity are growing in prevalence. Even so, hand-designing sufficiently
diverse and numerous simulated training tasks for these complex domains is
prohibitively labor-intensive. Domain randomization (DR) and procedural
generation (PG), offered as solutions to this problem, require simulators to
possess carefully-defined parameters which directly translate to meaningful
task diversity--a similarly prohibitive assumption. In this work, we present
DIVA, an evolutionary approach for generating diverse training tasks in such
complex, open-ended simulators. Like unsupervised environment design (UED)
methods, DIVA can be applied to arbitrary parameterizations, but can
additionally incorporate realistically-available domain knowledge--thus
inheriting the flexibility and generality of UED, and the supervised structure
embedded in well-designed simulators exploited by DR and PG. Our empirical
results showcase DIVA's unique ability to overcome complex parameterizations
and successfully train adaptive agent behavior, far outperforming competitive
baselines from prior literature. These findings highlight the potential of such
semi-supervised environment design (SSED) approaches, of which DIVA is the
first humble constituent, to enable training in realistic simulated domains,
and produce more robust and capable adaptive agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>-Guided Monte Carlo Tree Search for Symbolic Regression in Financial
  Fraud Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prashank Kadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing number of financial services available online, the rate
of financial fraud has also been increasing. The traffic and transaction rates
on the internet have increased considerably, leading to a need for fast
decision-making. Financial institutions also have stringent regulations that
often require transparency and explainability of the decision-making process.
However, most state-of-the-art algorithms currently used in the industry are
highly parameterized black-box models that rely on complex computations to
generate a score. These algorithms are inherently slow and lack the
explainability and speed of traditional rule-based learners. This work
introduces SR-MCTS (Symbolic Regression MCTS), which utilizes a foundational
GPT model to guide the MCTS, significantly enhancing its convergence speed and
the quality of the generated expressions which are further extracted to rules.
Our experiments show that SR-MCTS can detect fraud more efficiently than widely
used methods in the industry while providing substantial insights into the
decision-making process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM International Conference on Information and Knowledge Management
  2024 RAG - Enterprise</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Fairness of Generative Mobility Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Wang, Jack McFarland, Afra Mashhadi, Ekin Ugurel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work examines the fairness of generative mobility models, addressing the
often overlooked dimension of equity in model performance across geographic
regions. Predictive models built on crowd flow data are instrumental in
understanding urban structures and movement patterns; however, they risk
embedding biases, particularly in spatiotemporal contexts where model
performance may reflect and reinforce existing inequities tied to geographic
distribution. We propose a novel framework for assessing fairness by measuring
the utility and equity of generated traces. Utility is assessed via the Common
Part of Commuters (CPC), a similarity metric comparing generated and real
mobility flows, while fairness is evaluated using demographic parity. By
reformulating demographic parity to reflect the difference in CPC distribution
between two groups, our analysis reveals disparities in how various models
encode biases present in the underlying data. We utilized four models (Gravity,
Radiation, Deep Gravity, and Non-linear Gravity) and our results indicate that
traditional gravity and radiation models produce fairer outcomes, although Deep
Gravity achieves higher CPC. This disparity underscores a trade-off between
model accuracy and equity, with the feature-rich Deep Gravity model amplifying
pre-existing biases in community representations. Our findings emphasize the
importance of integrating fairness metrics in mobility modeling to avoid
perpetuating inequities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages, Accepted at the Network Mobility (NetMob) 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws for Pre-training Agents and World Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Pearce, Tabish Rashid, Dave Bignell, Raluca Georgescu, Sam Devlin, Katja Hofmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of embodied agents has been shown to improve by increasing
model parameters, dataset size, and compute. This has been demonstrated in
domains from robotics to video games, when generative learning objectives on
offline datasets (pre-training) are used to model an agent's behavior
(imitation learning) or their environment (world modeling). This paper
characterizes the role of scale in these tasks more precisely. Going beyond the
simple intuition that `bigger is better', we show that the same types of power
laws found in language modeling (e.g. between loss and optimal model size),
also arise in world modeling and imitation learning. However, the coefficients
of these laws are heavily influenced by the tokenizer, task \& architecture --
this has important implications on the optimal sizing of models and data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Unifying Interpretability and Control: Evaluation via
  Intervention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usha Bhalla, Suraj Srinivas, Asma Ghandeharioun, Himabindu Lakkaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing complexity and capability of large language models, a need
to understand model reasoning has emerged, often motivated by an underlying
goal of controlling and aligning models. While numerous interpretability and
steering methods have been proposed as solutions, they are typically designed
either for understanding or for control, seldom addressing both, with the
connection between interpretation and control more broadly remaining tenuous.
Additionally, the lack of standardized applications, motivations, and
evaluation metrics makes it difficult to assess these methods' practical
utility and efficacy. To address this, we propose intervention as a fundamental
goal of interpretability and introduce success criteria to evaluate how well
methods are able to control model behavior through interventions. We unify and
extend four popular interpretability methods--sparse autoencoders, logit lens,
tuned lens, and probing--into an abstract encoder-decoder framework. This
framework maps intermediate latent representations to human-interpretable
feature spaces, enabling interventions on these interpretable features, which
can then be mapped back to latent representations to control model outputs. We
introduce two new evaluation metrics: intervention success rate and the
coherence-intervention tradeoff, designed to measure the accuracy of
explanations and their utility in controlling model behavior. Our findings
reveal that (1) although current methods allow for intervention, they are
inconsistent across models and features, (2) lens-based methods outperform
others in achieving simple, concrete interventions, and (3) interventions often
compromise model performance and coherence, underperforming simpler
alternatives, such as prompting, for steering model behavior and highlighting a
critical shortcoming of current interpretability approaches in real-world
applications requiring control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Abnormal Stop Detection for Long Distance Coaches with
  Low-Frequency GPS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Deng, Junbiao Pang, Jiayu Xu, Haitao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our urban life, long distance coaches supply a convenient yet economic
approach to the transportation of the public. One notable problem is to
discover the abnormal stop of the coaches due to the important reason, i.e.,
illegal pick up on the way which possibly endangers the safety of passengers.
It has become a pressing issue to detect the coach abnormal stop with
low-quality GPS. In this paper, we propose an unsupervised method that helps
transportation managers to efficiently discover the Abnormal Stop Detection
(ASD) for long distance coaches. Concretely, our method converts the ASD
problem into an unsupervised clustering framework in which both the normal stop
and the abnormal one are decomposed. Firstly, we propose a stop duration model
for the low frequency GPS based on the assumption that a coach changes speed
approximately in a linear approach. Secondly, we strip the abnormal stops from
the normal stop points by the low rank assumption. The proposed method is
conceptually simple yet efficient, by leveraging low rank assumption to handle
normal stop points, our approach enables domain experts to discover the ASD for
coaches, from a case study motivated by traffic managers. Datset and code are
publicly available at: https://github.com/pangjunbiao/IPPs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational <span class="highlight-title">Low-Rank Adaptation</span> Using IVON <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bai Cong, Nico Daheim, Yuesong Shen, Daniel Cremers, Rio Yokota, Mohammad Emtiyaz Khan, Thomas Möllenhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that variational learning can significantly improve the accuracy and
calibration of Low-Rank Adaptation (LoRA) without a substantial increase in the
cost. We replace AdamW by the Improved Variational Online Newton (IVON)
algorithm to finetune large language models. For Llama-2 with 7 billion
parameters, IVON improves the accuracy over AdamW by 2.8% and expected
calibration error by 4.6%. The accuracy is also better than the other Bayesian
alternatives, yet the cost is lower and the implementation is easier. Our work
provides additional evidence for the effectiveness of IVON for large language
models. The code is available at
https://github.com/team-approx-bayes/ivon-lora.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at 38th Workshop on Fine-Tuning in Machine Learning
  (NeurIPS 2024). Code available at
  https://github.com/team-approx-bayes/ivon-lora</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BendVLM: Test-Time Debiasing of Vision-Language Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walter Gerych, Haoran Zhang, Kimia Hamidieh, Eileen Pan, Maanas Sharma, Thomas Hartvigsen, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language model (VLM) embeddings have been shown to encode biases
present in their training data, such as societal biases that prescribe negative
characteristics to members of various racial and gender identities. VLMs are
being quickly adopted for a variety of tasks ranging from few-shot
classification to text-guided image generation, making debiasing VLM embeddings
crucial. Debiasing approaches that fine-tune the VLM often suffer from
catastrophic forgetting. On the other hand, fine-tuning-free methods typically
utilize a "one-size-fits-all" approach that assumes that correlation with the
spurious attribute can be explained using a single linear direction across all
possible inputs. In this work, we propose Bend-VLM, a nonlinear,
fine-tuning-free approach for VLM embedding debiasing that tailors the
debiasing operation to each unique input. This allows for a more flexible
debiasing approach. Additionally, we do not require knowledge of the set of
inputs a priori to inference time, making our method more appropriate for
online, open-set tasks such as retrieval and text guided image generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Bayesian Mixture Model of Temporal Point Processes with Determinantal
  Point Process Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Dong, Shaoxin Ye, Yuwen Cao, Qiyu Han, Hongteng Xu, Hanfang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Asynchronous event sequence clustering aims to group similar event sequences
in an unsupervised manner. Mixture models of temporal point processes have been
proposed to solve this problem, but they often suffer from overfitting, leading
to excessive cluster generation with a lack of diversity. To overcome these
limitations, we propose a Bayesian mixture model of Temporal Point Processes
with Determinantal Point Process prior (TP$^2$DP$^2$) and accordingly an
efficient posterior inference algorithm based on conditional Gibbs sampling.
Our work provides a flexible learning framework for event sequence clustering,
enabling automatic identification of the potential number of clusters and
accurate grouping of sequences with similar features. It is applicable to a
wide range of parametric temporal point processes, including neural
network-based models. Experimental results on both synthetic and real-world
data suggest that our framework could produce moderately fewer yet more diverse
mixture components, and achieve outstanding results across multiple evaluation
metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remote Sensing-Based Assessment of Economic Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijian Pan, Yongchang Ma, Bolin Shen, Linyang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of our project is to use satellite data (including nighttime light
data and remote sensing images) to give us some statistical estimation of the
economic development level of a selected area (Singapore). Findings from the
project could inform policymakers about areas needing intervention or support
for economic development initiatives. Insights gained might aid in targeted
policy formulation for infrastructure, agriculture, urban planning, or resource
management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical-Computational Trade-offs for Greedy Recursive Partitioning
  Estimators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Shuo Tan, Jason M. Klusowski, Krishnakumar Balasubramanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models based on recursive partitioning such as decision trees and their
ensembles are popular for high-dimensional regression as they can potentially
avoid the curse of dimensionality. Because empirical risk minimization (ERM) is
computationally infeasible, these models are typically trained using greedy
algorithms. Although effective in many cases, these algorithms have been
empirically observed to get stuck at local optima. We explore this phenomenon
in the context of learning sparse regression functions over $d$ binary
features, showing that when the true regression function $f^*$ does not satisfy
the so-called Merged Staircase Property (MSP), greedy training requires
$\exp(\Omega(d))$ to achieve low estimation error. Conversely, when $f^*$ does
satisfy MSP, greedy training can attain small estimation error with only
$O(\log d)$ samples. This performance mirrors that of two-layer neural networks
trained with stochastic gradient descent (SGD) in the mean-field regime,
thereby establishing a head-to-head comparison between SGD-trained neural
networks and greedy recursive partitioning estimators. Furthermore, ERM-trained
recursive partitioning estimators achieve low estimation error with $O(\log d)$
samples irrespective of whether $f^*$ satisfies MSP, thereby demonstrating a
statistical-computational trade-off for greedy training. Our proofs are based
on a novel interpretation of greedy recursive partitioning using stochastic
process theory and a coupling technique that may be of independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Gap: Representation Spaces in Neuro-Symbolic AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Victor S. Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuro-symbolic AI is an effective method for improving the overall
performance of AI models by combining the advantages of neural networks and
symbolic learning. However, there are differences between the two in terms of
how they process data, primarily because they often use different data
representation methods, which is often an important factor limiting the overall
performance of the two. From this perspective, we analyzed 191 studies from
2013 by constructing a four-level classification framework. The first level
defines five types of representation spaces, and the second level focuses on
five types of information modalities that the representation space can
represent. Then, the third level describes four symbolic logic methods.
Finally, the fourth-level categories propose three collaboration strategies
between neural networks and symbolic learning. Furthermore, we conducted a
detailed analysis of 46 research based on their representation space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximate Frank-Wolfe Algorithm over Graph-structured Support Set 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijian Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this project, we reviewed a paper that deals graph-structured convex
optimization (GSCO) problem with the approximate Frank-Wolfe (FW) algorithm. We
analyzed and implemented the original algorithm and introduced some extensions
based on that. Then we conducted experiments to compare the results and
concluded that our backtracking line-search method effectively reduced the
number of iterations, while our new DMO method (Top-g+ optimal visiting) did
not make satisfying enough improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlearning in- vs. out-of-distribution data in LLMs under gradient-based
  method <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teodora Baluta, Pascal Lamblin, Daniel Tarlow, Fabian Pedregosa, Gintare Karolina Dziugaite
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning aims to solve the problem of removing the influence of
selected training examples from a learned model. Despite the increasing
attention to this problem, it remains an open research question how to evaluate
unlearning in large language models (LLMs), and what are the critical
properties of the data to be unlearned that affect the quality and efficiency
of unlearning. This work formalizes a metric to evaluate unlearning quality in
generative models, and uses it to assess the trade-offs between unlearning
quality and performance. We demonstrate that unlearning out-of-distribution
examples requires more unlearning steps but overall presents a better trade-off
overall. For in-distribution examples, however, we observe a rapid decay in
performance as unlearning progresses. We further evaluate how example's
memorization and difficulty affect unlearning under a classical gradient
ascent-based approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Safe Generative AI Workshop @ NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuro-Symbolic AI: Explainability, Challenges, and Future Trends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Victor S. Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability is an essential reason limiting the application of neural
networks in many vital fields. Although neuro-symbolic AI hopes to enhance the
overall explainability by leveraging the transparency of symbolic learning, the
results are less evident than imagined. This article proposes a classification
for explainability by considering both model design and behavior of 191 studies
from 2013, focusing on neuro-symbolic AI, hoping to inspire scholars who want
to understand the explainability of neuro-symbolic AI. Precisely, we classify
them into five categories by considering whether the form of bridging the
representation differences is readable as their design factor, if there are
representation differences between neural networks and symbolic logic learning,
and whether a model decision or prediction process is understandable as their
behavior factor: implicit intermediate representations and implicit prediction,
partially explicit intermediate representations and partially explicit
prediction, explicit intermediate representations or explicit prediction,
explicit intermediate representation and explicit prediction, unified
representation and explicit prediction. We also analyzed the research trends
and three significant challenges: unified representations, explainability and
transparency, and sufficient cooperation from neural networks and symbolic
learning. Finally, we put forward suggestions for future research in three
aspects: unified representations, enhancing model explainability, ethical
considerations, and social impact.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Traj<span class="highlight-title">GPT</span>: Controlled Synthetic Trajectory Generation Using a Multitask
  <span class="highlight-title">Transformer</span>-Based Spatiotemporal Model <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shang-Ling Hsu, Emmanuel Tung, John Krumm, Cyrus Shahabi, Khurram Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mobility modeling from GPS-trajectories and synthetic trajectory
generation are crucial for various applications, such as urban planning,
disaster management and epidemiology. Both of these tasks often require filling
gaps in a partially specified sequence of visits - a new problem that we call
"controlled" synthetic trajectory generation. Existing methods for
next-location prediction or synthetic trajectory generation cannot solve this
problem as they lack the mechanisms needed to constrain the generated sequences
of visits. Moreover, existing approaches (1) frequently treat space and time as
independent factors, an assumption that fails to hold true in real-world
scenarios, and (2) suffer from challenges in accuracy of temporal prediction as
they fail to deal with mixed distributions and the inter-relationships of
different modes with latent variables (e.g., day-of-the-week). These
limitations become even more pronounced when the task involves filling gaps
within sequences instead of solely predicting the next visit.
  We introduce TrajGPT, a transformer-based, multi-task, joint spatiotemporal
generative model to address these issues. Taking inspiration from large
language models, TrajGPT poses the problem of controlled trajectory generation
as that of text infilling in natural language. TrajGPT integrates the spatial
and temporal models in a transformer architecture through a Bayesian
probability model that ensures that the gaps in a visit sequence are filled in
a spatiotemporally consistent manner. Our experiments on public and private
datasets demonstrate that TrajGPT not only excels in controlled synthetic visit
generation but also outperforms competing models in next-location prediction
tasks - Relatively, TrajGPT achieves a 26-fold improvement in temporal accuracy
while retaining more than 98% of spatial accuracy on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 32nd ACM SIGSPATIAL International Conference on
  Advances in Geographic Information Systems (ACM SIGSPATIAL 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Game-Theoretic Defenses for Robust Conformal Prediction Against
  Adversarial Attacks in Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Luo, Jie Bao, Zhixin Zhou, Chuangyin Dang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks pose significant threats to the reliability and safety of
deep learning models, especially in critical domains such as medical imaging.
This paper introduces a novel framework that integrates conformal prediction
with game-theoretic defensive strategies to enhance model robustness against
both known and unknown adversarial perturbations. We address three primary
research questions: constructing valid and efficient conformal prediction sets
under known attacks (RQ1), ensuring coverage under unknown attacks through
conservative thresholding (RQ2), and determining optimal defensive strategies
within a zero-sum game framework (RQ3). Our methodology involves training
specialized defensive models against specific attack types and employing
maximum and minimum classifiers to aggregate defenses effectively. Extensive
experiments conducted on the MedMNIST datasets, including PathMNIST,
OrganAMNIST, and TissueMNIST, demonstrate that our approach maintains high
coverage guarantees while minimizing prediction set sizes. The game-theoretic
analysis reveals that the optimal defensive strategy often converges to a
singular robust model, outperforming uniform and simple strategies across all
evaluated datasets. This work advances the state-of-the-art in uncertainty
quantification and adversarial robustness, providing a reliable mechanism for
deploying deep learning models in adversarial environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Large Language Models with Integer Sequence Generation
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel O'Malley, Manish Bhattarai, Javier Santos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel benchmark where the large language model (LLM)
must write code that computes integer sequences from the Online Encyclopedia of
Integer Sequences (OEIS), a widely-used resource for mathematical sequences.
The benchmark is designed to evaluate both the correctness of the generated
code and its computational efficiency. Our benchmark reveals that the o1 series
of models outperform other frontier models from OpenAI, Anthropic, Meta, and
Google in accuracy and cheating rates across both easy and hard integer
sequences. In order to ensure models do not exploit memorized sequence values,
we introduce an automated cheating detection mechanism that flags the use of
lookup tables and validated this automation against human cheating evaluations.
This benchmark provides a meaningful challenge for current LLMs, offering
insights into their mathematical reasoning and code writing capabilities, which
can guide future research directions and model development in mathematical
reasoning and code synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComFairGNN: Community Fair Graph Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonas Sium, Qi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have become the leading approach for addressing
graph analytical problems in various real-world scenarios. However, GNNs may
produce biased predictions against certain demographic subgroups due to node
attributes and neighbors surrounding a node. Most current research on GNN
fairness focuses predominantly on debiasing GNNs using oversimplified fairness
evaluation metrics, which can give a misleading impression of fairness.
Understanding the potential evaluation paradoxes due to the complicated nature
of the graph structure is crucial for developing effective GNN debiasing
mechanisms. In this paper, we examine the effectiveness of current GNN
debiasing methods in terms of unfairness evaluation. Specifically, we introduce
a community-level strategy to measure bias in GNNs and evaluate debiasing
methods at this level. Further, We introduce ComFairGNN, a novel framework
designed to mitigate community-level bias in GNNs. Our approach employs a
learnable coreset-based debiasing function that addresses bias arising from
diverse local neighborhood distributions during GNNs neighborhood aggregation.
Comprehensive evaluations on three benchmark datasets demonstrate our model's
effectiveness in both accuracy and fairness metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust and Efficient Fine-tuning of LLMs with Bayesian
  Reparameterization of <span class="highlight-title">Low-Rank Adaptation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Seth, Arinjay Pathak, Ayan Sengupta, Natraj Raman, Sriram Gopalakrishnan, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are highly resource-intensive to fine-tune due
to their enormous size. While low-rank adaptation is a prominent
parameter-efficient fine-tuning approach, it suffers from sensitivity to
hyperparameter choices, leading to instability in model performance on
fine-tuning downstream tasks. This paper highlights the importance of effective
parameterization in low-rank fine-tuning to reduce estimator variance and
enhance the stability of final model outputs. We propose MonteCLoRA, an
efficient fine-tuning technique, employing Monte Carlo estimation to learn an
unbiased posterior estimation of low-rank parameters with low expected
variance, which stabilizes fine-tuned LLMs with only O(1) additional
parameters. MonteCLoRA shows significant improvements in accuracy and
robustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness
than existing efficient fine-tuning methods on natural language understanding
tasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with
pre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance
with 50% lower variance than the contemporary efficient fine-tuning methods.
The theoretical and empirical results presented in the paper underscore how
parameterization and hyperpriors balance exploration-exploitation in the
low-rank parametric space, therefore leading to more optimal and robust
parameter estimation during efficient fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 10 figures, 10 tables, Code:
  https://github.com/LCS2-IIITD/MonteCLoRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaGSL: Global-augmented Graph Structure Learning via Graph Information
  Bottleneck 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangjie Li, Jiangqing Song, Baoming Zhang, Gaoli Ruan, Junyuan Xie, Chongjun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) are prominent for their effectiveness in
processing graph data for semi-supervised node classification tasks. Most works
of GNNs assume that the observed structure accurately represents the underlying
node relationships. However, the graph structure is inevitably noisy or
incomplete in reality, which can degrade the quality of graph representations.
Therefore, it is imperative to learn a clean graph structure that balances
performance and robustness. In this paper, we propose a novel method named
\textit{Global-augmented Graph Structure Learning} (GaGSL), guided by the Graph
Information Bottleneck (GIB) principle. The key idea behind GaGSL is to learn a
compact and informative graph structure for node classification tasks.
Specifically, to mitigate the bias caused by relying solely on the original
structure, we first obtain augmented features and augmented structure through
global feature augmentation and global structure augmentation. We then input
the augmented features and augmented structure into a structure estimator with
different parameters for optimization and re-definition of the graph structure,
respectively. The redefined structures are combined to form the final graph
structure. Finally, we employ GIB based on mutual information to guide the
optimization of the graph structure to obtain the minimum sufficient graph
structure. Comprehensive evaluations across a range of datasets reveal the
outstanding performance and robustness of GaGSL compared with the
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of white noise in artificial neural networks trained for
  classification: performance and noise mitigation strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadezhda Semenova, Daniel Brunner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the hardware implementation of neural networks, leveraging
physical coupling and analog neurons has substantially increased in relevance.
Such nonlinear and complex physical networks provide significant advantages in
speed and energy efficiency, but are potentially susceptible to internal noise
when compared to digital emulations of such networks. In this work, we consider
how additive and multiplicative Gaussian white noise on the neuronal level can
affect the accuracy of the network when applied for specific tasks and
including a softmax function in the readout layer. We adapt several noise
reduction techniques to the essential setting of classification tasks, which
represent a large fraction of neural network computing. We find that these
adjusted concepts are highly effective in mitigating the detrimental impact of
noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classification with Conceptual Safeguards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailey Joren, Charles Marx, Berk Ustun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach to promote safety in classification tasks with
established concepts. Our approach -- called a conceptual safeguard -- acts as
a verification layer for models that predict a target outcome by first
predicting the presence of intermediate concepts. Given this architecture, a
safeguard ensures that a model meets a minimal level of accuracy by abstaining
from uncertain predictions. In contrast to a standard selective classifier, a
safeguard provides an avenue to improve coverage by allowing a human to confirm
the presence of uncertain concepts on instances on which it abstains. We
develop methods to build safeguards that maximize coverage without compromising
safety, namely techniques to propagate the uncertainty in concept predictions
and to flag salient concepts for human review. We benchmark our approach on a
collection of real-world and synthetic datasets, showing that it can improve
performance and coverage in deep learning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing classroom teaching with LLMs and RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizabeth A Mullins, Adrian Portillo, Kristalys Ruiz-Rohena, Aritran Piplai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have become a valuable source of information for our
daily inquiries. However, after training, its data source quickly becomes
out-of-date, making RAG a useful tool for providing even more recent or
pertinent data. In this work, we investigate how RAG pipelines, with the course
materials serving as a data source, might help students in K-12 education. The
initial research utilizes Reddit as a data source for up-to-date cybersecurity
information. Chunk size is evaluated to determine the optimal amount of context
needed to generate accurate answers. After running the experiment for different
chunk sizes, answer correctness was evaluated using RAGAs with average answer
correctness not exceeding 50 percent for any chunk size. This suggests that
Reddit is not a good source to mine for data for questions about cybersecurity
threats. The methodology was successful in evaluating the data source, which
has implications for its use to evaluate educational resources for
effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws for Precision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanishq Kumar, Zachary Ankner, Benjamin F. Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Ré, Aditi Raghunathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low precision training and inference affect both the quality and cost of
language models, but current scaling laws do not account for this. In this
work, we devise "precision-aware" scaling laws for both training and inference.
We propose that training in lower precision reduces the model's "effective
parameter count," allowing us to predict the additional loss incurred from
training in low precision and post-train quantization. For inference, we find
that the degradation introduced by post-training quantization increases as
models are trained on more data, eventually making additional pretraining data
actively harmful. For training, our scaling laws allow us to predict the loss
of a model with different parts in different precisions, and suggest that
training larger models in lower precision may be compute optimal. We unify the
scaling laws for post and pretraining quantization to arrive at a single
functional form that predicts degradation from training and inference in varied
precisions. We fit on over 465 pretraining runs and validate our predictions on
model sizes up to 1.7B parameters trained on up to 26B tokens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Analysis of U-Net-based models for Segmentation of Cardiac
  MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09980v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09980v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ketan Suhaas Saichandran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical imaging refers to the technologies and methods utilized to view the
human body and its inside, in order to diagnose, monitor, or even treat medical
disorders. This paper aims to explore the application of deep learning
techniques in the semantic segmentation of Cardiac short-axis MRI (Magnetic
Resonance Imaging) images, aiming to enhance the diagnosis, monitoring, and
treatment of medical disorders related to the heart. The focus centers on
implementing various architectures that are derivatives of U-Net, to
effectively isolate specific parts of the heart for comprehensive anatomical
and functional analysis. Through a combination of images, graphs, and
quantitative metrics, the efficacy of the models and their predictions are
showcased. Additionally, this paper addresses encountered challenges and
outline strategies for future improvements. This abstract provides a concise
overview of the efforts in utilizing deep learning for cardiac image
segmentation, emphasizing both the accomplishments and areas for further
refinement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-Models: An Architecture for Decoding LLM Behaviors Through
  Interpreted Embeddings and Natural Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02472v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02472v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Costarelli, Mat Allen, Severin Field
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) become increasingly integrated into our daily
lives, the potential harms from deceptive behavior underlie the need for
faithfully interpreting their decision-making. While traditional probing
methods have shown some effectiveness, they remain best for narrowly scoped
tasks while more comprehensive explanations are still necessary. To this end,
we investigate meta-models-an architecture using a "meta-model" that takes
activations from an "input-model" and answers natural language questions about
the input-model's behaviors. We evaluate the meta-model's ability to generalize
by training them on selected task types and assessing their out-of-distribution
performance in deceptive scenarios. Our findings show that meta-models
generalize well to out-of-distribution tasks and point towards opportunities
for future research in this area. Our code is available at
https://github.com/acostarelli/meta-models-public .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GD doesn't make the cut: Three ways that non-differentiability affects
  neural network training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08426v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08426v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Krishna Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper critically examines the fundamental distinctions between gradient
methods applied to non-differentiable functions (NGDMs) and classical gradient
descents (GDs) for differentiable functions, revealing significant gaps in
current deep learning optimization theory. We demonstrate that NGDMs exhibit
markedly different convergence properties compared to GDs, strongly challenging
the applicability of extensive neural network convergence literature based on
$L-smoothness$ to non-smooth neural networks. Our analysis reveals paradoxical
behavior of NDGM solutions for $L_{1}$-regularized problems, where increasing
regularization counterintuitively leads to larger $L_{1}$ norms of optimal
solutions. This finding calls into question widely adopted $L_{1}$ penalization
techniques for network pruning. We further challenge the common assumption that
optimization algorithms like RMSProp behave similarly in differentiable and
non-differentiable contexts. Expanding on the Edge of Stability phenomenon, we
demonstrate its occurrence in a broader class of functions, including Lipschitz
continuous convex differentiable functions. This finding raises important
questions about its relevance and interpretation in non-convex,
non-differentiable neural networks, particularly those using ReLU activations.
Our work identifies critical misunderstandings of NDGMs in influential
literature, stemming from an overreliance on strong smoothness assumptions.
These findings necessitate a reevaluation of optimization dynamics in deep
learning, emphasizing the crucial need for more nuanced theoretical foundations
in analyzing these complex systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Law Hypothesis for Multimodal Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06754v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06754v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyun Sun, Zhen Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a scaling law hypothesis for multimodal models processing text,
audio, images, and video within a shared token and embedding space. Our
framework predicts model performance based on modality-specific compression and
tokenization efficiency, extending established scaling laws from text-based
decoder models to mixed-modality systems. We explore whether leveraging more
training data in multiple modalities can reduce the size of the multimodal
model, enabling efficient deployment on resource-constrained devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptions of Linguistic Uncertainty by Language Models and Humans <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Catarina G Belem, Markelle Kelly, Mark Steyvers, Sameer Singh, Padhraic Smyth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  _Uncertainty expressions_ such as "probably" or "highly unlikely" are
pervasive in human language. While prior work has established that there is
population-level agreement in terms of how humans quantitatively interpret
these expressions, there has been little inquiry into the abilities of language
models in the same context. In this paper, we investigate how language models
map linguistic expressions of uncertainty to numerical responses. Our approach
assesses whether language models can employ theory of mind in this setting:
understanding the uncertainty of another agent about a particular statement,
independently of the model's own certainty about that statement. We find that 7
out of 10 models are able to map uncertainty expressions to probabilistic
responses in a human-like manner. However, we observe systematically different
behavior depending on whether a statement is actually true or false. This
sensitivity indicates that language models are substantially more susceptible
to bias based on their prior knowledge (as compared to humans). These findings
raise important questions and have broad implications for human-AI and AI-AI
communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring QUIC Dynamics: A Large-Scale <span class="highlight-title">Dataset</span> for Encrypted Traffic
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barak Gahtan, Robert J. Shahla, Alex M. Bronstein, Reuven Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  QUIC, a new and increasingly used transport protocol, addresses and resolves
the limitations of TCP by offering improved security, performance, and features
such as stream multiplexing and connection migration. These features, however,
also present challenges for network operators who need to monitor and analyze
web traffic. In this paper, we introduce VisQUIC, a labeled dataset comprising
over 100,000 QUIC traces from more than 44,000 websites (URLs), collected over
a four-month period. These traces provide the foundation for generating more
than seven million images, with configurable parameters of window length, pixel
resolution, normalization, and labels. These images enable an observer looking
at the interactions between a client and a server to analyze and gain insights
about QUIC encrypted connections. To illustrate the dataset's potential, we
offer a use-case example of an observer estimating the number of HTTP/3
responses/requests pairs in a given QUIC, which can reveal server behavior,
client--server interactions, and the load imposed by an observed connection. We
formulate the problem as a discrete regression problem, train a machine
learning (ML) model for it, and then evaluate it using the proposed dataset on
an example use case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The dataset and the supplementary material can be provided upon
  request</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GENOT: Entropic (Gromov) Wasserstein Flow Matching with Applications to
  Single-Cell Genomics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09254v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09254v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Klein, Théo Uscidda, Fabian Theis, Marco Cuturi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-cell genomics has significantly advanced our understanding of cellular
behavior, catalyzing innovations in treatments and precision medicine. However,
single-cell sequencing technologies are inherently destructive and can only
measure a limited array of data modalities simultaneously. This limitation
underscores the need for new methods capable of realigning cells. Optimal
transport (OT) has emerged as a potent solution, but traditional discrete
solvers are hampered by scalability, privacy, and out-of-sample estimation
issues. These challenges have spurred the development of neural network-based
solvers, known as neural OT solvers, that parameterize OT maps. Yet, these
models often lack the flexibility needed for broader life science applications.
To address these deficiencies, our approach learns stochastic maps (i.e.
transport plans), allows for any cost function, relaxes mass conservation
constraints and integrates quadratic solvers to tackle the complex challenges
posed by the (Fused) Gromov-Wasserstein problem. Utilizing flow matching as a
backbone, our method offers a flexible and effective framework. We demonstrate
its versatility and robustness through applications in cell development
studies, cellular drug response modeling, and cross-modality cell translation,
illustrating significant potential for enhancing therapeutic strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C3T: Cross-modal Transfer Through Time for Human Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhi Kamboj, Anh Duy Nguyen, Minh Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to unlock the potential of diverse sensors, we investigate a method
to transfer knowledge between modalities using the structure of a unified
multimodal representation space for Human Action Recognition (HAR). We
formalize and explore an understudied cross-modal transfer setting we term
Unsupervised Modality Adaptation (UMA), where the modality used in testing is
not used in supervised training, i.e. zero labeled instances of the test
modality are available during training. We develop three methods to perform
UMA: Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer
Through Time (C3T). Our extensive experiments on various camera+IMU datasets
compare these methods to each other in the UMA setting, and to their empirical
upper bound in the supervised setting. The results indicate C3T is the most
robust and highest performing by at least a margin of 8%, and nears the
supervised setting performance even in the presence of temporal noise. This
method introduces a novel mechanism for aligning signals across time-varying
latent vectors, extracted from the receptive field of temporal convolutions.
Our findings suggest that C3T has significant potential for developing
generalizable models for time-series sensor data, opening new avenues for
multi-modal learning in various applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active-Dormant Attention Heads: Mechanistically Demystifying
  Extreme-Token Phenomena in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Guo, Druv Pai, Yu Bai, Jiantao Jiao, Michael I. Jordan, Song Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practitioners have consistently observed three puzzling phenomena in
transformer-based large language models (LLMs): attention sinks, value-state
drains, and residual-state peaks, collectively referred to as extreme-token
phenomena. These phenomena are characterized by certain so-called "sink tokens"
receiving disproportionately high attention weights, exhibiting significantly
smaller value states, and having much larger residual-state norms than those of
other tokens. These extreme tokens give rise to various challenges in LLM
inference, quantization, and interpretability.
  We elucidate the mechanisms behind extreme-token phenomena. First, we show
that these phenomena arise in very simple architectures -- transformers with
one to three layers -- trained on a toy model, the Bigram-Backcopy (BB) task.
In this setting, we identify an active-dormant mechanism, where attention heads
become sinks for specific input domains while remaining non-sinks for others.
Our theoretical analysis of the training dynamics reveals that these phenomena
are driven by a mutual reinforcement mechanism. Building on these insights, we
propose strategies to mitigate extreme-token phenomena during pretraining,
including replacing softmax with ReLU and Adam with SGD. Next, we extend our
analysis to pretrained LLMs, including Llama and OLMo, showing that many
attention heads exhibit a similar active-dormant mechanism as in the BB task,
and that the mutual reinforcement mechanism also governs the emergence of
extreme-token phenomena during LLM pretraining. Our results reveal that many of
the static and dynamic properties of extreme-token phenomena predicted by the
BB task align with observations in pretrained LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An efficient likelihood-free Bayesian inference method based on
  sequential neural posterior estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12530v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12530v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Xiong, Xiliang Yang, Sanguo Zhang, Zhijian He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential neural posterior estimation (SNPE) techniques have been recently
proposed for dealing with simulation-based models with intractable likelihoods.
Unlike approximate Bayesian computation, SNPE techniques learn the posterior
from sequential simulation using neural network-based conditional density
estimators by minimizing a specific loss function. The SNPE method proposed by
Lueckmann et al. (2017) used a calibration kernel to boost the sample weights
around the observed data, resulting in a concentrated loss function. However,
the use of calibration kernels may increase the variances of both the empirical
loss and its gradient, making the training inefficient. To improve the
stability of SNPE, this paper proposes to use an adaptive calibration kernel
and several variance reduction techniques. The proposed method greatly speeds
up the process of training and provides a better approximation of the posterior
than the original SNPE method and some existing competitors as confirmed by
numerical experiments. We also manage to demonstrate the superiority of the
proposed method for a high-dimensional model with real-world dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topological obstruction to the training of shallow ReLU neural networks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Nurisso, Pierrick Leroy, Francesco Vaccarino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studying the interplay between the geometry of the loss landscape and the
optimization trajectories of simple neural networks is a fundamental step for
understanding their behavior in more complex settings. This paper reveals the
presence of topological obstruction in the loss landscape of shallow ReLU
neural networks trained using gradient flow. We discuss how the homogeneous
nature of the ReLU activation function constrains the training trajectories to
lie on a product of quadric hypersurfaces whose shape depends on the particular
initialization of the network's parameters. When the neural network's output is
a single scalar, we prove that these quadrics can have multiple connected
components, limiting the set of reachable parameters during training. We
analytically compute the number of these components and discuss the possibility
of mapping one to the other through neuron rescaling and permutation. In this
simple setting, we find that the non-connectedness results in a topological
obstruction, which, depending on the initialization, can make the global
optimum unreachable. We validate this result with numerical experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 5 figures, Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Latent Space Dynamics with Model-Form Uncertainties: A
  Stochastic Reduced-Order Modeling Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Yi Yong, Rudy Geelen, Johann Guilleminot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a probabilistic approach to represent and quantify
model-form uncertainties in the reduced-order modeling of complex systems using
operator inference techniques. Such uncertainties can arise in the selection of
an appropriate state-space representation, in the projection step that
underlies many reduced-order modeling methods, or as a byproduct of
considerations made during training, to name a few. Following previous works in
the literature, the proposed method captures these uncertainties by expanding
the approximation space through the randomization of the projection matrix.
This is achieved by combining Riemannian projection and retraction operators -
acting on a subset of the Stiefel manifold - with an information-theoretic
formulation. The efficacy of the approach is assessed on canonical problems in
fluid mechanics by identifying and quantifying the impact of model-form
uncertainties on the inferred operators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The ODE Method for Asymptotic Statistics in Stochastic Approximation and
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.14427v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.14427v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Borkar, Shuhang Chen, Adithya Devraj, Ioannis Kontoyiannis, Sean Meyn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper concerns the $d$-dimensional stochastic approximation recursion, $$
\theta_{n+1}= \theta_n + \alpha_{n + 1} f(\theta_n, \Phi_{n+1}) $$ where $ \{
\Phi_n \}$ is a stochastic process on a general state space, satisfying a
conditional Markov property that allows for parameter-dependent noise. The main
results are established under additional conditions on the mean flow and a
version of the Donsker-Varadhan Lyapunov drift condition known as (DV3):
  {(i)} An appropriate Lyapunov function is constructed that implies
convergence of the estimates in $L_4$.
  {(ii)} A functional central limit theorem (CLT) is established, as well as
the usual one-dimensional CLT for the normalized error. Moment bounds combined
with the CLT imply convergence of the normalized covariance $\textsf{E} [ z_n
z_n^T ]$ to the asymptotic covariance in the CLT, where $z_n{=:}
(\theta_n-\theta^*)/\sqrt{\alpha_n}$.
  {(iii)} The CLT holds for the normalized version $z^{\text{PR}}_n{=:}
\sqrt{n} [\theta^{\text{PR}}_n -\theta^*]$, of the averaged parameters
$\theta^{\text{PR}}_n {=:} n^{-1} \sum_{k=1}^n\theta_k$, subject to standard
assumptions on the step-size. Moreover, the covariance in the CLT coincides
with the minimal covariance of Polyak and Ruppert.
  {(iv)} An example is given where $f$ and $\bar{f}$ are linear in $\theta$,
and $\Phi$ is a geometrically ergodic Markov chain but does not satisfy (DV3).
While the algorithm is convergent, the second moment of $\theta_n$ is unbounded
and in fact diverges.
  {\bf This arXiv version 3 represents a major extension of the results in
prior versions.} The main results now allow for parameter-dependent noise, as
is often the case in applications to reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent <span class="highlight-title">Diffusion</span> Model for Conditional Reservoir Facies Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01968v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01968v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daesoo Lee, Oscar Ovanger, Jo Eidsvik, Erlend Aune, Jacob Skauvold, Ragnar Hauge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating accurate and geologically realistic reservoir facies based on
limited measurements is crucial for field development and reservoir management,
especially in the oil and gas sector. Traditional two-point geostatistics,
while foundational, often struggle to capture complex geological patterns.
Multi-point statistics offers more flexibility, but comes with its own
challenges related to pattern configurations and storage limits. With the rise
of Generative Adversarial Networks (GANs) and their success in various fields,
there has been a shift towards using them for facies generation. However,
recent advances in the computer vision domain have shown the superiority of
diffusion models over GANs. Motivated by this, a novel Latent Diffusion Model
is proposed, which is specifically designed for conditional generation of
reservoir facies. The proposed model produces high-fidelity facies realizations
that rigorously preserve conditioning data. It significantly outperforms a
GAN-based alternative. Our implementation on GitHub:
\url{https://github.com/ML4ITS/Latent-Diffusion-Model-for-Conditional-Reservoir-Facies-Generation}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted in Computers & Geosciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pre-Finetuning for Few-Shot Emotional Speech Recognition <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12921v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12921v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximillian Chen, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech models have long been known to overfit individual speakers for many
classification tasks. This leads to poor generalization in settings where the
speakers are out-of-domain or out-of-distribution, as is common in production
environments. We view speaker adaptation as a few-shot learning problem and
propose investigating transfer learning approaches inspired by recent success
with pre-trained models in natural language tasks. We propose pre-finetuning
speech models on difficult tasks to distill knowledge into few-shot downstream
classification objectives. We pre-finetune Wav2Vec2.0 on every permutation of
four multiclass emotional speech recognition corpora and evaluate our
pre-finetuned models through 33,600 few-shot fine-tuning trials on the
Emotional Speech Dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at INTERSPEECH 2023. 5 pages, 4 figures. Code available at
  https://github.com/maxlchen/Speech-PreFinetuning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by
  Exploring Refusal Loss Landscapes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00867v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00867v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are becoming a prominent generative AI tool,
where the user enters a query and the LLM generates an answer. To reduce harm
and misuse, efforts have been made to align these LLMs to human values using
advanced training techniques such as Reinforcement Learning from Human Feedback
(RLHF). However, recent studies have highlighted the vulnerability of LLMs to
adversarial jailbreak attempts aiming at subverting the embedded safety
guardrails. To address this challenge, this paper defines and investigates the
Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect
jailbreak attempts. Gradient Cuff exploits the unique properties observed in
the refusal loss landscape, including functional values and its smoothness, to
design an effective two-step detection strategy. Experimental results on two
aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak
attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can
significantly improve the LLM's rejection capability for malicious jailbreak
queries, while maintaining the model's performance for benign user queries by
adjusting the detection threshold.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. Project page:
  https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning empowered sensor fusion boosts infant movement
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09014v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09014v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomas Kulvicius, Dajie Zhang, Luise Poustka, Sven Bölte, Lennart Jahn, Sarah Flügge, Marc Kraft, Markus Zweckstetter, Karin Nielsen-Saines, Florentin Wörgötter, Peter B Marschik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To assess the integrity of the developing nervous system, the Prechtl general
movement assessment (GMA) is recognized for its clinical value in diagnosing
neurological impairments in early infancy. GMA has been increasingly augmented
through machine learning approaches intending to scale-up its application,
circumvent costs in the training of human assessors and further standardize
classification of spontaneous motor patterns. Available deep learning tools,
all of which are based on single sensor modalities, are however still
considerably inferior to that of well-trained human assessors. These approaches
are hardly comparable as all models are designed, trained and evaluated on
proprietary/silo-data sets. With this study we propose a sensor fusion approach
for assessing fidgety movements (FMs). FMs were recorded from 51 typically
developing participants. We compared three different sensor modalities
(pressure, inertial, and visual sensors). Various combinations and two sensor
fusion approaches (late and early fusion) for infant movement classification
were tested to evaluate whether a multi-sensor system outperforms single
modality assessments. Convolutional neural network (CNN) architectures were
used to classify movement patterns. The performance of the three-sensor fusion
(classification accuracy of 94.5%) was significantly higher than that of any
single modality evaluated. We show that the sensor fusion approach is a
promising avenue for automated classification of infant motor patterns. The
development of a robust sensor fusion system may significantly enhance AI-based
early recognition of neurofunctions, ultimately facilitating automated early
detection of neurodevelopmental conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Axioms for AI Alignment from Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luise Ge, Daniel Halpern, Evi Micha, Ariel D. Procaccia, Itai Shapira, Yevgeniy Vorobeychik, Junlin Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of reinforcement learning from human feedback (RLHF), the
reward function is generally derived from maximum likelihood estimation of a
random utility model based on pairwise comparisons made by humans. The problem
of learning a reward function is one of preference aggregation that, we argue,
largely falls within the scope of social choice theory. From this perspective,
we can evaluate different aggregation methods via established axioms, examining
whether these methods meet or fail well-known standards. We demonstrate that
both the Bradley-Terry-Luce Model and its broad generalizations fail to meet
basic axioms. In response, we develop novel rules for learning reward functions
with strong axiomatic guarantees. A key innovation from the standpoint of
social choice is that our problem has a linear structure, which greatly
restricts the space of feasible rules and leads to a new paradigm that we call
linear social choice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpreting CLIP: Insights on the Robustness to ImageNet Distribution
  Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Crabbé, Pau Rodríguez, Vaishaal Shankar, Luca Zappella, Arno Blaas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What distinguishes robust models from non-robust ones? While for ImageNet
distribution shifts it has been shown that such differences in robustness can
be traced back predominantly to differences in training data, so far it is not
known what that translates to in terms of what the model has learned. In this
work, we bridge this gap by probing the representation spaces of 16 robust
zero-shot CLIP vision encoders with various backbones (ResNets and ViTs) and
pretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and {DataComp}),
and comparing them to the representation spaces of less robust models with
identical backbones, but different (pre)training sets or objectives (CLIP
pretraining on ImageNet-Captions, and supervised training or finetuning on
ImageNet).Through this analysis, we generate three novel insights. Firstly, we
detect the presence of outlier features in robust zero-shot CLIP vision
encoders, which to the best of our knowledge is the first time these are
observed in non-language and non-transformer models. Secondly, we find the
existence of outlier features to be an indication of ImageNet shift robustness
in models, since we only find them in robust models in our analysis. Lastly, we
also investigate the number of unique encoded concepts in the representation
space and find zero-shot CLIP models to encode a higher number of unique
concepts in their representation space. However, we do not find this to be an
indicator of ImageNet shift robustness and hypothesize that it is rather
related to the language supervision. Since the presence of outlier features can
be detected without access to any data from shifted datasets, we believe that
they could be a useful tool for practitioners to get a feeling for the
distribution shift robustness of a pretrained model during deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Open Respiratory Acoustic Foundation Models: Pretraining and
  Benchmarking <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16148v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16148v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Zhang, Tong Xia, Jing Han, Yu Wu, Georgios Rizos, Yang Liu, Mohammed Mosuily, Jagmohan Chauhan, Cecilia Mascolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Respiratory audio, such as coughing and breathing sounds, has predictive
power for a wide range of healthcare applications, yet is currently
under-explored. The main problem for those applications arises from the
difficulty in collecting large labeled task-specific data for model
development. Generalizable respiratory acoustic foundation models pretrained
with unlabeled data would offer appealing advantages and possibly unlock this
impasse. However, given the safety-critical nature of healthcare applications,
it is pivotal to also ensure openness and replicability for any proposed
foundation model solution. To this end, we introduce OPERA, an OPEn Respiratory
Acoustic foundation model pretraining and benchmarking system, as the first
approach answering this need. We curate large-scale respiratory audio datasets
(~136K samples, over 400 hours), pretrain three pioneering foundation models,
and build a benchmark consisting of 19 downstream respiratory health tasks for
evaluation. Our pretrained models demonstrate superior performance (against
existing acoustic models pretrained with general audio on 16 out of 19 tasks)
and generalizability (to unseen datasets and new respiratory audio modalities).
This highlights the great promise of respiratory acoustic foundation models and
encourages more studies using OPERA as an open resource to accelerate research
on respiratory audio for health. The system is accessible from
https://github.com/evelyn0414/OPERA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by NeurIPS 2024 Track Datasets and Benchmarks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced
  Zero/Few-Shot Forecasting of Multivariate Time Series <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03955v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03955v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijay Ekambaram, Arindam Jati, Pankaj Dayama, Sumanta Mukherjee, Nam H. Nguyen, Wesley M. Gifford, Chandra Reddy, Jayant Kalagnanam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pre-trained models excel in zero/few-shot learning for language and
vision tasks but face challenges in multivariate time series (TS) forecasting
due to diverse data characteristics. Consequently, recent research efforts have
focused on developing pre-trained TS forecasting models. These models, whether
built from scratch or adapted from large language models (LLMs), excel in
zero/few-shot forecasting tasks. However, they are limited by slow performance,
high computational demands, and neglect of cross-channel and exogenous
correlations. To address this, we introduce Tiny Time Mixers (TTM), a compact
model (starting from 1M parameters) with effective transfer learning
capabilities, trained exclusively on public TS datasets. TTM, based on the
light-weight TSMixer architecture, incorporates innovations like adaptive
patching, diverse resolution sampling, and resolution prefix tuning to handle
pre-training on varied dataset resolutions with minimal model capacity.
Additionally, it employs multi-level modeling to capture channel correlations
and infuse exogenous signals during fine-tuning. TTM outperforms existing
popular benchmarks in zero/few-shot forecasting by (4-40%), while reducing
computational requirements significantly. Moreover, TTMs are lightweight and
can be executed even on CPU-only machines, enhancing usability and fostering
wider adoption in resource-constrained environments. The model weights for
reproducibility and research use are available at
https://huggingface.co/ibm/ttm-research-r2/, while enterprise-use weights under
the Apache license can be accessed as follows: the initial TTM-Q variant at
https://huggingface.co/ibm-granite/granite-timeseries-ttm-r1, and the latest
variants (TTM-B, TTM-E, TTM-A) weights are available at
https://huggingface.co/ibm-granite/granite-timeseries-ttm-r2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEG: Medical Knowledge-Augmented Large Language Models for Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Cabello, Carmen Martin-Turrero, Uchenna Akujuobi, Anders Søgaard, Carlos Bobed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering is a natural language understanding task that involves
reasoning over both explicit context and unstated, relevant domain knowledge.
Large language models (LLMs), which underpin most contemporary question
answering systems, struggle to induce how concepts relate in specialized
domains such as medicine. Existing medical LLMs are also costly to train. In
this work, we present MEG, a parameter-efficient approach for medical
knowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate
graph embeddings into the LLM, enabling it to leverage external knowledge in a
cost-effective way. We evaluate our method on four popular medical
multiple-choice datasets and show that LLMs greatly benefit from the factual
grounding provided by knowledge graph embeddings. MEG attains an average of
+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized
models like BioMistral. We also show results based on Llama-3. Finally, we show
that MEG's performance remains robust to the choice of graph encoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial <span class="highlight-title">Transformer</span>s for Radio Map Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01211v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01211v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pham Q. Viet, Daniel Romero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radio map estimation (RME) involves spatial interpolation of radio
measurements to predict metrics such as the received signal strength at
locations where no measurements were collected. The most popular estimators
nowadays project the measurement locations to a regular grid and complete the
resulting measurement tensor with a convolutional deep neural network.
Unfortunately, these approaches suffer from poor spatial resolution and require
a great number of parameters. The first contribution of this paper addresses
these limitations by means of an attention-based estimator named Spatial
TransfOrmer for Radio Map estimation (STORM). This scheme not only outperforms
the existing estimators, but also exhibits lower computational complexity,
translation equivariance, rotation equivariance, and full spatial resolution.
The second contribution is an extended transformer architecture that allows
STORM to perform active sensing, by which the next measurement location is
selected based on the previous measurements. This is particularly useful for
minimization of drive tests (MDT) in cellular networks, where operators request
user equipment to collect measurements. Finally, STORM is extensively validated
by experiments with one ray-tracing and two real-measurement datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Robustness of Machine Learning Models in Predicting Thermodynamic
  Properties: a Case of Searching for New Quasicrystal Approximants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13873v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13873v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fedor S. Avilov, Roman A. Eremin, Semen A. Budennyy, Innokentiy S. Humonen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite an artificial intelligence-assisted modeling of disordered crystals
is a widely used and well-tried method of new materials design, the issues of
its robustness, reliability, and stability are still not resolved and even not
discussed enough. To highlight it, in this work we composed a series of nested
intermetallic approximants of quasicrystals datasets and trained various
machine learning models on them correspondingly. Our qualitative and, what is
more important, quantitative assessment of the difference in the predictions
clearly shows that different reasonable changes in the training sample can lead
to the completely different set of the predicted potentially new materials. We
also showed the advantage of pre-training and proposed a simple yet effective
trick of sequential training to increase stability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuralClothSim: Neural Deformation Fields Meet the Thin Shell Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12970v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12970v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navami Kairanda, Marc Habermann, Christian Theobalt, Vladislav Golyanik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite existing 3D cloth simulators producing realistic results, they
predominantly operate on discrete surface representations (e.g. points and
meshes) with a fixed spatial resolution, which often leads to large memory
consumption and resolution-dependent simulations. Moreover, back-propagating
gradients through the existing solvers is difficult, and they cannot be easily
integrated into modern neural architectures. In response, this paper re-thinks
physically plausible cloth simulation: We propose NeuralClothSim, i.e., a new
quasistatic cloth simulator using thin shells, in which surface deformation is
encoded in neural network weights in the form of a neural field. Our
memory-efficient solver operates on a new continuous coordinate-based surface
representation called neural deformation fields (NDFs); it supervises NDF
equilibria with the laws of the non-linear Kirchhoff-Love shell theory with a
non-linear anisotropic material model. NDFs are adaptive: They 1) allocate
their capacity to the deformation details and 2) allow surface state queries at
arbitrary spatial resolutions without re-training. We show how to train
NeuralClothSim while imposing hard boundary conditions and demonstrate multiple
applications, such as material interpolation and simulation editing. The
experimental results highlight the effectiveness of our continuous neural
formulation. See our project page: https://4dqv.mpi-inf.mpg.de/NeuralClothSim/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 23 figures and 3 tables; project page:
  https://4dqv.mpi-inf.mpg.de/NeuralClothSim/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Cognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18624v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18624v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfredo Ibias, Hector Antona, Guillem Ramirez-Miranda, Enric Guinovart, Eduard Alarcon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised learning methods have a soft inspiration in cognition models. To
this day, the most successful unsupervised learning methods revolve around
clustering samples in a mathematical space. In this paper we propose a
state-of-the-art, primitive-based, unsupervised learning approach for
decision-making inspired by a novel cognition framework. This
representation-centric approach models the input space constructively as a
distributed hierarchical structure in an input-agnostic way. We compared our
approach with both current state-of-the-art unsupervised learning
classification, and with current state-of-the-art cancer type classification.
We show how our proposal outperforms previous state-of-the-art. We also
evaluate some cognition-like properties of our proposal where it not only
outperforms the compared algorithms (even supervised learning ones), but it
also shows a different, more cognition-like, behaviour.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Occam Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20194v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20194v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        B. N. Kausik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning neural network models must be large enough to adapt to their
problem domain, while small enough to avoid overfitting training data during
gradient descent. To balance these competing demands, overprovisioned deep
learning models such as transformers are trained for a single epoch on large
data sets, and hence inefficient with both computing resources and training
data. In response to these inefficiencies, we exploit learning theory to derive
Occam Gradient Descent, an algorithm that interleaves adaptive reduction of
model size to minimize generalization error, with gradient descent on model
weights to minimize fitting error. In contrast, traditional gradient descent
greedily minimizes fitting error without regard to generalization error. Our
algorithm simultaneously descends the space of weights and topological size of
any neural network without modification. With respect to loss, compute and
model size, our experiments show (a) on image classification benchmarks, linear
and convolutional neural networks trained with Occam Gradient Descent
outperform traditional gradient descent with or without post-train pruning; (b)
on a range of tabular data classification tasks, neural networks trained with
Occam Gradient Descent outperform traditional gradient descent, as well as
Random Forests; (c) on natural language transformers, Occam Gradient Descent
outperforms traditional gradient descent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A unified weighting framework for evaluating nearest neighbour
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Urs Lenz, Henri Bollaert, Chris Cornelis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first comprehensive and large-scale evaluation of classical
(NN), fuzzy (FNN) and fuzzy rough (FRNN) nearest neighbour classification. We
standardise existing proposals for nearest neighbour weighting with kernel
functions, applied to the distance values and/or ranks of the nearest
neighbours of a test instance. In particular, we show that the theoretically
optimal Samworth weights converge to a kernel. Kernel functions are closely
related to fuzzy negation operators, and we propose a new kernel based on Yager
negation. We also consider various distance and scaling measures, which we show
can be related to each other. Through a systematic series of experiments on 85
real-life classification datasets, we find that NN, FNN and FRNN all perform
best with Boscovich distance, and that NN and FRNN perform best with a
combination of Samworth rank- and distance-weights and scaling by the mean
absolute deviation around the median ($r_1$), the standard deviation ($r_2$) or
the semi-interquartile range ($r_{\infty}^*$), while FNN performs best with
only Samworth distance-weights and $r_1$- or $r_2$-scaling. However, NN
achieves comparable performance with Yager-$\frac{1}{2}$ distance-weights,
which are simpler to implement than a combination of Samworth distance- and
rank-weights. Finally, FRNN generally outperforms NN, which in turn performs
systematically better than FNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16103v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16103v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Robert, Mher Safaryan, Ionut-Vlad Modoranu, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LDAdam, a memory-efficient optimizer for training large models,
that performs adaptive optimization steps within lower dimensional subspaces,
while consistently exploring the full parameter space during training. This
strategy keeps the optimizer's memory footprint to a fraction of the model
size. LDAdam relies on a new projection-aware update rule for the optimizer
states that allows for transitioning between subspaces, i.e., estimation of the
statistics of the projected gradients. To mitigate the errors due to low-rank
projection, LDAdam integrates a new generalized error feedback mechanism, which
explicitly accounts for both gradient and optimizer state compression. We prove
the convergence of LDAdam under standard assumptions, and show that LDAdam
allows for accurate and efficient fine-tuning and pre-training of language
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Characterizing stable regions in the residual stream of LLMs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17113v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17113v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jett Janiak, Jacek Karwowski, Chatrik Singh Mangat, Giorgi Giglemiani, Nora Petrova, Stefan Heimersheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We identify stable regions in the residual stream of Transformers, where the
model's output remains insensitive to small activation changes, but exhibits
high sensitivity at region boundaries. These regions emerge during training and
become more defined as training progresses or model size increases. The regions
appear to be much larger than previously studied polytopes. Our analysis
suggests that these stable regions align with semantic distinctions, where
similar prompts cluster within regions, and activations from the same region
lead to similar next token predictions. This work provides a promising research
direction for understanding the complexity of neural networks, shedding light
on training dynamics, and advancing interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at Scientific Methods for Understanding Deep Learning
  (SciForDL) workshop at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplicity Bias of Two-Layer Networks beyond Linearly Separable Data <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17299v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17299v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Tsoy, Nikola Konstantinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simplicity bias, the propensity of deep models to over-rely on simple
features, has been identified as a potential reason for limited
out-of-distribution generalization of neural networks (Shah et al., 2020).
Despite the important implications, this phenomenon has been theoretically
confirmed and characterized only under strong dataset assumptions, such as
linear separability (Lyu et al., 2021). In this work, we characterize
simplicity bias for general datasets in the context of two-layer neural
networks initialized with small weights and trained with gradient flow.
Specifically, we prove that in the early training phases, network features
cluster around a few directions that do not depend on the size of the hidden
layer. Furthermore, for datasets with an XOR-like pattern, we precisely
identify the learned features and demonstrate that simplicity bias intensifies
during later training stages. These results indicate that features learned in
the middle stages of training may be more useful for OOD transfer. We support
this hypothesis with experiments on image data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2024, camera-ready version (expanded related work)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable Mutual Benefits from Federated Learning in Privacy-Sensitive
  Domains <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Tsoy, Anna Mihalkova, Teodora Todorova, Nikola Konstantinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-silo federated learning (FL) allows data owners to train accurate
machine learning models by benefiting from each others private datasets.
Unfortunately, the model accuracy benefits of collaboration are often
undermined by privacy defenses. Therefore, to incentivize client participation
in privacy-sensitive domains, a FL protocol should strike a delicate balance
between privacy guarantees and end-model accuracy. In this paper, we study the
question of when and how a server could design a FL protocol provably
beneficial for all participants. First, we provide necessary and sufficient
conditions for the existence of mutually beneficial protocols in the context of
mean estimation and convex stochastic optimization. We also derive protocols
that maximize the total clients' utility, given symmetric privacy preferences.
Finally, we design protocols maximizing end-model accuracy and demonstrate
their benefits in synthetic experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2024; Camera-ready version (updated references)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MCDFN: Supply Chain Demand Forecasting via an Explainable Multi-Channel
  Data Fusion Network Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15598v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15598v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Abrar Jahin, Asef Shahriar, Md Al Amin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate demand forecasting is crucial for optimizing supply chain
management. Traditional methods often fail to capture complex patterns from
seasonal variability and special events. Despite advancements in deep learning,
interpretable forecasting models remain a challenge. To address this, we
introduce the Multi-Channel Data Fusion Network (MCDFN), a hybrid architecture
that integrates Convolutional Neural Networks (CNN), Long Short-Term Memory
networks (LSTM), and Gated Recurrent Units (GRU) to enhance predictive
performance by extracting spatial and temporal features from time series data.
Our comparative benchmarking demonstrates that MCDFN outperforms seven other
deep-learning models, achieving superior metrics: MSE (23.5738), RMSE (4.8553),
MAE (3.9991), and MAPE (20.1575%). Additionally, MCDFN's predictions were
statistically indistinguishable from actual values, confirmed by a paired
t-test with a 5% p-value and a 10-fold cross-validated statistical paired
t-test. We apply explainable AI techniques like ShapTime and Permutation
Feature Importance to enhance interpretability. This research advances demand
forecasting methodologies and offers practical guidelines for integrating MCDFN
into supply chain systems, highlighting future research directions for
scalability and user-friendly deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regularized Projection Matrix Approximation with Applications to
  Community Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16598v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16598v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Zhai, Jialu Xu, Mingxin Wu, Xiaohui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a regularized projection matrix approximation framework
designed to recover cluster information from the affinity matrix. The model is
formulated as a projection approximation problem, incorporating an entry-wise
penalty function. We investigate three distinct penalty functions, each
specifically tailored to address bounded, positive, and sparse scenarios. To
solve this problem, we propose direct optimization on the Stiefel manifold,
utilizing the Cayley transformation along with the Alternating Direction Method
of Multipliers (ADMM) algorithm. Additionally, we provide a theoretical
analysis that establishes the convergence properties of ADMM, demonstrating
that the convergence point satisfies the KKT conditions of the original
problem. Numerical experiments conducted on both synthetic and real-world
datasets reveal that our regularized projection matrix approximation approach
significantly outperforms state-of-the-art methods in clustering performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep-Graph-Sprints: Accelerated Representation Learning in
  Continuous-Time Dynamic Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07712v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07712v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Naser Eddin, Jacopo Bono, David Aparício, Hugo Ferreira, Pedro Ribeiro, Pedro Bizarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous-time dynamic graphs (CTDGs) are essential for modeling
interconnected, evolving systems. Traditional methods for extracting knowledge
from these graphs often depend on feature engineering or deep learning. Feature
engineering is limited by the manual and time-intensive nature of crafting
features, while deep learning approaches suffer from high inference latency,
making them impractical for real-time applications. This paper introduces
Deep-Graph-Sprints (DGS), a novel deep learning architecture designed for
efficient representation learning on CTDGs with low-latency inference
requirements. We benchmark DGS against state-of-the-art (SOTA) feature
engineering and graph neural network methods using five diverse datasets. The
results indicate that DGS achieves competitive performance while inference
speed improves between 4x and 12x compared to other deep learning approaches on
our benchmark datasets. Our method effectively bridges the gap between deep
representation learning and low-latency application requirements for CTDGs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating alignment between humans and neural network representations
  in image-based learning tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09377v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09377v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Demircan, Tankred Saanum, Leonardo Pettini, Marcel Binz, Blazej M Baczkowski, Christian F Doeller, Mona M Garvert, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans represent scenes and objects in rich feature spaces, carrying
information that allows us to generalise about category memberships and
abstract functions with few examples. What determines whether a neural network
model generalises like a human? We tested how well the representations of $86$
pretrained neural network models mapped to human learning trajectories across
two tasks where humans had to learn continuous relationships and categories of
natural images. In these tasks, both human participants and neural networks
successfully identified the relevant stimulus features within a few trials,
demonstrating effective generalisation. We found that while training dataset
size was a core determinant of alignment with human choices, contrastive
training with multi-modal data (text and imagery) was a common feature of
currently publicly available models that predicted human generalisation.
Intrinsic dimensionality of representations had different effects on alignment
for different model types. Lastly, we tested three sets of human-aligned
representations and found no consistent improvements in predictive accuracy
compared to the baselines. In conclusion, pretrained neural networks can serve
to extract representations for cognitive models, as they appear to capture some
fundamental aspects of cognition that are transferable across tasks. Both our
paradigms and modelling approach offer a novel way to quantify alignment
between neural networks and humans and extend cognitive science into more
naturalistic domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Integrated Decision Gradients (IDG-DP) for
  Radar-based Human Activity Recognition <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idris Zakariyya, Linda Tran, Kaushik Bhargav Sivangi, Paul Henderson, Fani Deligianni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion analysis offers significant potential for healthcare monitoring
and early detection of diseases. The advent of radar-based sensing systems has
captured the spotlight for they are able to operate without physical contact
and they can integrate with pre-existing Wi-Fi networks. They are also seen as
less privacy-invasive compared to camera-based systems. However, recent
research has shown high accuracy in recognizing subjects or gender from radar
gait patterns, raising privacy concerns. This study addresses these issues by
investigating privacy vulnerabilities in radar-based Human Activity Recognition
(HAR) systems and proposing a novel method for privacy preservation using
Differential Privacy (DP) driven by attributions derived with Integrated
Decision Gradient (IDG) algorithm. We investigate Black-box Membership
Inference Attack (MIA) Models in HAR settings across various levels of
attacker-accessible information. We extensively evaluated the effectiveness of
the proposed IDG-DP method by designing a CNN-based HAR model and rigorously
assessing its resilience against MIAs. Experimental results demonstrate the
potential of IDG-DP in mitigating privacy attacks while maintaining utility
across all settings, particularly excelling against label-only and shadow model
black-box MIA attacks. This work represents a crucial step towards balancing
the need for effective radar-based HAR with robust privacy protection in
healthcare environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025. 12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Low-Cost Drone Detection and Classification in Low SNR
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18624v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18624v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Glüge, Matthias Nyfeler, Ahmad Aghaebrahimian, Nicola Ramagnano, Christof Schüpbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of drones, or unmanned aerial vehicles (UAVs), has raised
significant safety concerns due to their potential misuse in activities such as
espionage, smuggling, and infrastructure disruption. This paper addresses the
critical need for effective drone detection and classification systems that
operate independently of UAV cooperation. We evaluate various convolutional
neural networks (CNNs) for their ability to detect and classify drones using
spectrogram data derived from consecutive Fourier transforms of signal
components. The focus is on model robustness in low signal-to-noise ratio (SNR)
environments, which is critical for real-world applications. A comprehensive
dataset is provided to support future model development. In addition, we
demonstrate a low-cost drone detection system using a standard computer,
software-defined radio (SDR) and antenna, validated through real-world field
testing. On our development dataset, all models consistently achieved an
average balanced classification accuracy of >= 85% at SNR > -12dB. In the field
test, these models achieved an average balance accuracy of > 80%, depending on
transmitter distance and antenna direction. Our contributions include: a
publicly available dataset for model development, a comparative analysis of CNN
for drone detection under low SNR conditions, and the deployment and field
evaluation of a practical, low-cost detection system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract
  Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04620v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04620v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pragnya Ramjee, Bhuvan Sachdeva, Satvik Golechha, Shreyas Kulkarni, Geeta Fulari, Kaushik Murali, Mohit Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The healthcare landscape is evolving, with patients seeking reliable
information about their health conditions and available treatment options.
Despite the abundance of information sources, the digital age overwhelms
individuals with excess, often inaccurate information. Patients primarily trust
medical professionals, highlighting the need for expert-endorsed health
information. However, increased patient loads on experts has led to reduced
communication time, impacting information sharing. To address this gap, we
developed CataractBot, an experts-in-the-loop chatbot powered by LLMs, in
collaboration with an eye hospital in India. CataractBot answers cataract
surgery related questions instantly by querying a curated knowledge base and
provides expert-verified responses asynchronously. It has multimodal and
multilingual capabilities. In an in-the-wild deployment study with 55
participants, CataractBot proved valuable, providing anytime accessibility,
saving time, accommodating diverse literacy levels, alleviating power
differences, and adding a privacy layer between patients and doctors. Users
reported that their trust in the system was established through expert
verification. Broadly, our results could inform future work on designing
expert-mediated LLM bots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongEmbed: Extending Embedding Models for Long Context Retrieval <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12096v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12096v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, Sujian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models play a pivot role in modern NLP applications such as IR and
RAG. While the context limit of LLMs has been pushed beyond 1 million tokens,
embedding models are still confined to a narrow context window not exceeding 8k
tokens, refrained from application scenarios requiring long inputs such as
legal contracts. This paper explores context window extension of existing
embedding models, pushing the limit to 32k without requiring additional
training. First, we examine the performance of current embedding models for
long context retrieval on our newly constructed LongEmbed benchmark. LongEmbed
comprises two synthetic tasks and four carefully chosen real-world tasks,
featuring documents of varying length and dispersed target information.
Benchmarking results underscore huge room for improvement in these models.
Based on this, comprehensive experiments show that training-free context window
extension strategies like position interpolation can effectively extend the
context window of existing embedding models by several folds, regardless of
their original context being 512 or beyond 4k. Furthermore, for models
employing absolute position encoding (APE), we show the possibility of further
fine-tuning to harvest notable performance gains while strictly preserving
original behavior for short inputs. For models using rotary position embedding
(RoPE), significant enhancements are observed when employing RoPE-specific
methods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for
context window extension. To facilitate future research, we release E5-Base-4k
and E5-RoPE-Base, along with the LongEmbed benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Assist Humans without Inferring Rewards <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Myers, Evan Ellis, Sergey Levine, Benjamin Eysenbach, Anca Dragan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assistive agents should make humans' lives easier. Classically, such
assistance is studied through the lens of inverse reinforcement learning, where
an assistive agent (e.g., a chatbot, a robot) infers a human's intention and
then selects actions to help the human reach that goal. This approach requires
inferring intentions, which can be difficult in high-dimensional settings. We
build upon prior work that studies assistance through the lens of empowerment:
an assistive agent aims to maximize the influence of the human's actions such
that they exert a greater control over the environmental outcomes and can solve
tasks in fewer steps. We lift the major limitation of prior work in this
area--scalability to high-dimensional settings--with contrastive successor
representations. We formally prove that these representations estimate a
similar notion of empowerment to that studied by prior work and provide a
ready-made mechanism for optimizing it. Empirically, our proposed method
outperforms prior methods on synthetic benchmarks, and scales to Overcooked, a
cooperative game setting. Theoretically, our work connects ideas from
information theory, neuroscience, and reinforcement learning, and charts a path
for representations to play a critical role in solving assistive problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Neural Information Processing Systems (NeurIPS), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Classification by Coupling Data Mollification with Label
  Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Heinonen, Ba-Hien Tran, Michael Kampffmeyer, Maurizio Filippone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Introducing training-time augmentations is a key technique to enhance
generalization and prepare deep neural networks against test-time corruptions.
Inspired by the success of generative diffusion models, we propose a novel
approach of coupling data mollification, in the form of image noising and
blurring, with label smoothing to align predicted label confidences with image
degradation. The method is simple to implement, introduces negligible
overheads, and can be combined with existing augmentations. We demonstrate
improved robustness and uncertainty quantification on the corrupted image
benchmarks of the CIFAR and TinyImageNet datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Shen, Hanyan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning learns from a static dataset without
interacting with environments, which ensures security and thus owns a good
application prospect. However, directly applying naive reinforcement learning
algorithm usually fails in an offline environment due to inaccurate Q value
approximation caused by out-of-distribution (OOD) state-actions. It is an
effective way to solve this problem by penalizing the Q-value of OOD
state-actions. Among the methods of punishing OOD state-actions, count-based
methods have achieved good results in discrete domains in a simple form.
Inspired by it, a novel pseudo-count method for continuous domains called
Grid-Mapping Pseudo-Count method (GPC) is proposed by extending the count-based
method from discrete to continuous domains. Firstly, the continuous state and
action space are mapped to discrete space using Grid-Mapping, then the Q-values
of OOD state-actions are constrained through pseudo-count. Secondly, the
theoretical proof is given to show that GPC can obtain appropriate uncertainty
constraints under fewer assumptions than other pseudo-count methods. Thirdly,
GPC is combined with Soft Actor-Critic algorithm (SAC) to get a new algorithm
called GPC-SAC. Lastly, experiments on D4RL datasets are given to show that
GPC-SAC has better performance and less computational cost than other
algorithms that constrain the Q-value.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Performance Issues in Cloud Service Systems Based on
  Relational-Temporal Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10869v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10869v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenwei Gu, Jinyang Liu, Zhuangbin Chen, Jianping Zhang, Yuxin Su, Jiazhen Gu, Cong Feng, Zengyin Yang, Yongqiang Yang, Michael Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud systems are susceptible to performance issues, which may cause
service-level agreement violations and financial losses. In current practice,
crucial metrics are monitored periodically to provide insight into the
operational status of components. Identifying performance issues is often
formulated as an anomaly detection problem, which is tackled by analyzing each
metric independently. However, this approach overlooks the complex dependencies
existing among cloud components. Some graph neural network-based methods take
both temporal and relational information into account, however, the correlation
violations in the metrics that serve as indicators of underlying performance
issues are difficult for them to identify. Furthermore, a large volume of
components in a cloud system results in a vast array of noisy metrics. This
complexity renders it impractical for engineers to fully comprehend the
correlations, making it challenging to identify performance issues accurately.
To address these limitations, we propose Identifying Performance Issues based
on Relational-Temporal Features (ISOLATE ), a learning-based approach that
leverages both the relational and temporal features of metrics to identify
performance issues. In particular, it adopts a graph neural network with
attention to characterizing the relations among metrics and extracts long-term
and multi-scale temporal patterns using a GRU and a convolution network,
respectively. The learned graph attention weights can be further used to
localize the correlation-violated metrics. Moreover, to relieve the impact of
noisy data, ISOLATE utilizes a positive unlabeled learning strategy that tags
pseudo-labels based on a small portion of confirmed negative examples.
Extensive evaluation on both public and industrial datasets shows that ISOLATE
outperforms all baseline models with 0.945 F1-score and 0.920 Hit rate@3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACM Transactions on Software Engineering and Methodology
  (TOSEM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fuzzy K-Means Clustering without Cluster Centroids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Bao, Han Lu, Quanxue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fuzzy K-Means clustering is a critical technique in unsupervised data
analysis. Unlike traditional hard clustering algorithms such as K-Means, it
allows data points to belong to multiple clusters with varying degrees of
membership, determined through iterative optimization to establish optimal
cluster centers and memberships, thereby achieving fuzzy partitioning of data.
However, the performance of popular Fuzzy K-Means algorithms is sensitive to
the selection of initial cluster centroids and is also affected by noise when
updating mean cluster centroids. To address these challenges, this paper
proposes a novel Fuzzy \textit{K}-Means clustering algorithm that entirely
eliminates the reliance on cluster centroids, obtaining membership metrics
solely through distance matrix computation. This innovation enhances
flexibility in distance measurement between sample points, thus improving the
algorithm's performance and robustness. The paper also establishes theoretical
connections between the proposed model and popular Fuzzy K-Means clustering
techniques. Experimental results on several real datasets demonstrate the
effectiveness of the algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predictive Analytics of Varieties of Potatoes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03701v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03701v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabiana Ferracina, Bala Krishnamoorthy, Mahantesh Halappanavar, Shengwei Hu, Vidyasagar Sathuvalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the application of machine learning algorithms specifically to
enhance the selection process of Russet potato clones in breeding trials by
predicting their suitability for advancement. This study addresses the
challenge of efficiently identifying high-yield, disease-resistant, and
climate-resilient potato varieties that meet processing industry standards.
Leveraging manually collected data from trials in the state of Oregon, we
investigate the potential of a wide variety of state-of-the-art binary
classification models. The dataset includes 1086 clones, with data on 38
attributes recorded for each clone, focusing on yield, size, appearance, and
frying characteristics, with several control varieties planted consistently
across four Oregon regions from 2013-2021. We conduct a comprehensive analysis
of the dataset that includes preprocessing, feature engineering, and imputation
to address missing values. We focus on several key metrics such as accuracy,
F1-score, and Matthews correlation coefficient (MCC) for model evaluation. The
top-performing models, namely a neural network classifier (Neural Net),
histogram-based gradient boosting classifier (HGBC), and a support vector
machine classifier (SVM), demonstrate consistent and significant results. To
further validate our findings, we conduct a simulation study. By simulating
different data-generating scenarios, we assess model robustness and performance
through true positive, true negative, false positive, and false negative
distributions, area under the receiver operating characteristic curve (AUC-ROC)
and MCC. The simulation results highlight that non-linear models like SVM and
HGBC consistently show higher AUC-ROC and MCC than logistic regression (LR),
thus outperforming the traditional linear model across various distributions,
and emphasizing the importance of model selection and tuning in agricultural
trials.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor revision; to appear in Crop Sciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReMoDetect: Reward Models Recognize Aligned LLM's Generations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunseok Lee, Jihoon Tack, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable capabilities and easy accessibility of large language models
(LLMs) have significantly increased societal risks (e.g., fake news
generation), necessitating the development of LLM-generated text (LGT)
detection methods for safe usage. However, detecting LGTs is challenging due to
the vast number of LLMs, making it impractical to account for each LLM
individually; hence, it is crucial to identify the common characteristics
shared by these models. In this paper, we draw attention to a common feature of
recent powerful LLMs, namely the alignment training, i.e., training LLMs to
generate human-preferable texts. Our key finding is that as these aligned LLMs
are trained to maximize the human preferences, they generate texts with higher
estimated preferences even than human-written texts; thus, such texts are
easily detected by using the reward model (i.e., an LLM trained to model human
preference distribution). Based on this finding, we propose two training
schemes to further improve the detection ability of the reward model, namely
(i) continual preference fine-tuning to make the reward model prefer aligned
LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a
rephrased texts from human-written texts using aligned LLMs), which serves as a
median preference text corpus between LGTs and human-written texts to learn the
decision boundary better. We provide an extensive evaluation by considering six
text domains across twelve aligned LLMs, where our method demonstrates
state-of-the-art results. Code is available at
https://github.com/hyunseoklee-ai/ReMoDetect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference proceeding for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Lattice Sampling of Quantum Field Theories via Neural
  Operator-based Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00828v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00828v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bálint Máté, François Fleuret
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of sampling lattice field configurations on a lattice
from the Boltzmann distribution corresponding to some action. Since such
densities arise as approximationw of an underlying functional density, we frame
the task as an instance of operator learning. We propose to approximate a
time-dependent neural operator whose time integral provides a mapping between
the functional distributions of the free and target theories. Once a particular
lattice is chosen, the neural operator can be discretized to a
finite-dimensional, time-dependent vector field which in turn induces a
continuous normalizing flow between finite dimensional distributions over the
chosen lattice. This flow can then be trained to be a diffeormorphism between
the discretized free and target theories on the chosen lattice, and, by
construction, can be evaluated on different discretizations of spacetime. We
experimentally validate the proposal on the 2-dimensional $\phi^4$-theory to
explore to what extent such operator-based flow architectures generalize to
lattice sizes they were not trained on, and show that pretraining on smaller
lattices can lead to a speedup over training directly on the target lattice
size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fourier Analysis of Variational Quantum Circuits for Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Wiedmann, Maniraman Periyasamy, Daniel D. Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  VQC can be understood through the lens of Fourier analysis. It is already
well-known that the function space represented by any circuit architecture can
be described through a truncated Fourier sum. We show that the spectrum
available to that truncated Fourier sum is not entirely determined by the
encoding gates of the circuit, since the variational part of the circuit can
constrain certain coefficients to zero, effectively removing that frequency
from the spectrum. To the best of our knowledge, we give the first description
of the functional dependence of the Fourier coefficients on the variational
parameters as trigonometric polynomials. This allows us to provide an algorithm
which computes the exact spectrum of any given circuit and the corresponding
Fourier coefficients. Finally, we demonstrate that by comparing the Fourier
transform of the dataset to the available spectra, it is possible to predict
which VQC out of a given list of choices will be able to best fit the data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Commute Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01635v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01635v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhuo, Guang Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have shown remarkable success in learning from
graph-structured data. However, their application to directed graphs (digraphs)
presents unique challenges, primarily due to the inherent asymmetry in node
relationships. Traditional GNNs are adept at capturing unidirectional relations
but fall short in encoding the mutual path dependencies between nodes, such as
asymmetrical shortest paths typically found in digraphs. Recognizing this gap,
we introduce Commute Graph Neural Networks (CGNN), an approach that seamlessly
integrates node-wise commute time into the message passing scheme. The
cornerstone of CGNN is an efficient method for computing commute time using a
newly formulated digraph Laplacian. Commute time is then integrated into the
neighborhood aggregation process, with neighbor contributions weighted
according to their respective commute time to the central node in each layer.
It enables CGNN to directly capture the mutual, asymmetric relationships in
digraphs. Extensive experiments confirm the superior performance of CGNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Language Models Truly Grasp Mathematics? An Empirical
  Exp<span class="highlight-title">lora</span>tion From A Psychological Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14979v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14979v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xie, Shuoyoucheng Ma, Zhenhua Wang, Enze Wang, Kai Chen, Xiaobing Sun, Baosheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their proficiency in math tasks, the mechanisms underlying LLMs'
mathematical reasoning abilities remain a subject of debate. Recent studies
suggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning
by encouraging LLMs to employ human-like logical reasoning (System 2), enabling
them to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs
genuinely possess System 2-like logical reasoning, we introduced targeted
modifications to CRT problems. Our findings reveal that, despite the use of CoT
prompts, mainstream LLMs, including the latest o1-preview model, continue to
exhibit a significant error rate. Further analysis indicates that they
predominantly rely on System 1-like intuitive reasoning and pattern matching
derived from training data, rather than demonstrating mastery of mathematical
thinking. This discovery challenges the prevailing notion that LLMs possess
genuine logical reasoning abilities and that CoT can enhance them.
Consequently, this work may temper overly optimistic projections regarding
LLMs' advancement toward artificial general intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompt-Based Spatio-Temporal Graph Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Hu, Xu Liu, Zhencheng Fan, Yifang Yin, Shili Xiang, Savitha Ramasamy, Roger Zimmermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal graph neural networks have proven efficacy in capturing
complex dependencies for urban computing tasks such as forecasting and kriging.
Yet, their performance is constrained by the reliance on extensive data for
training on a specific task, thereby limiting their adaptability to new urban
domains with varied task demands. Although transfer learning has been proposed
to remedy this problem by leveraging knowledge across domains, the cross-task
generalization still remains under-explored in spatio-temporal graph transfer
learning due to the lack of a unified framework. To bridge the gap, we propose
Spatio-Temporal Graph Prompting (STGP), a prompt-based framework capable of
adapting to multi-diverse tasks in a data-scarce domain. Specifically, we first
unify different tasks into a single template and introduce a task-agnostic
network architecture that aligns with this template. This approach enables
capturing dependencies shared across tasks. Furthermore, we employ learnable
prompts to achieve domain and task transfer in a two-stage prompting pipeline,
facilitating the prompts to effectively capture domain knowledge and
task-specific properties. Our extensive experiments demonstrate that STGP
outperforms state-of-the-art baselines in three tasks-forecasting, kriging, and
extrapolation-achieving an improvement of up to 10.7%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Diffusion</span> Policy Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00588v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00588v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Z. Ren, Justin Lidard, Lars L. Ankile, Anthony Simeonov, Pulkit Agrawal, Anirudha Majumdar, Benjamin Burchfiel, Hongkai Dai, Max Simchowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic
framework including best practices for fine-tuning diffusion-based policies
(e.g. Diffusion Policy) in continuous control and robot learning tasks using
the policy gradient (PG) method from reinforcement learning (RL). PG methods
are ubiquitous in training RL policies with other policy parameterizations;
nevertheless, they had been conjectured to be less efficient for
diffusion-based policies. Surprisingly, we show that DPPO achieves the
strongest overall performance and efficiency for fine-tuning in common
benchmarks compared to other RL methods for diffusion-based policies and also
compared to PG fine-tuning of other policy parameterizations. Through
experimental investigation, we find that DPPO takes advantage of unique
synergies between RL fine-tuning and the diffusion parameterization, leading to
structured and on-manifold exploration, stable training, and strong policy
robustness. We further demonstrate the strengths of DPPO in a range of
realistic settings, including simulated robotic tasks with pixel observations,
and via zero-shot deployment of simulation-trained policies on robot hardware
in a long-horizon, multi-stage manipulation task. Website with code:
diffusion-ppo.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: diffusion-ppo.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Relational Inference for Evolving Multi-agent Interacting Systems <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01442v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01442v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beomseok Kang, Priyabrata Saha, Sudarshan Sharma, Biswadeep Chakraborty, Saibal Mukhopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel framework, Online Relational Inference (ORI), designed
to efficiently identify hidden interaction graphs in evolving multi-agent
interacting systems using streaming data. Unlike traditional offline methods
that rely on a fixed training set, ORI employs online backpropagation, updating
the model with each new data point, thereby allowing it to adapt to changing
environments in real-time. A key innovation is the use of an adjacency matrix
as a trainable parameter, optimized through a new adaptive learning rate
technique called AdaRelation, which adjusts based on the historical sensitivity
of the decoder to changes in the interaction graph. Additionally, a data
augmentation method named Trajectory Mirror (TM) is introduced to improve
generalization by exposing the model to varied trajectory patterns.
Experimental results on both synthetic datasets and real-world data (CMU MoCap
for human motion) demonstrate that ORI significantly improves the accuracy and
adaptability of relational inference in dynamic settings compared to existing
methods. This approach is model-agnostic, enabling seamless integration with
various neural relational inference (NRI) architectures, and offers a robust
solution for real-time applications in complex, evolving systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeafloorAI: A Large-scale Vision-Language <span class="highlight-title">Dataset</span> for Seafloor
  Geological <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00172v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00172v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kien X. Nguyen, Fengchun Qiao, Arthur Trembanis, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major obstacle to the advancements of machine learning models in marine
science, particularly in sonar imagery analysis, is the scarcity of AI-ready
datasets. While there have been efforts to make AI-ready sonar image dataset
publicly available, they suffer from limitations in terms of environment
setting and scale. To bridge this gap, we introduce SeafloorAI, the first
extensive AI-ready datasets for seafloor mapping across 5 geological layers
that is curated in collaboration with marine scientists. We further extend the
dataset to SeafloorGenAI by incorporating the language component in order to
facilitate the development of both vision- and language-capable machine
learning models for sonar imagery. The dataset consists of 62 geo-distributed
data surveys spanning 17,300 square kilometers, with 696K sonar images, 827K
annotated segmentation masks, 696K detailed language descriptions and
approximately 7M question-answer pairs. By making our data processing source
code publicly available, we aim to engage the marine science community to
enrich the data pool and inspire the machine learning community to develop more
robust models. This collaborative approach will enhance the capabilities and
applications of our datasets within both fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15265v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15265v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, Daochen Zha, Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, Vipin Chaudhary, Shuai Xu, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth in model size, fine-tuning the large pre-trained
language model has become increasingly difficult due to its extensive memory
usage. Previous works usually focus on reducing the number of trainable
parameters in the network. While the model parameters do contribute to memory
usage, the primary memory bottleneck during training arises from storing
feature maps, also known as activations, as they are crucial for gradient
calculation. Notably, neural networks are usually trained using stochastic
gradient descent. We argue that in stochastic optimization, models can handle
noisy gradients as long as the gradient estimator is unbiased with reasonable
variance. Following this motivation, we propose a new family of unbiased
estimators called WTA-CRS, for matrix production with reduced variance, which
only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the
context of tuning transformers, our proposed estimators exhibit lower variance
compared to existing ones. By replacing the linear operation with our
approximated one in transformers, we can achieve up to 2.7$\times$ peak memory
reduction with almost no accuracy drop and enables up to $6.4\times$ larger
batch size. Under the same hardware, WTA-CRS enables better down-streaming task
performance by applying larger models and/or faster training speed with larger
batch sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Undermining Image and Text Classification Algorithms Using Adversarial
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Langalibalele Lunga, Suhas Sreehari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are prone to adversarial attacks, where inputs can be
manipulated in order to cause misclassifications. While previous research has
focused on techniques like Generative Adversarial Networks (GANs), there's
limited exploration of GANs and Synthetic Minority Oversampling Technique
(SMOTE) in text and image classification models to perform adversarial attacks.
Our study addresses this gap by training various machine learning models and
using GANs and SMOTE to generate additional data points aimed at attacking text
classification models. Furthermore, we extend our investigation to face
recognition models, training a Convolutional Neural Network(CNN) and subjecting
it to adversarial attacks with fast gradient sign perturbations on key features
identified by GradCAM, a technique used to highlight key image characteristics
CNNs use in classification. Our experiments reveal a significant vulnerability
in classification models. Specifically, we observe a 20 % decrease in accuracy
for the top-performing text classification models post-attack, along with a 30
% decrease in facial recognition accuracy. This highlights the susceptibility
of these models to manipulation of input data. Adversarial attacks not only
compromise the security but also undermine the reliability of machine learning
systems. By showcasing the impact of adversarial attacks on both text
classification and face recognition models, our study underscores the urgent
need for develop robust defenses against such vulnerabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at Electronic Imaging Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Text-to-Image <span class="highlight-title">Diffusion</span> Models with Reward Backpropagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03739v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03739v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have recently emerged at the forefront of
image generation, powered by very large-scale unsupervised or weakly supervised
text-to-image training datasets. Due to their unsupervised training,
controlling their behavior in downstream tasks, such as maximizing
human-perceived image quality, image-text alignment, or ethical image
generation, is difficult. Recent works finetune diffusion models to downstream
reward functions using vanilla reinforcement learning, notorious for the high
variance of the gradient estimators. In this paper, we propose AlignProp, a
method that aligns diffusion models to downstream reward functions using
end-to-end backpropagation of the reward gradient through the denoising
process. While naive implementation of such backpropagation would require
prohibitive memory resources for storing the partial derivatives of modern
text-to-image models, AlignProp finetunes low-rank adapter weight modules and
uses gradient checkpointing, to render its memory usage viable. We test
AlignProp in finetuning diffusion models to various objectives, such as
image-text semantic alignment, aesthetics, compressibility and controllability
of the number of objects present, as well as their combinations. We show
AlignProp achieves higher rewards in fewer training steps than alternatives,
while being conceptually simpler, making it a straightforward choice for
optimizing diffusion models for differentiable reward functions of interest.
Code and Visualization results are available at https://align-prop.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is subsumed by a later paper of ours: arXiv:2407.08737</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Offline Active Learning on Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchen Wu, Yubai Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of active learning on graphs, which has crucial
applications in many real-world networks where labeling node responses is
expensive. In this paper, we propose an offline active learning method that
selects nodes to query by explicitly incorporating information from both the
network structure and node covariates. Building on graph signal recovery
theories and the random spectral sparsification technique, the proposed method
adopts a two-stage biased sampling strategy that takes both informativeness and
representativeness into consideration for node querying. Informativeness refers
to the complexity of graph signals that are learnable from the responses of
queried nodes, while representativeness refers to the capacity of queried nodes
to control generalization errors given noisy node-level information. We
establish a theoretical relationship between generalization error and the
number of nodes selected by the proposed method. Our theoretical results
demonstrate the trade-off between informativeness and representativeness in
active learning. Extensive numerical experiments show that the proposed method
is competitive with existing graph-based active learning methods, especially
when node covariates and responses contain noises. Additionally, the proposed
method is applicable to both regression and classification tasks on graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How <span class="highlight-title">Transformer</span>s Solve Propositional Logic Problems: A Mechanistic
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guan Zhe Hong, Nishanth Dikkala, Enming Luo, Cyrus Rashtchian, Xin Wang, Rina Panigrahy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown amazing performance on tasks that
require planning and reasoning. Motivated by this, we investigate the internal
mechanisms that underpin a network's ability to perform complex logical
reasoning. We first construct a synthetic propositional logic problem that
serves as a concrete test-bed for network training and evaluation. Crucially,
this problem demands nontrivial planning to solve, but we can train a small
transformer to achieve perfect accuracy. Building on our set-up, we then pursue
an understanding of precisely how a three-layer transformer, trained from
scratch, solves this problem. We are able to identify certain "planning" and
"reasoning" circuits in the network that necessitate cooperation between the
attention blocks to implement the desired logic. To expand our findings, we
then study a larger model, Mistral 7B. Using activation patching, we
characterize internal components that are critical in solving our logic
problem. Overall, our work systemically uncovers novel aspects of small and
large transformers, and continues the study of how they plan and reason.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Distributed Optimization with Delay-free Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Wu, Changxin Liu, Sindri Magnusson, Mikael Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing asynchronous distributed optimization algorithms often use
diminishing step-sizes that cause slow practical convergence, or use fixed
step-sizes that depend on and decrease with an upper bound of the delays. Not
only are such delay bounds hard to obtain in advance, but they also tend to be
large and rarely attained, resulting in unnecessarily slow convergence. This
paper develops asynchronous versions of two distributed algorithms, Prox-DGD
and DGD-ATC, for solving consensus optimization problems over undirected
networks. In contrast to alternatives, our algorithms can converge to the fixed
point set of their synchronous counterparts using step-sizes that are
independent of the delays. We establish convergence guarantees for convex and
strongly convex problems under both partial and total asynchrony. We also show
that the convergence speed of the two asynchronous methods adjusts to the
actual level of asynchrony rather than being constrained by the worst-case.
Numerical experiments demonstrate a strong practical performance of our
asynchronous algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages. arXiv admin note: text overlap with arXiv:2303.18034</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Implicit Bias of Gradient Descent on Separable Multiclass Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01350v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01350v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrithik Ravi, Clayton Scott, Daniel Soudry, Yutong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit bias describes the phenomenon where optimization-based training
algorithms, without explicit regularization, show a preference for simple
estimators even when more complex estimators have equal objective values.
Multiple works have developed the theory of implicit bias for binary
classification under the assumption that the loss satisfies an exponential tail
property. However, there is a noticeable gap in analysis for multiclass
classification, with only a handful of results which themselves are restricted
to the cross-entropy loss. In this work, we employ the framework of Permutation
Equivariant and Relative Margin-based (PERM) losses [Wang and Scott, 2024] to
introduce a multiclass extension of the exponential tail property. This class
of losses includes not only cross-entropy but also other losses. Using this
framework, we extend the implicit bias result of Soudry et al. [2018] to
multiclass classification. Furthermore, our proof techniques closely mirror
those of the binary case, thus illustrating the power of the PERM framework for
bridging the binary-multiclass gap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Table<span class="highlight-title">GPT</span>2: A Large Multimodal Model with Tabular Data Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02059v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02059v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aofeng Su, Aowen Wang, Chao Ye, Chen Zhou, Ga Zhang, Gang Chen, Guangcheng Zhu, Haobo Wang, Haokai Xu, Hao Chen, Haoze Li, Haoxuan Lan, Jiaming Tian, Jing Yuan, Junbo Zhao, Junlin Zhou, Kaizhe Shou, Liangyu Zha, Lin Long, Liyao Li, Pengzuo Wu, Qi Zhang, Qingyi Huang, Saisai Yang, Tao Zhang, Wentao Ye, Wufang Zhu, Xiaomeng Hu, Xijun Gu, Xinjie Sun, Xiang Li, Yuhang Yang, Zhiqing Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI
applications, presenting vast new opportunities across industries. Yet, the
integration of tabular data remains notably underdeveloped, despite its
foundational role in numerous real-world domains.
  This gap is critical for three main reasons. First, database or data
warehouse data integration is essential for advanced applications; second, the
vast and largely untapped resource of tabular data offers immense potential for
analysis; and third, the business intelligence domain specifically demands
adaptable, precise solutions that many current LLMs may struggle to provide.
  In response, we introduce TableGPT2, a model rigorously pre-trained and
fine-tuned with over 593.8K tables and 2.36M high-quality query-table-output
tuples, a scale of table-related data unprecedented in prior research. This
extensive training enables TableGPT2 to excel in table-centric tasks while
maintaining strong general language and coding abilities.
  One of TableGPT2's key innovations is its novel table encoder, specifically
designed to capture schema-level and cell-level information. This encoder
strengthens the model's ability to handle ambiguous queries, missing column
names, and irregular tables commonly encountered in real-world applications.
Similar to visual language models, this pioneering approach integrates with the
decoder to form a robust large multimodal model.
  We believe the results are compelling: over 23 benchmarking metrics,
TableGPT2 achieves an average performance improvement of 35.20% in the 7B model
and 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust
general-purpose capabilities intact.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully Hyperbolic Rotation for Knowledge Graph Embedding <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuyu Liang, Weihua Wang, Feilong Bao, Guanglai Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperbolic rotation is commonly used to effectively model knowledge graphs
and their inherent hierarchies. However, existing hyperbolic rotation models
rely on logarithmic and exponential mappings for feature transformation. These
models only project data features into hyperbolic space for rotation, limiting
their ability to fully exploit the hyperbolic space. To address this problem,
we propose a novel fully hyperbolic model designed for knowledge graph
embedding. Instead of feature mappings, we define the model directly in
hyperbolic space with the Lorentz model. Our model considers each relation in
knowledge graphs as a Lorentz rotation from the head entity to the tail entity.
We adopt the Lorentzian version distance as the scoring function for measuring
the plausibility of triplets. Extensive results on standard knowledge graph
completion benchmarks demonstrated that our model achieves competitive results
with fewer parameters. In addition, our model get the state-of-the-art
performance on datasets of CoDEx-s and CoDEx-m, which are more diverse and
challenging than before. Our code is available at
https://github.com/llqy123/FHRE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tang Li, Mengmeng Ma, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pretrained foundation models demonstrate exceptional performance and,
in some high-stakes applications, even surpass human experts. However, most of
these models are currently evaluated primarily on prediction accuracy,
overlooking the validity of the rationales behind their accurate predictions.
For the safe deployment of foundation models, there is a pressing need to
ensure double-correct predictions, i.e., correct prediction backed by correct
rationales. To achieve this, we propose a two-phase scheme: First, we curate a
new dataset that offers structured rationales for visual recognition tasks.
Second, we propose a rationale-informed optimization method to guide the model
in disentangling and localizing visual evidence for each rationale, without
requiring manual annotations. Extensive experiments and ablation studies
demonstrate that our model outperforms state-of-the-art models by up to 10.1%
in prediction accuracy across a wide range of tasks. Furthermore, our method
significantly improves the model's rationale correctness, improving
localization by 7.5% and disentanglement by 36.5%. Our dataset, source code,
and pretrained weights: https://github.com/deep-real/DCP
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 38th Conference on Neural Information
  Processing Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linguistic Collapse: Neural Collapse in (Large) Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Wu, Vardan Papyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural collapse ($\mathcal{NC}$) is a phenomenon observed in classification
tasks where top-layer representations collapse into their class means, which
become equinorm, equiangular and aligned with the classifiers. These behaviors
-- associated with generalization and robustness -- would manifest under
specific conditions: models are trained towards zero loss, with noise-free
labels belonging to balanced classes, which do not outnumber the model's hidden
dimension. Recent studies have explored $\mathcal{NC}$ in the absence of one or
more of these conditions to extend and capitalize on the associated benefits of
ideal geometries. Language modeling presents a curious frontier, as
\textit{training by token prediction} constitutes a classification task where
none of the conditions exist: the vocabulary is imbalanced and exceeds the
embedding dimension; different tokens might correspond to similar contextual
embeddings; and large language models (LLMs) in particular are typically only
trained for a few epochs. This paper empirically investigates the impact of
scaling the architectures and training of causal language models (CLMs) on
their progression towards $\mathcal{NC}$. We find that $\mathcal{NC}$
properties that develop with scale (and regularization) are linked to
generalization. Moreover, there is evidence of some relationship between
$\mathcal{NC}$ and generalization independent of scale. Our work thereby
underscores the generality of $\mathcal{NC}$ as it extends to the novel and
more challenging setting of language modeling. Downstream, we seek to inspire
further research on the phenomenon to deepen our understanding of LLMs -- and
neural networks at large -- and improve existing architectures based on
$\mathcal{NC}$-related properties. Our code is hosted on GitHub at
https://github.com/rhubarbwu/linguistic-collapse .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; 36 pages; 30 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-form factuality in large language models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can outperform crowdsourced human
annotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; 72 pages, 18 figures, 30 tables. Code at
  https://github.com/google-deepmind/long-form-factuality</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare
  Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Wang, Hao Li, Di Huang, Amir M. Rahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In digital healthcare, large language models (LLMs) have primarily been
utilized to enhance question-answering capabilities and improve patient
interactions. However, effective patient care necessitates LLM chains that can
actively gather information by posing relevant questions. This paper presents
HealthQ, a novel framework designed to evaluate the questioning capabilities of
LLM healthcare chains. We implemented several LLM chains, including
Retrieval-Augmented Generation (RAG), Chain of Thought (CoT), and reflective
chains, and introduced an LLM judge to assess the relevance and informativeness
of the generated questions. To validate HealthQ, we employed traditional
Natural Language Processing (NLP) metrics such as Recall-Oriented Understudy
for Gisting Evaluation (ROUGE) and Named Entity Recognition (NER)-based set
comparison, and constructed two custom datasets from public medical note
datasets, ChatDoctor and MTS-Dialog. Our contributions are threefold: we
provide the first comprehensive study on the questioning capabilities of LLMs
in healthcare conversations, develop a novel dataset generation pipeline, and
propose a detailed evaluation methodology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ bit2bit: 1-bit quanta video reconstruction via self-supervised photon
  prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23247v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23247v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehe Liu, Alexander Krull, Hector Basevi, Ales Leonardis, Michael W. Jenkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quanta image sensors, such as SPAD arrays, are an emerging sensor technology,
producing 1-bit arrays representing photon detection events over exposures as
short as a few nanoseconds. In practice, raw data are post-processed using
heavy spatiotemporal binning to create more useful and interpretable images at
the cost of degrading spatiotemporal resolution. In this work, we propose
bit2bit, a new method for reconstructing high-quality image stacks at the
original spatiotemporal resolution from sparse binary quanta image data.
Inspired by recent work on Poisson denoising, we developed an algorithm that
creates a dense image sequence from sparse binary photon data by predicting the
photon arrival location probability distribution. However, due to the binary
nature of the data, we show that the assumption of a Poisson distribution is
inadequate. Instead, we model the process with a Bernoulli lattice process from
the truncated Poisson. This leads to the proposal of a novel self-supervised
solution based on a masked loss function. We evaluate our method using both
simulated and real data. On simulated data from a conventional video, we
achieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06
photons per pixel per frame). We also present a novel dataset containing a wide
range of real SPAD high-speed videos under various challenging imaging
conditions. The scenes cover strong/weak ambient light, strong motion,
ultra-fast events, etc., which will be made available to the community, on
which we demonstrate the promise of our approach. Both reconstruction quality
and throughput substantially surpass the state-of-the-art methods (e.g., Quanta
Burst Photography (QBP)). Our approach significantly enhances the visualization
and usability of the data, enabling the application of existing analysis
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parameter Symmetry and Noise Equilibrium of Stochastic Gradient Descent <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07193v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07193v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Ziyin, Mingze Wang, Hongchao Li, Lei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symmetries are prevalent in deep learning and can significantly influence the
learning dynamics of neural networks. In this paper, we examine how exponential
symmetries -- a broad subclass of continuous symmetries present in the model
architecture or loss function -- interplay with stochastic gradient descent
(SGD). We first prove that gradient noise creates a systematic motion (a
``Noether flow") of the parameters $\theta$ along the degenerate direction to a
unique initialization-independent fixed point $\theta^*$. These points are
referred to as the {\it noise equilibria} because, at these points, noise
contributions from different directions are balanced and aligned. Then, we show
that the balance and alignment of gradient noise can serve as a novel
alternative mechanism for explaining important phenomena such as progressive
sharpening/flattening and representation formation within neural networks and
have practical implications for understanding techniques like representation
normalization and warmup.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GTA: Generative Trajectory Augmentation with Guidance for Offline
  Reinforcement Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16907v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16907v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewoo Lee, Sujin Yun, Taeyoung Yun, Jinkyoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline Reinforcement Learning (Offline RL) presents challenges of learning
effective decision-making policies from static datasets without any online
interactions. Data augmentation techniques, such as noise injection and data
synthesizing, aim to improve Q-function approximation by smoothing the learned
state-action region. However, these methods often fall short of directly
improving the quality of offline datasets, leading to suboptimal results. In
response, we introduce GTA, Generative Trajectory Augmentation, a novel
generative data augmentation approach designed to enrich offline data by
augmenting trajectories to be both high-rewarding and dynamically plausible.
GTA applies a diffusion model within the data augmentation framework. GTA
partially noises original trajectories and then denoises them with
classifier-free guidance via conditioning on amplified return value. Our
results show that GTA, as a general data augmentation strategy, enhances the
performance of widely used offline RL algorithms across various tasks with
unique challenges. Furthermore, we conduct a quality analysis of data augmented
by GTA and demonstrate that GTA improves the quality of the data. Our code is
available at https://github.com/Jaewoopudding/GTA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Previously accepted (Spotlight) to ICLR 2024 Workshop
  on Generative Models for Decision Making. Jaewoo Lee and Sujin Yun are equal
  contribution authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling Efficient On-Device Fine-Tuning of LLMs Using Only Inference
  Engines <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Gao, Amir Ziashahabi, Yue Niu, Salman Avestimehr, Murali Annavaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are currently pre-trained and fine-tuned on
large cloud servers. The next frontier is LLM personalization, where a
foundation model can be fine-tuned with user/task-specific data. Given the
sensitive nature of such private data, it is desirable to fine-tune these
models on edge devices to improve user trust. However, fine-tuning on
resource-constrained edge devices presents significant challenges due to
substantial memory and computational demands, as well as limited infrastructure
support. We observe that inference engines (e.g., ExecuTorch) can be repurposed
for fine-tuning by leveraging zeroth-order (ZO) optimization, which uses
multiple forward passes to approximate gradients. However, directly applying ZO
methods on edge devices is impractical due to the high computational cost of
multiple model perturbations required to achieve accuracy improvements. Based
on these observations, we propose a memory- and computation-efficient LLM
fine-tuning method for edge devices. Our approach has three key innovations:
(1) We introduce a parallelized randomized gradient estimation (P-RGE)
technique that achieves high parallel efficiency by leveraging outer-loop and
inner-loop parallelization. This enables multiple function queries and forward
passes to be executed in parallel, reducing training time. (2) We integrate
P-RGE with parameter-efficient fine-tuning methods (e.g. LoRA) to further
reduce computational and memory overhead. (3) We implement a P-RGE LoRA-FA
module that fully supports fine-tuning with ExecuTorch. Our approach requires
no modifications to ExecuTorch's runtime code, as it can be implemented with
server-side code changes only. Experiments demonstrate that P-RGE achieves
substantial runtime speedups and memory savings while improving fine-tuning
accuracy, paving the way for practical deployment of LLMs in real-time,
on-device applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 ENLSP-IV workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CONTRAST: Continual Multi-source Adaptation to Dynamic Distributions <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sk Miraj Ahmed, Fahim Faisal Niloy, Xiangyu Chang, Dripta S. Raychaudhuri, Samet Oymak, Amit K. Roy-Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting to dynamic data distributions is a practical yet challenging task.
One effective strategy is to use a model ensemble, which leverages the diverse
expertise of different models to transfer knowledge to evolving data
distributions. However, this approach faces difficulties when the dynamic test
distribution is available only in small batches and without access to the
original source data. To address the challenge of adapting to dynamic
distributions in such practical settings, we propose Continual Multi-source
Adaptation to Dynamic Distributions (CONTRAST), a novel method that optimally
combines multiple source models to adapt to the dynamic test data. CONTRAST has
two distinguishing features. First, it efficiently computes the optimal
combination weights to combine the source models to adapt to the test data
distribution continuously as a function of time. Second, it identifies which of
the source model parameters to update so that only the model which is most
correlated to the target data is adapted, leaving the less correlated ones
untouched; this mitigates the issue of ``forgetting" the source model
parameters by focusing only on the source model that exhibits the strongest
correlation with the test batch distribution. Through theoretical analysis we
show that the proposed method is able to optimally combine the source models
and prioritize updates to the model least prone to forgetting. Experimental
analysis on diverse datasets demonstrates that the combination of multiple
source models does at least as well as the best source (with hindsight
knowledge), and performance does not degrade as the test data distribution
changes over time (robust to forgetting).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Birdie: Advancing State Space Models with Reward-Driven Objectives and
  Curricula <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01030v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01030v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Blouir, Jimmy T. H. Smith, Antonios Anastasopoulos, Amarda Shehu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient state space models (SSMs), such as linear recurrent neural networks
and linear attention variants, offer computational advantages over Transformers
but struggle with tasks requiring long-range in-context retrieval-like text
copying, associative recall, and question answering over long contexts.
Previous efforts to address these challenges have focused on architectural
modifications, often reintroducing computational inefficiencies. In this paper,
we propose a novel training procedure, Birdie, that significantly enhances the
in-context retrieval capabilities of SSMs without altering their architecture.
Our approach combines bidirectional input processing with dynamic mixtures of
specialized pre-training objectives, optimized via reinforcement learning. We
introduce a new bidirectional SSM architecture that seamlessly transitions from
bidirectional context processing to causal generation. Experimental evaluations
demonstrate that Birdie markedly improves performance on retrieval-intensive
tasks such as multi-number phone book lookup, long paragraph
question-answering, and infilling. This narrows the performance gap with
Transformers, while retaining computational efficiency. Our findings highlight
the importance of training procedures in leveraging the fixed-state capacity of
SSMs, offering a new direction to advance their capabilities. All code and
pre-trained models are available at https://www.github.com/samblouir/birdie,
with support for JAX and PyTorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Abstracted Shapes as Tokens -- A Generalizable and Interpretable Model
  for Time-series Classification <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunshi Wen, Tengfei Ma, Tsui-Wei Weng, Lam M. Nguyen, Anak Agung Julius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In time-series analysis, many recent works seek to provide a unified view and
representation for time-series across multiple domains, leading to the
development of foundation models for time-series data. Despite diverse modeling
techniques, existing models are black boxes and fail to provide insights and
explanations about their representations. In this paper, we present VQShape, a
pre-trained, generalizable, and interpretable model for time-series
representation learning and classification. By introducing a novel
representation for time-series data, we forge a connection between the latent
space of VQShape and shape-level features. Using vector quantization, we show
that time-series from different domains can be described using a unified set of
low-dimensional codes, where each code can be represented as an abstracted
shape in the time domain. On classification tasks, we show that the
representations of VQShape can be utilized to build interpretable classifiers,
achieving comparable performance to specialist models. Additionally, in
zero-shot learning, VQShape and its codebook can generalize to previously
unseen datasets and domains that are not included in the pre-training process.
The code and pre-trained weights are available at
https://github.com/YunshiWen/VQShape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neural Information Processing Systems (NeurIPS) 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A multi-purpose automatic editing system based on lecture semantics for
  remote education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panwen Hu, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote teaching has become popular recently due to its convenience and
safety, especially under extreme circumstances like a pandemic. However, online
students usually have a poor experience since the information acquired from the
views provided by the broadcast platforms is limited. One potential solution is
to show more camera views simultaneously, but it is technically challenging and
distracting for the viewers. Therefore, an automatic multi-camera
directing/editing system, which aims at selecting the most concerned view at
each time instance to guide the attention of online students, is in urgent
demand. However, existing systems mostly make simple assumptions and focus on
tracking the position of the speaker instead of the real lecture semantics, and
therefore have limited capacities to deliver optimal information flow. To this
end, this paper proposes an automatic multi-purpose editing system based on the
lecture semantics, which can both direct the multiple video streams for
real-time broadcasting and edit the optimal video offline for review purposes.
Our system directs the views by semantically analyzing the class events while
following the professional directing rules, mimicking a human director to
capture the regions of interest from the viewpoint of the onsite students. We
conduct both qualitative and quantitative analyses to verify the effectiveness
of the proposed system and its components.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Sign Language Recognition System using Deep Learning with
  MediaPipe Holistic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharvani Srivastava, Sudhakar Singh,  Pooja, Shiv Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign languages are the language of hearing-impaired people who use visuals
like the hand, facial, and body movements for communication. There are
different signs and gestures representing alphabets, words, and phrases.
Nowadays approximately 300 sign languages are being practiced worldwide such as
American Sign Language (ASL), Chinese Sign Language (CSL), Indian Sign Language
(ISL), and many more. Sign languages are dependent on the vocal language of a
place. Unlike vocal or spoken languages, there are no helping words in sign
language like is, am, are, was, were, will, be, etc. As only a limited
population is well-versed in sign language, this lack of familiarity of sign
language hinders hearing-impaired people from communicating freely and easily
with everyone. This issue can be addressed by a sign language recognition (SLR)
system which has the capability to translate the sign language into vocal
language. In this paper, a continuous SLR system is proposed using a deep
learning model employing Long Short-Term Memory (LSTM), trained and tested on
an ISL primary dataset. This dataset is created using MediaPipe Holistic
pipeline for tracking face, hand, and body movements and collecting landmarks.
The system recognizes the signs and gestures in real-time with 88.23% accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, Wireless Pers Commun</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Concatenator: A Bayesian Approach To Real Time Concatenative
  Musaicing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Tralie, Ben Cantil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ``The Concatenator,'' a real time system for audio-guided
concatenative synthesis. Similarly to Driedger et al.'s ``musaicing'' (or
``audio mosaicing'') technique, we concatenate a set number of windows within a
corpus of audio to re-create the harmonic and percussive aspects of a target
audio stream. Unlike Driedger's NMF-based technique, however, we instead use an
explicitly Bayesian point of view, where corpus window indices are hidden
states and the target audio stream is an observation. We use a particle filter
to infer the best hidden corpus states in real-time. Our transition model
includes a tunable parameter to control the time-continuity of corpus grains,
and our observation model allows users to prioritize how quickly windows change
to match the target. Because the computational complexity of the system is
independent of the corpus size, our system scales to corpora that are hours
long, which is an important feature in the age of vast audio data collections.
Within The Concatenator module itself, composers can vary grain length, fit to
target, and pitch shift in real time while reacting to the sounds they hear,
enabling them to rapidly iterate ideas. To conclude our work, we evaluate our
system with extensive quantitative tests of the effects of parameters, as well
as a qualitative evaluation with artistic insights. Based on the quality of the
results, we believe the real-time capability unlocks new avenues for musical
expression and control, suitable for live performance and modular synthesis
integration, which furthermore represents an essential breakthrough in
concatenative synthesis technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, Accepted for Publication in The International
  Society for Music Information Retrieval Proceedings, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIAST: A Multimodal Piano <span class="highlight-title">Dataset</span> with Audio, Symbolic and Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hayeon Bang, Eunjin Choi, Megan Finch, Seungheon Doh, Seolhee Lee, Gyeong-Hoon Lee, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While piano music has become a significant area of study in Music Information
Retrieval (MIR), there is a notable lack of datasets for piano solo music with
text labels. To address this gap, we present PIAST (PIano dataset with Audio,
Symbolic, and Text), a piano music dataset. Utilizing a piano-specific taxonomy
of semantic tags, we collected 9,673 tracks from YouTube and added human
annotations for 2,023 tracks by music experts, resulting in two subsets:
PIAST-YT and PIAST-AT. Both include audio, text, tag annotations, and
transcribed MIDI utilizing state-of-the-art piano transcription and beat
tracking models. Among many possible tasks with the multi-modal dataset, we
conduct music tagging and retrieval using both audio and MIDI data and report
baseline performances to demonstrate its potential as a valuable resource for
MIR research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 3rd Workshop on NLP for Music and
  Audio (NLP4MusA 2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">89</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing The Language of Visual Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David M. Chan, Rodolfo Corona, Joonyong Park, Cheol Jun Cho, Yutong Bai, Trevor Darrell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the introduction of transformer-based models for vision and language
tasks, such as LLaVA and Chameleon, there has been renewed interest in the
discrete tokenized representation of images. These models often treat image
patches as discrete tokens, analogous to words in natural language, learning
joint alignments between visual and human languages. However, little is known
about the statistical behavior of these visual languages - whether they follow
similar frequency distributions, grammatical structures, or topologies as
natural languages. In this paper, we take a natural-language-centric approach
to analyzing discrete visual languages and uncover striking similarities and
fundamental differences. We demonstrate that, although visual languages adhere
to Zipfian distributions, higher token innovation drives greater entropy and
lower compression, with tokens predominantly representing object parts,
indicating intermediate granularity. We also show that visual languages lack
cohesive grammatical structures, leading to higher perplexity and weaker
hierarchical organization compared to natural languages. Finally, we
demonstrate that, while vision models align more closely with natural languages
than other models, this alignment remains significantly weaker than the
cohesion found within natural languages. Through these experiments, we
demonstrate how understanding the statistical properties of discrete visual
languages can inform the design of more effective computer vision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Needle Threading: Can LLMs Follow Threads through Near-Million-Scale
  Haystacks? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05000v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05000v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Roberts, Kai Han, Samuel Albanie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the context limits of Large Language Models (LLMs) increase, the range of
possible applications and downstream functions broadens. In many real-world
tasks, decisions depend on details scattered across collections of often
disparate documents containing mostly irrelevant information. Long-context LLMs
appear well-suited to this form of complex information retrieval and reasoning,
which has traditionally proven costly and time-consuming. However, although the
development of longer context models has seen rapid gains in recent years, our
understanding of how effectively LLMs use their context has not kept pace. To
address this, we conduct a set of retrieval experiments designed to evaluate
the capabilities of 17 leading LLMs, such as their ability to follow threads of
information through the context window. Strikingly, we find that many models
are remarkably threadsafe: capable of simultaneously following multiple threads
without significant loss in performance. Still, for many models, we find the
effective context limit is significantly shorter than the supported context
length, with accuracy decreasing as the context window grows. Our study also
highlights the important point that token counts from different tokenizers
should not be directly compared -- they often correspond to substantially
different numbers of written characters. We release our code and long-context
experimental data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIP is one of the most important multimodal foundational models today. What
powers CLIP's capabilities? The rich supervision signals provided by natural
language, the carrier of human knowledge, shape a powerful cross-modal
representation space. However, with the rapid advancements in large language
models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and
generation are continually being pushed. This raises an intriguing question:
can the capabilities of LLMs be harnessed to further improve multimodal
representation learning? The potential benefits of incorporating LLMs into CLIP
are clear. LLMs' strong textual understanding can fundamentally improve CLIP's
ability to handle image captions, drastically enhancing its ability to process
long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs
are trained on a vast corpus of text, possessing open-world knowledge. This
allows them to expand on caption information during training, increasing the
efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel
approach that embraces the power of LLMs to unlock CLIP's potential. By
fine-tuning the LLM in the caption space with contrastive learning, we extract
its textual capabilities into the output embeddings, significantly improving
the output layer's textual discriminability. We then design an efficient
training process where the fine-tuned LLM acts as a powerful teacher for CLIP's
visual encoder. Thanks to the LLM's presence, we can now incorporate longer and
more complex captions without being restricted by vanilla CLIP's text encoder's
context window and ability limitations. Our experiments demonstrate that this
approach brings substantial improvements in cross-modal tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture-of-<span class="highlight-title">Transformer</span>s: A Sparse and Scalable Architecture for
  Multi-Modal Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, Xi Victoria Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) has expanded to multi-modal
systems capable of processing text, images, and speech within a unified
framework. Training these models demands significantly larger datasets and
computational resources compared to text-only LLMs. To address the scaling
challenges, we introduce Mixture-of-Transformers (MoT), a sparse multi-modal
transformer architecture that significantly reduces pretraining computational
costs. MoT decouples non-embedding parameters of the model by modality --
including feed-forward networks, attention matrices, and layer normalization --
enabling modality-specific processing with global self-attention over the full
input sequence. We evaluate MoT across multiple settings and model scales. In
the Chameleon 7B setting (autoregressive text-and-image generation), MoT
matches the dense baseline's performance using only 55.8\% of the FLOPs. When
extended to include speech, MoT reaches speech performance comparable to the
dense baseline with only 37.2\% of the FLOPs. In the Transfusion setting, where
text and image are trained with different objectives, a 7B MoT model matches
the image modality performance of the dense baseline with one third of the
FLOPs, and a 760M MoT model outperforms a 1.4B dense baseline across key image
generation metrics. System profiling further highlights MoT's practical
benefits, achieving dense baseline image quality in 47.2\% of the wall-clock
time and text quality in 75.6\% of the wall-clock time (measured on AWS
p4de.24xlarge instances with NVIDIA A100 GPUs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Semantic Hub Hypothesis: Language Models Share Semantic
  Representations Across Languages and Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaofeng Wu, Xinyan Velocity Yu, Dani Yogatama, Jiasen Lu, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern language models can process inputs across diverse languages and
modalities. We hypothesize that models acquire this capability through learning
a shared representation space across heterogeneous data types (e.g., different
languages and modalities), which places semantically similar inputs near one
another, even if they are from different modalities/languages. We term this the
semantic hub hypothesis, following the hub-and-spoke model from neuroscience
(Patterson et al., 2007) which posits that semantic knowledge in the human
brain is organized through a transmodal semantic "hub" which integrates
information from various modality-specific "spokes" regions. We first show that
model representations for semantically equivalent inputs in different languages
are similar in the intermediate layers, and that this space can be interpreted
using the model's dominant pretraining language via the logit lens. This
tendency extends to other data types, including arithmetic expressions, code,
and visual/audio inputs. Interventions in the shared representation space in
one data type also predictably affect model outputs in other data types,
suggesting that this shared representations space is not simply a vestigial
byproduct of large-scale training on broad data, but something that is actively
utilized by the model during input processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuffixDecoding: A Model-Free Approach to Speeding Up Large Language
  Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Oliaro, Zhihao Jia, Daniel Campos, Aurick Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SuffixDecoding, a novel model-free approach to accelerating large
language model (LLM) inference through speculative decoding. Unlike existing
methods that rely on draft models or specialized decoding heads, SuffixDecoding
leverages suffix trees built from previously generated outputs to efficiently
predict candidate token sequences. Our approach enables flexible
tree-structured speculation without the overhead of maintaining and
orchestrating additional models. SuffixDecoding builds and dynamically updates
suffix trees to capture patterns in the generated text, using them to construct
speculation trees through a principled scoring mechanism based on empirical
token frequencies. SuffixDecoding requires only CPU memory which is plentiful
and underutilized on typical LLM serving nodes. We demonstrate that
SuffixDecoding achieves competitive speedups compared to model-based approaches
across diverse workloads including open-domain chat, code generation, and
text-to-SQL tasks. For open-ended chat and code generation tasks,
SuffixDecoding achieves up to $1.4\times$ higher output throughput than
SpecInfer and up to $1.1\times$ lower time-per-token (TPOT) latency. For a
proprietary multi-LLM text-to-SQL application, SuffixDecoding achieves up to
$2.9\times$ higher output throughput and $3\times$ lower latency than
speculative decoding. Our evaluation shows that SuffixDecoding maintains high
acceptance rates even with small reference corpora of 256 examples, while
continuing to improve performance as more historical outputs are incorporated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BitNet a4.8: 4-bit Activations for 1-bit LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Wang, Shuming Ma, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on the 1-bit Large Language Models (LLMs), such as BitNet
b1.58, presents a promising direction for reducing the inference cost of LLMs
while maintaining their performance. In this work, we introduce BitNet a4.8,
enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid
quantization and sparsification strategy to mitigate the quantization errors
introduced by the outlier channels. Specifically, we utilize 4-bit activations
for inputs to the attention and feed-forward network layers, while sparsifying
intermediate states followed with 8-bit quantization. Extensive experiments
demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58
with equivalent training costs, while being faster in inference with enabling
4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of
parameters and supports 3-bit KV cache, further enhancing the efficiency of
large-scale LLM deployment and inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position Paper On Diagnostic Uncertainty Estimation from Large Language
  Models: Next-Word Probability Is Not Pre-test Probability <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Gao, Skatje Myers, Shan Chen, Dmitriy Dligach, Timothy A Miller, Danielle Bitterman, Guanhua Chen, Anoop Mayampurath, Matthew Churpek, Majid Afshar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are being explored for diagnostic decision
support, yet their ability to estimate pre-test probabilities, vital for
clinical decision-making, remains limited. This study evaluates two LLMs,
Mistral-7B and Llama3-70B, using structured electronic health record data on
three diagnosis tasks. We examined three current methods of extracting LLM
probability estimations and revealed their limitations. We aim to highlight the
need for improved techniques in LLM confidence estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to GenAI4Health Workshop at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page
  Multi-document Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemin Cho, Debanjan Mahata, Ozan Irsoy, Yujie He, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document visual question answering (DocVQA) pipelines that answer questions
from documents have broad applications. Existing methods focus on handling
single-page documents with multi-modal language models (MLMs), or rely on
text-based retrieval-augmented generation (RAG) that uses text extraction tools
such as optical character recognition (OCR). However, there are difficulties in
applying these methods in real-world scenarios: (a) questions often require
information across different pages or documents, where MLMs cannot handle many
long documents; (b) documents often have important information in visual
elements such as figures, but text extraction tools ignore them. We introduce
M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various
document contexts (closed-domain and open-domain), question hops (single-hop
and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG
finds relevant documents and answers questions using a multi-modal retriever
and an MLM, so that it can efficiently handle single or many documents while
preserving visual information. Since previous DocVQA datasets ask questions in
the context of a specific document, we also present M3DocVQA, a new benchmark
for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages.
In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results
show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance
than many strong baselines, including state-of-the-art performance in
MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and
retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully
handle various scenarios, such as when relevant information exists across
multiple pages and when answer evidence only exists in images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://m3docrag.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating the Influence of Sequentially Correlated Literary Properties
  in Textual Classification: A Data-Centric Hypothesis-Testing Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gideon Yoffe, Nachum Dershowitz, Ariel Vishne, Barak Sober
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stylometry aims to distinguish authors by analyzing literary traits assumed
to reflect semi-conscious choices distinct from elements like genre or theme.
However, these components often overlap, complicating text classification based
solely on feature distributions. While some literary properties, such as
thematic content, are likely to manifest as correlations between adjacent text
units, others, like authorial style, may be independent thereof. We introduce a
hypothesis-testing approach to evaluate the influence of sequentially
correlated literary properties on text classification, aiming to determine when
these correlations drive classification. Using a multivariate binary
distribution, our method models sequential correlations between text units as a
stochastic process, assessing the likelihood of clustering across varying
adjacency scales. This enables us to examine whether classification is
dominated by sequentially correlated properties or remains independent. In
experiments on a diverse English prose corpus, our analysis integrates
traditional and neural embeddings within supervised and unsupervised
frameworks. Results demonstrate that our approach effectively identifies when
textual classification is not primarily influenced by sequentially correlated
literary properties, particularly in cases where texts differ in authorial
style or genre rather than by a single author within a similar genre.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>KB: Building Very Large Knowledge Bases from Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Hu, Shrestha Ghosh, Tuan-Phong Nugyen, Simon Razniewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General-domain knowledge bases (KB), in particular the "big three" --
Wikidata, Yago and DBpedia -- are the backbone of many intelligent
applications. While these three have seen steady development, comprehensive KB
construction at large has seen few fresh attempts. In this work, we propose to
build a large general-domain KB entirely from a large language model (LLM). We
demonstrate the feasibility of large-scale KB construction from LLMs, while
highlighting specific challenges arising around entity recognition, entity and
property canonicalization, and taxonomy construction. As a prototype, we use
GPT-4o-mini to construct GPTKB, which contains 105 million triples for more
than 2.9 million entities, at a cost 100x less than previous KBC projects. Our
work is a landmark for two fields: For NLP, for the first time, it provides
\textit{constructive} insights into the knowledge (or beliefs) of LLMs. For the
Semantic Web, it shows novel ways forward for the long-standing challenge of
general-domain KB construction. GPTKB is accessible at https://gptkb.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GASE: Generatively Augmented Sentence Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Frank, Haithem Afli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an approach to enhance sentence embeddings by applying generative
text models for data augmentation at inference time. Unlike conventional data
augmentation that utilises synthetic training data, our approach does not
require access to model parameters or the computational resources typically
required for fine-tuning state-of-the-art models. Generatively Augmented
Sentence Encoding uses diverse linguistic synthetic variants of input texts
generated by paraphrasing, summarising, or extracting keywords, followed by
pooling the original and synthetic embeddings. Experimental results on the
Massive Text Embedding Benchmark for Semantic Textual Similarity (STS)
demonstrate performance improvements across a range of embedding models using
different generative models for augmentation. We find that generative
augmentation leads to larger performance improvements for embedding models with
lower baseline performance. These findings suggest that integrating generative
augmentation at inference time adds semantic diversity and can enhance the
robustness and generalizability of sentence embeddings for embedding models.
Our results show that the degree to which generative augmentation can improve
STS performance depends not only on the embedding model but also on the
dataset. From a broader perspective, the approach allows trading training for
inference compute.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H. Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, Wei Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) for code have become indispensable in various
domains, including code generation, reasoning tasks and agent systems.While
open-access code LLMs are increasingly approaching the performance levels of
proprietary models, high-quality code LLMs suitable for rigorous scientific
investigation, particularly those with reproducible data processing pipelines
and transparent training protocols, remain limited. The scarcity is due to
various challenges, including resource constraints, ethical considerations, and
the competitive advantages of keeping models advanced. To address the gap, we
introduce OpenCoder, a top-tier code LLM that not only achieves performance
comparable to leading models but also serves as an ``open cookbook'' for the
research community. Unlike most prior efforts, we release not only model
weights and inference code, but also the reproducible training data, complete
data processing pipeline, rigorous experimental ablation results, and detailed
training protocols for open scientific research. Through this comprehensive
release, we identify the key ingredients for building a top-tier code LLM: (1)
code optimized heuristic rules for data cleaning and methods for data
deduplication, (2) recall of text corpus related to code and (3) high-quality
synthetic data in both annealing and supervised fine-tuning stages. By offering
this level of openness, we aim to broaden access to all aspects of a top-tier
code LLM, with OpenCoder serving as both a powerful model and an open
foundation to accelerate research, and enable reproducible advancements in code
AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sentiment Analysis of Spanish Political Party Tweets Using Pre-trained
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuqiao Song, Shunzhang Chen, Xinyi Cai, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Title: Sentiment Analysis of Spanish Political Party Communications on
Twitter Using Pre-trained Language Models
  Authors: Chuqiao Song, Shunzhang Chen, Xinyi Cai, Hao Chen
  Comments: 21 pages, 6 figures
  Abstract: This study investigates sentiment patterns within Spanish political
party communications on Twitter by leveraging BETO and RoBERTuito, two
pre-trained language models optimized for Spanish text. Using a dataset of
tweets from major Spanish political parties: PSOE, PP, Vox, Podemos, and
Ciudadanos, spanning 2019 to 2024, this research analyzes sentiment
distributions and explores the relationship between sentiment expression and
party ideology. The findings indicate that both models consistently identify a
predominant Neutral sentiment across all parties, with significant variations
in Negative and Positive sentiments that align with ideological distinctions.
Specifically, Vox exhibits higher levels of Negative sentiment, while PSOE
demonstrates relatively high Positive sentiment, supporting the hypothesis that
emotional appeals in political messaging reflect ideological stances. This
study underscores the potential of pre-trained language models for non-English
sentiment analysis on social media, providing insights into sentiment dynamics
that shape public discourse within Spain's multi-party political system.
  Keywords: Spanish politics, sentiment analysis, pre-trained language models,
Twitter, BETO, RoBERTuito, political ideology, multi-party system
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prompt-Guided Internal States for Hallucination Detection of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fujie Zhang, Peiqi Yu, Biao Yi, Baolei Zhang, Tong Li, Zheli Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities across
a variety of tasks in different domains. However, they sometimes generate
responses that are logically coherent but factually incorrect or misleading,
which is known as LLM hallucinations. Data-driven supervised methods train
hallucination detectors by leveraging the internal states of LLMs, but
detectors trained on specific domains often struggle to generalize well to
other domains. In this paper, we aim to enhance the cross-domain performance of
supervised detectors with only in-domain data. We propose a novel framework,
prompt-guided internal states for hallucination detection of LLMs, namely
PRISM. By utilizing appropriate prompts to guide changes in the structure
related to text truthfulness within the LLM's internal states, we make this
structure more salient and consistent across texts from different domains. We
integrated our framework with existing hallucination detection methods and
conducted experiments on datasets from different domains. The experimental
results indicate that our framework significantly enhances the cross-domain
generalization of existing hallucination detection methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VTechAGP: An Academic-to-General-Audience Text Paraphrase <span class="highlight-title">Dataset</span> and
  Benchmark Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Cheng, Jiaying Gong, Chenhan Yuan, William A. Ingram, Edward Fox, Hoda Eldardiry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing text simplification or paraphrase datasets mainly focus on
sentence-level text generation in a general domain. These datasets are
typically developed without using domain knowledge. In this paper, we release a
novel dataset, VTechAGP, which is the first academic-to-general-audience text
paraphrase dataset consisting of 4,938 document-level these and dissertation
academic and general-audience abstract pairs from 8 colleges authored over 25
years. We also propose a novel dynamic soft prompt generative language model,
DSPT5. For training, we leverage a contrastive-generative loss function to
learn the keyword vectors in the dynamic prompt. For inference, we adopt a
crowd-sampling decoding strategy at both semantic and structural levels to
further select the best output candidate. We evaluate DSPT5 and various
state-of-the-art large language models (LLMs) from multiple perspectives.
Results demonstrate that the SOTA LLMs does not provide satisfactory outcomes,
while the lightweight DSPT5 can achieve competitive results. To the best of our
knowledge, we are the first to build a benchmark dataset and solutions for
academic-to-general-audience text paraphrase dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Does Classical Chinese Help? Quantifying Cross-Lingual Transfer in
  Hanja and Kanbun 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyoung Song, Haneul Yoo, Jiho Jin, Kyunghyun Cho, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Historical and linguistic connections within the Sinosphere have led
researchers to use Classical Chinese resources for cross-lingual transfer when
processing historical documents from Korea and Japan. In this paper, we
question the assumption of cross-lingual transferability from Classical Chinese
to Hanja and Kanbun, the ancient written languages of Korea and Japan,
respectively. Our experiments across machine translation, named entity
recognition, and punctuation restoration tasks show minimal impact of Classical
Chinese datasets on language model performance for ancient Korean documents
written in Hanja, with performance differences within $\pm{}0.0068$ F1-score
for sequence labeling tasks and up to $+0.84$ BLEU score for translation. These
limitations persist consistently across various model sizes, architectures, and
domain-specific datasets. Our analysis reveals that the benefits of Classical
Chinese resources diminish rapidly as local language data increases for Hanja,
while showing substantial improvements only in extremely low-resource scenarios
for both Korean and Japanese historical documents. These mixed results
emphasize the need for careful empirical validation rather than assuming
benefits from indiscriminate cross-lingual transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LuxBank: The First Universal Dependency Treebank for Luxembourgish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Plum, Caroline Döhmer, Emilia Milano, Anne-Marie Lutgen, Christoph Purschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Universal Dependencies (UD) project has significantly expanded linguistic
coverage across 161 languages, yet Luxembourgish, a West Germanic language
spoken by approximately 400,000 people, has remained absent until now. In this
paper, we introduce LuxBank, the first UD Treebank for Luxembourgish,
addressing the gap in syntactic annotation and analysis for this `low-research'
language. We establish formal guidelines for Luxembourgish language annotation,
providing the foundation for the first large-scale quantitative analysis of its
syntax. LuxBank serves not only as a resource for linguists and language
learners but also as a tool for developing spell checkers and grammar checkers,
organising existing text archives and even training large language models. By
incorporating Luxembourgish into the UD framework, we aim to enhance the
understanding of syntactic variation within West Germanic languages and offer a
model for documenting smaller, semi-standardised languages. This work positions
Luxembourgish as a valuable resource in the broader linguistic and NLP
communities, contributing to the study of languages with limited research and
resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 22nd Workshop on Treebanks and Linguistic Theories (TLT
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kwai-STaR: Transform LLMs into State-Transition Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Lu, Yuhang Hu, Changyi Liu, Tianke Zhang, Zhenyu Yang, Zhixiang Ding, Shengsheng Qian, Meng Du, Ruiwen Kang, Kaiyu Tang, Fan Yang, Tingting Gao, Di Zhang, Hai-Tao Zheng, Bin Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning presents a significant challenge to the cognitive
capabilities of LLMs. Various methods have been proposed to enhance the
mathematical ability of LLMs. However, few recognize the value of state
transition for LLM reasoning. In this work, we define mathematical
problem-solving as a process of transiting from an initial unsolved state to
the final resolved state, and propose Kwai-STaR framework, which transforms
LLMs into State-Transition Reasoners to improve their intuitive reasoning
capabilities. Our approach comprises three main steps: (1) Define the state
space tailored to the mathematical reasoning. (2) Generate state-transition
data based on the state space. (3) Convert original LLMs into State-Transition
Reasoners via a curricular training strategy. Our experiments validate the
effectiveness of Kwai-STaR in enhancing mathematical reasoning: After training
on the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and
LLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard
dataset. Additionally, the state transition-based design endows Kwai-STaR with
remarkable training and inference efficiency. Further experiments are underway
to establish the generality of Kwai-STaR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlignXIE: Improving Multilingual Information Extraction by Cross-Lingual
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Zuo, Wenxuan Jiang, Wenxuan Liu, Zixuan Li, Long Bai, Hanbin Wang, Yutao Zeng, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empirical evidence suggests that LLMs exhibit spontaneous cross-lingual
alignment. Our findings suggest that although LLMs also demonstrate promising
cross-lingual alignment in Information Extraction, there remains significant
imbalance across languages, revealing an underlying deficiency in the IE
alignment. To address this issue, we propose AlignXIE, a powerful code-based
LLM that significantly enhances cross-lingual IE alignment through two
strategies. Firstly, AlignXIE formulates IE across different languages,
especially non-English ones, as code generation tasks, standardizing the
representation of various schemas using Python classes to ensure consistency of
the same ontology in different languages and align the schema. Secondly, it
incorporates an IE cross-lingual alignment phase through a translated instance
prediction task proposed in this paper to align the extraction process,
utilizing ParallelNER, an IE bilingual parallel dataset with 257,190 samples,
generated by our proposed LLM-based automatic pipeline for IE parallel data
construction, with manual annotation to ensure quality. Ultimately, we obtain
AlignXIE through multilingual IE instruction tuning. Although without training
in 9 unseen languages, AlignXIE surpasses ChatGPT by $30.17\%$ and SoTA by
$20.03\%$, thereby demonstrating superior cross-lingual IE capabilities.
Comprehensive evaluations on 63 IE benchmarks in Chinese and English under
various settings, demonstrate that AlignXIE significantly enhances
cross-lingual and multilingual IE through boosting the IE alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in
  Financial Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuewen Han, Neng Wang, Shangkun Che, Hongyang Yang, Kunpeng Zhang, Sean Xin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the application of generative artificial intelligence
(GenAI) in financial analysis and investment decision-making has gained
significant attention. However, most existing approaches rely on single-agent
systems, which fail to fully utilize the collaborative potential of multiple AI
agents. In this paper, we propose a novel multi-agent collaboration system
designed to enhance decision-making in financial investment research. The
system incorporates agent groups with both configurable group sizes and
collaboration structures to leverage the strengths of each agent group type. By
utilizing a sub-optimal combination strategy, the system dynamically adapts to
varying market conditions and investment scenarios, optimizing performance
across different tasks. We focus on three sub-tasks: fundamentals, market
sentiment, and risk analysis, by analyzing the 2023 SEC 10-K forms of 30
companies listed on the Dow Jones Index. Our findings reveal significant
performance variations based on the configurations of AI agents for different
tasks. The results demonstrate that our multi-agent collaboration system
outperforms traditional single-agent models, offering improved accuracy,
efficiency, and adaptability in complex financial environments. This study
highlights the potential of multi-agent systems in transforming financial
analysis and investment decision-making by integrating diverse analytical
perspectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A study of Vietnamese readability assessing through semantic and
  statistical features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung Tuan Le, Long Truong To, Manh Trong Nguyen, Quyen Nguyen, Trong-Hop Do
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the difficulty of a text involves assessing various textual
features that may impact the reader's text comprehension, yet current research
in Vietnamese has only focused on statistical features. This paper introduces a
new approach that integrates statistical and semantic approaches to assessing
text readability. Our research utilized three distinct datasets: the Vietnamese
Text Readability Dataset (ViRead), OneStopEnglish, and RACE, with the latter
two translated into Vietnamese. Advanced semantic analysis methods were
employed for the semantic aspect using state-of-the-art language models such as
PhoBERT, ViDeBERTa, and ViBERT. In addition, statistical methods were
incorporated to extract syntactic and lexical features of the text. We
conducted experiments using various machine learning models, including Support
Vector Machine (SVM), Random Forest, and Extra Trees and evaluated their
performance using accuracy and F1 score metrics. Our results indicate that a
joint approach that combines semantic and statistical features significantly
enhances the accuracy of readability classification compared to using each
method in isolation. The current study emphasizes the importance of considering
both statistical and semantic aspects for a more accurate assessment of text
difficulty in Vietnamese. This contribution to the field provides insights into
the adaptability of advanced language models in the context of Vietnamese text
readability. It lays the groundwork for future research in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieve<span class="highlight-title">GPT</span>: Merging Prompts and Mathematical Models for Enhanced
  Code-Mixed Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Deroy, Subhankar Maity
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-mixing, the integration of lexical and grammatical elements from
multiple languages within a single sentence, is a widespread linguistic
phenomenon, particularly prevalent in multilingual societies. In India, social
media users frequently engage in code-mixed conversations using the Roman
script, especially among migrant communities who form online groups to share
relevant local information. This paper focuses on the challenges of extracting
relevant information from code-mixed conversations, specifically within Roman
transliterated Bengali mixed with English. This study presents a novel approach
to address these challenges by developing a mechanism to automatically identify
the most relevant answers from code-mixed conversations. We have experimented
with a dataset comprising of queries and documents from Facebook, and Query
Relevance files (QRels) to aid in this task. Our results demonstrate the
effectiveness of our approach in extracting pertinent information from complex,
code-mixed digital conversations, contributing to the broader field of natural
language processing in multilingual and informal text environments. We use
GPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant
documents to frame a mathematical model which helps to detect relevant
documents corresponding to a query.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at FIRE 2024 (Track: Code-Mixed Information Retrieval from
  Social Media Data)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BhasaAnuvaad: A Speech Translation <span class="highlight-title">Dataset</span> for 14 Indian Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sparsh Jain, Ashwin Sankar, Devilal Choudhary, Dhairya Suman, Nikhil Narasimhan, Mohammed Safi Ur Rahman Khan, Anoop Kunchukuttan, Mitesh M Khapra, Raj Dabre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Translation (AST) datasets for Indian languages remain
critically scarce, with public resources covering fewer than 10 of the 22
official languages. This scarcity has resulted in AST systems for Indian
languages lagging far behind those available for high-resource languages like
English. In this paper, we first evaluate the performance of widely-used AST
systems on Indian languages, identifying notable performance gaps and
challenges. Our findings show that while these systems perform adequately on
read speech, they struggle significantly with spontaneous speech, including
disfluencies like pauses and hesitations. Additionally, there is a striking
absence of systems capable of accurately translating colloquial and informal
language, a key aspect of everyday communication. To this end, we introduce
BhasaAnuvaad, the largest publicly available dataset for AST involving 14
scheduled Indian languages spanning over 44,400 hours and 17M text segments.
BhasaAnuvaad contains data for English speech to Indic text, as well as Indic
speech to English text. This dataset comprises three key categories: (1)
Curated datasets from existing resources, (2) Large-scale web mining, and (3)
Synthetic data generation. By offering this diverse and expansive dataset, we
aim to bridge the resource gap and promote advancements in AST for low-resource
Indian languages, especially in handling spontaneous and informal speech
patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DISCO: DISCovering Overfittings as Causal Rules for Text Classification
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhang, Vinay Setty, Yumeng Wang, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of neural language models, the deployment of
over-parameterized models has surged, increasing the need for interpretable
explanations comprehensible to human inspectors. Existing post-hoc
interpretability methods, which often focus on unigram features of single input
textual instances, fail to capture the models' decision-making process fully.
Additionally, many methods do not differentiate between decisions based on
spurious correlations and those based on a holistic understanding of the input.
Our paper introduces DISCO, a novel method for discovering global, rule-based
explanations by identifying causal n-gram associations with model predictions.
This method employs a scalable sequence mining technique to extract relevant
text spans from training data, associate them with model predictions, and
conduct causality checks to distill robust rules that elucidate model behavior.
These rules expose potential overfitting and provide insights into misleading
feature combinations. We validate DISCO through extensive testing,
demonstrating its superiority over existing methods in offering comprehensive
insights into complex model behaviors. Our approach successfully identifies all
shortcuts manually introduced into the training data (100% detection rate on
the MultiRC dataset), resulting in an 18.8% regression in model performance --
a capability unmatched by any other method. Furthermore, DISCO supports
interactive explanations, enabling human inspectors to distinguish spurious
causes in the rule-based output. This alleviates the burden of abundant
instance-wise explanations and helps assess the model's risk when encountering
out-of-distribution (OOD) data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hands-On Tutorial: Labeling with LLM and Human-in-the-Loop <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekaterina Artemova, Akim Tsvigun, Dominik Schlechtweg, Natalia Fedorova, Sergei Tilga, Boris Obmoroshev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training and deploying machine learning models relies on a large amount of
human-annotated data. As human labeling becomes increasingly expensive and
time-consuming, recent research has developed multiple strategies to speed up
annotation and reduce costs and human workload: generating synthetic training
data, active learning, and hybrid labeling. This tutorial is oriented toward
practical applications: we will present the basics of each strategy, highlight
their benefits and limitations, and discuss in detail real-life case studies.
Additionally, we will walk through best practices for managing human annotators
and controlling the quality of the final dataset. The tutorial includes a
hands-on workshop, where attendees will be guided in implementing a hybrid
annotation setup. This tutorial is designed for NLP practitioners from both
research and industry backgrounds who are involved in or interested in
optimizing data labeling projects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented at COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FASSILA: A Corpus for Algerian Dialect Fake News Detection and Sentiment
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Abdedaiem, Abdelhalim Hafedh Dahou, Mohamed Amine Cheragui, Brigitte Mathiak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of low-resource languages, the Algerian dialect (AD) faces
challenges due to the absence of annotated corpora, hindering its effective
processing, notably in Machine Learning (ML) applications reliant on corpora
for training and assessment. This study outlines the development process of a
specialized corpus for Fake News (FN) detection and sentiment analysis (SA) in
AD called FASSILA. This corpus comprises 10,087 sentences, encompassing over
19,497 unique words in AD, and addresses the significant lack of linguistic
resources in the language and covers seven distinct domains. We propose an
annotation scheme for FN detection and SA, detailing the data collection,
cleaning, and labelling process. Remarkable Inter-Annotator Agreement indicates
that the annotation scheme produces consistent annotations of high quality.
Subsequent classification experiments using BERT-based models and ML models are
presented, demonstrate promising results and highlight avenues for further
research. The dataset is made freely available on GitHub
(https://github.com/amincoding/FASSILA) to facilitate future advancements in
the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 Figuers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Calibrated Listwise Reranking with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyang Ren, Yuhao Wang, Kun Zhou, Wayne Xin Zhao, Wenjie Wang, Jing Liu, Ji-Rong Wen, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), with advanced linguistic capabilities, have
been employed in reranking tasks through a sequence-to-sequence approach. In
this paradigm, multiple passages are reranked in a listwise manner and a
textual reranked permutation is generated. However, due to the limited context
window of LLMs, this reranking paradigm requires a sliding window strategy to
iteratively handle larger candidate sets. This not only increases computational
costs but also restricts the LLM from fully capturing all the comparison
information for all candidates. To address these challenges, we propose a novel
self-calibrated listwise reranking method, which aims to leverage LLMs to
produce global relevance scores for ranking. To achieve it, we first propose
the relevance-aware listwise reranking framework, which incorporates explicit
list-view relevance scores to improve reranking efficiency and enable global
comparison across the entire candidate set. Second, to ensure the comparability
of the computed scores, we propose self-calibrated training that uses
point-view relevance assessments generated internally by the LLM itself to
calibrate the list-view relevance assessments. Extensive experiments and
comprehensive analysis on the BEIR benchmark and TREC Deep Learning Tracks
demonstrate the effectiveness and efficiency of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tibyan Corpus: Balanced and Comprehensive Error Coverage Corpus Using
  Chat<span class="highlight-title">GPT</span> for Arabic Grammatical Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahlam Alrehili, Areej Alhothali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing (NLP) utilizes text data augmentation to overcome
sample size constraints. Increasing the sample size is a natural and widely
used strategy for alleviating these challenges. In this study, we chose Arabic
to increase the sample size and correct grammatical errors. Arabic is
considered one of the languages with limited resources for grammatical error
correction (GEC). Furthermore, QALB-14 and QALB-15 are the only datasets used
in most Arabic grammatical error correction research, with approximately 20,500
parallel examples, which is considered low compared with other languages.
Therefore, this study aims to develop an Arabic corpus called "Tibyan" for
grammatical error correction using ChatGPT. ChatGPT is used as a data augmenter
tool based on a pair of Arabic sentences containing grammatical errors matched
with a sentence free of errors extracted from Arabic books, called guide
sentences. Multiple steps were involved in establishing our corpus, including
the collection and pre-processing of a pair of Arabic texts from various
sources, such as books and open-access corpora. We then used ChatGPT to
generate a parallel corpus based on the text collected previously, as a guide
for generating sentences with multiple types of errors. By engaging linguistic
experts to review and validate the automatically generated sentences, we
ensured that they were correct and error-free. The corpus was validated and
refined iteratively based on feedback provided by linguistic experts to improve
its accuracy. Finally, we used the Arabic Error Type Annotation tool (ARETA) to
analyze the types of errors in the Tibyan corpus. Our corpus contained 49 of
errors, including seven types: orthography, morphology, syntax, semantics,
punctuation, merge, and split. The Tibyan corpus contains approximately 600 K
tokens.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The State and Fate of Summarization <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noam Dahan, Gabriel Stanovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic summarization has consistently attracted attention, due to its
versatility and wide application in various downstream tasks. Despite its
popularity, we find that annotation efforts have largely been disjointed, and
have lacked common terminology. Consequently, it is challenging to discover
existing resources or identify coherent research directions. To address this,
we survey a large body of work spanning 133 datasets in over 100 languages,
creating a novel ontology covering sample properties, collection methods and
distribution. With this ontology we make key observations, including the lack
in accessible high-quality datasets for low-resource languages, and the field's
over-reliance on the news domain and on automatically collected distant
supervision. Finally, we make available a web interface that allows users to
interact and explore our ontology and dataset collection, as well as a template
for a summarization data card, which can be used to streamline future research
into a more coherent body of work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multistage Fine-tuning Strategies for Automatic Speech Recognition in
  Low-resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leena G Pillai, Kavya Manohar, Basil K Raju, Elizabeth Sherly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel multistage fine-tuning strategy designed to
enhance automatic speech recognition (ASR) performance in low-resource
languages using OpenAI's Whisper model. In this approach we aim to build ASR
model for languages with limited digital resources by sequentially adapting the
model across linguistically similar languages. We experimented this on the
Malasar language, a Dravidian language spoken by approximately ten thousand
people in the Western Ghats of South India. Malasar language faces critical
challenges for technological intervention due to its lack of a native script
and absence of digital or spoken data resources. Working in collaboration with
Wycliffe India and Malasar community members, we created a spoken Malasar
corpus paired with transcription in Tamil script, a closely related major
language. In our approach to build ASR model for Malasar, we first build an
intermediate Tamil ASR, leveraging higher data availability for Tamil annotated
speech. This intermediate model is subsequently fine-tuned on Malasar data,
allowing for more effective ASR adaptation despite limited resources. The
multistage fine-tuning strategy demonstrated significant improvements over
direct fine-tuning on Malasar data alone, achieving a word error rate (WER) of
51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning
method. Further a WER reduction to 47.3% was achieved through punctuation
removal in post-processing, which addresses formatting inconsistencies that
impact evaluation. Our results underscore the effectiveness of sequential
multistage fine-tuning combined with targeted post-processing as a scalable
strategy for ASR system development in low-resource languages, especially where
linguistic similarities can be leveraged to bridge gaps in training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pruning Literals for Highly Efficient Explainability at Word Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Kumar Yadav, Bimal Bhattarai, Abhik Jana, Lei Jiao, Seid Muhie Yimam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing an explainable model becomes crucial now for Natural Language
Processing(NLP) since most of the state-of-the-art machine learning models
provide a limited explanation for the prediction. In the spectrum of an
explainable model, Tsetlin Machine(TM) is promising because of its capability
of providing word-level explanation using proposition logic. However, concern
rises over the elaborated combination of literals (propositional logic) in the
clause that makes the model difficult for humans to comprehend, despite having
a transparent learning process. In this paper, we design a post-hoc pruning of
clauses that eliminate the randomly placed literals in the clause thereby
making the model more efficiently interpretable than the vanilla TM.
Experiments on the publicly available YELP-HAT Dataset demonstrate that the
proposed pruned TM's attention map aligns more with the human attention map
than the vanilla TM's attention map. In addition, the pairwise similarity
measure also surpasses the attention map-based neural network models. In terms
of accuracy, the proposed pruning method does not degrade the accuracy
significantly but rather enhances the performance up to 4% to 9% in some test
data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Best Practices for Distilling Large Language Models into BERT for Web
  Search Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dezhi Ye, Junwei Hu, Jiabin Fan, Bowen Tian, Jie Liu, Haijin Liang, Jin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have highlighted the significant potential of Large Language
Models (LLMs) as zero-shot relevance rankers. These methods predominantly
utilize prompt learning to assess the relevance between queries and documents
by generating a ranked list of potential documents. Despite their promise, the
substantial costs associated with LLMs pose a significant challenge for their
direct implementation in commercial search systems. To overcome this barrier
and fully exploit the capabilities of LLMs for text ranking, we explore
techniques to transfer the ranking expertise of LLMs to a more compact model
similar to BERT, using a ranking loss to enable the deployment of less
resource-intensive models. Specifically, we enhance the training of LLMs
through Continued Pre-Training, taking the query as input and the clicked title
and summary as output. We then proceed with supervised fine-tuning of the LLM
using a rank loss, assigning the final token as a representative of the entire
sentence. Given the inherent characteristics of autoregressive language models,
only the final token </s> can encapsulate all preceding tokens. Additionally,
we introduce a hybrid point-wise and margin MSE loss to transfer the ranking
knowledge from LLMs to smaller models like BERT. This method creates a viable
solution for environments with strict resource constraints. Both offline and
online evaluations have confirmed the efficacy of our approach, and our model
has been successfully integrated into a commercial web search engine as of
February 2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Arxiv Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Reasoning Improves Tool Use in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Alazraki, Marek Rei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  External tools help large language models (LLMs) succeed at tasks where they
would otherwise typically fail. In existing frameworks, LLMs learn tool use
either by in-context demonstrations or via full model fine-tuning on annotated
data. As these approaches do not easily scale, a recent trend is to abandon
them in favor of lightweight, parameter-efficient tuning paradigms. These
methods allow quickly alternating between the frozen LLM and its specialised
fine-tuned version, by switching on or off a handful of additional custom
parameters. Hence, we postulate that the generalization ability of the frozen
model can be leveraged to improve tool selection. We present Tool selECTion via
meta-reasONing (TECTON), a two-phase system that first reasons over a task
using a custom fine-tuned LM head and outputs candidate tools. Then, with the
custom head disabled, it meta-reasons (i.e., it reasons over the previous
reasoning process) to make a final choice. We show that TECTON results in
substantial gains - both in-distribution and out-of-distribution - on a range
of math reasoning datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tomato, Tomahto, Tomate: Measuring the Role of Shared Semantics among
  Subwords in Multilingual Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhang, Jing Lu, Vinh Q. Tran, Tal Schuster, Donald Metzler, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human understanding of language is robust to different word choices as far as
they represent similar semantic concepts. To what extent does our human
intuition transfer to language models, which represent all subwords as distinct
embeddings? In this work, we take an initial step on measuring the role of
shared semantics among subwords in the encoder-only multilingual language
models (mLMs). To this end, we form "semantic tokens" by merging the
semantically similar subwords and their embeddings, and evaluate the updated
mLMs on 5 heterogeneous multilingual downstream tasks. Results show that the
general shared semantics could get the models a long way in making the
predictions on mLMs with different tokenizers and model sizes. Inspections on
the grouped subwords show that they exhibit a wide range of semantic
similarities, including synonyms and translations across many languages and
scripts. Lastly, we found the zero-shot results with semantic tokens are on par
or even better than the original models on certain classification tasks,
suggesting that the shared subword-level semantics may serve as the anchors for
cross-lingual transferring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thanos: Enhancing Conversational Agents with Skill-of-Mind-Infused Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young-Jun Lee, Dokyong Lee, Junyoung Youn, Kyeongjin Oh, Ho-Jin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To increase social bonding with interlocutors, humans naturally acquire the
ability to respond appropriately in a given situation by considering which
conversational skill is most suitable for the response - a process we call
skill-of-mind. For large language model (LLM)-based conversational agents,
planning appropriate conversational skills, as humans do, is challenging due to
the complexity of social dialogue, especially in interactive scenarios. To
address this, we propose a skill-of-mind-annotated conversation dataset, named
Multifaceted Skill-of-Mind, which includes multi-turn and multifaceted
conversational skills across various interactive scenarios (e.g., long-term,
counseling, task-oriented), grounded in diverse social contexts (e.g.,
demographics, persona, rules of thumb). This dataset consists of roughly 100K
conversations. Using this dataset, we introduce a new family of
skill-of-mind-infused LLMs, named Thanos, with model sizes of 1B, 3B, and 8B
parameters. With extensive experiments, these models successfully demonstrate
the skill-of-mind process and exhibit strong generalizability in inferring
multifaceted skills across a variety of domains. Moreover, we show that Thanos
significantly enhances the quality of responses generated by LLM-based
conversational agents and promotes prosocial behavior in human evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/passing2961/Thanos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ML-Promise: A Multilingual <span class="highlight-title">Dataset</span> for Corporate Promise Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yohei Seki, Hakusen Shu, Anaïs Lhuissier, Hanwool Lee, Juyeon Kang, Min-Yuh Day, Chung-Chi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Promises made by politicians, corporate leaders, and public figures have a
significant impact on public perception, trust, and institutional reputation.
However, the complexity and volume of such commitments, coupled with
difficulties in verifying their fulfillment, necessitate innovative methods for
assessing their credibility. This paper introduces the concept of Promise
Verification, a systematic approach involving steps such as promise
identification, evidence assessment, and the evaluation of timing for
verification. We propose the first multilingual dataset, ML-Promise, which
includes English, French, Chinese, Japanese, and Korean, aimed at facilitating
in-depth verification of promises, particularly in the context of
Environmental, Social, and Governance (ESG) reports. Given the growing emphasis
on corporate environmental contributions, this dataset addresses the challenge
of evaluating corporate promises, especially in light of practices like
greenwashing. Our findings also explore textual and image-based baselines, with
promising results from retrieval-augmented generation (RAG) approaches. This
work aims to foster further discourse on the accountability of public
commitments across multiple languages and domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient Localization Improves Lifelong Pretraining of Language Models <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jared Fernandez, Yonatan Bisk, Emma Strubell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) trained on web-scale text corpora have been
shown to capture world knowledge in their parameters. However, the mechanism by
which language models store different types of knowledge is poorly understood.
In this work, we examine two types of knowledge relating to temporally
sensitive entities and demonstrate that each type is localized to different
sets of parameters within the LLMs. We hypothesize that the lack of
consideration of the locality of knowledge in existing continual learning
methods contributes to both: the failed uptake of new information, and
catastrophic forgetting of previously learned information. We observe that
sequences containing references to updated and newly mentioned entities exhibit
larger gradient norms in a subset of layers. We demonstrate that targeting
parameter updates to these relevant layers can improve the performance of
continually pretraining on language containing temporal drift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ACCIO: Table Understanding Enhanced via Contrastive Learning with
  Aggregations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Whanhee Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The attention to table understanding using recent natural language models has
been growing. However, most related works tend to focus on learning the
structure of the table directly. Just as humans improve their understanding of
sentences by comparing them, they can also enhance their understanding by
comparing tables. With this idea, in this paper, we introduce ACCIO, tAble
understanding enhanCed via Contrastive learnIng with aggregatiOns, a novel
approach to enhancing table understanding by contrasting original tables with
their pivot summaries through contrastive learning. ACCIO trains an encoder to
bring these table pairs closer together. Through validation via column type
annotation, ACCIO achieves competitive performance with a macro F1 score of
91.1 compared to state-of-the-art methods. This work represents the first
attempt to utilize pairs of tables for table embedding, promising significant
advancements in table comprehension. Our code is available at
https://github.com/whnhch/ACCIO/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One fish, two fish, but not the whole sea: Alignment reduces language
  models' conceptual diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonia K. Murthy, Tomer Ullman, Jennifer Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Researchers in social science and psychology have recently proposed using
large language models (LLMs) as replacements for humans in behavioral research.
In addition to arguments about whether LLMs accurately capture population-level
patterns, this has raised questions about whether LLMs capture human-like
conceptual diversity. Separately, it is debated whether post-training alignment
(RLHF or RLAIF) affects models' internal diversity. Inspired by human studies,
we use a new way of measuring the conceptual diversity of
synthetically-generated LLM "populations" by relating the internal variability
of simulated individuals to the population-level variability. We use this
approach to evaluate non-aligned and aligned LLMs on two domains with rich
human behavioral data. While no model reaches human-like diversity, aligned
models generally display less diversity than their instruction fine-tuned
counterparts. Our findings highlight potential trade-offs between increasing
models' value alignment and decreasing the diversity of their conceptual
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DELIFT: Data Efficient Language model Instruction Fine Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishika Agarwal, Krishna Killamsetty, Lucian Popa, Marina Danilevksy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large language models (LLMs) is essential for enhancing their
performance on specific tasks but is often resource-intensive due to redundant
or uninformative data. To address this inefficiency, we introduce DELIFT (Data
Efficient Language model Instruction Fine-Tuning), a novel algorithm that
systematically optimizes data selection across the three key stages of
fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g.,
reasoning, question-answering), and (3) continual fine-tuning (e.g.,
incorporating new data versions). Unlike existing methods that focus on
single-stage optimization or rely on computationally intensive gradient
calculations, DELIFT operates efficiently across all stages. Central to our
approach is a pairwise utility metric that quantifies how beneficial a data
sample is for improving the model's responses to other samples, effectively
measuring the informational value relative to the model's current capabilities.
By leveraging different submodular functions applied to this metric, DELIFT
selects diverse and optimal subsets that are useful across all stages of
fine-tuning. Experiments across various tasks and model scales demonstrate that
DELIFT can reduce the fine-tuning data size by up to 70% without compromising
performance, offering significant computational savings and outperforming
existing methods in both efficiency and efficacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Calibration of Win Rate Estimation with LLM Evaluators <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Gao, Gonghan Xu, Zhe Wang, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) show the potential of using
LLMs as evaluators for assessing the quality of text generations from LLMs.
However, applying LLM evaluators naively to compare or judge between different
systems can lead to unreliable results due to the intrinsic win rate estimation
bias of LLM evaluators. In order to mitigate this problem, we propose two
calibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian
Dawid-Skene, both of which leverage Bayesian inference to more accurately infer
the true win rate of generative language models. We empirically validate our
methods on six datasets covering story generation, summarization, and
instruction following tasks. We show that both our methods are effective in
improving the accuracy of win rate estimation using LLMs as evaluators,
offering a promising direction for reliable automatic text quality evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational <span class="highlight-title">Low-Rank Adaptation</span> Using IVON <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bai Cong, Nico Daheim, Yuesong Shen, Daniel Cremers, Rio Yokota, Mohammad Emtiyaz Khan, Thomas Möllenhoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that variational learning can significantly improve the accuracy and
calibration of Low-Rank Adaptation (LoRA) without a substantial increase in the
cost. We replace AdamW by the Improved Variational Online Newton (IVON)
algorithm to finetune large language models. For Llama-2 with 7 billion
parameters, IVON improves the accuracy over AdamW by 2.8% and expected
calibration error by 4.6%. The accuracy is also better than the other Bayesian
alternatives, yet the cost is lower and the implementation is easier. Our work
provides additional evidence for the effectiveness of IVON for large language
models. The code is available at
https://github.com/team-approx-bayes/ivon-lora.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at 38th Workshop on Fine-Tuning in Machine Learning
  (NeurIPS 2024). Code available at
  https://github.com/team-approx-bayes/ivon-lora</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring short-form factuality in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, William Fedus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SimpleQA, a benchmark that evaluates the ability of language
models to answer short, fact-seeking questions. We prioritized two properties
in designing this eval. First, SimpleQA is challenging, as it is adversarially
collected against GPT-4 responses. Second, responses are easy to grade, because
questions are created such that there exists only a single, indisputable
answer. Each answer in SimpleQA is graded as either correct, incorrect, or not
attempted. A model with ideal behavior would get as many questions correct as
possible while not attempting the questions for which it is not confident it
knows the correct answer. SimpleQA is a simple, targeted evaluation for whether
models "know what they know," and our hope is that this benchmark will remain
relevant for the next few generations of frontier models. SimpleQA can be found
at https://github.com/openai/simple-evals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Blog post: https://openai.com/index/introducing-simpleqa/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust and Efficient Fine-tuning of LLMs with Bayesian
  Reparameterization of <span class="highlight-title">Low-Rank Adaptation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vaibhav Seth, Arinjay Pathak, Ayan Sengupta, Natraj Raman, Sriram Gopalakrishnan, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are highly resource-intensive to fine-tune due
to their enormous size. While low-rank adaptation is a prominent
parameter-efficient fine-tuning approach, it suffers from sensitivity to
hyperparameter choices, leading to instability in model performance on
fine-tuning downstream tasks. This paper highlights the importance of effective
parameterization in low-rank fine-tuning to reduce estimator variance and
enhance the stability of final model outputs. We propose MonteCLoRA, an
efficient fine-tuning technique, employing Monte Carlo estimation to learn an
unbiased posterior estimation of low-rank parameters with low expected
variance, which stabilizes fine-tuned LLMs with only O(1) additional
parameters. MonteCLoRA shows significant improvements in accuracy and
robustness, achieving up to 3.8% higher accuracy and 8.6% greater robustness
than existing efficient fine-tuning methods on natural language understanding
tasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with
pre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance
with 50% lower variance than the contemporary efficient fine-tuning methods.
The theoretical and empirical results presented in the paper underscore how
parameterization and hyperpriors balance exploration-exploitation in the
low-rank parametric space, therefore leading to more optimal and robust
parameter estimation during efficient fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 10 figures, 10 tables, Code:
  https://github.com/LCS2-IIITD/MonteCLoRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws for Precision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanishq Kumar, Zachary Ankner, Benjamin F. Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Ré, Aditi Raghunathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low precision training and inference affect both the quality and cost of
language models, but current scaling laws do not account for this. In this
work, we devise "precision-aware" scaling laws for both training and inference.
We propose that training in lower precision reduces the model's "effective
parameter count," allowing us to predict the additional loss incurred from
training in low precision and post-train quantization. For inference, we find
that the degradation introduced by post-training quantization increases as
models are trained on more data, eventually making additional pretraining data
actively harmful. For training, our scaling laws allow us to predict the loss
of a model with different parts in different precisions, and suggest that
training larger models in lower precision may be compute optimal. We unify the
scaling laws for post and pretraining quantization to arrive at a single
functional form that predicts degradation from training and inference in varied
precisions. We fit on over 465 pretraining runs and validate our predictions on
model sizes up to 1.7B parameters trained on up to 26B tokens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CodeTree: Agent-guided Tree Search for Code Generation with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jierui Li, Hung Le, Yinbo Zhou, Caiming Xiong, Silvio Savarese, Doyen Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained on massive amounts of code and text data, large language models
(LLMs) have demonstrated remarkable achievements in performing code generation
tasks. With additional execution-based feedback, these models can act as agents
with capabilities to self-refine and improve generated code autonomously.
However, on challenging coding tasks with extremely large search space, current
agentic approaches still struggle with multi-stage planning, generating, and
debugging. To address this problem, we propose CodeTree, a framework for LLM
agents to efficiently explore the search space in different stages of the code
generation process. Specifically, we adopted a unified tree structure to
explicitly explore different coding strategies, generate corresponding coding
solutions, and subsequently refine the solutions. In each stage, critical
decision-making (ranking, termination, expanding) of the exploration process is
guided by both the environmental execution-based feedback and
LLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code
generation benchmarks and demonstrated the significant performance gains of
CodeTree against strong baselines. Using GPT-4o as the base model, we
consistently achieved top results of 95.1 on HumanEval, 98.7 on MBPP, and 43.0
on CodeContests. On the challenging SWEBench benchmark, our approach led to
significant performance gains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing Transparency and Accuracy: A Comparative Analysis of
  Rule-Based and Deep Learning Models in Political Bias Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Nunez Martinez, Sonja Schmer-Galunder, Zoey Liu, Sangpil Youm, Chathuri Jayaweera, Bonnie J. Dorr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unchecked spread of digital information, combined with increasing
political polarization and the tendency of individuals to isolate themselves
from opposing political viewpoints, has driven researchers to develop systems
for automatically detecting political bias in media. This trend has been
further fueled by discussions on social media. We explore methods for
categorizing bias in US news articles, comparing rule-based and deep learning
approaches. The study highlights the sensitivity of modern self-learning
systems to unconstrained data ingestion, while reconsidering the strengths of
traditional rule-based systems. Applying both models to left-leaning (CNN) and
right-leaning (FOX) news articles, we assess their effectiveness on data beyond
the original training and test sets.This analysis highlights each model's
accuracy, offers a framework for exploring deep-learning explainability, and
sheds light on political bias in US news media. We contrast the opaque
architecture of a deep learning model with the transparency of a linguistically
informed rule-based model, showing that the rule-based model performs
consistently across different data conditions and offers greater transparency,
whereas the deep learning model is dependent on the training set and struggles
with unseen data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive
  Clinical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan S. Ilgen, Emma Pierson, Pang Wei Koh, Yulia Tsvetkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Users typically engage with LLMs interactively, yet most existing benchmarks
evaluate them in a static, single-turn format, posing reliability concerns in
interactive scenarios. We identify a key obstacle towards reliability: LLMs are
trained to answer any question, even with incomplete context or insufficient
knowledge. In this paper, we propose to change the static paradigm to an
interactive one, develop systems that proactively ask questions to gather more
information and respond reliably, and introduce an benchmark - MediQ - to
evaluate question-asking ability in LLMs. MediQ simulates clinical interactions
consisting of a Patient System and an adaptive Expert System; with potentially
incomplete initial information, the Expert refrains from making diagnostic
decisions when unconfident, and instead elicits missing details via follow-up
questions. We provide a pipeline to convert single-turn medical benchmarks into
an interactive format. Our results show that directly prompting
state-of-the-art LLMs to ask questions degrades performance, indicating that
adapting LLMs to proactive information-seeking settings is nontrivial. We
experiment with abstention strategies to better estimate model confidence and
decide when to ask questions, improving diagnostic accuracy by 22.3%; however,
performance still lags compared to an (unrealistic in practice) upper bound
with complete information upfront. Further analyses show improved interactive
performance with filtering irrelevant contexts and reformatting conversations.
Overall, we introduce a novel problem towards LLM reliability, an interactive
MediQ benchmark and a novel question-asking system, and highlight directions to
extend LLMs' information-seeking abilities in critical domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talking the Talk Does Not Entail Walking the Walk: On the Limits of
  Large Language Models in Lexical Entailment Recognition <span class="chip">EMNLP-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Candida M. Greco, Lucio La Cava, Andrea Tagarelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verbs form the backbone of language, providing the structure and meaning to
sentences. Yet, their intricate semantic nuances pose a longstanding challenge.
Understanding verb relations through the concept of lexical entailment is
crucial for comprehending sentence meanings and grasping verb dynamics. This
work investigates the capabilities of eight Large Language Models in
recognizing lexical entailment relations among verbs through differently
devised prompting strategies and zero-/few-shot settings over verb pairs from
two lexical databases, namely WordNet and HyperLex. Our findings unveil that
the models can tackle the lexical entailment recognition task with moderately
good performance, although at varying degree of effectiveness and under
different conditions. Also, utilizing few-shot prompting can enhance the
models' performance. However, perfectly solving the task arises as an unmet
challenge for all examined LLMs, which raises an emergence for further research
developments on this topic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at The 2024 Conference on Empirical Methods
  in Natural Language Processing (EMNLP-2024) - Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TinyStyler: Efficient Few-Shot Text Style Transfer with Authorship
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Horvitz, Ajay Patel, Kanishk Singh, Chris Callison-Burch, Kathleen McKeown, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of text style transfer is to transform the style of texts while
preserving their original meaning, often with only a few examples of the target
style. Existing style transfer methods generally rely on the few-shot
capabilities of large language models or on complex controllable text
generation approaches that are inefficient and underperform on fluency metrics.
We introduce TinyStyler, a lightweight but effective approach, which leverages
a small language model (800M params) and pre-trained authorship embeddings to
perform efficient, few-shot text style transfer. We evaluate on the challenging
task of authorship style transfer and find TinyStyler outperforms strong
approaches such as GPT-4. We also evaluate TinyStyler's ability to perform text
attribute style transfer (formal $\leftrightarrow$ informal) with automatic and
human evaluations and find that the approach outperforms recent controllable
text generation methods. Our model has been made publicly available at
https://huggingface.co/tinystyler/tinystyler .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptions of Linguistic Uncertainty by Language Models and Humans <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Catarina G Belem, Markelle Kelly, Mark Steyvers, Sameer Singh, Padhraic Smyth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  _Uncertainty expressions_ such as "probably" or "highly unlikely" are
pervasive in human language. While prior work has established that there is
population-level agreement in terms of how humans quantitatively interpret
these expressions, there has been little inquiry into the abilities of language
models in the same context. In this paper, we investigate how language models
map linguistic expressions of uncertainty to numerical responses. Our approach
assesses whether language models can employ theory of mind in this setting:
understanding the uncertainty of another agent about a particular statement,
independently of the model's own certainty about that statement. We find that 7
out of 10 models are able to map uncertainty expressions to probabilistic
responses in a human-like manner. However, we observe systematically different
behavior depending on whether a statement is actually true or false. This
sensitivity indicates that language models are substantially more susceptible
to bias based on their prior knowledge (as compared to humans). These findings
raise important questions and have broad implications for human-AI and AI-AI
communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Rigour of Scientific Writing: Criteria, Analysis, and Insights <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph James, Chenghao Xiao, Yucheng Li, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rigour is crucial for scientific research as it ensures the reproducibility
and validity of results and findings. Despite its importance, little work
exists on modelling rigour computationally, and there is a lack of analysis on
whether these criteria can effectively signal or measure the rigour of
scientific papers in practice. In this paper, we introduce a bottom-up,
data-driven framework to automatically identify and define rigour criteria and
assess their relevance in scientific writing. Our framework includes rigour
keyword extraction, detailed rigour definition generation, and salient criteria
identification. Furthermore, our framework is domain-agnostic and can be
tailored to the evaluation of scientific rigour for different areas,
accommodating the distinct salient criteria across fields. We conducted
comprehensive experiments based on datasets collected from two high impact
venues for Machine Learning and NLP (i.e., ICLR and ACL) to demonstrate the
effectiveness of our framework in modelling rigour. In addition, we analyse
linguistic patterns of rigour, revealing that framing certainty is crucial for
enhancing the perception of scientific rigour, while suggestion certainty and
probability uncertainty diminish it.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Findings at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Personalized Large Language Models <span class="chip">ICDM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stanisław Woźniak, Bartłomiej Koptyra, Arkadiusz Janz, Przemysław Kazienko, Jan Kocoń
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have significantly advanced Natural Language
Processing (NLP) tasks in recent years. However, their universal nature poses
limitations in scenarios requiring personalized responses, such as
recommendation systems and chatbots. This paper investigates methods to
personalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on
subjective tasks. Results demonstrate that personalized fine-tuning improves
model reasoning compared to non-personalized models. Experiments on datasets
for emotion recognition and hate speech detection show consistent performance
gains with personalized methods across different LLM architectures. These
findings underscore the importance of personalization for enhancing LLM
capabilities in subjective text perception tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SENTIRE 2024 (ICDM Workshops):
  https://sentic.net/sentire2024wozniak.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational
  Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated
  Multi-shot Jailbreaks) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Priyanshu, Supriti Vijay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating the
safety of Large Language Models (LLMs) against multi-turn conversational
attacks. Building upon the SORRY-Bench dataset, we propose a simple yet
effective method for generating adversarial prompts by breaking down harmful
queries into seemingly innocuous sub-questions. Our approach achieves a maximum
increase of +46.22\% in Attack Success Rates (ASRs) across GPT-4, GPT-4o,
GPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods. We
demonstrate that this technique poses a challenge to current LLM safety
measures and highlights the need for more robust defenses against subtle,
multi-turn attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pre-Finetuning for Few-Shot Emotional Speech Recognition <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12921v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12921v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximillian Chen, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech models have long been known to overfit individual speakers for many
classification tasks. This leads to poor generalization in settings where the
speakers are out-of-domain or out-of-distribution, as is common in production
environments. We view speaker adaptation as a few-shot learning problem and
propose investigating transfer learning approaches inspired by recent success
with pre-trained models in natural language tasks. We propose pre-finetuning
speech models on difficult tasks to distill knowledge into few-shot downstream
classification objectives. We pre-finetune Wav2Vec2.0 on every permutation of
four multiclass emotional speech recognition corpora and evaluate our
pre-finetuned models through 33,600 few-shot fine-tuning trials on the
Emotional Speech Dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at INTERSPEECH 2023. 5 pages, 4 figures. Code available at
  https://github.com/maxlchen/Speech-PreFinetuning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by
  Exploring Refusal Loss Landscapes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00867v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00867v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are becoming a prominent generative AI tool,
where the user enters a query and the LLM generates an answer. To reduce harm
and misuse, efforts have been made to align these LLMs to human values using
advanced training techniques such as Reinforcement Learning from Human Feedback
(RLHF). However, recent studies have highlighted the vulnerability of LLMs to
adversarial jailbreak attempts aiming at subverting the embedded safety
guardrails. To address this challenge, this paper defines and investigates the
Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect
jailbreak attempts. Gradient Cuff exploits the unique properties observed in
the refusal loss landscape, including functional values and its smoothness, to
design an effective two-step detection strategy. Experimental results on two
aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak
attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can
significantly improve the LLM's rejection capability for malicious jailbreak
queries, while maintaining the model's performance for benign user queries by
adjusting the detection threshold.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. Project page:
  https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SYNTHEVAL: Hybrid Behavioral Testing of NLP Models with Synthetic
  CheckLists <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17437v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17437v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raoyuan Zhao, Abdullatif Köksal, Yihong Liu, Leonie Weissweiler, Anna Korhonen, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional benchmarking in NLP typically involves using static held-out test
sets. However, this approach often results in an overestimation of performance
and lacks the ability to offer comprehensive, interpretable, and dynamic
assessments of NLP models. Recently, works like DynaBench (Kiela et al., 2021)
and CheckList (Ribeiro et al., 2020) have addressed these limitations through
behavioral testing of NLP models with test types generated by a multistep
human-annotated pipeline. Unfortunately, manually creating a variety of test
types requires much human labor, often at prohibitive cost. In this work, we
propose SYNTHEVAL, a hybrid behavioral testing framework that leverages large
language models (LLMs) to generate a wide range of test types for a
comprehensive evaluation of NLP models. SYNTHEVAL first generates sentences via
LLMs using controlled generation, and then identifies challenging examples by
comparing the predictions made by LLMs with task-specific NLP models. In the
last stage, human experts investigate the challenging examples, manually design
templates, and identify the types of failures the taskspecific models
consistently exhibit. We apply SYNTHEVAL to two classification tasks, sentiment
analysis and toxic language detection, and show that our framework is effective
in identifying weaknesses of strong models on these tasks. We share our code in
https://github.com/Loreley99/SynthEval_CheckList.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 - Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MEG: Medical Knowledge-Augmented Large Language Models for Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Cabello, Carmen Martin-Turrero, Uchenna Akujuobi, Anders Søgaard, Carlos Bobed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering is a natural language understanding task that involves
reasoning over both explicit context and unstated, relevant domain knowledge.
Large language models (LLMs), which underpin most contemporary question
answering systems, struggle to induce how concepts relate in specialized
domains such as medicine. Existing medical LLMs are also costly to train. In
this work, we present MEG, a parameter-efficient approach for medical
knowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate
graph embeddings into the LLM, enabling it to leverage external knowledge in a
cost-effective way. We evaluate our method on four popular medical
multiple-choice datasets and show that LLMs greatly benefit from the factual
grounding provided by knowledge graph embeddings. MEG attains an average of
+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized
models like BioMistral. We also show results based on Llama-3. Finally, we show
that MEG's performance remains robust to the choice of graph encoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MILPaC: A Novel Benchmark for Evaluating Translation of Legal Text to
  Indian Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayan Mahapatra, Debtanu Datta, Shubham Soni, Adrijit Goswami, Saptarshi Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most legal text in the Indian judiciary is written in complex English due to
historical reasons. However, only a small fraction of the Indian population is
comfortable in reading English. Hence legal text needs to be made available in
various Indian languages, possibly by translating the available legal text from
English. Though there has been a lot of research on translation to and between
Indian languages, to our knowledge, there has not been much prior work on such
translation in the legal domain. In this work, we construct the first
high-quality legal parallel corpus containing aligned text units in English and
nine Indian languages, that includes several low-resource languages. We also
benchmark the performance of a wide variety of Machine Translation (MT) systems
over this corpus, including commercial MT systems, open-source MT systems and
Large Language Models. Through a comprehensive survey by Law practitioners, we
check how satisfied they are with the translations by some of these MT systems,
and how well automatic MT evaluation metrics agree with the opinions of Law
practitioners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in ACM Transactions on Asian and Low-Resource
  Language Information Processing (TALLIP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Speculation Lookahead Accelerates Speculative Decoding of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04304v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04304v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Mamou, Oren Pereg, Daniel Korat, Moshe Berchansky, Nadav Timor, Moshe Wasserblat, Roy Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding is commonly used for reducing the inference latency of
large language models. Its effectiveness depends highly on the speculation
lookahead (SL)-the number of tokens generated by the draft model at each
iteration. In this work we show that the common practice of using the same SL
for all iterations (static SL) is suboptimal. We introduce DISCO (DynamIc
SpeCulation lookahead Optimization), a novel method for dynamically selecting
the SL. Our experiments with four datasets show that DISCO reaches an average
speedup of 10% compared to the best static SL baseline, while generating the
exact same text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wave Network: An Ultra-Small Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02674v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02674v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Victor S. Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an innovative token representation and update method in a new
ultra-small language model: the Wave network. Specifically, we use a complex
vector to represent each token, encoding both global and local semantics of the
input text. A complex vector consists of two components: a magnitude vector
representing the global semantics of the input text, and a phase vector
capturing the relationships between individual tokens and global semantics.
Experiments on the AG News text classification task demonstrate that, when
generating complex vectors from randomly initialized token embeddings, our
single-layer Wave Network achieves 90.91% accuracy with wave interference and
91.66% with wave modulation - outperforming a single Transformer layer using
BERT pre-trained embeddings by 19.23% and 19.98%, respectively, and approaching
the accuracy of the pre-trained and fine-tuned BERT base model (94.64%).
Additionally, compared to BERT base, the Wave Network reduces video memory
usage and training time by 77.34% and 85.62% during wave modulation. In
summary, we used a 2.4-million-parameter small language model to achieve
accuracy comparable to a 100-million-parameter BERT model in text
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14125v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14125v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingnan Zheng, Han Wang, An Zhang, Tai D. Nguyen, Jun Sun, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) can elicit unintended and even harmful content
when misaligned with human values, posing severe risks to users and society. To
mitigate these risks, current evaluation benchmarks predominantly employ
expert-designed contextual scenarios to assess how well LLMs align with human
values. However, the labor-intensive nature of these benchmarks limits their
test scope, hindering their ability to generalize to the extensive variety of
open-world use cases and identify rare but crucial long-tail risks.
Additionally, these static tests fail to adapt to the rapid evolution of LLMs,
making it hard to evaluate timely alignment issues. To address these
challenges, we propose ALI-Agent, an evaluation framework that leverages the
autonomous abilities of LLM-powered agents to conduct in-depth and adaptive
alignment assessments. ALI-Agent operates through two principal stages:
Emulation and Refinement. During the Emulation stage, ALI-Agent automates the
generation of realistic test scenarios. In the Refinement stage, it iteratively
refines the scenarios to probe long-tail risks. Specifically, ALI-Agent
incorporates a memory module to guide test scenario generation, a tool-using
module to reduce human labor in tasks such as evaluating feedback from target
LLMs, and an action module to refine tests. Extensive experiments across three
aspects of human values--stereotypes, morality, and legality--demonstrate that
ALI-Agent, as a general evaluation framework, effectively identifies model
misalignment. Systematic analysis also validates that the generated test
scenarios represent meaningful use cases, as well as integrate enhanced
measures to probe long-tail risks. Our code is available at
https://github.com/SophieZheng998/ALI-Agent.git
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongEmbed: Extending Embedding Models for Long Context Retrieval <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12096v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12096v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, Sujian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding models play a pivot role in modern NLP applications such as IR and
RAG. While the context limit of LLMs has been pushed beyond 1 million tokens,
embedding models are still confined to a narrow context window not exceeding 8k
tokens, refrained from application scenarios requiring long inputs such as
legal contracts. This paper explores context window extension of existing
embedding models, pushing the limit to 32k without requiring additional
training. First, we examine the performance of current embedding models for
long context retrieval on our newly constructed LongEmbed benchmark. LongEmbed
comprises two synthetic tasks and four carefully chosen real-world tasks,
featuring documents of varying length and dispersed target information.
Benchmarking results underscore huge room for improvement in these models.
Based on this, comprehensive experiments show that training-free context window
extension strategies like position interpolation can effectively extend the
context window of existing embedding models by several folds, regardless of
their original context being 512 or beyond 4k. Furthermore, for models
employing absolute position encoding (APE), we show the possibility of further
fine-tuning to harvest notable performance gains while strictly preserving
original behavior for short inputs. For models using rotary position embedding
(RoPE), significant enhancements are observed when employing RoPE-specific
methods, such as NTK and SelfExtend, indicating RoPE's superiority over APE for
context window extension. To facilitate future research, we release E5-Base-4k
and E5-RoPE-Base, along with the LongEmbed benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOVA3: Learning to Visual Question Answering, Asking and Assessment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Hengyuan Zhao, Pan Zhou, Difei Gao, Zechen Bai, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering, asking, and assessment are three innate human traits
crucial for understanding the world and acquiring knowledge. By enhancing these
capabilities, humans can more effectively utilize data, leading to better
comprehension and learning outcomes. Current Multimodal Large Language Models
(MLLMs) primarily focus on question answering, often neglecting the full
potential of questioning and assessment skills. Inspired by the human learning
mechanism, we introduce LOVA3, an innovative framework named "Learning tO
Visual question Answering, Asking and Assessment," designed to equip MLLMs with
these additional capabilities. Our approach involves the creation of two
supplementary training tasks GenQA and EvalQA, aiming at fostering the skills
of asking and assessing questions in the context of images. To develop the
questioning ability, we compile a comprehensive set of multimodal foundational
tasks. For assessment, we introduce a new benchmark called EvalQABench,
comprising 64,000 training samples (split evenly between positive and negative
samples) and 5,000 validation and testing samples. We posit that enhancing
MLLMs with the capabilities to answer, ask, and assess questions will enhance
their multimodal comprehension, ultimately improving overall performance. To
validate this hypothesis, we train MLLMs using the LOVA3 framework and evaluate
them on a range of multimodal datasets and benchmarks. Our results demonstrate
consistent performance gains, underscoring the critical role of these
additional tasks in fostering comprehensive intelligence in MLLMs. The code is
available at https://github.com/showlab/LOVA3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. The code is available at
  https://github.com/showlab/LOVA3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReMoDetect: Reward Models Recognize Aligned LLM's Generations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunseok Lee, Jihoon Tack, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable capabilities and easy accessibility of large language models
(LLMs) have significantly increased societal risks (e.g., fake news
generation), necessitating the development of LLM-generated text (LGT)
detection methods for safe usage. However, detecting LGTs is challenging due to
the vast number of LLMs, making it impractical to account for each LLM
individually; hence, it is crucial to identify the common characteristics
shared by these models. In this paper, we draw attention to a common feature of
recent powerful LLMs, namely the alignment training, i.e., training LLMs to
generate human-preferable texts. Our key finding is that as these aligned LLMs
are trained to maximize the human preferences, they generate texts with higher
estimated preferences even than human-written texts; thus, such texts are
easily detected by using the reward model (i.e., an LLM trained to model human
preference distribution). Based on this finding, we propose two training
schemes to further improve the detection ability of the reward model, namely
(i) continual preference fine-tuning to make the reward model prefer aligned
LGTs even further and (ii) reward modeling of Human/LLM mixed texts (a
rephrased texts from human-written texts using aligned LLMs), which serves as a
median preference text corpus between LGTs and human-written texts to learn the
decision boundary better. We provide an extensive evaluation by considering six
text domains across twelve aligned LLMs, where our method demonstrates
state-of-the-art results. Code is available at
https://github.com/hyunseoklee-ai/ReMoDetect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference proceeding for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciEval: A Multi-Level Large Language Model Evaluation Benchmark for
  Scientific Research <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13149v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13149v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been growing interest in using Large Language Models
(LLMs) for scientific research. Numerous benchmarks have been proposed to
evaluate the ability of LLMs for scientific research. However, current
benchmarks are mostly based on pre-collected objective questions. This design
suffers from data leakage problem and lacks the evaluation of subjective Q/A
ability. In this paper, we propose SciEval, a comprehensive and
multi-disciplinary evaluation benchmark to address these issues. Based on
Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate
scientific research ability. In particular, we design a "dynamic" subset based
on scientific principles to prevent evaluation from potential data leakage.
Both objective and subjective questions are included in SciEval. These
characteristics make SciEval a more effective benchmark for scientific research
ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs
show that, although GPT-4 achieves SOTA performance compared to other LLMs,
there is still substantial room for improvement, especially for dynamic
questions. The codes and data are publicly available on
https://github.com/OpenDFM/SciEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 17 figures, 12 tables. Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Large Language Models Truly Grasp Mathematics? An Empirical
  Exp<span class="highlight-title">lora</span>tion From A Psychological Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14979v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14979v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xie, Shuoyoucheng Ma, Zhenhua Wang, Enze Wang, Kai Chen, Xiaobing Sun, Baosheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their proficiency in math tasks, the mechanisms underlying LLMs'
mathematical reasoning abilities remain a subject of debate. Recent studies
suggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning
by encouraging LLMs to employ human-like logical reasoning (System 2), enabling
them to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs
genuinely possess System 2-like logical reasoning, we introduced targeted
modifications to CRT problems. Our findings reveal that, despite the use of CoT
prompts, mainstream LLMs, including the latest o1-preview model, continue to
exhibit a significant error rate. Further analysis indicates that they
predominantly rely on System 1-like intuitive reasoning and pattern matching
derived from training data, rather than demonstrating mastery of mathematical
thinking. This discovery challenges the prevailing notion that LLMs possess
genuine logical reasoning abilities and that CoT can enhance them.
Consequently, this work may temper overly optimistic projections regarding
LLMs' advancement toward artificial general intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Collaborative Content Moderation Framework for Toxicity Detection
  based on Conformalized Estimates of Annotation Disagreement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Villate-Castillo, Javier Del Ser, Borja Sanz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content moderation typically combines the efforts of human moderators and
machine learning models. However, these systems often rely on data where
significant disagreement occurs during moderation, reflecting the subjective
nature of toxicity perception. Rather than dismissing this disagreement as
noise, we interpret it as a valuable signal that highlights the inherent
ambiguity of the content,an insight missed when only the majority label is
considered. In this work, we introduce a novel content moderation framework
that emphasizes the importance of capturing annotation disagreement. Our
approach uses multitask learning, where toxicity classification serves as the
primary task and annotation disagreement is addressed as an auxiliary task.
Additionally, we leverage uncertainty estimation techniques, specifically
Conformal Prediction, to account for both the ambiguity in comment annotations
and the model's inherent uncertainty in predicting toxicity and
disagreement.The framework also allows moderators to adjust thresholds for
annotation disagreement, offering flexibility in determining when ambiguity
should trigger a review. We demonstrate that our joint approach enhances model
performance, calibration, and uncertainty estimation, while offering greater
parameter efficiency and improving the review process in comparison to
single-task methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical
  Questioning for Socratic Code Debugging <span class="chip">EMNLP'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11709v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11709v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Kargupta, Ishika Agarwal, Dilek Hakkani-Tur, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Socratic questioning is an effective teaching strategy, encouraging critical
thinking and problem-solving. The conversational capabilities of large language
models (LLMs) show great potential for providing scalable, real-time student
guidance. However, current LLMs often give away solutions directly, making them
ineffective instructors. We tackle this issue in the code debugging domain with
TreeInstruct, an Instructor agent guided by a novel state space-based planning
algorithm. TreeInstruct asks probing questions to help students independently
identify and resolve errors. It estimates a student's conceptual and
syntactical knowledge to dynamically construct a question tree based on their
responses and current knowledge state, effectively addressing both independent
and dependent mistakes concurrently in a multi-turn interaction setting. In
addition to using an existing single-bug debugging benchmark, we construct a
more challenging multi-bug dataset of 150 coding problems, incorrect solutions,
and bug fixes -- all carefully constructed and annotated by experts. Extensive
evaluation shows TreeInstruct's state-of-the-art performance on both datasets,
proving it to be a more effective instructor than baselines. Furthermore, a
real-world case study with five students of varying skill levels further
demonstrates TreeInstruct's ability to guide students to debug their code
efficiently with minimal turns and highly Socratic questioning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at: https://github.com/agarwalishika/TreeInstruct
  Accepted at EMNLP'24 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAD: Personalized Alignment of LLMs at Decoding-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04070v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04070v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhe Chen, Xiaotian Zhang, Meng Luo, Wenhao Chai, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning with personalized preferences, which vary significantly across
cultural, educational, and political differences, poses a significant challenge
due to the computational costs and data demands of traditional alignment
methods. In response, this paper presents Personalized Alignment at
Decoding-time (PAD), a novel framework designed to align LLM outputs with
diverse personalized preferences during the inference phase, eliminating the
need for additional training. By introducing a unique personalized reward
modeling strategy, this framework decouples the text generation process from
personalized preferences, facilitating the generation of generalizable
token-level personalized rewards. The PAD algorithm leverages these rewards to
guide the decoding process, dynamically tailoring the base model's predictions
to personalized preferences. Extensive experimental results demonstrate that
PAD not only outperforms existing training-based alignment methods in terms of
aligning with diverse preferences but also shows significant generalizability
to preferences unseen during training and scalability across different base
models. This work advances the capability of LLMs to meet user needs in
real-time applications, presenting a substantial step forward in personalized
LLM alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper presents Personalized Alignment at Decoding-time (PAD), a
  novel framework designed to align LLM outputs with diverse personalized
  preferences during the inference phase</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Translation of Circumlocution in Arabic Short Stories into English 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dalal Waadallah Shehab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the translation of circumlocution from Arabic to
English in a corpus of short stories by renowned Arabic authors. By analyzing
the source and target texts, the study aims to identify and categorize
circumlocution instances in Arabic and their corresponding renditions in
English. The study employs Nida's (1964) translation theory as a framework to
assess the appropriateness of the translation strategies employed. It examines
the extent to which translators successfully rendered Arabic circumlocution
into English, identifying potential challenges and limitations in the
translation process. The findings reveal significant similarities between
Arabic circumlocution categories and English metadiscourse categories,
particularly in terms of textual and interpersonal functions. However, the
study also highlights instances where translators encountered difficulties in
accurately conveying the nuances of circumlocution, often resorting to
strategies like addition, subtraction, and alteration.https://ntu.edu.iq/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SciDFM: A Large Language Model with Mixture-of-Experts for Science <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangtai Sun, Danyu Luo, Da Ma, Zihan Zhao, Baocai Chen, Zhennan Shen, Su Zhu, Lu Chen, Xin Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a significant upsurge of interest in leveraging
large language models (LLMs) to assist scientific discovery. However, most LLMs
only focus on general science, while they lack domain-specific knowledge, such
as chemical molecules and amino acid sequences. To bridge these gaps, we
introduce SciDFM, a mixture-of-experts LLM, which is trained from scratch and
is able to conduct college-level scientific reasoning and understand molecules
and amino acid sequences. We collect a large-scale training corpus containing
numerous scientific papers and books from different disciplines as well as data
from domain-specific databases. We further fine-tune the pre-trained model on
lots of instruction data to improve performances on downstream benchmarks. From
experiment results, we show that SciDFM achieves strong performance on general
scientific benchmarks such as SciEval and SciQ, and it reaches a SOTA
performance on domain-specific benchmarks among models of similar size. We
further analyze the expert layers and show that the results of expert selection
vary with data from different disciplines. To benefit the broader research
community, we open-source SciDFM at
https://huggingface.co/OpenDFM/SciDFM-MoE-A5.6B-v1.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, 9 tables. Technical Report, accepted by NeurIPS
  2024 Workshop FM4Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphTeam: Facilitating Large Language Model-based Graph Analysis via
  Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Chen Qian, Chuan Shi, Cheng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs are widely used for modeling relational data in real-world scenarios,
such as social networks and urban computing. Existing LLM-based graph analysis
approaches either integrate graph neural networks (GNNs) for specific machine
learning tasks, limiting their transferability, or rely solely on LLMs'
internal reasoning ability, resulting in suboptimal performance. To address
these limitations, we take advantage of recent advances in LLM-based agents,
which have shown capabilities of utilizing external knowledge or tools for
problem solving. By simulating human problem-solving strategies such as analogy
and collaboration, we propose a multi-agent system based on LLMs named
GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from
three modules, and the agents with different specialities can collaborate with
each other to address complex problems. Specifically, (1) input-output
normalization module: the question agent extracts and refines four key
arguments from the original question, facilitating the problem understanding,
and the answer agent organizes the results to meet the output requirement; (2)
external knowledge retrieval module: we first build a knowledge base consisting
of relevant documentation and experience information, and then the search agent
retrieves the most relevant entries for each question. (3) problem-solving
module: given the retrieved information from search agent, the coding agent
uses established algorithms via programming to generate solutions, and in case
the coding agent does not work, the reasoning agent will directly compute the
results without programming. Extensive experiments on six graph analysis
benchmarks demonstrate that GraphTeam achieves state-of-the-art performance
with an average 25.85% improvement over the best baseline in terms of accuracy.
The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What is lost in Normalization? Exploring Pitfalls in Multilingual ASR
  Model Evaluations <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02449v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02449v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavya Manohar, Leena G Pillai, Elizabeth Sherly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the pitfalls in evaluating multilingual automatic speech
recognition (ASR) models, with a particular focus on Indic language scripts. We
investigate the text normalization routine employed by leading ASR models,
including OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer,
and their unintended consequences on performance metrics. Our research reveals
that current text normalization practices, while aiming to standardize ASR
outputs for fair comparison, by removing inconsistencies such as variations in
spelling, punctuation, and special characters, are fundamentally flawed when
applied to Indic scripts. Through empirical analysis using text similarity
scores and in-depth linguistic examination, we demonstrate that these flaws
lead to artificially improved performance metrics for Indic languages. We
conclude by proposing a shift towards developing text normalization routines
that leverage native linguistic expertise, ensuring more robust and accurate
evaluations of multilingual ASR models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15265v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15265v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, Daochen Zha, Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, Vipin Chaudhary, Shuai Xu, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth in model size, fine-tuning the large pre-trained
language model has become increasingly difficult due to its extensive memory
usage. Previous works usually focus on reducing the number of trainable
parameters in the network. While the model parameters do contribute to memory
usage, the primary memory bottleneck during training arises from storing
feature maps, also known as activations, as they are crucial for gradient
calculation. Notably, neural networks are usually trained using stochastic
gradient descent. We argue that in stochastic optimization, models can handle
noisy gradients as long as the gradient estimator is unbiased with reasonable
variance. Following this motivation, we propose a new family of unbiased
estimators called WTA-CRS, for matrix production with reduced variance, which
only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the
context of tuning transformers, our proposed estimators exhibit lower variance
compared to existing ones. By replacing the linear operation with our
approximated one in transformers, we can achieve up to 2.7$\times$ peak memory
reduction with almost no accuracy drop and enables up to $6.4\times$ larger
batch size. Under the same hardware, WTA-CRS enables better down-streaming task
performance by applying larger models and/or faster training speed with larger
batch sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Quality of Answers for Retrieval-Augmented Generation: A
  Strong LLM Is All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18064v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18064v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Wang, Alberto Garcia Hernandez, Roman Kyslyi, Nicholas Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive study of answer quality evaluation in
Retrieval-Augmented Generation (RAG) applications using vRAG-Eval, a novel
grading system that is designed to assess correctness, completeness, and
honesty. We further map the grading of quality aspects aforementioned into a
binary score, indicating an accept or reject decision, mirroring the intuitive
"thumbs-up" or "thumbs-down" gesture commonly used in chat applications. This
approach suits factual business contexts where a clear decision opinion is
essential. Our assessment applies vRAG-Eval to two Large Language Models
(LLMs), evaluating the quality of answers generated by a vanilla RAG
application. We compare these evaluations with human expert judgments and find
a substantial alignment between GPT-4's assessments and those of human experts,
reaching 83% agreement on accept or reject decisions. This study highlights the
potential of LLMs as reliable evaluators in closed-domain, closed-ended
settings, particularly when human evaluations require significant resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How <span class="highlight-title">Transformer</span>s Solve Propositional Logic Problems: A Mechanistic
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guan Zhe Hong, Nishanth Dikkala, Enming Luo, Cyrus Rashtchian, Xin Wang, Rina Panigrahy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown amazing performance on tasks that
require planning and reasoning. Motivated by this, we investigate the internal
mechanisms that underpin a network's ability to perform complex logical
reasoning. We first construct a synthetic propositional logic problem that
serves as a concrete test-bed for network training and evaluation. Crucially,
this problem demands nontrivial planning to solve, but we can train a small
transformer to achieve perfect accuracy. Building on our set-up, we then pursue
an understanding of precisely how a three-layer transformer, trained from
scratch, solves this problem. We are able to identify certain "planning" and
"reasoning" circuits in the network that necessitate cooperation between the
attention blocks to implement the desired logic. To expand our findings, we
then study a larger model, Mistral 7B. Using activation patching, we
characterize internal components that are critical in solving our logic
problem. Overall, our work systemically uncovers novel aspects of small and
large transformers, and continues the study of how they plan and reason.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and
  Iterative Sub-SQL Refinement for Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07930v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07930v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Xie, Gaochen Wu, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent In-Context Learning based methods have achieved remarkable success in
Text-to-SQL task. However, there is still a large gap between the performance
of these models and human performance on datasets with complex database schema
and difficult questions, such as BIRD. Besides, existing work has neglected to
supervise intermediate steps when solving questions iteratively with question
decomposition methods, and the schema linking methods used in these works are
very rudimentary. To address these issues, we propose MAG-SQL, a multi-agent
generative approach with soft schema linking and iterative Sub-SQL refinement.
In our framework, an entity-based method with tables' summary is used to select
the columns in database, and a novel targets-conditions decomposition method is
introduced to decompose those complex questions. Additionally, we build a
iterative generating module which includes a Sub-SQL Generator and Sub-SQL
Refiner, introducing external oversight for each step of generation. Through a
series of ablation studies, the effectiveness of each agent in our framework
has been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL
achieves an execution accuracy of 61.08%, compared to the baseline accuracy of
46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL.
Besides, our approach makes similar progress on Spider. The codes are available
at https://github.com/LancelotXWX/MAG-SQL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Employing Large Language Models for Text-to-SQL Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15186v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15186v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Shi, Zhengju Tang, Nan Zhang, Xiaotong Zhang, Zhi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing volume of data in relational databases and the expertise
needed for writing SQL queries pose challenges for users to access and analyze
data. Text-to-SQL (Text2SQL) solves the issues by utilizing natural language
processing (NLP) techniques to convert natural language into SQL queries. With
the development of Large Language Models (LLMs), a range of LLM-based Text2SQL
methods have emerged. This survey provides a comprehensive review of LLMs in
Text2SQL tasks. We review benchmark datasets, prompt engineering methods,
fine-tuning methods, and base models in LLM-based Text2SQL methods. We provide
insights in each part and discuss future directions in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Oscars of AI Theater: A <span class="highlight-title">Survey</span> on Role-Playing with Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11484v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11484v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Yan Wang, Yang Deng, Jia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey explores the burgeoning field of role-playing with language
models, focusing on their development from early persona-based models to
advanced character-driven simulations facilitated by Large Language Models
(LLMs). Initially confined to simple persona consistency due to limited model
capabilities, role-playing tasks have now expanded to embrace complex character
portrayals involving character consistency, behavioral alignment, and overall
attractiveness. We provide a comprehensive taxonomy of the critical components
in designing these systems, including data, models and alignment, agent
architecture and evaluation. This survey not only outlines the current
methodologies and challenges, such as managing dynamic personal profiles and
achieving high-level persona consistency but also suggests avenues for future
research in improving the depth and realism of role-playing applications. The
goal is to guide future research by offering a structured overview of current
methodologies and identifying potential areas for improvement. Related
resources and papers are available at
https://github.com/nuochenpku/Awesome-Role-Play-Papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FactTest: Factuality Testing in Large Language Models with Finite-Sample
  and Distribution-Free Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02603v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02603v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Nie, Xiaotian Hou, Shuhang Lin, James Zou, Huaxiu Yao, Linjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The propensity of Large Language Models (LLMs) to generate hallucinations and
non-factual content undermines their reliability in high-stakes domains, where
rigorous control over Type I errors (the conditional probability of incorrectly
classifying hallucinations as truthful content) is essential. Despite its
importance, formal verification of LLM factuality with such guarantees remains
largely unexplored. In this paper, we introduce FactTest, a novel framework
that statistically assesses whether a LLM can confidently provide correct
answers to given questions with high-probability correctness guarantees. We
formulate factuality testing as hypothesis testing problem to enforce an upper
bound of Type I errors at user-specified significance levels. Notably, we prove
that our framework also ensures strong Type II error control under mild
conditions and can be extended to maintain its effectiveness when covariate
shifts exist. Our approach is distribution-free and works for any number of
human-annotated samples. It is model-agnostic and applies to any black-box or
white-box LM. Extensive experiments on question-answering (QA) and
multiple-choice benchmarks demonstrate that FactTest effectively detects
hallucinations and improves the model's ability to abstain from answering
unknown questions, leading to an over 40% accuracy improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linguistic Collapse: Neural Collapse in (Large) Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Wu, Vardan Papyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural collapse ($\mathcal{NC}$) is a phenomenon observed in classification
tasks where top-layer representations collapse into their class means, which
become equinorm, equiangular and aligned with the classifiers. These behaviors
-- associated with generalization and robustness -- would manifest under
specific conditions: models are trained towards zero loss, with noise-free
labels belonging to balanced classes, which do not outnumber the model's hidden
dimension. Recent studies have explored $\mathcal{NC}$ in the absence of one or
more of these conditions to extend and capitalize on the associated benefits of
ideal geometries. Language modeling presents a curious frontier, as
\textit{training by token prediction} constitutes a classification task where
none of the conditions exist: the vocabulary is imbalanced and exceeds the
embedding dimension; different tokens might correspond to similar contextual
embeddings; and large language models (LLMs) in particular are typically only
trained for a few epochs. This paper empirically investigates the impact of
scaling the architectures and training of causal language models (CLMs) on
their progression towards $\mathcal{NC}$. We find that $\mathcal{NC}$
properties that develop with scale (and regularization) are linked to
generalization. Moreover, there is evidence of some relationship between
$\mathcal{NC}$ and generalization independent of scale. Our work thereby
underscores the generality of $\mathcal{NC}$ as it extends to the novel and
more challenging setting of language modeling. Downstream, we seek to inspire
further research on the phenomenon to deepen our understanding of LLMs -- and
neural networks at large -- and improve existing architectures based on
$\mathcal{NC}$-related properties. Our code is hosted on GitHub at
https://github.com/rhubarbwu/linguistic-collapse .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; 36 pages; 30 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-form factuality in large language models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can outperform crowdsourced human
annotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; 72 pages, 18 figures, 30 tables. Code at
  https://github.com/google-deepmind/long-form-factuality</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HealthQ: Unveiling Questioning Capabilities of LLM Chains in Healthcare
  Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19487v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19487v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Wang, Hao Li, Di Huang, Amir M. Rahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In digital healthcare, large language models (LLMs) have primarily been
utilized to enhance question-answering capabilities and improve patient
interactions. However, effective patient care necessitates LLM chains that can
actively gather information by posing relevant questions. This paper presents
HealthQ, a novel framework designed to evaluate the questioning capabilities of
LLM healthcare chains. We implemented several LLM chains, including
Retrieval-Augmented Generation (RAG), Chain of Thought (CoT), and reflective
chains, and introduced an LLM judge to assess the relevance and informativeness
of the generated questions. To validate HealthQ, we employed traditional
Natural Language Processing (NLP) metrics such as Recall-Oriented Understudy
for Gisting Evaluation (ROUGE) and Named Entity Recognition (NER)-based set
comparison, and constructed two custom datasets from public medical note
datasets, ChatDoctor and MTS-Dialog. Our contributions are threefold: we
provide the first comprehensive study on the questioning capabilities of LLMs
in healthcare conversations, develop a novel dataset generation pipeline, and
propose a detailed evaluation methodology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deploying Multi-task Online Server with Large Language Model <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yincen Qu, Chao Ma, Xiangying Dai, Hui Zhou, Yiting Wu, Hengyue Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the industry, numerous tasks are deployed online. Traditional approaches
often tackle each task separately by its own network, which leads to excessive
costs for developing and scaling models, especially in the context of large
language models. Although multi-task methods can save costs through parameter
sharing, they often struggle to outperform single-task methods in real-world
applications. To tackle these challenges, we present a three-stage multi-task
learning framework for large language models. It involves task filtering,
followed by fine-tuning on high-resource tasks, and finally fine-tuning on all
tasks. We conducted comprehensive experiments in single-task and multi-task
settings. Our approach, exemplified on different benchmarks, demonstrates that
it is able to achieve performance comparable to the single-task method while
reducing up to 90.9\% of its overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2025 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $B^4$: A Black-Box Scrubbing Attack on LLM Watermarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01222v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01222v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baizhou Huang, Xiao Pu, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking has emerged as a prominent technique for LLM-generated content
detection by embedding imperceptible patterns. Despite supreme performance, its
robustness against adversarial attacks remains underexplored. Previous work
typically considers a grey-box attack setting, where the specific type of
watermark is already known. Some even necessitates knowledge about
hyperparameters of the watermarking method. Such prerequisites are unattainable
in real-world scenarios. Targeting at a more realistic black-box threat model
with fewer assumptions, we here propose $B^4$, a black-box scrubbing attack on
watermarks. Specifically, we formulate the watermark scrubbing attack as a
constrained optimization problem by capturing its objectives with two
distributions, a Watermark Distribution and a Fidelity Distribution. This
optimization problem can be approximately solved using two proxy distributions.
Experimental results across 12 different settings demonstrate the superior
performance of $B^4$ compared with other baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FinCon: A Synthesized LLM Multi-Agent System with Conceptual Verbal
  Reinforcement for Enhanced Financial Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06567v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06567v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yupeng Cao, Zhi Chen, Jordan W. Suchow, Rong Liu, Zhenyu Cui, Zhaozhuo Xu, Denghui Zhang, Koduvayur Subbalakshmi, Guojun Xiong, Yueru He, Jimin Huang, Dong Li, Qianqian Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated notable potential in
conducting complex tasks and are increasingly utilized in various financial
applications. However, high-quality sequential financial investment
decision-making remains challenging. These tasks require multiple interactions
with a volatile environment for every decision, demanding sufficient
intelligence to maximize returns and manage risks. Although LLMs have been used
to develop agent systems that surpass human teams and yield impressive
investment returns, opportunities to enhance multi-sourced information
synthesis and optimize decision-making outcomes through timely experience
refinement remain unexplored. Here, we introduce the FinCon, an LLM-based
multi-agent framework with CONceptual verbal reinforcement tailored for diverse
FINancial tasks. Inspired by effective real-world investment firm
organizational structures, FinCon utilizes a manager-analyst communication
hierarchy. This structure allows for synchronized cross-functional agent
collaboration towards unified goals through natural language interactions and
equips each agent with greater memory capacity than humans. Additionally, a
risk-control component in FinCon enhances decision quality by episodically
initiating a self-critiquing mechanism to update systematic investment beliefs.
The conceptualized beliefs serve as verbal reinforcement for the future agent's
behavior and can be selectively propagated to the appropriate node that
requires knowledge updates. This feature significantly improves performance
while reducing unnecessary peer-to-peer communication costs. Moreover, FinCon
demonstrates strong generalization capabilities in various financial tasks,
including single stock trading and portfolio management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM Applications, LLM Agents, Financial Technology, Quantitative
  Finance, Algorithmic Trading, Cognitive Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Birdie: Advancing State Space Models with Reward-Driven Objectives and
  Curricula <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01030v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01030v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Blouir, Jimmy T. H. Smith, Antonios Anastasopoulos, Amarda Shehu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient state space models (SSMs), such as linear recurrent neural networks
and linear attention variants, offer computational advantages over Transformers
but struggle with tasks requiring long-range in-context retrieval-like text
copying, associative recall, and question answering over long contexts.
Previous efforts to address these challenges have focused on architectural
modifications, often reintroducing computational inefficiencies. In this paper,
we propose a novel training procedure, Birdie, that significantly enhances the
in-context retrieval capabilities of SSMs without altering their architecture.
Our approach combines bidirectional input processing with dynamic mixtures of
specialized pre-training objectives, optimized via reinforcement learning. We
introduce a new bidirectional SSM architecture that seamlessly transitions from
bidirectional context processing to causal generation. Experimental evaluations
demonstrate that Birdie markedly improves performance on retrieval-intensive
tasks such as multi-number phone book lookup, long paragraph
question-answering, and infilling. This narrows the performance gap with
Transformers, while retaining computational efficiency. Our findings highlight
the importance of training procedures in leveraging the fixed-state capacity of
SSMs, offering a new direction to advance their capabilities. All code and
pre-trained models are available at https://www.github.com/samblouir/birdie,
with support for JAX and PyTorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-06T00:00:00Z">2024-11-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">92</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical Adaptation of Large Language and Vision-Language Models: Are We
  Making Progress? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel P. Jeong, Saurabh Garg, Zachary C. Lipton, Michael Oberst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several recent works seek to develop foundation models specifically for
medical applications, adapting general-purpose large language models (LLMs) and
vision-language models (VLMs) via continued pretraining on publicly available
biomedical corpora. These works typically claim that such domain-adaptive
pretraining (DAPT) improves performance on downstream medical tasks, such as
answering medical licensing exam questions. In this paper, we compare seven
public "medical" LLMs and two VLMs against their corresponding base models,
arriving at a different conclusion: all medical VLMs and nearly all medical
LLMs fail to consistently improve over their base models in the zero-/few-shot
prompting regime for medical question-answering (QA) tasks. For instance,
across the tasks and model pairs we consider in the 3-shot setting, medical
LLMs only outperform their base models in 12.1% of cases, reach a (statistical)
tie in 49.8% of cases, and are significantly worse than their base models in
the remaining 38.2% of cases. Our conclusions are based on (i) comparing each
medical model head-to-head, directly against the corresponding base model; (ii)
optimizing the prompts for each model separately; and (iii) accounting for
statistical uncertainty in comparisons. While these basic practices are not
consistently adopted in the literature, our ablations show that they
substantially impact conclusions. Our findings suggest that state-of-the-art
general-domain models may already exhibit strong medical knowledge and
reasoning capabilities, and offer recommendations to strengthen the conclusions
of future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference as Long Paper (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Consistency Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, Jane Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-alignment, whereby models learn to improve themselves without human
annotation, is a rapidly growing research area. However, existing techniques
often fail to improve complex reasoning tasks due to the difficulty of
assigning correct rewards. An orthogonal approach that is known to improve
correctness is self-consistency, a method applied at inference time based on
multiple sampling in order to find the most consistent answer. In this work, we
extend the self-consistency concept to help train models. We thus introduce
self-consistency preference optimization (ScPO), which iteratively trains
consistent answers to be preferred over inconsistent ones on unsupervised new
problems. We show ScPO leads to large improvements over conventional reward
model training on reasoning tasks such as GSM8K and MATH, closing the gap with
supervised training with gold answers or preferences, and that combining ScPO
with standard supervised learning improves results even further. On ZebraLogic,
ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and
Claude-3 Haiku.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How <span class="highlight-title">Transformer</span>s Solve Propositional Logic Problems: A Mechanistic
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guan Zhe Hong, Nishanth Dikkala, Enming Luo, Cyrus Rashtchian, Rina Panigrahy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown amazing performance on tasks that
require planning and reasoning. Motivated by this, we investigate the internal
mechanisms that underpin a network's ability to perform complex logical
reasoning. We first construct a synthetic propositional logic problem that
serves as a concrete test-bed for network training and evaluation. Crucially,
this problem demands nontrivial planning to solve, but we can train a small
transformer to achieve perfect accuracy. Building on our set-up, we then pursue
an understanding of precisely how a three-layer transformer, trained from
scratch, solves this problem. We are able to identify certain "planning" and
"reasoning" circuits in the network that necessitate cooperation between the
attention blocks to implement the desired logic. To expand our findings, we
then study a larger model, Mistral 7B. Using activation patching, we
characterize internal components that are critical in solving our logic
problem. Overall, our work systemically uncovers novel aspects of small and
large transformers, and continues the study of how they plan and reason.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Summarization of Opinionated Political Documents with Varied
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Deas, Kathleen McKeown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global partisan hostility and polarization has increased, and this
polarization is heightened around presidential elections. Models capable of
generating accurate summaries of diverse perspectives can help reduce such
polarization by exposing users to alternative perspectives. In this work, we
introduce a novel dataset and task for independently summarizing each political
perspective in a set of passages from opinionated news articles. For this task,
we propose a framework for evaluating different dimensions of perspective
summary performance. We benchmark 10 models of varying sizes and architectures
through both automatic and human evaluation. While recent models like GPT-4o
perform well on this task, we find that all models struggle to generate
summaries faithful to the intended perspective. Our analysis of summaries
focuses on how extraction behavior depends on the features of the input
documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Collaborative Content Moderation Framework for Toxicity Detection
  based on Conformalized Estimates of Annotation Disagreement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Villate-Castillo, Javier Del Ser, Borja Sanz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content moderation typically combines the efforts of human moderators and
machine learning models.However, these systems often rely on data where
significant disagreement occurs during moderation, reflecting the subjective
nature of toxicity perception.Rather than dismissing this disagreement as
noise, we interpret it as a valuable signal that highlights the inherent
ambiguity of the content,an insight missed when only the majority label is
considered.In this work, we introduce a novel content moderation framework that
emphasizes the importance of capturing annotation disagreement. Our approach
uses multitask learning, where toxicity classification serves as the primary
task and annotation disagreement is addressed as an auxiliary
task.Additionally, we leverage uncertainty estimation techniques, specifically
Conformal Prediction, to account for both the ambiguity in comment annotations
and the model's inherent uncertainty in predicting toxicity and
disagreement.The framework also allows moderators to adjust thresholds for
annotation disagreement, offering flexibility in determining when ambiguity
should trigger a review.We demonstrate that our joint approach enhances model
performance, calibration, and uncertainty estimation, while offering greater
parameter efficiency and improving the review process in comparison to
single-task methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for
  Evaluating Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhan Li, Ziyao Shangguan, Yilun Zhao, Deyuan Li, Yixin Liu, Arman Cohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing benchmarks for evaluating foundation models mainly focus on
single-document, text-only tasks. However, they often fail to fully capture the
complexity of research workflows, which typically involve interpreting
non-textual data and gathering information across multiple documents. To
address this gap, we introduce M3SciQA, a multi-modal, multi-document
scientific question answering benchmark designed for a more comprehensive
evaluation of foundation models. M3SciQA consists of 1,452 expert-annotated
questions spanning 70 natural language processing paper clusters, where each
cluster represents a primary paper along with all its cited documents,
mirroring the workflow of comprehending a single paper by requiring multi-modal
and multi-document data. With M3SciQA, we conduct a comprehensive evaluation of
18 foundation models. Our results indicate that current foundation models still
significantly underperform compared to human experts in multi-modal information
retrieval and in reasoning across multiple scientific documents. Additionally,
we explore the implications of these findings for the future advancement of
applying foundation models in multi-modal scientific literature analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beemo: Benchmark of Expert-edited Machine-generated Outputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekaterina Artemova, Jason Lucas, Saranya Venkatraman, Jooyoung Lee, Sergei Tilga, Adaku Uchendu, Vladislav Mikhailov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid proliferation of large language models (LLMs) has increased the
volume of machine-generated texts (MGTs) and blurred text authorship in various
domains. However, most existing MGT benchmarks include single-author texts
(human-written and machine-generated). This conventional design fails to
capture more practical multi-author scenarios, where the user refines the LLM
response for natural flow, coherence, and factual correctness. Our paper
introduces the Benchmark of Expert-edited Machine-generated Outputs (Beemo),
which includes 6.5k texts written by humans, generated by ten
instruction-finetuned LLMs, and edited by experts for various use cases,
ranging from creative writing to summarization. Beemo additionally comprises
13.1k machine-generated and LLM-edited texts, allowing for diverse MGT
detection evaluation across various edit types. We document Beemo's creation
protocol and present the results of benchmarking 33 configurations of MGT
detectors in different experimental setups. We find that expert-based editing
evades MGT detection, while LLM-edited texts are unlikely to be recognized as
human-written. Beemo and all materials are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prompt Engineering Using <span class="highlight-title">GPT</span> for Word-Level Code-Mixed Language
  Identification in Low-Resource Dravidian Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Deroy, Subhankar Maity
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Identification (LI) is crucial for various natural language
processing tasks, serving as a foundational step in applications such as
sentiment analysis, machine translation, and information retrieval. In
multilingual societies like India, particularly among the youth engaging on
social media, text often exhibits code-mixing, blending local languages with
English at different linguistic levels. This phenomenon presents formidable
challenges for LI systems, especially when languages intermingle within single
words. Dravidian languages, prevalent in southern India, possess rich
morphological structures yet suffer from under-representation in digital
platforms, leading to the adoption of Roman or hybrid scripts for
communication. This paper introduces a prompt based method for a shared task
aimed at addressing word-level LI challenges in Dravidian languages. In this
work, we leveraged GPT-3.5 Turbo to understand whether the large language
models is able to correctly classify words into correct categories. Our
findings show that the Kannada model consistently outperformed the Tamil model
across most metrics, indicating a higher accuracy and reliability in
identifying and categorizing Kannada language instances. In contrast, the Tamil
model showed moderate performance, particularly needing improvement in
precision and recall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at FIRE 2024 (Track: Word-level Language Identification in
  Dravidian Languages)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WorryWords: Norms of Anxiety Association for over 44k English Words 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saif M. Mohammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anxiety, the anticipatory unease about a potential negative outcome, is a
common and beneficial human emotion. However, there is still much that is not
known, such as how anxiety relates to our body and how it manifests in
language. This is especially pertinent given the increasing impact of
anxiety-related disorders. In this work, we introduce WorryWords, the first
large-scale repository of manually derived word--anxiety associations for over
44,450 English words. We show that the anxiety associations are highly
reliable. We use WorryWords to study the relationship between anxiety and other
emotion constructs, as well as the rate at which children acquire anxiety words
with age. Finally, we show that using WorryWords alone, one can accurately
track the change of anxiety in streams of text. The lexicon enables a wide
variety of anxiety-related research in psychology, NLP, public health, and
social sciences. WorryWords (and its translations to over 100 languages) is
freely available. http://saifmohammad.com/worrywords.html
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Really is Commonsense Knowledge? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quyet V. Do, Junze Li, Tung-Duong Vuong, Zhaowei Wang, Yangqiu Song, Xiaojuan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonsense datasets have been well developed in Natural Language Processing,
mainly through crowdsource human annotation. However, there are debates on the
genuineness of commonsense reasoning benchmarks. In specific, a significant
portion of instances in some commonsense benchmarks do not concern commonsense
knowledge. That problem would undermine the measurement of the true commonsense
reasoning ability of evaluated models. It is also suggested that the problem
originated from a blurry concept of commonsense knowledge, as distinguished
from other types of knowledge. To demystify all of the above claims, in this
study, we survey existing definitions of commonsense knowledge, ground into the
three frameworks for defining concepts, and consolidate them into a
multi-framework unified definition of commonsense knowledge (so-called
consolidated definition). We then use the consolidated definition for
annotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets
to examine the above claims. Our study shows that there exists a large portion
of non-commonsense-knowledge instances in the two datasets, and a large
performance gap on these two subsets where Large Language Models (LLMs) perform
worse on commonsense-knowledge instances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data will be released together with the next version of the
  paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Does A Text Preprocessing Pipeline Affect Ontology Syntactic
  Matching? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generic text preprocessing pipeline, comprising Tokenisation,
Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been
implemented in many ontology matching (OM) systems. However, the lack of
standardisation in text preprocessing creates diversity in mapping results. In
this paper, we investigate the effect of the text preprocessing pipeline on OM
tasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation
Initiative (OAEI) track repositories with 49 distinct alignments indicate: (1)
Tokenisation and Normalisation are currently more effective than Stop Words
Removal and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and
Stemming is task-specific. We recommend standalone Lemmatisation or Stemming
with post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer
perform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS)
Tagging does not help Lemmatisation. To repair less effective Stop Words
Removal and Stemming/Lemmatisation used in OM tasks, we propose a novel
context-based pipeline repair approach that significantly improves matching
correctness and overall matching performance. We also discuss the use of text
preprocessing pipeline in the new era of large language models (LLMs).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 26 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactions Across Blocks in Post-Training Quantization of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khasmamad Shabanovi, Lukas Wiest, Vladimir Golkov, Daniel Cremers, Thomas Pfeil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization is widely employed to reduce the computational
demands of neural networks. Typically, individual substructures, such as layers
or blocks of layers, are quantized with the objective of minimizing
quantization errors in their pre-activations by fine-tuning the corresponding
weights. Deriving this local objective from the global objective of minimizing
task loss involves two key simplifications: assuming substructures are mutually
independent and ignoring the knowledge of subsequent substructures as well as
the task loss. In this work, we assess the effects of these simplifications on
weight-only quantization of large language models. We introduce two multi-block
fine-tuning strategies and compare them against the baseline of fine-tuning
single transformer blocks. The first captures correlations of weights across
blocks by jointly optimizing multiple quantized blocks. The second incorporates
knowledge of subsequent blocks by minimizing the error in downstream
pre-activations rather than focusing solely on the quantized block. Our
findings indicate that the effectiveness of these methods depends on the
specific network model, with no impact on some models but demonstrating
significant benefits for others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation data contamination in LLMs: how do we measure it and (when)
  does it matter? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaditya K. Singh, Muhammed Yusuf Kocyigit, Andrew Poulton, David Esiobu, Maria Lomeli, Gergely Szilvasy, Dieuwke Hupkes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hampering the interpretation of benchmark scores, evaluation data
contamination has become a growing concern in the evaluation of LLMs, and an
active area of research studies its effects. While evaluation data
contamination is easily understood intuitively, it is surprisingly difficult to
define precisely which samples should be considered contaminated and,
consequently, how it impacts benchmark scores. We propose that these questions
should be addressed together and that contamination metrics can be assessed
based on whether models benefit from the examples they mark contaminated. We
propose a novel analysis method called ConTAM, and show with a large scale
survey of existing and novel n-gram based contamination metrics across 13
benchmarks and 7 models from 2 different families that ConTAM can be used to
better understand evaluation data contamination and its effects. We find that
contamination may have a much larger effect than reported in recent LLM
releases and benefits models differently at different scales. We also find that
considering only the longest contaminated substring provides a better signal
than considering a union of all contaminated substrings, and that doing model
and benchmark specific threshold analysis greatly increases the specificity of
the results. Lastly, we investigate the impact of hyperparameter choices,
finding that, among other things, both using larger values of n and
disregarding matches that are infrequent in the pre-training data lead to many
false negatives. With ConTAM, we provide a method to empirically ground
evaluation data contamination metrics in downstream effects. With our
exploration, we shed light on how evaluation data contamination can impact LLMs
and provide insight into the considerations important when doing contamination
analysis. We end our paper by discussing these in more detail and providing
concrete suggestions for future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAGulator: Lightweight Out-of-Context Detectors for Grounded Text
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Poey, Jiajun Liu, Qishuai Zhong, Adrien Chenailler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time detection of out-of-context LLM outputs is crucial for enterprises
looking to safely adopt RAG applications. In this work, we train lightweight
models to discriminate LLM-generated text that is semantically out-of-context
from retrieved text documents. We preprocess a combination of summarisation and
semantic textual similarity datasets to construct training data using minimal
resources. We find that DeBERTa is not only the best-performing model under
this pipeline, but it is also fast and does not require additional text
preprocessing or feature engineering. While emerging work demonstrates that
generative LLMs can also be fine-tuned and used in complex data pipelines to
achieve state-of-the-art performance, we note that speed and resource limits
are important considerations for on-premise deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lexicalization Is All You Need: Examining the Impact of Lexical
  Knowledge in a Compositional QALD System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Maria Schmidt, Mohammad Fazleh Elahi, Philipp Cimiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we examine the impact of lexicalization on Question Answering
over Linked Data (QALD). It is well known that one of the key challenges in
interpreting natural language questions with respect to SPARQL lies in bridging
the lexical gap, that is mapping the words in the query to the correct
vocabulary elements. We argue in this paper that lexicalization, that is
explicit knowledge about the potential interpretations of a word with respect
to the given vocabulary, significantly eases the task and increases the
performance of QA systems. Towards this goal, we present a compositional QA
system that can leverage explicit lexical knowledge in a compositional manner
to infer the meaning of a question in terms of a SPARQL query. We show that
such a system, given lexical knowledge, has a performance well beyond current
QA systems, achieving up to a $35.8\%$ increase in the micro $F_1$ score
compared to the best QA system on QALD-9. This shows the importance and
potential of including explicit lexical knowledge. In contrast, we show that
LLMs have limited abilities to exploit lexical knowledge, with only marginal
improvements compared to a version without lexical knowledge. This shows that
LLMs have no ability to compositionally interpret a question on the basis of
the meaning of its parts, a key feature of compositional approaches. Taken
together, our work shows new avenues for QALD research, emphasizing the
importance of lexicalization and compositionality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24th International Conference on Knowledge Engineering and Knowledge
  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Analysis of Gender Depiction in the Comedias of Calderón
  de la Barca 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allison Keith, Antonio Rojas Castro, Sebastian Padó
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In theatre, playwrights use the portrayal of characters to explore culturally
based gender norms. In this paper, we develop quantitative methods to study
gender depiction in the non-religious works (comedias) of Pedro Calder\'on de
la Barca, a prolific Spanish 17th century author. We gather insights from a
corpus of more than 100 plays by using a gender classifier and applying model
explainability (attribution) methods to determine which text features are most
influential in the model's decision to classify speech as 'male' or 'female',
indicating the most gendered elements of dialogue in Calder\'on's comedias in a
human accessible manner. We find that female and male characters are portrayed
differently and can be identified by the gender prediction model at practically
useful accuracies (up to f=0.83). Analysis reveals semantic aspects of gender
portrayal, and demonstrates that the model is even useful in providing a
relatively accurate scene-by-scene prediction of cross-dressing characters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi3Hate: Multimodal, Multilingual, and Multicultural Hate Speech
  Detection with Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Duc Bui, Katharina von der Wense, Anne Lauscher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Warning: this paper contains content that may be offensive or upsetting
  Hate speech moderation on global platforms poses unique challenges due to the
multimodal and multilingual nature of content, along with the varying cultural
perceptions. How well do current vision-language models (VLMs) navigate these
nuances? To investigate this, we create the first multimodal and multilingual
parallel hate speech dataset, annotated by a multicultural set of annotators,
called Multi3Hate. It contains 300 parallel meme samples across 5 languages:
English, German, Spanish, Hindi, and Mandarin. We demonstrate that cultural
background significantly affects multimodal hate speech annotation in our
dataset. The average pairwise agreement among countries is just 74%,
significantly lower than that of randomly selected annotator groups. Our
qualitative analysis indicates that the lowest pairwise label agreement-only
67% between the USA and India-can be attributed to cultural factors. We then
conduct experiments with 5 large VLMs in a zero-shot setting, finding that
these models align more closely with annotations from the US than with those
from other cultures, even when the memes and prompts are presented in the
dominant language of the other culture. Code and dataset are available at
https://github.com/MinhDucBui/Multi3Hate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Polynomial Composition Activations: Unleashing the Dynamics of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Zhuo, Ya Wang, Yutao Zeng, Xiaoqing Li, Xun Zhou, Jinwen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have found extensive applications across various domains due to
the powerful fitting capabilities. This success can be partially attributed to
their inherent nonlinearity. Thus, in addition to the ReLU function employed in
the original transformer architecture, researchers have explored alternative
modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment
representational capacity. In this paper, we propose a novel category of
polynomial composition activations (PolyCom), designed to optimize the dynamics
of transformers. Theoretically, we provide a comprehensive mathematical
analysis of PolyCom, highlighting its enhanced expressivity and efficacy
relative to other activation functions. Notably, we demonstrate that networks
incorporating PolyCom achieve the $\textbf{optimal approximation rate}$,
indicating that PolyCom networks require minimal parameters to approximate
general smooth functions in Sobolev spaces. We conduct empirical experiments on
the pre-training configurations of large language models (LLMs), including both
dense and sparse architectures. By substituting conventional activation
functions with PolyCom, we enable LLMs to capture higher-order interactions
within the data, thus improving performance metrics in terms of accuracy and
convergence rates. Extensive experimental results demonstrate the effectiveness
of our method, showing substantial improvements over other activation
functions. Code is available at https://github.com/BryceZhuo/PolyCom.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEG: Medical Knowledge-Augmented Large Language Models for Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Cabello, Carmen Martin-Turrero, Uchenna Akujuobi, Anders Søgaard, Carlos Bobed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering is a natural language understanding task that involves
reasoning over both explicit context and unstated, relevant domain knowledge.
Large language models (LLMs), which underpin most contemporary question
answering systems, struggle to induce how concepts relate in specialized
domains such as medicine. Existing medical LLMs are also costly to train. In
this work, we present MEG, a parameter-efficient approach for medical
knowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate
graph embeddings into the LLM, enabling it to leverage external knowledge in a
cost-effective way. We evaluate our method on four popular medical
multiple-choice datasets and show that LLMs greatly benefit from the factual
grounding provided by knowledge graph embeddings. MEG attains an average of
+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized
models like BioMistral. We also show results based on Llama-3. Finally, we show
that MEG's performance remains robust to the choice of graph encoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the
  Way Forward <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashi Kumar, Iuliia Thorbecke, Sergio Burdisso, Esaú Villatoro-Tello, Manjunath K E, Kadri Hacioğlu, Pradeep Rangappa, Petr Motlicek, Aravind Ganapathiraju, Andreas Stolcke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has demonstrated that training a linear connector between
speech foundation encoders and large language models (LLMs) enables this
architecture to achieve strong ASR capabilities. Despite the impressive
results, it remains unclear whether these simple approaches are robust enough
across different scenarios and speech conditions, such as domain shifts and
different speech perturbations. In this paper, we address these questions by
conducting various ablation experiments using a recent and widely adopted
approach called SLAM-ASR. We present novel empirical findings that offer
insights on how to effectively utilize the SLAM-ASR architecture across a wide
range of settings. Our main findings indicate that the SLAM-ASR exhibits poor
performance in cross-domain evaluation settings. Additionally, speech
perturbations within in-domain data, such as changes in speed or the presence
of additive noise, can significantly impact performance. Our findings offer
critical insights for fine-tuning and configuring robust LLM-based ASR models,
tailored to different data characteristics and computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2025 SALMA Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mamba<span class="highlight-title">PEFT</span>: Exploring <span class="highlight-title">Parameter-Efficient Fine-Tuning</span> for Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masakazu Yoshimura, Teruaki Hayashi, Yota Maeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An ecosystem of Transformer-based models has been established by building
large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a
crucial technology for deploying these models to downstream tasks with minimal
cost while achieving effective performance. Recently, Mamba, a State Space
Model (SSM)-based model, has attracted attention as a potential alternative to
Transformers. While many large-scale Mamba-based models have been proposed,
efficiently adapting pre-trained Mamba-based models to downstream tasks remains
unexplored. In this paper, we conduct an exploratory analysis of PEFT methods
for Mamba. We investigate the effectiveness of existing PEFT methods for
Transformers when applied to Mamba. We also modify these methods to better
align with the Mamba architecture. Additionally, we propose new Mamba-specific
PEFT methods that leverage the distinctive structure of Mamba. Our experiments
indicate that PEFT performs more effectively for Mamba than Transformers.
Lastly, we demonstrate how to effectively combine multiple PEFT methods and
provide a framework that outperforms previous works. To ensure reproducibility,
we will release the code after publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Novice to Expert: LLM Agent Policy Optimization via Step-wise
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhirui Deng, Zhicheng Dou, Yutao Zhu, Ji-Rong Wen, Ruibin Xiong, Mang Wang, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The outstanding capabilities of large language models (LLMs) render them a
crucial component in various autonomous agent systems. While traditional
methods depend on the inherent knowledge of LLMs without fine-tuning, more
recent approaches have shifted toward the reinforcement learning strategy to
further enhance agents' ability to solve complex interactive tasks with
environments and tools. However, previous approaches are constrained by the
sparse reward issue, where existing datasets solely provide a final scalar
reward for each multi-step reasoning chain, potentially leading to
ineffectiveness and inefficiency in policy learning. In this paper, we
introduce StepAgent, which utilizes step-wise reward to optimize the agent's
reinforcement learning process. Inheriting the spirit of novice-to-expert
theory, we first compare the actions of the expert and the agent to
automatically generate intermediate rewards for fine-grained optimization.
Additionally, we propose implicit-reward and inverse reinforcement learning
techniques to facilitate agent reflection and policy adjustment. Further
theoretical analysis demonstrates that the action distribution of the agent can
converge toward the expert action distribution over multiple training cycles.
Experimental results across various datasets indicate that StepAgent
outperforms existing baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengxiang Wang, Ranjie Duan, Peng Xiao, Xiaojun Jia, YueFeng Chen, Chongwen Wang, Jialing Tao, Hang Su, <span class="highlight-author">Jun Zhu</span>, Hui Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate outstanding performance in their
reservoir of knowledge and understanding capabilities, but they have also been
shown to be prone to illegal or unethical reactions when subjected to jailbreak
attacks. To ensure their responsible deployment in critical applications, it is
crucial to understand the safety capabilities and vulnerabilities of LLMs.
Previous works mainly focus on jailbreak in single-round dialogue, overlooking
the potential jailbreak risks in multi-round dialogues, which are a vital way
humans interact with and extract information from LLMs. Some studies have
increasingly concentrated on the risks associated with jailbreak in multi-round
dialogues. These efforts typically involve the use of manually crafted
templates or prompt engineering techniques. However, due to the inherent
complexity of multi-round dialogues, their jailbreak performance is limited. To
solve this problem, we propose a novel multi-round dialogue jailbreaking agent,
emphasizing the importance of stealthiness in identifying and mitigating
potential threats to human values posed by LLMs. We propose a risk
decomposition strategy that distributes risks across multiple rounds of queries
and utilizes psychological strategies to enhance attack strength. Extensive
experiments show that our proposed method surpasses other attack methods and
achieves state-of-the-art attack success rate. We will make the corresponding
code and dataset available for future research. The code will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The natural stability of autonomous morphology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erich Round, Louise Esher, Sacha Beniamine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous morphology, such as inflection class systems and paradigmatic
distribution patterns, is widespread and diachronically resilient in natural
language. Why this should be so has remained unclear given that autonomous
morphology imposes learning costs, offers no clear benefit relative to its
absence and could easily be removed by the analogical forces which are
constantly reshaping it. Here we propose an explanation for the resilience of
autonomous morphology, in terms of a diachronic dynamic of attraction and
repulsion between morphomic categories, which emerges spontaneously from a
simple paradigm cell filling process. Employing computational evolutionary
models, our key innovation is to bring to light the role of `dissociative
evidence', i.e., evidence for inflectional distinctiveness which a rational
reasoner will have access to during analogical inference. Dissociative evidence
creates a repulsion dynamic which prevents morphomic classes from collapsing
together entirely, i.e., undergoing complete levelling. As we probe alternative
models, we reveal the limits of conditional entropy as a measure for
predictability in systems that are undergoing change. Finally, we demonstrate
that autonomous morphology, far from being `unnatural' (e.g.
\citealt{Aronoff1994}), is rather the natural (emergent) consequence of a
natural (rational) process of inference applied to inflectional systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication by the journal Morphology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Effects of Human-written Paraphrases in LLM-generated
  Text Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiu Ting Lau, Arkaitz Zubiaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Generation has been rapidly developing with the advent of
large language models (LLMs). While their usage has sparked significant
attention from the general public, it is important for readers to be aware when
a piece of text is LLM-generated. This has brought about the need for building
models that enable automated LLM-generated text detection, with the aim of
mitigating potential negative outcomes of such content. Existing LLM-generated
detectors show competitive performances in telling apart LLM-generated and
human-written text, but this performance is likely to deteriorate when
paraphrased texts are considered. In this study, we devise a new data
collection strategy to collect Human & LLM Paraphrase Collection (HLPC), a
first-of-its-kind dataset that incorporates human-written texts and
paraphrases, as well as LLM-generated texts and paraphrases. With the aim of
understanding the effects of human-written paraphrases on the performance of
state-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark
detectors, we perform classification experiments that incorporate human-written
paraphrases, watermarked and non-watermarked LLM-generated documents from GPT
and OPT, and LLM-generated paraphrases from DIPPER and BART. The results show
that the inclusion of human-written paraphrases has a significant impact of
LLM-generated detector performance, promoting TPR@1%FPR with a possible
trade-off of AUROC and accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Recent Large Language Models on Generating
  Hospital Discharge Summaries for Lung Cancer Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Li, Fang Li, Kirk Roberts, Licong Cui, Cui Tao, Hua Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating discharge summaries is a crucial yet time-consuming task in
clinical practice, essential for conveying pertinent patient information and
facilitating continuity of care. Recent advancements in large language models
(LLMs) have significantly enhanced their capability in understanding and
summarizing complex medical texts. This research aims to explore how LLMs can
alleviate the burden of manual summarization, streamline workflow efficiencies,
and support informed decision-making in healthcare settings. Clinical notes
from a cohort of 1,099 lung cancer patients were utilized, with a subset of 50
patients for testing purposes, and 102 patients used for model fine-tuning.
This study evaluates the performance of multiple LLMs, including GPT-3.5,
GPT-4, GPT-4o, and LLaMA 3 8b, in generating discharge summaries. Evaluation
metrics included token-level analysis (BLEU, ROUGE-1, ROUGE-2, ROUGE-L) and
semantic similarity scores between model-generated summaries and
physician-written gold standards. LLaMA 3 8b was further tested on clinical
notes of varying lengths to examine the stability of its performance. The study
found notable variations in summarization capabilities among LLMs. GPT-4o and
fine-tuned LLaMA 3 demonstrated superior token-level evaluation metrics, while
LLaMA 3 consistently produced concise summaries across different input lengths.
Semantic similarity scores indicated GPT-4o and LLaMA 3 as leading models in
capturing clinical relevance. This study contributes insights into the efficacy
of LLMs for generating discharge summaries, highlighting LLaMA 3's robust
performance in maintaining clarity and relevance across varying clinical
contexts. These findings underscore the potential of automated summarization
tools to enhance documentation precision and efficiency, ultimately improving
patient care and operational capability in healthcare settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with
  Captions in 28 Languages <span class="chip">EMNLP 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Mohamed, Runjia Li, Ibrahim Said Ahmad, Kilichbek Haydarov, Philip Torr, Kenneth Ward Church, Mohamed Elhoseiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in vision and language has made considerable progress thanks to
benchmarks such as COCO. COCO captions focused on unambiguous facts in English;
ArtEmis introduced subjective emotions and ArtELingo introduced some
multilinguality (Chinese and Arabic). However we believe there should be more
multilinguality. Hence, we present ArtELingo-28, a vision-language benchmark
that spans $\textbf{28}$ languages and encompasses approximately
$\textbf{200,000}$ annotations ($\textbf{140}$ annotations per image).
Traditionally, vision research focused on unambiguous class labels, whereas
ArtELingo-28 emphasizes diversity of opinions over languages and cultures. The
challenge is to build machine learning systems that assign emotional captions
to images. Baseline results will be presented for three novel conditions:
Zero-Shot, Few-Shot and One-vs-All Zero-Shot. We find that cross-lingual
transfer is more successful for culturally-related languages. Data and code are
provided at www.artelingo.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Accepted at EMNLP 24, for more details see www.artelingo.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Number Cookbook: Number Understanding of Language Models and How to
  Improve It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotong Yang, Yi Hu, Shijia Kang, Zhouchen Lin, Muhan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can solve an increasing number of complex
reasoning tasks while making surprising mistakes in basic numerical
understanding and processing (such as 9.11 > 9.9). The latter ability is
essential for tackling complex arithmetic and mathematical problems and serves
as a foundation for most reasoning tasks, but previous work paid little
attention to it or only discussed several restricted tasks (like integer
addition). In this paper, we comprehensively investigate the numerical
understanding and processing ability (NUPA) of LLMs. Firstly, we introduce a
benchmark covering four common numerical representations and 17 distinct
numerical tasks in four major categories, resulting in 41 meaningful
combinations in total. These tasks are derived from primary and secondary
education curricula, encompassing nearly all everyday numerical understanding
and processing scenarios, and the rules of these tasks are very simple and
clear. Through the benchmark, we find that current LLMs fail frequently in many
of the tasks. To study the problem, we train small models with existing and
potential techniques for enhancing NUPA (such as special tokenizers, PEs, and
number formats), comprehensively evaluating their effectiveness using our
testbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and
find that 1) naive finetuning can improve NUPA a lot on many but not all tasks,
and 2) surprisingly, techniques designed to enhance NUPA prove ineffective for
finetuning pretrained models. We further explore the impact of chain-of-thought
techniques on NUPA. Our work takes a preliminary step towards understanding and
improving NUPA of LLMs. Our benchmark and code are released at
https://github.com/GraphPKU/number_cookbook.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms
  in Aligned Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anaelia Ovalle, Krunoslav Lehman Pavasovic, Louis Martin, Luke Zettlemoyer, Eric Michael Smith, Adina Williams, Levent Sagun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural-language assistants are designed to provide users with helpful
responses while avoiding harmful outputs, largely achieved through alignment to
human preferences. Yet there is limited understanding of whether alignment
techniques may inadvertently perpetuate or even amplify harmful biases
inherited from their pre-aligned base models. This issue is compounded by the
choice of bias evaluation benchmarks in popular preference-finetuned models,
which predominantly focus on dominant social categories, such as binary gender,
thereby limiting insights into biases affecting underrepresented groups.
Towards addressing this gap, we center transgender, nonbinary, and other
gender-diverse identities to investigate how alignment procedures interact with
pre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a
comprehensive survey of bias evaluation modalities across leading
preference-finetuned LLMs, highlighting critical gaps in gender-diverse
representation, 2) systematic evaluation of gender-diverse biases across 12
models spanning Direct Preference Optimization (DPO) stages, uncovering harms
popular bias benchmarks fail to detect, and 3) a flexible framework for
measuring harmful biases in implicit reward signals applicable to other social
contexts. Our findings reveal that DPO-aligned models are particularly
sensitive to supervised finetuning (SFT), and can amplify two forms of
real-world gender-diverse harms from their base models: stigmatization and
gender non-affirmative language. We conclude with recommendations tailored to
DPO and broader alignment practices, advocating for the adoption of
community-informed bias evaluation frameworks to more effectively identify and
address underrepresented harms in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2024 Neurips Queer in AI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QUILL: Quotation Generation Enhancement of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Xiao, Bowei Zhang, Qianyu He, Jiaqing Liang, Feng Wei, Jinglei Chen, Zujie Liang, Deqing Yang, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large language models (LLMs) have become excellent writing assistants,
they still struggle with quotation generation. This is because they either
hallucinate when providing factual quotations or fail to provide quotes that
exceed human expectations. To bridge the gap, we systematically study how to
evaluate and improve LLMs' performance in quotation generation tasks. We first
establish a holistic and automatic evaluation system for quotation generation
task, which consists of five criteria each with corresponding automatic metric.
To improve the LLMs' quotation generation abilities, we construct a bilingual
knowledge base that is broad in scope and rich in dimensions, containing up to
32,022 quotes. Moreover, guided by our critiria, we further design a
quotation-specific metric to rerank the retrieved quotations from the knowledge
base. Extensive experiments show that our metrics strongly correlate with human
preferences. Existing LLMs struggle to generate desired quotes, but our
quotation knowledge base and reranking metric help narrow this gap. Our dataset
and code are publicly available at https://github.com/GraceXiaoo/QUILL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Moral Beliefs across LLMs through a Pluralistic Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuelin Liu, Yanfei Zhu, Shucheng Zhu, Pengyuan Liu, Ying Liu, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proper moral beliefs are fundamental for language models, yet assessing these
beliefs poses a significant challenge. This study introduces a novel
three-module framework to evaluate the moral beliefs of four prominent large
language models. Initially, we constructed a dataset containing 472 moral
choice scenarios in Chinese, derived from moral words. The decision-making
process of the models in these scenarios reveals their moral principle
preferences. By ranking these moral choices, we discern the varying moral
beliefs held by different language models. Additionally, through moral debates,
we investigate the firmness of these models to their moral choices. Our
findings indicate that English language models, namely ChatGPT and Gemini,
closely mirror moral decisions of the sample of Chinese university students,
demonstrating strong adherence to their choices and a preference for
individualistic moral beliefs. In contrast, Chinese models such as Ernie and
ChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their
moral choices and debates. This study also uncovers gender bias embedded within
the moral beliefs of all examined language models. Our methodology offers an
innovative means to assess moral beliefs in both artificial and human
intelligence, facilitating a comparison of moral values across different
cultures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deploying Multi-task Online Server with Large Language Model <span class="chip">COLING2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yincen Qu, Chao Ma, Yiting Wu, Xiangying Dai, Hui Zhou, Hengyue Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the industry, numerous tasks are deployed online. Traditional approaches
often tackle each task separately by its own network, which leads to excessive
costs for developing and scaling models, especially in the context of large
language models. Although multi-task methods can save costs through parameter
sharing, they often struggle to outperform single-task methods in real-world
applications. To tackle these challenges, we present a three-stage multi-task
learning framework for large language models. It involves task filtering,
followed by fine-tuning on high-resource tasks, and finally fine-tuning on all
tasks. We conducted comprehensive experiments in single-task and multi-task
settings. Our approach, exemplified on different benchmarks, demonstrates that
it is able to achieve performance comparable to the single-task method while
reducing up to 90.9\% of its overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING2025 under submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Medprompt to o1: Exp<span class="highlight-title">lora</span>tion of Run-Time Strategies for Medical
  Challenge Problems and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harsha Nori, Naoto Usuyama, Nicholas King, Scott Mayer McKinney, Xavier Fernandes, Sheng Zhang, Eric Horvitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Run-time steering strategies like Medprompt are valuable for guiding large
language models (LLMs) to top performance on challenging tasks. Medprompt
demonstrates that a general LLM can be focused to deliver state-of-the-art
performance on specialized domains like medicine by using a prompt to elicit a
run-time strategy involving chain of thought reasoning and ensembling. OpenAI's
o1-preview model represents a new paradigm, where a model is designed to do
run-time reasoning before generating final responses. We seek to understand the
behavior of o1-preview on a diverse set of medical challenge problem
benchmarks. Following on the Medprompt study with GPT-4, we systematically
evaluate the o1-preview model across various medical benchmarks. Notably, even
without prompting techniques, o1-preview largely outperforms the GPT-4 series
with Medprompt. We further systematically study the efficacy of classic prompt
engineering strategies, as represented by Medprompt, within the new paradigm of
reasoning models. We found that few-shot prompting hinders o1's performance,
suggesting that in-context learning may no longer be an effective steering
approach for reasoning-native models. While ensembling remains viable, it is
resource-intensive and requires careful cost-performance optimization. Our cost
and accuracy analysis across run-time strategies reveals a Pareto frontier,
with GPT-4o representing a more affordable option and o1-preview achieving
state-of-the-art performance at higher cost. Although o1-preview offers top
performance, GPT-4o with steering strategies like Medprompt retains value in
specific contexts. Moreover, we note that the o1-preview model has reached
near-saturation on many existing medical benchmarks, underscoring the need for
new, challenging benchmarks. We close with reflections on general directions
for inference-time computation with LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The American Sign Language Knowledge Graph: Infusing ASL Models with
  Linguistic Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lee Kezar, Nidhi Munikote, Zian Zeng, Zed Sehyr, Naomi Caselli, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models for American Sign Language (ASL) could make language
technologies substantially more accessible to those who sign. To train models
on tasks such as isolated sign recognition (ISR) and ASL-to-English
translation, datasets provide annotated video examples of ASL signs. To
facilitate the generalizability and explainability of these models, we
introduce the American Sign Language Knowledge Graph (ASLKG), compiled from
twelve sources of expert linguistic knowledge. We use the ASLKG to train
neuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of
91% on ISR, 14% for predicting the semantic features of unseen signs, and 36%
for classifying the topic of Youtube-ASL videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multilingual Sentiment Lexicon for Low-Resource Language Translation
  using Large Languages Models and Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melusi Malinga, Isaac Lupanda, Mike Wa Nkongolo, Phil van Deventer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  South Africa and the Democratic Republic of Congo (DRC) present a complex
linguistic landscape with languages such as Zulu, Sepedi, Afrikaans, French,
English, and Tshiluba (Ciluba), which creates unique challenges for AI-driven
translation and sentiment analysis systems due to a lack of accurately labeled
data. This study seeks to address these challenges by developing a multilingual
lexicon designed for French and Tshiluba, now expanded to include translations
in English, Afrikaans, Sepedi, and Zulu. The lexicon enhances cultural
relevance in sentiment classification by integrating language-specific
sentiment scores. A comprehensive testing corpus is created to support
translation and sentiment analysis tasks, with machine learning models such as
Random Forest, Support Vector Machine (SVM), Decision Trees, and Gaussian Naive
Bayes (GNB) trained to predict sentiment across low resource languages (LRLs).
Among them, the Random Forest model performed particularly well, capturing
sentiment polarity and handling language-specific nuances effectively.
Furthermore, Bidirectional Encoder Representations from Transformers (BERT), a
Large Language Model (LLM), is applied to predict context-based sentiment with
high accuracy, achieving 99% accuracy and 98% precision, outperforming other
models. The BERT predictions were clarified using Explainable AI (XAI),
improving transparency and fostering confidence in sentiment classification.
Overall, findings demonstrate that the proposed lexicon and machine learning
models significantly enhance translation and sentiment analysis for LRLs in
South Africa and the DRC, laying a foundation for future AI models that support
underrepresented languages, with applications across education, governance, and
business in multilingual contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is part of a PhD proposal in Information Technology at the
  University of Pretoria, supervised by Dr. Mike Wa Nkongolo and co-supervised
  by Dr. Phil van Deventer, under the Low-Resource Language Processing Lab in
  the Department of Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Bilingual Capabilities of Language Models to Support Diverse
  Linguistic Practices in Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anand Syamkumar, Nora Tseng, Kaycie Barron, Shanglin Yang, Shamya Karumbaiah, Rheeya Uppal, Junjie Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) offer promise in generating educational content,
providing instructor feedback, and reducing teacher workload on assessments.
While prior studies have focused on studying LLM-powered learning analytics,
limited research has examined how effective LLMs are in a bilingual context. In
this paper, we study the effectiveness of multilingual large language models
(MLLMs) across monolingual (English-only, Spanish-only) and bilingual
(Spanglish) student writing. We present a learning analytics use case that
details LLM performance in assessing acceptable and unacceptable explanations
of Science and Social Science concepts. Our findings reveal a significant bias
in the grading performance of pre-trained models for bilingual writing compared
to English-only and Spanish-only writing. Following this, we fine-tune
open-source MLLMs including Llama 3.1 and Mistral NeMo using synthetic datasets
generated in English, Spanish, and Spanglish. Our experiments indicate that the
models perform significantly better for all three languages after fine-tuning
with bilingual data. This study highlights the potential of enhancing MLLM
effectiveness to support authentic language practices amongst bilingual
learners. It also aims to illustrate the value of incorporating non-English
languages into the design and implementation of language models in education.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Capabilities Approach to Studying Bias and Harm in Language
  Technologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hellina Hailu Nigatu, Zeerak Talat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mainstream Natural Language Processing (NLP) research has ignored the
majority of the world's languages. In moving from excluding the majority of the
world's languages to blindly adopting what we make for English, we first risk
importing the same harms we have at best mitigated and at least measured for
English. However, in evaluating and mitigating harms arising from adopting new
technologies into such contexts, we often disregard (1) the actual community
needs of Language Technologies, and (2) biases and fairness issues within the
context of the communities. In this extended abstract, we consider fairness,
bias, and inclusion in Language Technologies through the lens of the
Capabilities Approach. The Capabilities Approach centers on what people are
capable of achieving, given their intersectional social, political, and
economic contexts instead of what resources are (theoretically) available to
them. We detail the Capabilities Approach, its relationship to multilingual and
multicultural evaluation, and how the framework affords meaningful
collaboration with community members in defining and measuring the harms of
Language Technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the New Perspectives on Bias and Discrimination in
  Language Technology workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unfair Alignment: Examining Safety Alignment Across Vision Encoder
  Layers in Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saketh Bachu, Erfan Shayegani, Trishna Chakraborty, Rohit Lal, Arindam Dutta, Chengyu Song, Yue Dong, Nael Abu-Ghazaleh, Amit K. Roy-Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have improved significantly in multi-modal
tasks, but their more complex architecture makes their safety alignment more
challenging than the alignment of large language models (LLMs). In this paper,
we reveal an unfair distribution of safety across the layers of VLM's vision
encoder, with earlier and middle layers being disproportionately vulnerable to
malicious inputs compared to the more robust final layers. This 'cross-layer'
vulnerability stems from the model's inability to generalize its safety
training from the default architectural settings used during training to unseen
or out-of-distribution scenarios, leaving certain layers exposed. We conduct a
comprehensive analysis by projecting activations from various intermediate
layers and demonstrate that these layers are more likely to generate harmful
outputs when exposed to malicious inputs. Our experiments with LLaVA-1.5 and
Llama 3.2 show discrepancies in attack success rates and toxicity scores across
layers, indicating that current safety alignment strategies focused on a single
default layer are insufficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models are Hidden Reasoners: Unlocking Latent Reasoning
  Capabilities via Self-Rewarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, Huan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive capabilities, but still
struggle with complex reasoning tasks requiring multiple steps. While
prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at
inference time, optimizing reasoning capabilities during training remains
challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled
framework that formulates reasoning as sampling from a latent distribution and
optimizes it via variational approaches. LaTRO enables LLMs to concurrently
improve both their reasoning process and ability to evaluate reasoning quality,
without requiring external feedback or reward models. We validate LaTRO through
experiments on GSM8K and ARC-Challenge datasets using multiple model
architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of
12.5% over base models and 9.6% over supervised fine-tuning across
Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that
pre-trained LLMs possess latent reasoning capabilities that can be unlocked and
enhanced through our proposed optimization approach in a self-improvement
manner. The code of LaTRO is available at
\url{https://github.com/SalesforceAIResearch/LaTRO}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diversity Helps Jailbreak Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiliang Zhao, Daniel Ben-Levi, Junfeng Yang, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We have uncovered a powerful jailbreak technique that leverages large
language models' ability to diverge from prior context, enabling them to bypass
safety constraints and generate harmful outputs. By simply instructing the LLM
to deviate and obfuscate previous attacks, our method dramatically outperforms
existing approaches, achieving up to a 62% higher success rate in compromising
nine leading chatbots, including GPT-4, Gemini, and Llama, while using only 13%
of the queries. This revelation exposes a critical flaw in current LLM safety
training, suggesting that existing methods may merely mask vulnerabilities
rather than eliminate them. Our findings sound an urgent alarm for the need to
revolutionize testing methodologies to ensure robust and reliable LLM security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.02119</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Multimodal Features of Spontaneous Voice Assistant Commands
  for Mild Cognitive Impairment Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nana Lin, Youxiang Zhu, Xiaohui Liang, John A. Batsis, Caroline Summerour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mild cognitive impairment (MCI) is a major public health concern due to its
high risk of progressing to dementia. This study investigates the potential of
detecting MCI with spontaneous voice assistant (VA) commands from 35 older
adults in a controlled setting. Specifically, a command-generation task is
designed with pre-defined intents for participants to freely generate commands
that are more associated with cognitive ability than read commands. We develop
MCI classification and regression models with audio, textual, intent, and
multimodal fusion features. We find the command-generation task outperforms the
command-reading task with an average classification accuracy of 82%, achieved
by leveraging multimodal fusion features. In addition, generated commands
correlate more strongly with memory and attention subdomains than read
commands. Our results confirm the effectiveness of the command-generation task
and imply the promise of using longitudinal in-home commands for MCI detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM
  Data Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting dataset contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is sensitive to varying degrees of contamination and can
highlight significant performance improvements due to leakage of the training
set of multimodal benchmarks. Furthermore, We also explore the possibility of
contamination originating from the pre-training phase of LLMs used by MLLMs and
the fine-tuning phase of MLLMs, offering new insights into the stages at which
contamination may be introduced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crystal: Illuminating LLM Abilities on Language and Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhua Tao, Junbo Li, Bowen Tan, Hongyi Wang, William Marshall, Bhargav M Kanakiya, Joel Hestness, Natalia Vassilieva, Zhiqiang Shen, Eric P. Xing, Zhengzhong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) specializing in code generation (which are also
often referred to as code LLMs), e.g., StarCoder and Code Llama, play
increasingly critical roles in various software development scenarios. It is
also crucial for code LLMs to possess both code generation and natural language
abilities for many specific applications, such as code snippet retrieval using
natural language or code explanations. The intricate interaction between
acquiring language and coding skills complicates the development of strong code
LLMs. Furthermore, there is a lack of thorough prior studies on the LLM
pretraining strategy that mixes code and natural language. In this work, we
propose a pretraining strategy to enhance the integration of natural language
and coding capabilities within a single LLM. Specifically, it includes two
phases of training with appropriately adjusted code/language ratios. The
resulting model, Crystal, demonstrates remarkable capabilities in both domains.
Specifically, it has natural language and coding performance comparable to that
of Llama 2 and Code Llama, respectively. Crystal exhibits better data
efficiency, using 1.4 trillion tokens compared to the more than 2 trillion
tokens used by Llama 2 and Code Llama. We verify our pretraining strategy by
analyzing the training process and observe consistent improvements in most
benchmarks. We also adopted a typical application adaptation phase with a
code-centric data mixture, only to find that it did not lead to enhanced
performance or training efficiency, underlining the importance of a carefully
designed data recipe. To foster research within the community, we commit to
open-sourcing every detail of the pretraining, including our training datasets,
code, loggings and 136 checkpoints throughout the training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen
  Representations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walter Simoncini, Spyros Gidaris, Andrei Bursuc, Yuki M. Asano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces FUNGI, Features from UNsupervised GradIents, a method
to enhance the features of transformer encoders by leveraging self-supervised
gradients. Our method is simple: given any pretrained model, we first compute
gradients from various self-supervised objectives for each input. These
gradients are projected to a lower dimension and then concatenated with the
model's output embedding. The resulting features are evaluated on k-nearest
neighbor classification over 11 datasets from vision, 5 from natural language
processing, and 2 from audio. Across backbones spanning various sizes and
pretraining strategies, FUNGI features provide consistent performance
improvements over the embeddings. We also show that using FUNGI features can
benefit linear classification, clustering and image retrieval, and that they
significantly improve the retrieval-based in-context scene understanding
abilities of pretrained models, for example improving upon DINO by +17% for
semantic segmentation - without any training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code available at
  https://github.com/WalterSimoncini/fungivision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hasan Iqbal, Yuxia Wang, Minghan Wang, Georgi Georgiev, Jiahui Geng, Iryna Gurevych, Preslav Nakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increased use of large language models (LLMs) across a variety of
real-world applications calls for automatic tools to check the factual accuracy
of their outputs, as LLMs often hallucinate. This is difficult as it requires
assessing the factuality of free-form open-domain responses. While there has
been a lot of research on this topic, different papers use different evaluation
benchmarks and measures, which makes them hard to compare and hampers future
progress. To mitigate these issues, we developed OpenFactCheck, a unified
framework, with three modules: (i) RESPONSEEVAL, which allows users to easily
customize an automatic fact-checking system and to assess the factuality of all
claims in an input document using that system, (ii) LLMEVAL, which assesses the
overall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate
automatic fact-checking systems. OpenFactCheck is open-sourced
(https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python
library (https://pypi.org/project/openfactcheck/) and also as a web service
(http://app.openfactcheck.com). A video describing the system is available at
https://youtu.be/-i9VKL0HleI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 Figures, 3 Tables, Accepted at EMNLP 2024 System
  Demonstration. arXiv admin note: substantial text overlap with
  arXiv:2405.05583</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Token Generation in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivi Chatzi, Nina Corvelo Benz, Eleni Straitouri, Stratis Tsirtsis, Manuel Gomez-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  "Sure, I am happy to generate a story for you: Captain Lyra stood at the helm
of her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]
Lyra's eyes welled up with tears as she realized the bitter truth - she had
sacrificed everything for fleeting riches, and lost the love of her crew, her
family, and herself." Although this story, generated by a large language model,
is captivating, one may wonder -- how would the story have unfolded if the
model had chosen "Captain Maeve" as the protagonist instead? We cannot know.
State-of-the-art large language models are stateless -- they maintain no
internal memory or state. Given a prompt, they generate a sequence of tokens as
an output using an autoregressive process. As a consequence, they cannot reason
about counterfactual alternatives to tokens they have generated in the past. In
this work, our goal is to enhance them with this functionality. To this end, we
develop a causal model of token generation that builds upon the Gumbel-Max
structural causal model. Our model allows any large language model to perform
counterfactual token generation at almost no cost in comparison with vanilla
token generation, it is embarrassingly simple to implement, and it does not
require any fine-tuning nor prompt engineering. We implement our model on Llama
3 8B-Instruct and Ministral-8B-Instruct and conduct a qualitative and a
quantitative analysis of counterfactually generated text. We conclude with a
demonstrative application of counterfactual token generation for bias
detection, unveiling interesting insights about the model of the world
constructed by large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teaching Models to Improve on Tape 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01483v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01483v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liat Bezalel, Eyal Orgad, Amir Globerson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often struggle when prompted to generate content
under specific constraints. However, in such cases it is often easy to check
whether these constraints are satisfied or violated. Recent works have shown
that LLMs can benefit from such "corrective feedback". Here we claim that this
skill of LLMs can be significantly enhanced via training. We introduce an RL
framework for teaching models to use such rewards, by simulating interaction
sessions, and rewarding the model according to its ability to satisfy the
constraints. We refer to our method as CORGI (Controlled Generation with RL for
Guided Interaction), and evaluate it on a variety of controlled generation
tasks using unlabeled training data. We find that CORGI consistently
outperforms the baseline reinforcement learning method that does not
incorporate conversational feedback. Furthermore, CORGI's interactive framework
enables meta-learning, allowing the LLM to generalize better to guided
interaction in new tasks. Our results clearly show that conversational
optimization, when combined with reinforcement learning, significantly improves
the effectiveness of LLMs in controlled generation contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diverging Preferences: When do Annotators Disagree and do Models Know? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael JQ Zhang, Zhilin Wang, Jena D. Hwang, Yi Dong, Olivier Delalleau, Yejin Choi, Eunsol Choi, Xiang Ren, Valentina Pyatkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine diverging preferences in human-labeled preference datasets. We
develop a taxonomy of disagreement sources spanning 10 categories across four
high-level classes -- task underspecification, response style, refusals, and
annotation errors. We find that the majority of disagreements are in opposition
with standard reward modeling approaches, which are designed with the
assumption that annotator disagreement is noise. We then explore how these
findings impact two areas of LLM development: reward modeling and evaluation.
In our experiments, we demonstrate how standard reward modeling methods, like
the Bradley-Terry model, fail to differentiate whether a given preference
judgment is the result of unanimous agreement among annotators or the majority
opinion among diverging user preferences. We also find that these tendencies
are also echoed by popular LLM-as-Judge evaluation methods, which consistently
identify a winning response in cases of diverging preferences. These findings
highlight remaining challenges in LLM evaluations, which are greatly influenced
by divisive features like response style, and in developing pluralistically
aligned LLMs. To address these issues, we develop methods for identifying
diverging preferences to mitigate their influence on evaluation and training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pretraining and Updates of Domain-Specific LLM: A Case Study in the
  Japanese Business Domain <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Takahashi, Takahiro Omi, Kosuke Arima, Tatsuya Ishigaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Large Language Models (LLMs) in various languages has been
advancing, but the combination of non-English languages with domain-specific
contexts remains underexplored. This paper presents our findings from training
and evaluating a Japanese business domain-specific LLM designed to better
understand business-related documents, such as the news on current affairs,
technical reports, and patents. Additionally, LLMs in this domain require
regular updates to incorporate the most recent knowledge. Therefore, we also
report our findings from the first experiments and evaluations involving
updates to this LLM using the latest article data, which is an important
problem setting that has not been addressed in previous research. From our
experiments on a newly created benchmark dataset for question answering in the
target domain, we found that (1) our pretrained model improves QA accuracy
without losing general knowledge, and (2) a proper mixture of the latest and
older texts in the training data for the update is necessary. Our pretrained
model and business domain benchmark are publicly available to support further
studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at PACLIC 38</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ News Reporter: A Multi-lingual LLM Framework for Broadcast T.V News <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarun Jain, Yufei Gao, Sridhar Vanga, Karan Singla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have fast become an essential tools to many
conversational chatbots due to their ability to provide coherent answers for
varied queries. Datasets used to train these LLMs are often a mix of generic
and synthetic samples, thus lacking the verification needed to provide correct
and verifiable answers for T.V. News.
  We collect and share a large collection of QA pairs extracted from
transcripts of news recordings from various news-channels across the United
States. Resultant QA pairs are then used to fine-tune an off-the-shelf LLM
model. Our model surpasses base models of similar size on several open LLM
benchmarks. We further integrate and propose a RAG method to improve
contextualization of our answers and also point it to a verifiable news
recording.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, under review at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Context-Aware Preference Modeling for Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silviu Pitis, Ziang Xiao, Nicolas Le Roux, Alessandro Sordoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While finetuning language models from pairwise preferences has proven
remarkably effective, the underspecified nature of natural language presents
critical challenges. Direct preference feedback is uninterpretable, difficult
to provide where multidimensional criteria may apply, and often inconsistent,
either because it is based on incomplete instructions or provided by diverse
principals. To address these challenges, we consider the two-step preference
modeling procedure that first resolves the under-specification by selecting a
context, and then evaluates preference with respect to the chosen context. We
decompose reward modeling error according to these two steps, which suggests
that supervising context in addition to context-specific preference may be a
viable approach to aligning models with diverse human preferences. For this to
work, the ability of models to evaluate context-specific preference is
critical. To this end, we contribute context-conditioned preference datasets
and accompanying experiments that investigate the ability of language models to
evaluate context-specific preference. We use our datasets to (1) show that
existing preference models benefit from, but fail to fully consider, added
context, (2) finetune a context-aware reward model with context-specific
performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3)
investigate the value of context-aware preference modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. 10 pages (29 with references and appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ZeMing Gong, Austin T. Wang, Xiaoliang Huo, Joakim Bruslund Haurum, Scott C. Lowe, Graham W. Taylor, Angel X. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring biodiversity is crucial for understanding ecosystem health. While
prior works have developed machine learning models for taxonomic classification
of photographic images and DNA separately, in this work, we introduce a
multimodal approach combining both, using CLIP-style contrastive learning to
align images, barcode DNA, and text-based representations of taxonomic labels
in a unified embedding space. This allows for accurate classification of both
known and unknown insect species without task-specific fine-tuning, leveraging
contrastive learning for the first time to fuse DNA and image data. Our method
surpasses previous single-modality approaches in accuracy by over 8% on
zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages with 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Causal Reasoning in Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16676v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16676v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longxuan Yu, Delin Chen, Siheng Xiong, Qingyang Wu, Qingzhen Liu, Dawei Li, Zhikai Chen, Xiaoze Liu, Liangming Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal reasoning (CR) is a crucial aspect of intelligence, essential for
problem-solving, decision-making, and understanding the world. While large
language models (LLMs) can generate rationales for their outputs, their ability
to reliably perform causal reasoning remains uncertain, often falling short in
tasks requiring a deep understanding of causality. In this survey, we provide a
comprehensive review of research aimed at enhancing LLMs for causal reasoning.
We categorize existing methods based on the role of LLMs: either as reasoning
engines or as helpers providing knowledge or data to traditional CR methods,
followed by a detailed discussion of the methodologies in each category. We
then evaluate the performance of LLMs on various causal reasoning tasks,
providing key findings and in-depth analysis. Finally, we provide insights from
current studies and highlight promising directions for future research. We aim
for this work to serve as a comprehensive resource, fostering further
advancements in causal reasoning with LLMs. Resources are available at
https://github.com/chendl02/Awesome-LLM-causal-reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BABILong: Testing the Limits of LLMs with Long Context
  Reasoning-in-a-Haystack <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10149v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10149v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, Mikhail Burtsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the input context sizes of large language models (LLMs) have
increased dramatically. However, existing evaluation methods have not kept
pace, failing to comprehensively assess the efficiency of models in handling
long contexts. To bridge this gap, we introduce the BABILong benchmark,
designed to test language models' ability to reason across facts distributed in
extremely long documents. BABILong includes a diverse set of 20 reasoning
tasks, including fact chaining, simple induction, deduction, counting, and
handling lists/sets. These tasks are challenging on their own, and even more
demanding when the required facts are scattered across long natural text. Our
evaluations show that popular LLMs effectively utilize only 10-20\% of the
context and their performance declines sharply with increased reasoning
complexity. Among alternatives to in-context reasoning, Retrieval-Augmented
Generation methods achieve a modest 60\% accuracy on single-fact question
answering, independent of context length. Among context extension methods, the
highest performance is demonstrated by recurrent memory transformers after
fine-tuning, enabling the processing of lengths up to 50 million tokens. The
BABILong benchmark is extendable to any length to support the evaluation of new
upcoming models with increased capabilities, and we provide splits up to 10
million token lengths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Morphological Compositional Generalization in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, Lonneke van der Plas, Duygu Ataman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant progress in
various natural language generation and understanding tasks. However, their
linguistic generalization capabilities remain questionable, raising doubts
about whether these models learn language similarly to humans. While humans
exhibit compositional generalization and linguistic creativity in language use,
the extent to which LLMs replicate these abilities, particularly in morphology,
is under-explored. In this work, we systematically investigate the
morphological generalization abilities of LLMs through the lens of
compositionality. We define morphemes as compositional primitives and design a
novel suite of generative and discriminative tasks to assess morphological
productivity and systematicity. Focusing on agglutinative languages such as
Turkish and Finnish, we evaluate several state-of-the-art instruction-finetuned
multilingual models, including GPT-4 and Gemini. Our analysis shows that LLMs
struggle with morphological compositional generalization particularly when
applied to novel word roots, with performance declining sharply as
morphological complexity increases. While models can identify individual
morphological combinations better than chance, their performance lacks
systematicity, leading to significant accuracy gaps compared to humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChartInsights: Evaluating Multimodal Large Language Models for Low-Level
  Chart Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07001v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07001v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wu, Lutao Yan, Leixian Shen, Yunhai Wang, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chart question answering (ChartQA) tasks play a critical role in interpreting
and extracting insights from visualization charts. While recent advancements in
multimodal large language models (MLLMs) like GPT-4o have shown promise in
high-level ChartQA tasks, such as chart captioning, their effectiveness in
low-level ChartQA tasks (e.g., identifying correlations) remains underexplored.
In this paper, we address this gap by evaluating MLLMs on low-level ChartQA
using a newly curated dataset, ChartInsights, which consists of 22,347 (chart,
task, query, answer) covering 10 data analysis tasks across 7 chart types. We
systematically evaluate 19 advanced MLLMs, including 12 open-source and 7
closed-source models. The average accuracy rate across these models is 39.8%,
with GPT-4o achieving the highest accuracy at 69.17%. To further explore the
limitations of MLLMs in low-level ChartQA, we conduct experiments that alter
visual elements of charts (e.g., changing color schemes, adding image noise) to
assess their impact on the task effectiveness. Furthermore, we propose a new
textual prompt strategy, Chain-of-Charts, tailored for low-level ChartQA tasks,
which boosts performance by 14.41%, achieving an accuracy of 83.58%. Finally,
incorporating a visual prompt strategy that directs attention to relevant
visual elements further improves accuracy to 84.32%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA
  <span class="highlight-title">Dataset</span> and Self-adaptive Planning Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Pengjun Xie, Philip S. Yu, Fei Huang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Retrieval Augmented Generation (mRAG) plays an important role in
mitigating the "hallucination" issue inherent in multimodal large language
models (MLLMs). Although promising, existing heuristic mRAGs typically
predefined fixed retrieval processes, which causes two issues: (1) Non-adaptive
Retrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws
cannot be adequately reflected by current knowledge-seeking visual question
answering (VQA) datasets, since the most required knowledge can be readily
obtained with a standard two-step retrieval. To bridge the dataset gap, we
first construct Dyn-VQA dataset, consisting of three types of "dynamic"
questions, which require complex knowledge retrieval strategies variable in
query, tool, and time: (1) Questions with rapidly changing answers. (2)
Questions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments
on Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient
and precisely relevant knowledge for dynamic questions due to their rigid
retrieval processes. Hence, we further propose the first self-adaptive planning
agent for multimodal retrieval, OmniSearch. The underlying idea is to emulate
the human behavior in question solution which dynamically decomposes complex
multimodal questions into sub-question chains with retrieval action. Extensive
experiments prove the effectiveness of our OmniSearch, also provide direction
for advancing mRAG. The code and dataset will be open-sourced at
https://github.com/Alibaba-NLP/OmniSearch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ElectionSim: Massive Population Election Simulation Powered by Large
  Language Model Driven Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20746v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20746v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinnong Zhang, Jiayu Lin, Libo Sun, Weihong Qi, Yihang Yang, Yue Chen, Hanjia Lyu, Xinyi Mou, Siming Chen, Jiebo Luo, Xuanjing Huang, Shiping Tang, Zhongyu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The massive population election simulation aims to model the preferences of
specific groups in particular election scenarios. It has garnered significant
attention for its potential to forecast real-world social trends. Traditional
agent-based modeling (ABM) methods are constrained by their ability to
incorporate complex individual background information and provide interactive
prediction results. In this paper, we introduce ElectionSim, an innovative
election simulation framework based on large language models, designed to
support accurate voter simulations and customized distributions, together with
an interactive platform to dialogue with simulated voters. We present a
million-level voter pool sampled from social media platforms to support
accurate individual simulation. We also introduce PPE, a poll-based
presidential election benchmark to assess the performance of our framework
under the U.S. presidential election scenario. Through extensive experiments
and analyses, we demonstrate the effectiveness and robustness of our framework
in U.S. presidential election simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CIBench: Evaluating Your LLMs with a Code Interpreter Plugin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10499v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10499v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyu Zhang, Songyang Zhang, Yingfan Hu, Haowen Shen, Kuikun Liu, Zerun Ma, Fengzhe Zhou, Wenwei Zhang, Xuming He, Dahua Lin, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While LLM-Based agents, which use external tools to solve complex problems,
have made significant progress, benchmarking their ability is challenging,
thereby hindering a clear understanding of their limitations. In this paper, we
propose an interactive evaluation framework, named CIBench, to comprehensively
assess LLMs' ability to utilize code interpreters for data science tasks. Our
evaluation framework includes an evaluation dataset and two evaluation modes.
The evaluation dataset is constructed using an LLM-human cooperative approach
and simulates an authentic workflow by leveraging consecutive and interactive
IPython sessions. The two evaluation modes assess LLMs' ability with and
without human assistance. We conduct extensive experiments to analyze the
ability of 24 LLMs on CIBench and provide valuable insights for future LLMs in
code interpreter utilization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review. The first three authors contribute equally, and
  Songyang Zhang is the project leader</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Fine Line: Navigating Large Language Model Pretraining with
  Down-streaming Capability Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01204v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01204v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yang, Junzhuo Li, Xinyao Niu, Xinrun Du, Songyang Gao, Haoran Zhang, Zhaoliang Chen, Xingwei Qu, Ruibin Yuan, Yizhi Li, Jiaheng Liu, Stephen W. Huang, Shawn Yue, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncovering early-stage metrics that reflect final model performance is one
core principle for large-scale pretraining. The existing scaling law
demonstrates the power-law correlation between pretraining loss and training
flops, which serves as an important indicator of the current training state for
large language models. However, this principle only focuses on the model's
compression properties on the training data, resulting in an inconsistency with
the ability improvements on the downstream tasks. Some follow-up works
attempted to extend the scaling-law to more complex metrics (such as
hyperparameters), but still lacked a comprehensive analysis of the dynamic
differences among various capabilities during pretraining. To address the
aforementioned limitations, this paper undertakes a comprehensive comparison of
model capabilities at various pretraining intermediate checkpoints. Through
this analysis, we confirm that specific downstream metrics exhibit similar
training dynamics across models of different sizes, up to 67 billion
parameters. In addition to our core findings, we've reproduced Amber and
OpenLLaMA, releasing their intermediate checkpoints. This initiative offers
valuable resources to the research community and facilitates the verification
and exploration of LLM pretraining by open-source researchers. Besides, we
provide empirical summaries, including performance comparisons of different
models and capabilities, and tuition of key metrics for different training
phases. Based on these findings, we provide a more user-friendly strategy for
evaluating the optimization state, offering guidance for establishing a stable
pretraining process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based
  Contrastive Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyuan Zhang, Chenming Shang, Sizhe Wang, Dongdong Zhang, Renliang Sun, Yiyao Yu, Yujiu Yang, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although fine-tuning Large Language Models (LLMs) with multilingual data can
rapidly enhance the multilingual capabilities of LLMs, they still exhibit a
performance gap between the dominant language (e.g., English) and non-dominant
ones due to the imbalance of training data across languages. To further enhance
the performance of non-dominant languages, we propose ShifCon, a Shift-based
Contrastive framework that aligns the internal forward process of other
languages toward that of the dominant one. Specifically, it shifts the
representations of non-dominant languages into the dominant language subspace,
allowing them to access relatively rich information encoded in the model
parameters. The enriched representations are then shifted back into their
original language subspace before generation. Moreover, we introduce a subspace
distance metric to pinpoint the optimal layer area for shifting representations
and employ multilingual contrastive learning to further enhance the alignment
of representations within this area. Experiments demonstrate that our ShifCon
framework significantly enhances the performance of non-dominant languages,
particularly for low-resource ones. Further analysis offers extra insights to
verify the effectiveness of ShifCon and propel future research
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PersianRAG: A Retrieval-Augmented Generation System for Persian Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Hosseini, Mohammad Sobhan Zare, Amir Hossein Mohammadi, Arefeh Kazemi, Zahra Zojaji, Mohammad Ali Nematbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented generation (RAG) models, which integrate large-scale
pre-trained generative models with external retrieval mechanisms, have shown
significant success in various natural language processing (NLP) tasks.
However, applying RAG models in Persian language as a low-resource language,
poses distinct challenges. These challenges primarily involve the
preprocessing, embedding, retrieval, prompt construction, language modeling,
and response evaluation of the system. In this paper, we address the challenges
towards implementing a real-world RAG system for Persian language called
PersianRAG. We propose novel solutions to overcome these obstacles and evaluate
our approach using several Persian benchmark datasets. Our experimental results
demonstrate the capability of the PersianRAG framework to enhance question
answering task in Persian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and
  Cross-Cultural Embedding Models and Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gagan Bhatia, El Moatez Billah Nagoudi, Abdellah El Mekki, Fakhraddin Alwajih, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce {\bf Swan}, a family of embedding models centred around the
Arabic language, addressing both small-scale and large-scale use cases. Swan
includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on
ArMistral, a pretrained Arabic large language model. To evaluate these models,
we propose ArabicMTEB, a comprehensive benchmark suite that assesses
cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text
embedding performance, covering eight diverse tasks and spanning 94 datasets.
Swan-Large achieves state-of-the-art results, outperforming
Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently
surpasses Multilingual-E5-base. Our extensive evaluations demonstrate that Swan
models are both dialectally and culturally aware, excelling across various
Arabic domains while offering significant monetary efficiency. This work
significantly advances the field of Arabic language modelling and provides
valuable resources for future research and applications in Arabic natural
language processing. Our models and benchmark will be made publicly accessible
for research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large
  Language Models <span class="chip">EMNLP24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18680v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18680v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Chen, Xianghu Yue, Xiaoxue Gao, Chen Zhang, Luis Fernando D'Haro, Robby T. Tan, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various audio-LLMs (ALLMs) have been explored recently for tackling different
audio tasks simultaneously using a single, unified model. While existing
evaluations of ALLMs primarily focus on single-audio tasks, real-world
applications often involve processing multiple audio streams simultaneously. To
bridge this gap, we propose the first multi-audio evaluation (MAE) benchmark
that consists of 20 datasets from 11 multi-audio tasks encompassing both speech
and sound scenarios. Comprehensive experiments on MAE demonstrate that the
existing ALLMs, while being powerful in comprehending primary audio elements in
individual audio inputs, struggling to handle multi-audio scenarios. To this
end, we propose a novel multi-audio-LLM (MALLM) to capture audio context among
multiple similar audios using discriminative learning on our proposed synthetic
data. The results demonstrate that the proposed MALLM outperforms all baselines
and achieves high data efficiency using synthetic data without requiring human
annotations. The proposed MALLM opens the door for ALLMs towards multi-audio
processing era and brings us closer to replicating human auditory capabilities
in machines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP24 Findings. Data available at
  https://github.com/MatthewCYM/MALLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeTikZify: Synthesizing Graphics Programs for Scientific Figures and
  Sketches with TikZ <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Belouadi, Simone Paolo Ponzetto, Steffen Eger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating high-quality scientific figures can be time-consuming and
challenging, even though sketching ideas on paper is relatively easy.
Furthermore, recreating existing figures that are not stored in formats
preserving semantic information is equally complex. To tackle this problem, we
introduce DeTikZify, a novel multimodal language model that automatically
synthesizes scientific figures as semantics-preserving TikZ graphics programs
based on sketches and existing figures. To achieve this, we create three new
datasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k
human-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn
sketches with their corresponding scientific figures; and MetaFig, a collection
of diverse scientific figures and associated metadata. We train DeTikZify on
MetaFig and DaTikZv2, along with synthetically generated sketches learned from
SketchFig. We also introduce an MCTS-based inference algorithm that enables
DeTikZify to iteratively refine its outputs without the need for additional
training. Through both automatic and human evaluation, we demonstrate that
DeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ
programs, with the MCTS algorithm effectively boosting its performance. We make
our code, models, and datasets publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 (spotlight); Project page:
  https://github.com/potamides/DeTikZify</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Compositional Data Augmentation for Scientific Keyphrase Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mael Houbre, Florian Boudin, Beatrice Daille, Akiko Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art models for keyphrase generation require large amounts of
training data to achieve good performance. However, obtaining keyphrase-labeled
documents can be challenging and costly. To address this issue, we present a
self-compositional data augmentation method. More specifically, we measure the
relatedness of training documents based on their shared keyphrases, and combine
similar documents to generate synthetic samples. The advantage of our method
lies in its ability to create additional training samples that keep domain
coherence, without relying on external data or resources. Our results on
multiple datasets spanning three different domains, demonstrate that our method
consistently improves keyphrase generation. A qualitative analysis of the
generated keyphrases for the Computer Science domain confirms this improvement
towards their representativity property.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to JCDL 2024. This is the author's version of the work. It
  is posted here for your personal use. Not for redistribution. The definitive
  version was published in the proceedings of the 2024 ACM/IEEE Joint
  Conference on Digital Libraries (JCDL 24)
  https://doi.org/10.1145/3677389.3702504</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated
  Parameters by Tencent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02265v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02265v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian, Saiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Jun Xia, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie Yu, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jianqiang Ma, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao Yu, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, Jie Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Hunyuan-Large, which is currently the largest
open-source Transformer-based mixture of experts model, with a total of 389
billion parameters and 52 billion activation parameters, capable of handling up
to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior
performance across various benchmarks including language understanding and
generation, logical reasoning, mathematical problem-solving, coding,
long-context, and aggregated tasks, where it outperforms LLama3.1-70B and
exhibits comparable performance when compared to the significantly larger
LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale
synthetic data that is orders larger than in previous literature, a mixed
expert routing strategy, a key-value cache compression technique, and an
expert-specific learning rate strategy. Additionally, we also investigate the
scaling laws and learning rate schedule of mixture of experts models, providing
valuable insights and guidances for future model development and optimization.
The code and checkpoints of Hunyuan-Large are released to facilitate future
innovations and applications.
  Codes: https://github.com/Tencent/Hunyuan-Large
  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FactTest: Factuality Testing in Large Language Models with Finite-Sample
  and Distribution-Free Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Nie, Xiaotian Hou, Shuhang Lin, James Zou, Huaxiu Yao, Linjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The propensity of Large Language Models (LLMs) to generate hallucinations and
non-factual content undermines their reliability in high-stakes domains, where
rigorous control over Type I errors (the conditional probability of incorrectly
classifying hallucinations as truthful content) is essential. Despite its
importance, formal verification of LLM factuality with such guarantees remains
largely unexplored. In this paper, we introduce FactTest, a novel framework
that statistically assesses whether an LLM can confidently provide correct
answers to given questions with high-probability correctness guarantees. We
formulate factuality testing as hypothesis testing problem to enforce an upper
bound of Type I errors at user-specified significance levels. Notably, we prove
that our framework also ensures strong Type II error control under mild
conditions and can be extended to maintain its effectiveness when covariate
shifts exist. %These analyses are amenable to the principled NP framework. Our
approach is distribution-free and works for any number of human-annotated
samples. It is model-agnostic and applies to any black-box or white-box LM.
Extensive experiments on question-answering (QA) and multiple-choice benchmarks
demonstrate that \approach effectively detects hallucinations and improves the
model's ability to abstain from answering unknown questions, leading to an over
40% accuracy improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Webpage UIs for Text-Rich Visual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13824v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13824v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich visual understanding-the ability to process environments where
dense textual content is integrated with visuals-is crucial for multimodal
large language models (MLLMs) to interact effectively with structured
environments. To enhance this capability, we propose synthesizing general
multimodal instructions from webpage UIs using text-based large language models
(LLMs). Despite lacking direct visual input, text-based LLMs are able to
process structured text representations from webpage accessibility trees. These
instructions are then paired with UI screenshots to train multimodal models. We
introduce MultiUI, a dataset containing 7.3 million samples from 1 million
websites, covering diverse multimodal tasks and UI layouts. Models trained on
MultiUI not only excel in web UI tasks-achieving up to a 48% improvement on
VisualWebBench and a 19.1% boost in element accuracy on a web agent dataset
Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to
non-UI domains, such as document understanding, OCR, and chart interpretation.
These results highlight the broad applicability of web UI data for advancing
text-rich visual understanding across various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Packing Analysis: Packing Is More Appropriate for Large Models or
  <span class="highlight-title">Dataset</span>s in Supervised Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhe Wang, Guoyin Wang, Yizhong Wang, Jiwei Li, Eduard Hovy, Chen Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Packing, initially utilized in the pre-training phase, is an optimization
technique designed to maximize hardware resource efficiency by combining
different training sequences to fit the model's maximum input length. Although
it has demonstrated effectiveness during pre-training, there remains a lack of
comprehensive analysis for the supervised fine-tuning (SFT) stage on the
following points: (1) whether packing can effectively enhance training
efficiency while maintaining performance, (2) the suitable size of the model
and dataset for fine-tuning with the packing method, and (3) whether packing
unrelated or related training samples might cause the model to either
excessively disregard or over-rely on the context.
  In this paper, we perform extensive comparisons between SFT methods using
padding and packing, covering SFT datasets ranging from 69K to 1.2M and models
from 8B to 70B. This provides the first comprehensive analysis of the
advantages and limitations of packing versus padding, as well as practical
considerations for implementing packing in various training scenarios. Our
analysis covers various benchmarks, including knowledge, reasoning, and coding,
as well as GPT-based evaluations, time efficiency, and other fine-tuning
parameters. We also open-source our code for fine-tuning and evaluation and
provide checkpoints fine-tuned on datasets of different sizes, aiming to
advance future research on packing methods. Code is available at:
https://github.com/ShuheWang1998/Packing-Analysis?tab=readme-ov-file.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Power of Question Translation Training in Multilingual Reasoning:
  Broadened Scope and Deepened Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01345v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01345v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Zhu, Shujian Huang, Fei Yuan, Cheng Chen, Jiajun Chen, Alexandra Birch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bridging the significant gap between large language model's English and
non-English performance presents a great challenge. While some previous studies
attempt to mitigate this gap with translated training data, the recently
proposed question alignment framework leverages the model's English expertise
to improve multilingual performance with minimum usage of expensive,
error-prone translation. In this paper, we explore how broadly this method can
be applied by examining its effects in reasoning with and without
chain-of-thought, as well as with program-of-thought. We also explore applying
this framework to extremely large language models in an efficient manner, such
as through proxy-tuning. Experiment results on multilingual reasoning
benchmarks mGSM, mSVAMP, xCSQA and xNLI demonstrate that we can extend question
alignment framework to boost multilingual performance across diverse reasoning
scenarios, model families, and sizes. For instance, when applied to the LLaMA2
models, it brings an average accuracy improvements of 12.2% on mGSM even with
the 70B model. To understand the mechanism of its success, we analyze
representation space, generated response and data scales, and reveal how
question translation training strengthens language alignment within LLMs and
shapes their working patterns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping
  Backward Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunghyeon Woo, Baeseong Park, Byeongwook Kim, Minjung Jo, Se Jung Kwon, Dongsuk Jeon, Dongsoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved significant success across various
domains. However, training these LLMs typically involves substantial memory and
computational costs during both forward and backward propagation. While
parameter-efficient fine-tuning (PEFT) considerably reduces the training memory
associated with parameters, it does not address the significant computational
costs and activation memory. In this paper, we propose Dropping Backward
Propagation (DropBP), a novel approach designed to reduce computational costs
and activation memory while maintaining accuracy. DropBP randomly drops layers
during backward propagation, which is essentially equivalent to training
shallow submodules generated by undropped layers and residual connections.
Additionally, DropBP calculates the sensitivity of each layer to assign an
appropriate drop rate, thereby stabilizing the training process. DropBP is not
only applicable to full fine-tuning but can also be orthogonally integrated
with all types of PEFT by dropping layers during backward propagation.
Specifically, DropBP can reduce training time by 44% with comparable accuracy
to the baseline, accelerate convergence to the same perplexity by 1.5x, and
enable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.
Furthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100
GPU and 117% on an Intel Gaudi2 HPU. The code is available at
https://github.com/WooSunghyeon/dropbp.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09324v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09324v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Xu, Fan Liu, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Large Language Models (LLMs) have demonstrated significant
capabilities in executing complex tasks in a zero-shot manner, they are
susceptible to jailbreak attacks and can be manipulated to produce harmful
outputs. Recently, a growing body of research has categorized jailbreak attacks
into token-level and prompt-level attacks. However, previous work primarily
overlooks the diverse key factors of jailbreak attacks, with most studies
concentrating on LLM vulnerabilities and lacking exploration of
defense-enhanced LLMs. To address these issues, we introduced
$\textbf{JailTrickBench}$ to evaluate the impact of various attack settings on
LLM performance and provide a baseline for jailbreak attacks, encouraging the
adoption of a standardized evaluation framework. Specifically, we evaluate the
eight key factors of implementing jailbreak attacks on LLMs from both
target-level and attack-level perspectives. We further conduct seven
representative jailbreak attacks on six defense methods across two widely used
datasets, encompassing approximately 354 experiments with about 55,000 GPU
hours on A800-80G. Our experimental results highlight the need for standardized
benchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is
available at https://github.com/usail-hkust/JailTrickBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CheX-<span class="highlight-title">GPT</span>: Harnessing Large Language Models for Enhanced Chest X-ray
  Report Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jawook Gu, Kihyun You, Han-Cheol Cho, Jiho Kim, Eun Kyoung Hong, Byungseok Roh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Free-text radiology reports present a rich data source for various medical
tasks, but effectively labeling these texts remains challenging. Traditional
rule-based labeling methods fall short of capturing the nuances of diverse
free-text patterns. Moreover, models using expert-annotated data are limited by
data scarcity and pre-defined classes, impacting their performance, flexibility
and scalability. To address these issues, our study offers three main
contributions: 1) We demonstrate the potential of GPT as an adept labeler using
carefully designed prompts. 2) Utilizing only the data labeled by GPT, we
trained a BERT-based labeler, CheX-GPT, which operates faster and more
efficiently than its GPT counterpart. 3) To benchmark labeler performance, we
introduced a publicly available expert-annotated test set, MIMIC-500,
comprising 500 cases from the MIMIC validation set. Our findings demonstrate
that CheX-GPT not only excels in labeling accuracy over existing models, but
also showcases superior efficiency, flexibility, and scalability, supported by
our introduction of the MIMIC-500 dataset for robust benchmarking. Code and
models are available at https://github.com/Soombit-ai/CheXGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wave Network: An Ultra-Small Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Victor S. Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an innovative token representation and update method in a new
ultra-small language model: the Wave network. Specifically, we use a complex
vector to represent each token, encoding both global and local semantics of the
input text. A complex vector consists of two components: a magnitude vector
representing the global semantics of the input text, and a phase vector
capturing the relationships between individual tokens and global semantics.
Experiments on the AG News text classification task demonstrate that, when
generating complex vectors from randomly initialized token embeddings, our
single-layer Wave Network achieves 90.91% accuracy with wave interference and
91.66% with wave modulation - outperforming a single Transformer layer using
BERT pre-trained embeddings by 19.23% and 19.98%, respectively, and approaching
the accuracy of the pre-trained and fine-tuned BERT base model (94.64%).
Additionally, compared to BERT base, the Wave Network reduces video memory
usage and training time by 77.34% and 85.62% during wave modulation. In
summary, we used a 2.4-million-parameter small language model to achieve
accuracy comparable to a 100-million-parameter BERT model in text
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMLU-Pro: A More Robust and Challenging Multi-Task Language
  Understanding Benchmark <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01574v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01574v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the age of large-scale language models, benchmarks like the Massive
Multitask Language Understanding (MMLU) have been pivotal in pushing the
boundaries of what AI can achieve in language comprehension and reasoning
across diverse domains. However, as models continue to improve, their
performance on these benchmarks has begun to plateau, making it increasingly
difficult to discern differences in model capabilities. This paper introduces
MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven
MMLU benchmark by integrating more challenging, reasoning-focused questions and
expanding the choice set from four to ten options. Additionally, MMLU-Pro
eliminates the trivial and noisy questions in MMLU. Our experimental results
show that MMLU-Pro not only raises the challenge, causing a significant drop in
accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability
under varying prompts. With 24 different prompt styles tested, the sensitivity
of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in
MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT)
reasoning achieved better performance on MMLU-Pro compared to direct answering,
which is in stark contrast to the findings on the original MMLU, indicating
that MMLU-Pro includes more complex reasoning questions. Our assessments
confirm that MMLU-Pro is a more discriminative benchmark to better track
progress in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has been accepted and published at NeurIPS 2024 Track
  Datasets and Benchmarks (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perception Compressor:A training-free prompt compression method in long
  context scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwei Tang, Jin Xu, Tingwei Lu, Zhicheng Zhang, Yiming Zhao, Lin Hai, Hai-Tao Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate exceptional capabilities in various
scenarios. However, they suffer from much redundant information and are
sensitive to the position of key information (relevant to the input question)
in long context scenarios, leading to inferior performance. To address these
challenges, we present Perception Compressor, a training-free prompt
compression method. It includes a perception retriever that leverages guiding
questions and instruction to retrieve the most relevant demonstrations, a
dual-slope ratio allocator to dynamically allocate compression ratios and
open-book ratios, and a semi-guided iterative compression that retains key
information at the token level while removing tokens that distract the LLM. We
conduct extensive experiments on long context benchmarks, i.e.,
NaturalQuestions, LongBench, and MuSiQue. Experiment results show that
Perception Compressor outperforms existing methods by a large margin, achieving
state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AudioBench: A Universal Benchmark for Audio Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16020v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16020v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, Nancy F. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce AudioBench, a universal benchmark designed to evaluate Audio
Large Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26
datasets, among which, 7 are newly proposed datasets. The evaluation targets
three main aspects: speech understanding, audio scene understanding, and voice
understanding (paralinguistic). Despite recent advancements, there lacks a
comprehensive benchmark for AudioLLMs on instruction following capabilities
conditioned on audio signals. AudioBench addresses this gap by setting up
datasets as well as desired evaluation metrics. Besides, we also evaluated the
capabilities of five popular models and found that no single model excels
consistently across all tasks. We outline the research outlook for AudioLLMs
and anticipate that our open-sourced evaluation toolkit, data, and leaderboard
will offer a robust testbed for future model developments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v4 - Add acknowledgment and slight update on structure; Code:
  https://github.com/AudioLLMs/AudioBench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $B^4$: A Black-Box Scrubbing Attack on LLM Watermarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baizhou Huang, Xiao Pu, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking has emerged as a prominent technique for LLM-generated content
detection by embedding imperceptible patterns. Despite supreme performance, its
robustness against adversarial attacks remains underexplored. Previous work
typically considers a grey-box attack setting, where the specific type of
watermark is already known. Some even necessitates knowledge about
hyperparameters of the watermarking method. Such prerequisites are unattainable
in real-world scenarios. Targeting at a more realistic black-box threat model
with fewer assumptions, we here propose $\mathcal{B}^4$, a black-box scrubbing
attack on watermarks. Specifically, we formulate the watermark scrubbing attack
as a constrained optimization problem by capturing its objectives with two
distributions, a Watermark Distribution and a Fidelity Distribution. This
optimization problem can be approximately solved using two proxy distributions.
Experimental results across 12 different settings demonstrate the superior
performance of $\mathcal{B}^4$ compared with other baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Pretraining Using a Large Corpus Machine-Translated from a
  Single Source Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Wang, Yao Lu, Maurice Weber, Max Ryabinin, Yihong Chen, Raphael Tang, Pontus Stenetorp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  English, as a very high-resource language, enables the pretraining of
high-quality large language models (LLMs). The same cannot be said for most
other languages, as leading LLMs still underperform for non-English languages,
likely due to a gap in the quality and diversity of the available multilingual
pretraining corpora. In this work, we find that machine-translated text from a
single high-quality source language can contribute significantly to the
pretraining of multilingual LLMs. We translate FineWeb-Edu, a high-quality
English web dataset, into French, German, and Spanish, resulting in a final
300B-token dataset, which we call TransWeb-Edu, and pretrain a 1.3B-parameter
model, CuatroLLM, from scratch on this dataset. Across five non-English
reasoning tasks, we show that CuatroLLM matches or outperforms state-of-the-art
multilingual models trained using closed data, such as Llama3.2 and Gemma2,
despite using an order of magnitude less data, such as about 6% of the tokens
used for Llama3.2's training. We further demonstrate that with additional
domain-specific pretraining, amounting to less than 1% of TransWeb-Edu,
CuatroLLM surpasses the state of the art in multilingual reasoning. To promote
reproducibility, we release our corpus, models, and training pipeline under
open licenses at hf.co/britllm/CuatroLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiently and Effectively: A Two-stage Approach to Balance Plaintext
  and Encrypted Text for Traffic Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19687v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19687v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Peng, Lei Cui, Wei Cai, Zhenquan Ding, Zhiyu Hao, Xiaochun Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Encrypted traffic classification is the task of identifying the application
or service associated with encrypted network traffic. One effective approach
for this task is to use deep learning methods to encode the raw traffic bytes
directly and automatically extract features for classification (byte-based
models). However, current byte-based models input raw traffic bytes, whether
plaintext or encrypted text, for automated feature extraction, neglecting the
distinct impacts of plaintext and encrypted text on downstream tasks.
Additionally, these models primarily focus on improving classification
accuracy, with little emphasis on the efficiency of models. In this paper, for
the first time, we analyze the impact of plaintext and encrypted text on the
model's effectiveness and efficiency. Based on our observations and findings,
we propose a two-phase approach to balance the trade-off between plaintext and
encrypted text in traffic classification. Specifically, Stage one is to
Determine whether the Plain text is enough to be accurately Classified (DPC)
using the proposed DPC Selector. This stage quickly identifies samples that can
be classified using plaintext, leveraging explicit byte features in plaintext
to enhance model's efficiency. Stage two aims to adaptively make a
classification with the result from stage one. This stage incorporates
encrypted text information for samples that cannot be classified using
plaintext alone, ensuring the model's effectiveness on traffic classification
tasks. Experiments on two datasets demonstrate that our proposed model achieves
state-of-the-art results in both effectiveness and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating LLMs for Targeted Concept Simplification for Domain-Specific
  Texts <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumit Asthana, Hannah Rashkin, Elizabeth Clark, Fantine Huot, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One useful application of NLP models is to support people in reading complex
text from unfamiliar domains (e.g., scientific articles). Simplifying the
entire text makes it understandable but sometimes removes important details. On
the contrary, helping adult readers understand difficult concepts in context
can enhance their vocabulary and knowledge. In a preliminary human study, we
first identify that lack of context and unfamiliarity with difficult concepts
is a major reason for adult readers' difficulty with domain-specific text. We
then introduce "targeted concept simplification," a simplification task for
rewriting text to help readers comprehend text containing unfamiliar concepts.
We also introduce WikiDomains, a new dataset of 22k definitions from 13
academic domains paired with a difficult concept within each definition. We
benchmark the performance of open-source and commercial LLMs and a simple
dictionary baseline on this task across human judgments of ease of
understanding and meaning preservation. Interestingly, our human judges
preferred explanations about the difficult concept more than simplification of
the concept phrase. Further, no single model achieved superior performance
across all quality dimensions, and automated metrics also show low correlations
with human evaluations of concept simplification ($\sim0.2$), opening up rich
avenues for research on personalized human reading comprehension support.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in proceedings of EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Document Parsing Unveiled: Techniques, Challenges, and Prospects for
  Structured Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qintong Zhang, Victor Shea-Jay Huang, Bin Wang, Junyuan Zhang, Zhengren Wang, Hao Liang, Shawn Wang, Matthieu Lin, Conghui He, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document parsing is essential for converting unstructured and semi-structured
documents-such as contracts, academic papers, and invoices-into structured,
machine-readable data. Document parsing extract reliable structured data from
unstructured inputs, providing huge convenience for numerous applications.
Especially with recent achievements in Large Language Models, document parsing
plays an indispensable role in both knowledge base construction and training
data generation. This survey presents a comprehensive review of the current
state of document parsing, covering key methodologies, from modular pipeline
systems to end-to-end models driven by large vision-language models. Core
components such as layout detection, content extraction (including text,
tables, and mathematical expressions), and multi-modal data integration are
examined in detail. Additionally, this paper discusses the challenges faced by
modular document parsing systems and vision-language models in handling complex
layouts, integrating multiple modules, and recognizing high-density text. It
emphasizes the importance of developing larger and more diverse datasets and
outlines future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Creative Short Story Generation in Humans and Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Claire Stevenson, Lonneke van der Plas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Storytelling is a fundamental aspect of human communication, relying heavily
on creativity to produce narratives that are novel, appropriate, and
surprising. While large language models (LLMs) have recently demonstrated the
ability to generate high-quality stories, their creative capabilities remain
underexplored. Previous research has either focused on creativity tests
requiring short responses or primarily compared model performance in story
generation to that of professional writers. However, the question of whether
LLMs exhibit creativity in writing short stories on par with the average human
remains unanswered. In this work, we conduct a systematic analysis of
creativity in short story generation across LLMs and everyday people. Using a
five-sentence creative story task, commonly employed in psychology to assess
human creativity, we automatically evaluate model- and human-generated stories
across several dimensions of creativity, including novelty, surprise, and
diversity. Our findings reveal that while LLMs can generate stylistically
complex stories, they tend to fall short in terms of creativity when compared
to average human writers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRSQA -- Graph Reasoning-Structured Question Answering <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anish Pahilajani, Devasha Trivedi, Jincen Shuai, Khin S. Yone, Samyak Rajesh Jain, Namyong Park, Ryan A. Rossi, Nesreen K. Ahmed, Franck Dernoncourt, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have excelled in multi-hop question-answering
(M-QA) due to their advanced reasoning abilities. However, the impact of the
inherent reasoning structures on LLM M-QA performance remains unclear, largely
due to the absence of QA datasets that provide fine-grained reasoning
structures. To address this gap, we introduce the Graph Reasoning-Structured
Question Answering Dataset (GRS-QA), which includes both semantic contexts and
reasoning structures for QA pairs. Unlike existing M-QA datasets, where
different reasoning structures are entangled together, GRS-QA explicitly
captures intricate reasoning pathways by constructing reasoning graphs, where
nodes represent textual contexts and edges denote logical flows. These
reasoning graphs of different structures enable a fine-grained evaluation of
LLM reasoning capabilities across various reasoning structures. Our empirical
analysis reveals that LLMs perform differently when handling questions with
varying reasoning structures. This finding facilitates the exploration of
textual structures as compared with semantics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 24 figures, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transcoders Find Interpretable LLM Feature Circuits <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Dunefsky, Philippe Chlenski, Neel Nanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key goal in mechanistic interpretability is circuit analysis: finding
sparse subgraphs of models corresponding to specific behaviors or capabilities.
However, MLP sublayers make fine-grained circuit analysis on transformer-based
language models difficult. In particular, interpretable features -- such as
those found by sparse autoencoders (SAEs) -- are typically linear combinations
of extremely many neurons, each with its own nonlinearity to account for.
Circuit analysis in this setting thus either yields intractably large circuits
or fails to disentangle local and global behavior. To address this we explore
transcoders, which seek to faithfully approximate a densely activating MLP
layer with a wider, sparsely-activating MLP layer. We introduce a novel method
for using transcoders to perform weights-based circuit analysis through MLP
sublayers. The resulting circuits neatly factorize into input-dependent and
input-invariant terms. We then successfully train transcoders on language
models with 120M, 410M, and 1.4B parameters, and find them to perform at least
on par with SAEs in terms of sparsity, faithfulness, and
human-interpretability. Finally, we apply transcoders to reverse-engineer
unknown circuits in the model, and we obtain novel insights regarding the
"greater-than circuit" in GPT2-small. Our results suggest that transcoders can
prove effective in decomposing model computations involving MLPs into
interpretable circuits. Code is available at
https://github.com/jacobdunefsky/transcoder_circuits/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 6 figures, 4 tables, 2 algorithms. NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teach Better or Show Smarter? On Instructions and Exemplars in Automatic
  Prompt Optimization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Sercan O. Arik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated remarkable capabilities, but their
performance is heavily reliant on effective prompt engineering. Automatic
prompt optimization (APO) methods are designed to automate this and can be
broadly categorized into those targeting instructions (instruction
optimization, IO) vs. those targeting exemplars (exemplar optimization, EO).
Despite their shared objective, these have evolved rather independently, with
IO receiving more research attention recently. This paper seeks to bridge this
gap by comprehensively comparing the performance of representative IO and EO
techniques both isolation and combination on a diverse set of challenging
tasks. Our findings reveal that intelligently reusing model-generated
input-output pairs obtained from evaluating prompts on the validation set as
exemplars, consistently improves performance on top of IO methods but is
currently under-investigated. We also find that despite the recent focus on IO,
how we select exemplars can outweigh how we optimize instructions, with EO
strategies as simple as random search outperforming state-of-the-art IO methods
with seed instructions without any optimization. Moreover, we observe a synergy
between EO and IO, with optimal combinations surpassing the individual
contributions. We conclude that studying exemplar optimization both as a
standalone method and its optimal combination with instruction optimization
remain a crucial aspect of APO and deserve greater consideration in future
research, even in the era of highly capable instruction-following models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Expanded version of the NeurIPS 2024 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptions to Beliefs: Exploring Precursory Inferences for Theory of
  Mind in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06004v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06004v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chani Jung, Dongkwan Kim, Jiho Jin, Jiseon Kim, Yeon Seonwoo, Yejin Choi, Alice Oh, Hyunwoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While humans naturally develop theory of mind (ToM), the capability to
understand other people's mental states and beliefs, state-of-the-art large
language models (LLMs) underperform on simple ToM benchmarks. We posit that we
can extend our understanding of LLMs' ToM abilities by evaluating key human ToM
precursors$-$perception inference and perception-to-belief inference$-$in LLMs.
We introduce two datasets, Percept-ToMi and Percept-FANToM, to evaluate these
precursory inferences for ToM in LLMs by annotating characters' perceptions on
ToMi and FANToM, respectively. Our evaluation of eight state-of-the-art LLMs
reveals that the models generally perform well in perception inference while
exhibiting limited capability in perception-to-belief inference (e.g., lack of
inhibitory control). Based on these results, we present PercepToM, a novel ToM
method leveraging LLMs' strong perception inference capability while
supplementing their limited perception-to-belief inference. Experimental
results demonstrate that PercepToM significantly enhances LLM's performance,
especially in false belief scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Differential Diagnosis with Dual-Inference Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07330v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07330v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Zhou, Mingquan Lin, Sirui Ding, Jiashuo Wang, Genevieve B. Melton, James Zou, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic differential diagnosis (DDx) is an essential medical task that
generates a list of potential diseases as differentials based on patient
symptom descriptions. In practice, interpreting these differential diagnoses
yields significant value but remains under-explored. Given the powerful
capabilities of large language models (LLMs), we investigated using LLMs for
interpretable DDx. Specifically, we curated the first DDx dataset with
expert-derived interpretation on 570 clinical notes. Besides, we proposed
Dual-Inf, a novel framework that enabled LLMs to conduct bidirectional
inference (i.e., from symptoms to diagnoses and vice versa) for DDx
interpretation. Both human and automated evaluation validated its efficacy in
predicting and elucidating differentials across four base LLMs. In addition,
Dual-Inf could reduce interpretation errors and hold promise for rare disease
explanations. To the best of our knowledge, it is the first work that
customizes LLMs for DDx explanation and comprehensively evaluates their
interpretation performance. Overall, our study bridges a critical gap in DDx
interpretation and enhances clinical decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Textless Speech-to-Speech Translation With Limited Parallel Data <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15405v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15405v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuj Diwan, Anirudh Srinivasan, David Harwath, Eunsol Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing speech-to-speech translation (S2ST) models fall into two camps: they
either leverage text as an intermediate step or require hundreds of hours of
parallel speech data. Both approaches are incompatible with textless languages
or language pairs with limited parallel data. We present PFB, a framework for
training textless S2ST models that require just dozens of hours of parallel
speech data. We first pretrain a model on large-scale monolingual speech data,
finetune it with a small amount of parallel speech data (20-60 hours), and
lastly train with an unsupervised backtranslation objective. We train and
evaluate our models for English-to-German, German-to-English and
Marathi-to-English translation on three different domains (European Parliament,
Common Voice, and All India Radio) with single-speaker synthesized speech.
Evaluated using the ASR-BLEU metric, our models achieve reasonable performance
on all three domains, with some being within 1-2 points of our higher-resourced
topline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-4V Cannot Generate Radiology Reports Yet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12176v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12176v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Jiang, Chacha Chen, Dang Nguyen, Benjamin M. Mervak, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GPT-4V's purported strong multimodal abilities raise interests in using it to
automate radiology report writing, but there lacks thorough evaluations. In
this work, we perform a systematic evaluation of GPT-4V in generating radiology
reports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt
to directly generate reports using GPT-4V through different prompting
strategies and find that it fails terribly in both lexical metrics and clinical
efficacy metrics. To understand the low performance, we decompose the task into
two steps: 1) the medical image reasoning step of predicting medical condition
labels from images; and 2) the report synthesis step of generating reports from
(groundtruth) conditions. We show that GPT-4V's performance in image reasoning
is consistently low across different prompts. In fact, the distributions of
model-predicted labels remain constant regardless of which groundtruth
conditions are present on the image, suggesting that the model is not
interpreting chest X-rays meaningfully. Even when given groundtruth conditions
in report synthesis, its generated reports are less correct and less
natural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt
on the viability of using GPT-4V in a radiology workflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 3 figures, code:
  https://github.com/ChicagoHAI/cxr-eval-gpt-4v</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INQUIRE: A Natural World Text-to-Image Retrieval Benchmark <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara Beery, Grant Van Horn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce INQUIRE, a text-to-image retrieval benchmark designed to
challenge multimodal vision-language models on expert-level queries. INQUIRE
includes iNaturalist 2024 (iNat24), a new dataset of five million natural world
images, along with 250 expert-level retrieval queries. These queries are paired
with all relevant images comprehensively labeled within iNat24, comprising
33,000 total matches. Queries span categories such as species identification,
context, behavior, and appearance, emphasizing tasks that require nuanced image
understanding and domain expertise. Our benchmark evaluates two core retrieval
tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)
INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed
evaluation of a range of recent multimodal models demonstrates that INQUIRE
poses a significant challenge, with the best models failing to achieve an
mAP@50 above 50%. In addition, we show that reranking with more powerful
multimodal models can enhance retrieval performance, yet there remains a
significant margin for improvement. By focusing on scientifically-motivated
ecological challenges, INQUIRE aims to bridge the gap between AI capabilities
and the needs of real-world scientific inquiry, encouraging the development of
retrieval systems that can assist with accelerating ecological and biodiversity
research. Our dataset and code are available at
https://inquire-benchmark.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">116</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Community Forensics: Using Thousands of Generators to Train Fake Image
  Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongsoo Park, Andrew Owens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the key challenges of detecting AI-generated images is spotting images
that have been created by previously unseen generative models. We argue that
the limited diversity of the training data is a major obstacle to addressing
this problem, and we propose a new dataset that is significantly larger and
more diverse than prior work. As part of creating this dataset, we
systematically download thousands of text-to-image latent diffusion models and
sample images from them. We also collect images from dozens of popular open
source and commercial models. The resulting dataset contains 2.7M images that
have been sampled from 4803 different models. These images collectively capture
a wide range of scene content, generator architectures, and image processing
settings. Using this dataset, we study the generalization abilities of fake
image detectors. Our experiments suggest that detection performance improves as
the number of models in the training set increases, even when these models have
similar architectures. We also find that detection performance improves as the
diversity of the models increases, and that our trained detectors generalize
better than those trained on other datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For
  Autonomous Visual Robot Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreya Gummadi, Mateus V. Gasparino, Deepak Vasisht, Girish Chowdhary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Centralized learning requires data to be aggregated at a central server,
which poses significant challenges in terms of data privacy and bandwidth
consumption. Federated learning presents a compelling alternative, however,
vanilla federated learning methods deployed in robotics aim to learn a single
global model across robots that works ideally for all. But in practice one
model may not be well suited for robots deployed in various environments. This
paper proposes Federated-EmbedCluster (Fed-EC), a clustering-based federated
learning framework that is deployed with vision based autonomous robot
navigation in diverse outdoor environments. The framework addresses the key
federated learning challenge of deteriorating model performance of a single
global model due to the presence of non-IID data across real-world robots.
Extensive real-world experiments validate that Fed-EC reduces the communication
size by 23x for each robot while matching the performance of centralized
learning for goal-oriented navigation and outperforms local learning. Fed-EC
can transfer previously learnt models to new robots that join the cluster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned
  Vision-Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maya Varma, Jean-Benoit Delbrouck, Zhihong Chen, Akshay Chaudhari, Curtis Langlotz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuned vision-language models (VLMs) often capture spurious correlations
between image features and textual attributes, resulting in degraded zero-shot
performance at test time. Existing approaches for addressing spurious
correlations (i) primarily operate at the global image-level rather than
intervening directly on fine-grained image features and (ii) are predominantly
designed for unimodal settings. In this work, we present RaVL, which takes a
fine-grained perspective on VLM robustness by discovering and mitigating
spurious correlations using local image features rather than operating at the
global image level. Given a fine-tuned VLM, RaVL first discovers spurious
correlations by leveraging a region-level clustering approach to identify
precise image features contributing to zero-shot classification errors. Then,
RaVL mitigates the identified spurious correlation with a novel region-aware
loss function that enables the VLM to focus on relevant regions and ignore
spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with
various model architectures, data domains, and learned spurious correlations.
Our results show that RaVL accurately discovers (191% improvement over the
closest baseline) and mitigates (8.2% improvement on worst-group image
classification accuracy) spurious correlations. Qualitative evaluations on
general-domain and medical-domain VLMs confirm our findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Textual Decomposition Then Sub-motion-space Scattering for
  Open-Vocabulary Motion Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Fan, Jiangning Zhang, Ran Yi, Jingyu Gong, Yabiao Wang, Yating Wang, Xin Tan, Chengjie Wang, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-motion generation is a crucial task in computer vision, which
generates the target 3D motion by the given text. The existing annotated
datasets are limited in scale, resulting in most existing methods overfitting
to the small datasets and unable to generalize to the motions of the open
domain. Some methods attempt to solve the open-vocabulary motion generation
problem by aligning to the CLIP space or using the Pretrain-then-Finetuning
paradigm. However, the current annotated dataset's limited scale only allows
them to achieve mapping from sub-text-space to sub-motion-space, instead of
mapping between full-text-space and full-motion-space (full mapping), which is
the key to attaining open-vocabulary motion generation. To this end, this paper
proposes to leverage the atomic motion (simple body part motions over a short
time period) as an intermediate representation, and leverage two orderly
coupled steps, i.e., Textual Decomposition and Sub-motion-space Scattering, to
address the full mapping problem. For Textual Decomposition, we design a
fine-grained description conversion algorithm, and combine it with the
generalization ability of a large language model to convert any given motion
text into atomic texts. Sub-motion-space Scattering learns the compositional
process from atomic motions to the target motions, to make the learned
sub-motion-space scattered to form the full-motion-space. For a given motion of
the open domain, it transforms the extrapolation into interpolation and thereby
significantly improves generalization. Our network, $DSO$-Net, combines textual
$d$ecomposition and sub-motion-space $s$cattering to solve the
$o$pen-vocabulary motion generation. Extensive experiments demonstrate that our
DSO-Net achieves significant improvements over the state-of-the-art methods on
open-vocabulary motion generation. Code is available at
https://vankouf.github.io/DSONet/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://vankouf.github.io/DSONet/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H-POPE: Hierarchical Polling-based Probing Evaluation of Hallucinations
  in Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nhi Pham, Michael Schott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By leveraging both texts and images, large vision language models (LVLMs)
have shown significant progress in various multi-modal tasks. Nevertheless,
these models often suffer from hallucinations, e.g., they exhibit
inconsistencies between the visual input and the textual output. To address
this, we propose H-POPE, a coarse-to-fine-grained benchmark that systematically
assesses hallucination in object existence and attributes. Our evaluation shows
that models are prone to hallucinations on object existence, and even more so
on fine-grained attributes. We further investigate whether these models rely on
visual input to formulate the output texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Poster at https://sites.google.com/berkeley.edu/bb-stat/home</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pseudo-labeling with Keyword Refining for Few-Supervised Video
  Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping Li, Tao Wang, Xinkui Zhao, Xianghua Xu, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video captioning generate a sentence that describes the video content.
Existing methods always require a number of captions (\eg, 10 or 20) per video
to train the model, which is quite costly. In this work, we explore the
possibility of using only one or very few ground-truth sentences, and introduce
a new task named few-supervised video captioning. Specifically, we propose a
few-supervised video captioning framework that consists of lexically
constrained pseudo-labeling module and keyword-refined captioning module.
Unlike the random sampling in natural language processing that may cause
invalid modifications (\ie, edit words), the former module guides the model to
edit words using some actions (\eg, copy, replace, insert, and delete) by a
pretrained token-level classifier, and then fine-tunes candidate sentences by a
pretrained language model. Meanwhile, the former employs the repetition
penalized sampling to encourage the model to yield concise pseudo-labeled
sentences with less repetition, and selects the most relevant sentences upon a
pretrained video-text model. Moreover, to keep semantic consistency between
pseudo-labeled sentences and video content, we develop the transformer-based
keyword refiner with the video-keyword gated fusion strategy to emphasize more
on relevant words. Extensive experiments on several benchmarks demonstrate the
advantages of the proposed approach in both few-supervised and fully-supervised
scenarios. The code implementation is available at
https://github.com/mlvccn/PKG_VidCap
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 figures, Accepted in Pattern Recognition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-branch Spatio-Temporal Graph Neural Network For Efficient Ice
  Layer Thickness Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zesheng Liu, Maryam Rahnemoonfar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding spatio-temporal patterns in polar ice layers is essential for
tracking changes in ice sheet balance and assessing ice dynamics. While
convolutional neural networks are widely used in learning ice layer patterns
from raw echogram images captured by airborne snow radar sensors, noise in the
echogram images prevents researchers from getting high-quality results.
Instead, we focus on geometric deep learning using graph neural networks,
aiming to build a spatio-temporal graph neural network that learns from
thickness information of the top ice layers and predicts for deeper layers. In
this paper, we developed a novel multi-branch spatio-temporal graph neural
network that used the GraphSAGE framework for spatio features learning and a
temporal convolution operation to capture temporal changes, enabling different
branches of the network to be more specialized and focusing on a single
learning task. We found that our proposed multi-branch network can consistently
outperform the current fused spatio-temporal graph neural network in both
accuracy and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Characteristic Descriptors with Images for Human-Expert-like
  Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharat Chandra Yalavarthi, Nalini Ratha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In mission-critical domains such as law enforcement and medical diagnosis,
the ability to explain and interpret the outputs of deep learning models is
crucial for ensuring user trust and supporting informed decision-making.
Despite advancements in explainability, existing methods often fall short in
providing explanations that mirror the depth and clarity of those given by
human experts. Such expert-level explanations are essential for the dependable
application of deep learning models in law enforcement and medical contexts.
Additionally, we recognize that most explanations in real-world scenarios are
communicated primarily through natural language. Addressing these needs, we
propose a novel approach that utilizes characteristic descriptors to explain
model decisions by identifying their presence in images, thereby generating
expert-like explanations. Our method incorporates a concept bottleneck layer
within the model architecture, which calculates the similarity between image
and descriptor encodings to deliver inherent and faithful explanations. Through
experiments in face recognition and chest X-ray diagnosis, we demonstrate that
our approach offers a significant contrast over existing techniques, which are
often limited to the use of saliency maps. We believe our approach represents a
significant step toward making deep learning systems more accountable,
transparent, and trustworthy in the critical domains of face recognition and
medical diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synomaly Noise and Multi-Stage <span class="highlight-title">Diffusion</span>: A Novel Approach for
  Unsupervised Anomaly Detection in Ultrasound Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Bi, Lucie Huang, Ricarda Clarenbach, Reza Ghotbi, Angelos Karlas, Nassir Navab, Zhongliang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound (US) imaging is widely used in routine clinical practice due to
its advantages of being radiation-free, cost-effective, and portable. However,
the low reproducibility and quality of US images, combined with the scarcity of
expert-level annotation, make the training of fully supervised segmentation
models challenging. To address these issues, we propose a novel unsupervised
anomaly detection framework based on a diffusion model that incorporates a
synthetic anomaly (Synomaly) noise function and a multi-stage diffusion
process. Synomaly noise introduces synthetic anomalies into healthy images
during training, allowing the model to effectively learn anomaly removal. The
multi-stage diffusion process is introduced to progressively denoise images,
preserving fine details while improving the quality of anomaly-free
reconstructions. The generated high-fidelity counterfactual healthy images can
further enhance the interpretability of the segmentation models, as well as
provide a reliable baseline for evaluating the extent of anomalies and
supporting clinical decision-making. Notably, the unsupervised anomaly
detection model is trained purely on healthy images, eliminating the need for
anomalous training samples and pixel-level annotations. We validate the
proposed approach on carotid US, brain MRI, and liver CT datasets. The
experimental results demonstrate that the proposed framework outperforms
existing state-of-the-art unsupervised anomaly detection methods, achieving
performance comparable to fully supervised segmentation models in the US
dataset. Additionally, ablation studies underline the importance of
hyperparameter selection for Synomaly noise and the effectiveness of the
multi-stage diffusion process in enhancing model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local vs distributed representations: What is the right basis for
  interpretability? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Colin, Lore Goetschalckx, Thomas Fel, Victor Boutin, Jay Gopal, Thomas Serre, Nuria Oliver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much of the research on the interpretability of deep neural networks has
focused on studying the visual features that maximally activate individual
neurons. However, recent work has cast doubts on the usefulness of such local
representations for understanding the behavior of deep neural networks because
individual neurons tend to respond to multiple unrelated visual patterns, a
phenomenon referred to as "superposition". A promising alternative to
disentangle these complex patterns is learning sparsely distributed vector
representations from entire network layers, as the resulting basis vectors
seemingly encode single identifiable visual patterns consistently. Thus, one
would expect the resulting code to align better with human perceivable visual
patterns, but supporting evidence remains, at best, anecdotal. To fill this
gap, we conducted three large-scale psychophysics experiments collected from a
pool of 560 participants. Our findings provide (i) strong evidence that
features obtained from sparse distributed representations are easier to
interpret by human observers and (ii) that this effect is more pronounced in
the deepest layers of a neural network. Complementary analyses also reveal that
(iii) features derived from sparse distributed representations contribute more
to the model's decision. Overall, our results highlight that distributed
representations constitute a superior basis for interpretability, underscoring
a need for the field to move beyond the interpretation of local neural codes in
favor of sparsely distributed ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ET-SEED: Efficient Trajectory-Level SE(3) Equivariant <span class="highlight-title">Diffusion</span> Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenrui Tie, Yue Chen, Ruihai Wu, Boxuan Dong, Zeyi Li, Chongkai Gao, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning, e.g., diffusion policy, has been proven effective in
various robotic manipulation tasks. However, extensive demonstrations are
required for policy robustness and generalization. To reduce the demonstration
reliance, we leverage spatial symmetry and propose ET-SEED, an efficient
trajectory-level SE(3) equivariant diffusion model for generating action
sequences in complex robot manipulation tasks. Further, previous equivariant
diffusion models require the per-step equivariance in the Markov process,
making it difficult to learn policy under such strong constraints. We
theoretically extend equivariant Markov kernels and simplify the condition of
equivariant diffusion process, thereby significantly improving training
efficiency for trajectory-level SE(3) equivariant diffusion policy in an
end-to-end manner. We evaluate ET-SEED on representative robotic manipulation
tasks, involving rigid body, articulated and deformable object. Experiments
demonstrate superior data efficiency and manipulation proficiency of our
proposed method, as well as its ability to generalize to unseen configurations
with only a few demonstrations. Website: https://et-seed.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to CoRL 2024 Workshop on X-Embodiment Robot Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReEdit: Multimodal Exemplar-Based Image Editing with <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Srivastava, Tarun Ram Menta, Abhinav Java, Avadhoot Jadhav, Silky Singh, Surgan Jandial, Balaji Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern Text-to-Image (T2I) Diffusion models have revolutionized image editing
by enabling the generation of high-quality photorealistic images. While the de
facto method for performing edits with T2I models is through text instructions,
this approach non-trivial due to the complex many-to-many mapping between
natural language and images. In this work, we address exemplar-based image
editing -- the task of transferring an edit from an exemplar pair to a content
image(s). We propose ReEdit, a modular and efficient end-to-end framework that
captures edits in both text and image modalities while ensuring the fidelity of
the edited image. We validate the effectiveness of ReEdit through extensive
comparisons with state-of-the-art baselines and sensitivity analyses of key
design choices. Our results demonstrate that ReEdit consistently outperforms
contemporary approaches both qualitatively and quantitatively. Additionally,
ReEdit boasts high practical applicability, as it does not require any
task-specific optimization and is four times faster than the next best
baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First three authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion
  Segmentation <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyuan Ding, Yixiong Liang, Shichao Kan, Qing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High resolution is crucial for precise segmentation in fundus images, yet
handling high-resolution inputs incurs considerable GPU memory costs, with
diminishing performance gains as overhead increases. To address this issue
while tackling the challenge of segmenting tiny objects, recent studies have
explored local-global fusion methods. These methods preserve fine details using
local regions and capture long-range context information from downscaled global
images. However, the necessity of multiple forward passes inevitably incurs
significant computational overhead, adversely affecting inference speed. In
this paper, we propose HRDecoder, a simple High-Resolution Decoder network for
fundus lesion segmentation. It integrates a high-resolution representation
learning module to capture fine-grained local features and a high-resolution
fusion module to fuse multi-scale predictions. Our method effectively improves
the overall segmentation accuracy of fundus lesions while consuming reasonable
memory and computational overhead, and maintaining satisfying inference speed.
Experimental results on the IDRID and DDR datasets demonstrate the
effectiveness of our method. Code is available at
https://github.com/CVIU-CSU/HRDecoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures, accepted by MICCAI 2024, the revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Face Reconstruction from Face Embeddings using Adapter to a Face
  Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hatef Otroshi Shahreza, Anjith George, Sébastien Marcel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition systems extract embedding vectors from face images and use
these embeddings to verify or identify individuals. Face reconstruction attack
(also known as template inversion) refers to reconstructing face images from
face embeddings and using the reconstructed face image to enter a face
recognition system. In this paper, we propose to use a face foundation model to
reconstruct face images from the embeddings of a blackbox face recognition
model. The foundation model is trained with 42M images to generate face images
from the facial embeddings of a fixed face recognition model. We propose to use
an adapter to translate target embeddings into the embedding space of the
foundation model. The generated images are evaluated on different face
recognition models and different datasets, demonstrating the effectiveness of
our method to translate embeddings of different face recognition models. We
also evaluate the transferability of reconstructed face images when attacking
different face recognition models. Our experimental results show that our
reconstructed face images outperform previous reconstruction attacks against
face recognition models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy Score-based Pseudo-Label Filtering and Adaptive Loss for
  Imbalanced Semi-supervised SAR target recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzheng Zhang, Yuqing Luo, Guopeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic target recognition (ATR) is an important use case for synthetic
aperture radar (SAR) image interpretation. Recent years have seen significant
advancements in SAR ATR technology based on semi-supervised learning. However,
existing semi-supervised SAR ATR algorithms show low recognition accuracy in
the case of class imbalance. This work offers a non-balanced semi-supervised
SAR target recognition approach using dynamic energy scores and adaptive loss.
First, an energy score-based method is developed to dynamically select
unlabeled samples near to the training distribution as pseudo-labels during
training, assuring pseudo-label reliability in long-tailed distribution
circumstances. Secondly, loss functions suitable for class imbalances are
proposed, including adaptive margin perception loss and adaptive hard triplet
loss, the former offsets inter-class confusion of classifiers, alleviating the
imbalance issue inherent in pseudo-label generation. The latter effectively
tackles the model's preference for the majority class by focusing on complex
difficult samples during training. Experimental results on extremely imbalanced
SAR datasets demonstrate that the proposed method performs well under the dual
constraints of scarce labels and data imbalance, effectively overcoming the
model bias caused by data imbalance and achieving high-precision target
recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Act in Collusion: A Persistent Distributed Multi-Target Backdoor in
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Liu, Wu Yang, Chen Xu, Jiguang Lv, Huanran Wang, Yuhang Zhang, Shuchun Xu, Dapeng Man
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning, a novel paradigm designed to protect data privacy, is
vulnerable to backdoor attacks due to its distributed nature. Current research
often designs attacks based on a single attacker with a single backdoor,
overlooking more realistic and complex threats in federated learning. We
propose a more practical threat model for federated learning: the distributed
multi-target backdoor. In this model, multiple attackers control different
clients, embedding various triggers and targeting different classes,
collaboratively implanting backdoors into the global model via central
aggregation. Empirical validation shows that existing methods struggle to
maintain the effectiveness of multiple backdoors in the global model. Our key
insight is that similar backdoor triggers cause parameter conflicts and
injecting new backdoors disrupts gradient directions, significantly weakening
some backdoors performance. To solve this, we propose a Distributed
Multi-Target Backdoor Attack (DMBA), ensuring efficiency and persistence of
backdoors from different malicious clients. To avoid parameter conflicts, we
design a multi-channel dispersed frequency trigger strategy to maximize trigger
differences. To mitigate gradient interference, we introduce backdoor replay in
local training to neutralize conflicting gradients. Extensive validation shows
that 30 rounds after the attack, Attack Success Rates of three different
backdoors from various clients remain above 93%. The code will be made publicly
available after the review period.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-supervised Representation Learning for Cell Event Recognition
  through Time Arrow Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cangxiong Chen, Vinay P. Namboodiri, Julia E. Sero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The spatio-temporal nature of live-cell microscopy data poses challenges in
the analysis of cell states which is fundamental in bioimaging. Deep-learning
based segmentation or tracking methods rely on large amount of high quality
annotations to work effectively. In this work, we explore an alternative
solution: using feature maps obtained from self-supervised representation
learning (SSRL) on time arrow prediction (TAP) for the downstream supervised
task of cell event recognition. We demonstrate through extensive experiments
and analysis that this approach can achieve better performance with limited
annotation compared to models trained from end to end using fully supervised
approach. Our analysis also provides insight into applications of the SSRL
using TAP in live-cell microscopy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROBIN: Robust and Invisible Watermarks for <span class="highlight-title">Diffusion</span> Models with
  Adversarial Optimization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huayang Huang, Yu Wu, Qian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking generative content serves as a vital tool for authentication,
ownership protection, and mitigation of potential misuse. Existing watermarking
methods face the challenge of balancing robustness and concealment. They
empirically inject a watermark that is both invisible and robust and passively
achieve concealment by limiting the strength of the watermark, thus reducing
the robustness. In this paper, we propose to explicitly introduce a watermark
hiding process to actively achieve concealment, thus allowing the embedding of
stronger watermarks. To be specific, we implant a robust watermark in an
intermediate diffusion state and then guide the model to hide the watermark in
the final generated image. We employ an adversarial optimization algorithm to
produce the optimal hiding prompt guiding signal for each watermark. The prompt
embedding is optimized to minimize artifacts in the generated image, while the
watermark is optimized to achieve maximum strength. The watermark can be
verified by reversing the generation process. Experiments on various diffusion
models demonstrate the watermark remains verifiable even under significant
image tampering and shows superior invisibility compared to other
state-of-the-art robust watermarking methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedRISE: Rating Induced Sign Election of Gradients for Byzantine
  Tolerant Federated Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Geo Benjamin, Mothilal Asokan, Mohammad Yaqub, Karthik Nandakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most common defense strategies against model poisoning in
federated learning is to employ a robust aggregator mechanism that makes the
training more resilient. Many of the existing Byzantine robust aggregators
provide theoretical guarantees and are empirically effective against certain
categories of attacks. However, we observe that certain high-strength attacks
can subvert the aggregator and collapse the training. In addition, most
aggregators require identifying tolerant settings to converge. Impact of
attacks becomes more pronounced when the number of Byzantines is near-majority,
and becomes harder to evade if the attacker is omniscient with access to data,
honest updates and aggregation methods. Motivated by these observations, we
develop a robust aggregator called FedRISE for cross-silo FL that is consistent
and less susceptible to poisoning updates by an omniscient attacker. The
proposed method explicitly determines the optimal direction of each gradient
through a sign-voting strategy that uses variance-reduced sparse gradients. We
argue that vote weighting based on the cosine similarity of raw gradients is
misleading, and we introduce a sign-based gradient valuation function that
ignores the gradient magnitude. We compare our method against 8 robust
aggregators under 6 poisoning attacks on 3 datasets and architectures. Our
results show that existing robust aggregators collapse for at least some
attacks under severe settings, while FedRISE demonstrates better robustness
because of a stringent gradient inclusion formulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a work under submission/review process</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mamba<span class="highlight-title">PEFT</span>: Exploring <span class="highlight-title">Parameter-Efficient Fine-Tuning</span> for Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masakazu Yoshimura, Teruaki Hayashi, Yota Maeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An ecosystem of Transformer-based models has been established by building
large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a
crucial technology for deploying these models to downstream tasks with minimal
cost while achieving effective performance. Recently, Mamba, a State Space
Model (SSM)-based model, has attracted attention as a potential alternative to
Transformers. While many large-scale Mamba-based models have been proposed,
efficiently adapting pre-trained Mamba-based models to downstream tasks remains
unexplored. In this paper, we conduct an exploratory analysis of PEFT methods
for Mamba. We investigate the effectiveness of existing PEFT methods for
Transformers when applied to Mamba. We also modify these methods to better
align with the Mamba architecture. Additionally, we propose new Mamba-specific
PEFT methods that leverage the distinctive structure of Mamba. Our experiments
indicate that PEFT performs more effectively for Mamba than Transformers.
Lastly, we demonstrate how to effectively combine multiple PEFT methods and
provide a framework that outperforms previous works. To ensure reproducibility,
we will release the code after publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Edge Computing-Based Solution for Real-Time Leaf Disease
  Classification using Thermal Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Públio Elon Correa da Silva, Jurandy Almeida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) technologies can transform agriculture by improving crop
health monitoring and management, thus improving food safety. In this paper, we
explore the potential of edge computing for real-time classification of leaf
diseases using thermal imaging. We present a thermal image dataset for plant
disease classification and evaluate deep learning models, including
InceptionV3, MobileNetV1, MobileNetV2, and VGG-16, on resource-constrained
devices like the Raspberry Pi 4B. Using pruning and quantization-aware
training, these models achieve inference times up to 1.48x faster on Edge TPU
Max for VGG16, and up to 2.13x faster with precision reduction on Intel NCS2
for MobileNetV1, compared to high-end GPUs like the RTX 3090, while maintaining
state-of-the-art accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Enhancement of Haar Cascade Algorithm Applied to Face Recognition for
  Gate Pass Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clarence A. Antipona, Romeo R. Magsino, Raymund M. Dioses, Khatalyn E. Mata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study is focused on enhancing the Haar Cascade Algorithm to decrease the
false positive and false negative rate in face matching and face detection to
increase the accuracy rate even under challenging conditions. The face
recognition library was implemented with Haar Cascade Algorithm in which the
128-dimensional vectors representing the unique features of a face are encoded.
A subprocess was applied where the grayscale image from Haar Cascade was
converted to RGB to improve the face encoding. Logical process and face
filtering are also used to decrease non-face detection. The Enhanced Haar
Cascade Algorithm produced a 98.39% accuracy rate (21.39% increase), 63.59%
precision rate, 98.30% recall rate, and 72.23% in F1 Score. In comparison, the
Haar Cascade Algorithm achieved a 46.70% to 77.00% accuracy rate, 44.15%
precision rate, 98.61% recall rate, and 47.01% in F1 Score. Both algorithms
used the Confusion Matrix Test with 301,950 comparisons using the same dataset
of 550 images. The 98.39% accuracy rate shows a significant decrease in false
positive and false negative rates in facial recognition. Face matching and face
detection are more accurate in images with complex backgrounds, lighting
variations, and occlusions, or even those with similar attributes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalize or Detect? Towards Robust Semantic Segmentation Under
  Multiple Distribution Shifts <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhitong Gao, Bingnan Li, Mathieu Salzmann, Xuming He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In open-world scenarios, where both novel classes and domains may exist, an
ideal segmentation model should detect anomaly classes for safety and
generalize to new domains. However, existing methods often struggle to
distinguish between domain-level and semantic-level distribution shifts,
leading to poor out-of-distribution (OOD) detection or domain generalization
performance. In this work, we aim to equip the model to generalize effectively
to covariate-shift regions while precisely identifying semantic-shift regions.
To achieve this, we design a novel generative augmentation method to produce
coherent images that incorporate both anomaly (or novel) objects and various
covariate shifts at both image and object levels. Furthermore, we introduce a
training strategy that recalibrates uncertainty specifically for semantic
shifts and enhances the feature extractor to align features associated with
domain shifts. We validate the effectiveness of our method across benchmarks
featuring both semantic and domain shifts. Our method achieves state-of-the-art
performance across all benchmarks for both OOD detection and domain
generalization. Code is available at
https://github.com/gaozhitong/MultiShiftSeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM
  Data Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting dataset contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is sensitive to varying degrees of contamination and can
highlight significant performance improvements due to leakage of the training
set of multimodal benchmarks. Furthermore, We also explore the possibility of
contamination originating from the pre-training phase of LLMs used by MLLMs and
the fine-tuning phase of MLLMs, offering new insights into the stages at which
contamination may be introduced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SA3DIP: Segment Any 3D Instance with Potential 3D Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Yang, Xu Gu, Xingyilang Yin, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of 2D foundation models has sparked research into adapting
them for open-world 3D instance segmentation. Recent methods introduce a
paradigm that leverages superpoints as geometric primitives and incorporates 2D
multi-view masks from Segment Anything model (SAM) as merging guidance,
achieving outstanding zero-shot instance segmentation results. However, the
limited use of 3D priors restricts the segmentation performance. Previous
methods calculate the 3D superpoints solely based on estimated normal from
spatial coordinates, resulting in under-segmentation for instances with similar
geometry. Besides, the heavy reliance on SAM and hand-crafted algorithms in 2D
space suffers from over-segmentation due to SAM's inherent part-level
segmentation tendency. To address these issues, we propose SA3DIP, a novel
method for Segmenting Any 3D Instances via exploiting potential 3D Priors.
Specifically, on one hand, we generate complementary 3D primitives based on
both geometric and textural priors, which reduces the initial errors that
accumulate in subsequent procedures. On the other hand, we introduce
supplemental constraints from the 3D space by using a 3D detector to guide a
further merging process. Furthermore, we notice a considerable portion of
low-quality ground truth annotations in ScanNetV2 benchmark, which affect the
fair evaluations. Thus, we present ScanNetV2-INS with complete ground truth
labels and supplement additional instances for 3D class-agnostic instance
segmentation. Experimental evaluations on various 2D-3D datasets demonstrate
the effectiveness and robustness of our approach. Our code and proposed
ScanNetV2-INS dataset are available HERE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GS2Pose: Tow-stage 6D Object Pose Estimation Guided by Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jilan Mei, Junbo Li, Cai Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new method for accurate and robust 6D pose estimation
of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose
can utilize the reconstruction results without requiring a high-quality CAD
model, which means it only requires segmented RGBD images as input.
Specifically, GS2Pose employs a two-stage structure consisting of coarse
estimation followed by refined estimation. In the coarse stage, a lightweight
U-Net network with a polarization attention mechanism, called Pose-Net, is
designed. By using the 3DGS model for supervised training, Pose-Net can
generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose
formulates a pose regression algorithm following the idea of reprojection or
Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to
extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that
refines the coarse pose by comparing the input images with the rendered images.
GS-Refiner also selectively updates parameters in the 3DGS model to achieve
environmental adaptation, thereby enhancing the algorithm's robustness and
flexibility to illuminative variation, occlusion, and other challenging
disruptive factors. GS2Pose was evaluated through experiments conducted on the
LineMod dataset, where it was compared with similar algorithms, yielding highly
competitive results. The code for GS2Pose will soon be released on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VQA$^2$:Visual Question Answering for Video Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Jia, Zicheng Zhang, Jiaying Qian, Haoning Wu, Wei Sun, Chunyi Li, Xiaohong Liu, Weisi Lin, Guangtao Zhai, Xiongkuo Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent and proliferation of large multi-modal models (LMMs) have
introduced a new paradigm to video-related computer vision fields, including
training and inference methods based on visual question answering (VQA). These
methods enable models to handle multiple downstream tasks robustly. Video
Quality Assessment (VQA), a classic field in low-level visual quality
evaluation, originally focused on quantitative video quality scoring. However,
driven by advances in LMMs, it is now evolving towards more comprehensive
visual quality understanding tasks. Visual question answering has significantly
improved low-level visual evaluation within the image domain recently. However,
related work is almost nonexistent in the video domain, leaving substantial
room for improvement. To address this gap, we introduce the VQA2 Instruction
Dataset the first visual question answering instruction dataset entirely
focuses on video quality assessment, and based on it, we propose the VQA2
series models The VQA2 Instruction Dataset consists of three stages and covers
various video types, containing 157,735 instruction question-answer pairs,
including both manually annotated and synthetic data. We conduct extensive
experiments on both video quality scoring and video quality understanding
tasks. Results demonstrate that the VQA2 series models achieve state-of-the-art
(SOTA) performance in quality scoring tasks, and their performance in visual
quality question answering surpasses the renowned GPT-4o. Additionally, our
final model, the VQA2-Assistant, performs well across both scoring and
question-answering tasks, validating its versatility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harmformer: Harmonic Networks Meet <span class="highlight-title">Transformer</span>s for Continuous
  Roto-Translation Equivariance <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomáš Karella, Adam Harmanec, Jan Kotera, Jan Blažek, Filip Šroubek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CNNs exhibit inherent equivariance to image translation, leading to efficient
parameter and data usage, faster learning, and improved robustness. The concept
of translation equivariant networks has been successfully extended to rotation
transformation using group convolution for discrete rotation groups and
harmonic functions for the continuous rotation group encompassing $360^\circ$.
We explore the compatibility of the SA mechanism with full rotation
equivariance, in contrast to previous studies that focused on discrete
rotation. We introduce the Harmformer, a harmonic transformer with a
convolutional stem that achieves equivariance for both translation and
continuous rotation. Accompanied by an end-to-end equivariance proof, the
Harmformer not only outperforms previous equivariant transformers, but also
demonstrates inherent stability under any continuous rotation, even without
seeing rotated samples during training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appears in NeurIPS 2024 Workshop on Symmetry and Geometry in Neural
  Representations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sub-DM:Subspace <span class="highlight-title">Diffusion</span> Model with Orthogonal Decomposition for MRI
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Guan, Qinrong Cai, Wei Li, Qiuyun Fan, Dong Liang, Qiegen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion model-based approaches recently achieved re-markable success in MRI
reconstruction, but integration into clinical routine remains challenging due
to its time-consuming convergence. This phenomenon is partic-ularly notable
when directly apply conventional diffusion process to k-space data without
considering the inherent properties of k-space sampling, limiting k-space
learning efficiency and image reconstruction quality. To tackle these
challenges, we introduce subspace diffusion model with orthogonal
decomposition, a method (referred to as Sub-DM) that restrict the diffusion
process via projections onto subspace as the k-space data distribution evolves
toward noise. Particularly, the subspace diffusion model circumvents the
inference challenges posed by the com-plex and high-dimensional characteristics
of k-space data, so the highly compact subspace ensures that diffusion process
requires only a few simple iterations to produce accurate prior information.
Furthermore, the orthogonal decomposition strategy based on wavelet transform
hin-ders the information loss during the migration of the vanilla diffusion
process to the subspace. Considering the strate-gy is approximately reversible,
such that the entire pro-cess can be reversed. As a result, it allows the
diffusion processes in different spaces to refine models through a mutual
feedback mechanism, enabling the learning of ac-curate prior even when dealing
with complex k-space data. Comprehensive experiments on different datasets
clearly demonstrate that the superiority of Sub-DM against state of-the-art
methods in terms of reconstruction speed and quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deferred Poisoning: Making the Model More Vulnerable via Hessian
  Singularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao He, Jinyu Tian, Xianwei Zheng, Li Dong, Yuanman Li, Leo Yu Zhang, Jiantao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that deep learning models are very vulnerable to
poisoning attacks. Many defense methods have been proposed to address this
issue. However, traditional poisoning attacks are not as threatening as
commonly believed. This is because they often cause differences in how the
model performs on the training set compared to the validation set. Such
inconsistency can alert defenders that their data has been poisoned, allowing
them to take the necessary defensive actions. In this paper, we introduce a
more threatening type of poisoning attack called the Deferred Poisoning Attack.
This new attack allows the model to function normally during the training and
validation phases but makes it very sensitive to evasion attacks or even
natural noise. We achieve this by ensuring the poisoned model's loss function
has a similar value as a normally trained model at each input sample but with a
large local curvature. A similar model loss ensures that there is no obvious
inconsistency between the training and validation accuracy, demonstrating high
stealthiness. On the other hand, the large curvature implies that a small
perturbation may cause a significant increase in model loss, leading to
substantial performance degradation, which reflects a worse robustness. We
fulfill this purpose by making the model have singular Hessian information at
the optimal point via our proposed Singularization Regularization term. We have
conducted both theoretical and empirical analyses of the proposed method and
validated its effectiveness through experiments on image classification tasks.
Furthermore, we have confirmed the hazards of this form of poisoning attack
under more general scenarios using natural noise, offering a new perspective
for research in the field of security.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homotopy Continuation Made Easy: Regression-based Online Simulation of
  Starting Problem-Solution Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Zhang, Zijia Dai, Wanting Xu, Laurent Kneip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While automatically generated polynomial elimination templates have sparked
great progress in the field of 3D computer vision, there remain many problems
for which the degree of the constraints or the number of unknowns leads to
intractability. In recent years, homotopy continuation has been introduced as a
plausible alternative. However, the method currently depends on expensive
parallel tracking of all possible solutions in the complex domain, or a
classification network for starting problem-solution pairs trained over a
limited set of real-world examples. Our innovation consists of employing a
regression network trained in simulation to directly predict a solution from
input correspondences, followed by an online simulator that invents a
consistent problem-solution pair. Subsequently, homotopy continuation is
applied to track that single solution back to the original problem. We apply
this elegant combination to generalized camera resectioning, and also introduce
a new solution to the challenging generalized relative pose and scale problem.
As demonstrated, the proposed method successfully compensates the raw error
committed by the regressor alone, and leads to state-of-the-art efficiency and
success rates while running on CPU resources, only.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document
  VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marlon Tobaben, Mohamed Ali Souibgui, Rubèn Tito, Khanh Nguyen, Raouf Kerkouche, Kangsoo Jung, Joonas Jälkö, Lei Kang, Andrey Barsky, Vincent Poulain d'Andecy, Aurélie Joseph, Aashiq Muhamed, Kevin Kuo, Virginia Smith, Yusuke Yamasaki, Takumi Fukami, Kenta Niwa, Iifan Tyou, Hiro Ishii, Rio Yokota, Ragul N, Rintu Kutum, Josep Llados, Ernest Valveny, Antti Honkela, Mario Fritz, Dimosthenis Karatzas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA)
competition challenged the community to develop provably private and
communication-efficient solutions in a federated setting for a real-life use
case: invoice processing. The competition introduced a dataset of real invoice
documents, along with associated questions and answers requiring information
extraction and reasoning over the document images. Thereby, it brings together
researchers and expertise from the document analysis, privacy, and federated
learning communities. Participants fine-tuned a pre-trained, state-of-the-art
Document Visual Question Answering model provided by the organizers for this
new domain, mimicking a typical federated invoice processing setup. The base
model is a multi-modal generative language model, and sensitive information
could be exposed through either the visual or textual input modality.
Participants proposed elegant solutions to reduce communication costs while
maintaining a minimum utility threshold in track 1 and to protect all
information from each document provider using differential privacy in track 2.
The competition served as a new testbed for developing and testing private
federated learning methods, simultaneously raising awareness about privacy
within the document image analysis and recognition community. Ultimately, the
competition analysis provides best practices and recommendations for
successfully running privacy-focused federated learning challenges in the
future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relation Learning and Aggregate-attention for Multi-person Motion
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kehua Qu, Rui Ding, Jin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-person motion prediction is an emerging and intricate task with broad
real-world applications. Unlike single person motion prediction, it considers
not just the skeleton structures or human trajectories but also the
interactions between others. Previous methods use various networks to achieve
impressive predictions but often overlook that the joints relations within an
individual (intra-relation) and interactions among groups (inter-relation) are
distinct types of representations. These methods often lack explicit
representation of inter&intra-relations, and inevitably introduce undesired
dependencies. To address this issue, we introduce a new collaborative framework
for multi-person motion prediction that explicitly modeling these relations:a
GCN-based network for intra-relations and a novel reasoning network for
inter-relations.Moreover, we propose a novel plug-and-play aggregation module
called the Interaction Aggregation Module (IAM), which employs an
aggregate-attention mechanism to seamlessly integrate these relations.
Experiments indicate that the module can also be applied to other dual-path
models. Extensive experiments on the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS-3D, as
well as synthesized datasets Mix1 & Mix2 (9 to 15 persons), demonstrate that
our method achieves state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Fourier Filtering Network with Contrastive Learning for
  UAV-based Unaligned Bi-modal Salient Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Lyu, Pak-Hei Yeung, Xiufei Cheng, Xiaosheng Yu, Chengdong Wu, Jagath C. Rajapakse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial vehicle (UAV)-based bi-modal salient object detection (BSOD)
aims to segment salient objects in a scene utilizing complementary cues in
unaligned RGB and thermal image pairs. However, the high computational expense
of existing UAV-based BSOD models limits their applicability to real-world UAV
devices. To address this problem, we propose an efficient Fourier filter
network with contrastive learning that achieves both real-time and accurate
performance. Specifically, we first design a semantic contrastive alignment
loss to align the two modalities at the semantic level, which facilitates
mutual refinement in a parameter-free way. Second, inspired by the fast Fourier
transform that obtains global relevance in linear complexity, we propose
synchronized alignment fusion, which aligns and fuses bi-modal features in the
channel and spatial dimensions by a hierarchical filtering mechanism. Our
proposed model, AlignSal, reduces the number of parameters by 70.0%, decreases
the floating point operations by 49.4%, and increases the inference speed by
152.5% compared to the cutting-edge BSOD model (i.e., MROS). Extensive
experiments on the UAV RGB-T 2400 and three weakly aligned datasets demonstrate
that AlignSal achieves both real-time inference speed and better performance
and generalizability compared to sixteen state-of-the-art BSOD models across
most evaluation metrics. In addition, our ablation studies further verify
AlignSal's potential in boosting the performance of existing aligned BSOD
models on UAV-based unaligned data. The code is available at:
https://github.com/JoshuaLPF/AlignSal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PX2Tooth: Reconstructing the 3D Point Cloud Teeth from a Single
  Panoramic X-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Ma, Huikai Wu, Zikai Xiao, Yang Feng, Jian Wu, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing the 3D anatomical structures of the oral cavity, which
originally reside in the cone-beam CT (CBCT), from a single 2D Panoramic
X-ray(PX) remains a critical yet challenging task, as it can effectively reduce
radiation risks and treatment costs during the diagnostic in digital dentistry.
However, current methods are either error-prone or only trained/evaluated on
small-scale datasets (less than 50 cases), resulting in compromised
trustworthiness. In this paper, we propose PX2Tooth, a novel approach to
reconstruct 3D teeth using a single PX image with a two-stage framework. First,
we design the PXSegNet to segment the permanent teeth from the PX images,
providing clear positional, morphological, and categorical information for each
tooth. Subsequently, we design a novel tooth generation network (TGNet) that
learns to transform random point clouds into 3D teeth. TGNet integrates the
segmented patch information and introduces a Prior Fusion Module (PFM) to
enhance the generation quality, especially in the root apex region. Moreover,
we construct a dataset comprising 499 pairs of CBCT and Panoramic X-rays.
Extensive experiments demonstrate that PX2Tooth can achieve an Intersection
over Union (IoU) of 0.793, significantly surpassing previous methods,
underscoring the great potential of artificial intelligence in digital
dentistry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ma W, Wu H, Xiao Z, et al. PX2Tooth: Reconstructing the 3D Point
  Cloud Teeth from a Single Panoramic X-Ray[C]//International Conference on
  Medical Image Computing and Computer-Assisted Intervention. Cham: Springer
  Nature Switzerland, 2024: 411-421</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimation of Psychosocial Work Environment Exposures Through Video
  Object Detection. Proof of Concept Using CCTV Footage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claus D. Hansen, Thuy Hai Le, David Campos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper examines the use of computer vision algorithms to estimate aspects
of the psychosocial work environment using CCTV footage. We present a proof of
concept for a methodology that detects and tracks people in video footage and
estimates interactions between customers and employees by estimating their
poses and calculating the duration of their encounters. We propose a pipeline
that combines existing object detection and tracking algorithms (YOLOv8 and
DeepSORT) with pose estimation algorithms (BlazePose) to estimate the number of
customers and employees in the footage as well as the duration of their
encounters. We use a simple rule-based approach to classify the interactions as
positive, neutral or negative based on three different criteria: distance,
duration and pose. The proposed methodology is tested on a small dataset of
CCTV footage. While the data is quite limited in particular with respect to the
quality of the footage, we have chosen this case as it represents a typical
setting where the method could be applied. The results show that the object
detection and tracking part of the pipeline has a reasonable performance on the
dataset with a high degree of recall and reasonable accuracy. At this stage,
the pose estimation is still limited to fully detect the type of interactions
due to difficulties in tracking employees in the footage. We conclude that the
method is a promising alternative to self-reported measures of the psychosocial
work environment and could be used in future studies to obtain external
observations of the work environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, presented at IWOAR 9th International Workshop on
  Sensor-Based Activity Recognition and Artificial Intelligence, September
  26-27, Potsdam, Germany</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Dynamic MRI Reconstruction with Global-to-local <span class="highlight-title">Diffusion</span>
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Guan, Kunlong Zhang, Qi Qi, Dong Wang, Ziwen Ke, Shaoyu Wang, Dong Liang, Qiegen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently demonstrated considerable advancement in the
generation and reconstruction of magnetic resonance imaging (MRI) data. These
models exhibit great potential in handling unsampled data and reducing noise,
highlighting their promise as generative models. However, their application in
dynamic MRI remains relatively underexplored. This is primarily due to the
substantial amount of fully-sampled data typically required for training, which
is difficult to obtain in dynamic MRI due to its spatio-temporal complexity and
high acquisition costs. To address this challenge, we propose a dynamic MRI
reconstruction method based on a time-interleaved acquisition scheme, termed
the Glob-al-to-local Diffusion Model. Specifically, fully encoded
full-resolution reference data are constructed by merging under-sampled k-space
data from adjacent time frames, generating two distinct bulk training datasets
for global and local models. The global-to-local diffusion framework
alternately optimizes global information and local image details, enabling
zero-shot reconstruction. Extensive experiments demonstrate that the proposed
method performs well in terms of noise reduction and detail preservation,
achieving reconstruction quality comparable to that of supervised approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ These Maps Are Made by Propagation: Adapting Deep Stereo Networks to
  Road Scenarios with Decisive Disparity <span class="highlight-title">Diffusion</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuang-Wei Liu, Yikang Zhang, Qijun Chen, Ioannis Pitas, Rui Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereo matching has emerged as a cost-effective solution for road surface 3D
reconstruction, garnering significant attention towards improving both
computational efficiency and accuracy. This article introduces decisive
disparity diffusion (D3Stereo), marking the first exploration of dense deep
feature matching that adapts pre-trained deep convolutional neural networks
(DCNNs) to previously unseen road scenarios. A pyramid of cost volumes is
initially created using various levels of learned representations.
Subsequently, a novel recursive bilateral filtering algorithm is employed to
aggregate these costs. A key innovation of D3Stereo lies in its alternating
decisive disparity diffusion strategy, wherein intra-scale diffusion is
employed to complete sparse disparity images, while inter-scale inheritance
provides valuable prior information for higher resolutions. Extensive
experiments conducted on our created UDTIRI-Stereo and Stereo-Road datasets
underscore the effectiveness of D3Stereo strategy in adapting pre-trained DCNNs
and its superior performance compared to all other explicit programming-based
algorithms designed specifically for road surface 3D reconstruction. Additional
experiments conducted on the Middlebury dataset with backbone DCNNs pre-trained
on the ImageNet database further validate the versatility of D3Stereo strategy
in tackling general stereo matching problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining Human Activity Recognition with SHAP: Validating Insights
  with Perturbation and Quantitative Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Tempel, Espen Alexander F. Ihlen, Lars Adde, Inga Strümke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Human Activity Recognition (HAR), understanding the intricacy of body
movements within high-risk applications is essential. This study uses SHapley
Additive exPlanations (SHAP) to explain the decision-making process of Graph
Convolution Networks (GCNs) when classifying activities with skeleton data. We
employ SHAP to explain two real-world datasets: one for cerebral palsy (CP)
classification and the widely used NTU RGB+D 60 action recognition dataset. To
test the explanation, we introduce a novel perturbation approach that modifies
the model's edge importance matrix, allowing us to evaluate the impact of
specific body key points on prediction outcomes. To assess the fidelity of our
explanations, we employ informed perturbation, targeting body key points
identified as important by SHAP and comparing them against random perturbation
as a control condition. This perturbation enables a judgment on whether the
body key points are truly influential or non-influential based on the SHAP
values. Results on both datasets show that body key points identified as
important through SHAP have the largest influence on the accuracy, specificity,
and sensitivity metrics. Our findings highlight that SHAP can provide granular
insights into the input feature contribution to the prediction outcome of GCNs
in HAR tasks. This demonstrates the potential for more interpretable and
trustworthy models in high-stakes applications like healthcare or
rehabilitation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuning Vision-Language Model for Automated Engineering Drawing
  Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Tayyab Khan, Lequn Chen, Ye Han Ng, Wenhe Feng, Nicholas Yew Jin Tan, Seung Ki Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric Dimensioning and Tolerancing (GD&T) plays a critical role in
manufacturing by defining acceptable variations in part features to ensure
component quality and functionality. However, extracting GD&T information from
2D engineering drawings is a time-consuming and labor-intensive task, often
relying on manual efforts or semi-automated tools. To address these challenges,
this study proposes an automated and computationally efficient GD&T extraction
method by fine-tuning Florence-2, an open-source vision-language model (VLM).
The model is trained on a dataset of 400 drawings with ground truth annotations
provided by domain experts. For comparison, two state-of-the-art closed-source
VLMs, GPT-4o and Claude-3.5-Sonnet, are evaluated on the same dataset. All
models are assessed using precision, recall, F1-score, and hallucination
metrics. Due to the computational cost and impracticality of fine-tuning large
closed-source VLMs for domain-specific tasks, GPT-4o and Claude-3.5-Sonnet are
evaluated in a zero-shot setting. In contrast, Florence-2, a smaller model with
0.23 billion parameters, is optimized through full-parameter fine-tuning across
three distinct experiments, each utilizing datasets augmented to different
levels. The results show that Florence-2 achieves a 29.95% increase in
precision, a 37.75% increase in recall, a 52.40% improvement in F1-score, and a
43.15% reduction in hallucination rate compared to the best-performing
closed-source model. These findings highlight the effectiveness of fine-tuning
smaller, open-source VLMs like Florence-2, offering a practical and efficient
solution for automated GD&T extraction to support downstream manufacturing
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper has been submitted to the 9th International Conference on
  Innovation in Artificial Intelligence (ICIAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical
  Object Rearrangement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Lu, Jianbo Ye, John Leonard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for
detecting physical object rearrangements in 3D scenes. Our approach estimates
3D object-level changes by comparing two sets of unaligned images taken at
different times. Leveraging 3DGS's novel view rendering and EfficientSAM's
zero-shot segmentation capabilities, we detect 2D object-level changes, which
are then associated and fused across views to estimate 3D changes. Our method
can detect changes in cluttered environments using sparse post-change images
within as little as 18s, using as few as a single new image. It does not rely
on depth input, user instructions, object classes, or object models -- An
object is recognized simply if it has been re-arranged. Our approach is
evaluated on both public and self-collected real-world datasets, achieving up
to 14% higher accuracy and three orders of magnitude faster performance
compared to the state-of-the-art radiance-field-based change detection method.
This significant performance boost enables a broad range of downstream
applications, where we highlight three key use cases: object reconstruction,
robot workspace reset, and 3DGS model update. Our code and data will be made
available at https://github.com/520xyxyzq/3DGS-CD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Based Multi-Modal Sensor Fusion for Autonomous Driving <span class="chip">ICPR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Depanshu Sani, Saket Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing demand for robust scene understanding in mobile robotics and
autonomous driving has highlighted the importance of integrating multiple
sensing modalities. By combining data from diverse sensors like cameras and
LIDARs, fusion techniques can overcome the limitations of individual sensors,
enabling a more complete and accurate perception of the environment. We
introduce a novel approach to multi-modal sensor fusion, focusing on developing
a graph-based state representation that supports critical decision-making
processes in autonomous driving. We present a Sensor-Agnostic Graph-Aware
Kalman Filter [3], the first online state estimation technique designed to fuse
multi-modal graphs derived from noisy multi-sensor data. The estimated
graph-based state representations serve as a foundation for advanced
applications like Multi-Object Tracking (MOT), offering a comprehensive
framework for enhancing the situational awareness and safety of autonomous
systems. We validate the effectiveness of our proposed framework through
extensive experiments conducted on both synthetic and real-world driving
datasets (nuScenes). Our results showcase an improvement in MOTA and a
reduction in estimated position errors (MOTP) and identity switches (IDS) for
tracked objects using the SAGA-KF. Furthermore, we highlight the capability of
such a framework to develop methods that can leverage heterogeneous information
(like semantic objects and geometric structures) from various sensing
modalities, enabling a more holistic approach to scene understanding and
enhancing the safety and effectiveness of autonomous systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An extended abstract accepted at Young Researchers' Symposium, ICVGIP
  '24. This extended abstract contains the following: 1. Short summary of our
  work, SAGA-KF, accepted at ICPR'24. 2. A proposal that was awarded the
  Qualcomm Innovation Fellowship'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OccLoff: Learning Optimized Feature Fusion for 3D Occupancy Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Zhang, Yiran Ding, Zixin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D semantic occupancy prediction is crucial for finely representing the
surrounding environment, which is essential for ensuring the safety in
autonomous driving. Existing fusion-based occupancy methods typically involve
performing a 2D-to-3D view transformation on image features, followed by
computationally intensive 3D operations to fuse these with LiDAR features,
leading to high computational costs and reduced accuracy. Moreover, current
research on occupancy prediction predominantly focuses on designing specific
network architectures, often tailored to particular models, with limited
attention given to the more fundamental aspect of semantic feature learning.
This gap hinders the development of more transferable methods that could
enhance the performance of various occupancy models. To address these
challenges, we propose OccLoff, a framework that Learns to Optimize Feature
Fusion for 3D occupancy prediction. Specifically, we introduce a sparse fusion
encoder with entropy masks that directly fuses 3D and 2D features, improving
model accuracy while reducing computational overhead. Additionally, we propose
a transferable proxy-based loss function and an adaptive hard sample weighting
algorithm, which enhance the performance of several state-of-the-art methods.
Extensive evaluations on the nuScenes and SemanticKITTI benchmarks demonstrate
the superiority of our framework, and ablation studies confirm the
effectiveness of each proposed module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AMNCutter: Affinity-Attention-Guided Multi-View Normalized Cutter for
  Unsupervised Surgical Instrument Segmentation <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Sheng, Jianan Fan, Dongnan Liu, Ron Kikinis, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical instrument segmentation (SIS) is pivotal for robotic-assisted
minimally invasive surgery, assisting surgeons by identifying surgical
instruments in endoscopic video frames. Recent unsupervised surgical instrument
segmentation (USIS) methods primarily rely on pseudo-labels derived from
low-level features such as color and optical flow, but these methods show
limited effectiveness and generalizability in complex and unseen endoscopic
scenarios. In this work, we propose a label-free unsupervised model featuring a
novel module named Multi-View Normalized Cutter (m-NCutter). Different from
previous USIS works, our model is trained using a graph-cutting loss function
that leverages patch affinities for supervision, eliminating the need for
pseudo-labels. The framework adaptively determines which affinities from which
levels should be prioritized. Therefore, the low- and high-level features and
their affinities are effectively integrated to train a label-free unsupervised
model, showing superior effectiveness and generalization ability. We conduct
comprehensive experiments across multiple SIS datasets to validate our
approach's state-of-the-art (SOTA) performance, robustness, and exceptional
potential as a pre-trained model. Our code is released at
https://github.com/MingyuShengSMY/AMNCutter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by the 2025 IEEE Winter Conference on
  Applications of Computer Vision (WACV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Where Do We Stand with Implicit Neural Representations? A Technical and
  Performance <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amer Essakine, Yanqi Cheng, Chun-Wun Cheng, Lipei Zhang, Zhongying Deng, Lei Zhu, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representations (INRs) have emerged as a paradigm in
knowledge representation, offering exceptional flexibility and performance
across a diverse range of applications. INRs leverage multilayer perceptrons
(MLPs) to model data as continuous implicit functions, providing critical
advantages such as resolution independence, memory efficiency, and
generalisation beyond discretised data structures. Their ability to solve
complex inverse problems makes them particularly effective for tasks including
audio reconstruction, image representation, 3D object reconstruction, and
high-dimensional data synthesis. This survey provides a comprehensive review of
state-of-the-art INR methods, introducing a clear taxonomy that categorises
them into four key areas: activation functions, position encoding, combined
strategies, and network structure optimisation. We rigorously analyse their
critical properties, such as full differentiability, smoothness, compactness,
and adaptability to varying resolutions while also examining their strengths
and limitations in addressing locality biases and capturing fine details. Our
experimental comparison offers new insights into the trade-offs between
different approaches, showcasing the capabilities and challenges of the latest
INR techniques across various tasks. In addition to identifying areas where
current methods excel, we highlight key limitations and potential avenues for
improvement, such as developing more expressive activation functions, enhancing
positional encoding mechanisms, and improving scalability for complex,
high-dimensional data. This survey serves as a roadmap for researchers,
offering practical guidance for future exploration in the field of INRs. We aim
to foster new methodologies by outlining promising research directions for INRs
and applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards 3D Semantic Scene Completion for Autonomous Driving: A
  Meta-Learning Framework Empowered by Deformable Large-Kernel Attention and
  Mamba Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yansong Qu, Zilin Huang, Zihao Sheng, Tiantian Chen, Sikai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic scene completion (SSC) is essential for achieving comprehensive
perception in autonomous driving systems. However, existing SSC methods often
overlook the high deployment costs in real-world applications. Traditional
architectures, such as 3D Convolutional Neural Networks (3D CNNs) and
self-attention mechanisms, face challenges in efficiently capturing long-range
dependencies within 3D voxel grids, limiting their effectiveness. To address
these issues, we introduce MetaSSC, a novel meta-learning-based framework for
SSC that leverages deformable convolution, large-kernel attention, and the
Mamba (D-LKA-M) model. Our approach begins with a voxel-based semantic
segmentation (SS) pretraining task, aimed at exploring the semantics and
geometry of incomplete regions while acquiring transferable meta-knowledge.
Using simulated cooperative perception datasets, we supervise the perception
training of a single vehicle using aggregated sensor data from multiple nearby
connected autonomous vehicles (CAVs), generating richer and more comprehensive
labels. This meta-knowledge is then adapted to the target domain through a
dual-phase training strategy that does not add extra model parameters, enabling
efficient deployment. To further enhance the model's capability in capturing
long-sequence relationships within 3D voxel grids, we integrate Mamba blocks
with deformable convolution and large-kernel attention into the backbone
network. Extensive experiments demonstrate that MetaSSC achieves
state-of-the-art performance, significantly outperforming competing models
while also reducing deployment costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Touchstone Benchmark: Are We on the Right Way for Evaluating AI
  Algorithms for Medical Segmentation? <span class="chip">NeurIPS-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro R. A. S. Bassi, Wenxuan Li, Yucheng Tang, Fabian Isensee, Zifu Wang, Jieneng Chen, Yu-Cheng Chou, Yannick Kirchhoff, Maximilian Rokuss, Ziyan Huang, Jin Ye, Junjun He, Tassilo Wald, Constantin Ulrich, Michael Baumgartner, Saikat Roy, Klaus H. Maier-Hein, Paul Jaeger, Yiwen Ye, Yutong Xie, Jianpeng Zhang, Ziyang Chen, Yong Xia, Zhaohu Xing, Lei Zhu, Yousef Sadegheih, Afshin Bozorgpour, Pratibha Kumari, Reza Azad, Dorit Merhof, Pengcheng Shi, Ting Ma, Yuxin Du, Fan Bai, Tiejun Huang, Bo Zhao, Haonan Wang, Xiaomeng Li, Hanxue Gu, Haoyu Dong, Jichen Yang, Maciej A. Mazurowski, Saumya Gupta, Linshan Wu, Jiaxin Zhuang, Hao Chen, Holger Roth, Daguang Xu, Matthew B. Blaschko, Sergio Decherchi, Andrea Cavalli, Alan L. Yuille, Zongwei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we test AI performance? This question seems trivial, but it isn't.
Standard benchmarks often have problems such as in-distribution and small-size
test sets, oversimplified metrics, unfair comparisons, and short-term outcome
pressure. As a consequence, good performance on standard benchmarks does not
guarantee success in real-world scenarios. To address these problems, we
present Touchstone, a large-scale collaborative segmentation benchmark of 9
types of abdominal organs. This benchmark is based on 5,195 training CT scans
from 76 hospitals around the world and 5,903 testing CT scans from 11
additional hospitals. This diverse test set enhances the statistical
significance of benchmark results and rigorously evaluates AI algorithms across
various out-of-distribution scenarios. We invited 14 inventors of 19 AI
algorithms to train their algorithms, while our team, as a third party,
independently evaluated these algorithms on three test sets. In addition, we
also evaluated pre-existing AI frameworks--which, differing from algorithms,
are more flexible and can support different algorithms--including MONAI from
NVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are
committed to expanding this benchmark to encourage more innovation of AI
algorithms for the medical domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Stereo Depth Estimation with Multi-Spectral Images Across All
  Lighting Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Qin, Jialei Xu, Wenbo Zhao, Junjun Jiang, Xianming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation under adverse conditions remains a significant challenge.
Recently, multi-spectral depth estimation, which integrates both visible light
and thermal images, has shown promise in addressing this issue. However,
existing algorithms struggle with precise pixel-level feature matching,
limiting their ability to fully exploit geometric constraints across different
spectra. To address this, we propose a novel framework incorporating stereo
depth estimation to enforce accurate geometric constraints. In particular, we
treat the visible light and thermal images as a stereo pair and utilize a
Cross-modal Feature Matching (CFM) Module to construct a cost volume for
pixel-level matching. To mitigate the effects of poor lighting on stereo
matching, we introduce Degradation Masking, which leverages robust monocular
thermal depth estimation in degraded regions. Our method achieves
state-of-the-art (SOTA) performance on the Multi-Spectral Stereo (MS2) dataset,
with qualitative evaluations demonstrating high-quality depth maps under
varying lighting conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure Consistent Gaussian Splatting with Matching Prior for Few-shot
  Novel View Synthesis <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Peng, Wangze Xu, Luyang Tang, Liwei Liao, Jianbo Jiao, Ronggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the substantial progress of novel view synthesis, existing methods,
either based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian
Splatting (3DGS), suffer significant degradation when the input becomes sparse.
Numerous efforts have been introduced to alleviate this problem, but they still
struggle to synthesize satisfactory results efficiently, especially in the
large scene. In this paper, we propose SCGaussian, a Structure Consistent
Gaussian Splatting method using matching priors to learn 3D consistent scene
structure. Considering the high interdependence of Gaussian attributes, we
optimize the scene structure in two folds: rendering geometry and, more
importantly, the position of Gaussian primitives, which is hard to be directly
constrained in the vanilla 3DGS due to the non-structure property. To achieve
this, we present a hybrid Gaussian representation. Besides the ordinary
non-structure Gaussian primitives, our model also consists of ray-based
Gaussian primitives that are bound to matching rays and whose optimization of
their positions is restricted along the ray. Thus, we can utilize the matching
correspondence to directly enforce the position of these Gaussian primitives to
converge to the surface points where rays intersect. Extensive experiments on
forward-facing, surrounding, and complex large scenes show the effectiveness of
our approach with state-of-the-art performance and high efficiency. Code is
available at https://github.com/prstrive/SCGaussian.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junming Lin, Zheng Fang, Chi Chen, Zihao Wan, Fuwen Luo, Peng Li, Yang Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Multimodal Large Language Models (MLLMs) has
expanded their capabilities from image comprehension to video understanding.
However, most of these MLLMs focus primarily on offline video comprehension,
necessitating extensive processing of all video frames before any queries can
be made. This presents a significant gap compared to the human ability to
watch, listen, think, and respond to streaming inputs in real time,
highlighting the limitations of current MLLMs. In this paper, we introduce
StreamingBench, the first comprehensive benchmark designed to evaluate the
streaming video understanding capabilities of MLLMs. StreamingBench assesses
three core aspects of streaming video understanding: (1) real-time visual
understanding, (2) omni-source understanding, and (3) contextual understanding.
The benchmark consists of 18 tasks, featuring 900 videos and 4,500
human-curated QA pairs. Each video features five questions presented at
different time points to simulate a continuous streaming scenario. We conduct
experiments on StreamingBench with 13 open-source and proprietary MLLMs and
find that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and
GPT-4o perform significantly below human-level streaming video understanding
capabilities. We hope our work can facilitate further advancements for MLLMs,
empowering them to approach human-level video comprehension and interaction in
more realistic scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross Feature Fusion of Fundus Image and Generated Lesion Map for
  Referable Diabetic Retinopathy Classification <span class="chip">ACCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dahyun Mok, Junghyun Bum, Le Duc Tai, Hyunseung Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diabetic Retinopathy (DR) is a primary cause of blindness, necessitating
early detection and diagnosis. This paper focuses on referable DR
classification to enhance the applicability of the proposed method in clinical
practice. We develop an advanced cross-learning DR classification method
leveraging transfer learning and cross-attention mechanisms. The proposed
method employs the Swin U-Net architecture to segment lesion maps from DR
fundus images. The Swin U-Net segmentation model, enriched with DR lesion
insights, is transferred to generate a lesion map. Both the fundus image and
its segmented lesion map are used as complementary inputs for the
classification model. A cross-attention mechanism is deployed to improve the
model's ability to capture fine-grained details from the input pairs. Our
experiments, utilizing two public datasets, FGADR and EyePACS, demonstrate a
superior accuracy of 94.6%, surpassing current state-of-the-art methods by
4.4%. To this end, we aim for the proposed method to be seamlessly integrated
into clinical workflows, enhancing accuracy and efficiency in identifying
referable DR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCV 2024 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADMIRE: a locally adaptive single-image, non-uniformity correction and
  denoising algorithm: application to uncooled IR camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yohann Tendero, Jerome Gilles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new way to correct for the non-uniformity (NU) and the noise in
uncooled infrared-type images. This method works on static images, needs no
registration, no camera motion and no model for the non uniformity. The
proposed method uses an hybrid scheme including an automatic locally-adaptive
contrast adjustment and a state-of-the-art image denoising method. It permits
to correct for a fully non-linear NU and the noise efficiently using only one
image. We compared it with total variation on real raw and simulated NU
infrared images. The strength of this approach lies in its simplicity, low
computational cost. It needs no test-pattern or calibration and produces no
"ghost-artefact".
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LCP-Fusion: A Neural Implicit SLAM with Enhanced Local Constraints and
  Computable Prior <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Wang, Yinan Deng, Yi Yang, Yufeng Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently the dense Simultaneous Localization and Mapping (SLAM) based on
neural implicit representation has shown impressive progress in hole filling
and high-fidelity mapping. Nevertheless, existing methods either heavily rely
on known scene bounds or suffer inconsistent reconstruction due to drift in
potential loop-closure regions, or both, which can be attributed to the
inflexible representation and lack of local constraints. In this paper, we
present LCP-Fusion, a neural implicit SLAM system with enhanced local
constraints and computable prior, which takes the sparse voxel octree structure
containing feature grids and SDF priors as hybrid scene representation,
enabling the scalability and robustness during mapping and tracking. To enhance
the local constraints, we propose a novel sliding window selection strategy
based on visual overlap to address the loop-closure, and a practical warping
loss to constrain relative poses. Moreover, we estimate SDF priors as coarse
initialization for implicit features, which brings additional explicit
constraints and robustness, especially when a light but efficient adaptive
early ending is adopted. Experiments demonstrate that our method achieve better
localization accuracy and reconstruction consistency than existing RGB-D
implicit SLAM, especially in challenging real scenes (ScanNet) as well as
self-captured scenes with unknown scene bounds. The code is available at
https://github.com/laliwang/LCP-Fusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Attention for Robust RGB-T Pedestrian Detection in Real-World
  Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arunkumar Rathinam, Leo Pauly, Abd El Rahman Shabayek, Wassim Rharbaoui, Anis Kacem, Vincent Gaudillière, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multispectral pedestrian detection has gained significant attention in recent
years, particularly in autonomous driving applications. To address the
challenges posed by adversarial illumination conditions, the combination of
thermal and visible images has demonstrated its advantages. However, existing
fusion methods rely on the critical assumption that the RGB-Thermal (RGB-T)
image pairs are fully overlapping. These assumptions often do not hold in
real-world applications, where only partial overlap between images can occur
due to sensors configuration. Moreover, sensor failure can cause loss of
information in one modality. In this paper, we propose a novel module called
the Hybrid Attention (HA) mechanism as our main contribution to mitigate
performance degradation caused by partial overlap and sensor failure, i.e. when
at least part of the scene is acquired by only one sensor. We propose an
improved RGB-T fusion algorithm, robust against partial overlap and sensor
failure encountered during inference in real-world applications. We also
leverage a mobile-friendly backbone to cope with resource constraints in
embedded systems. We conducted experiments by simulating various partial
overlap and sensor failure scenarios to evaluate the performance of our
proposed method. The results demonstrate that our approach outperforms
state-of-the-art methods, showcasing its superiority in handling real-world
challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Robotics and Automation Letters,
  October 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Personalized Federated Learning via Comprehensive Knowledge
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengju Wang, Bochao Liu, Weijia Guo, Yong Li, Shiming Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a distributed machine learning paradigm designed to
protect data privacy. However, data heterogeneity across various clients
results in catastrophic forgetting, where the model rapidly forgets previous
knowledge while acquiring new knowledge. To address this challenge,
personalized federated learning has emerged to customize a personalized model
for each client. However, the inherent limitation of this mechanism is its
excessive focus on personalization, potentially hindering the generalization of
those models. In this paper, we present a novel personalized federated learning
method that uses global and historical models as teachers and the local model
as the student to facilitate comprehensive knowledge distillation. The
historical model represents the local model from the last round of client
training, containing historical personalized knowledge, while the global model
represents the aggregated model from the last round of server aggregation,
containing global generalized knowledge. By applying knowledge distillation, we
effectively transfer global generalized knowledge and historical personalized
knowledge to the local model, thus mitigating catastrophic forgetting and
enhancing the general performance of personalized models. Extensive
experimental results demonstrate the significant advantages of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE SMC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The American Sign Language Knowledge Graph: Infusing ASL Models with
  Linguistic Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lee Kezar, Nidhi Munikote, Zian Zeng, Zed Sehyr, Naomi Caselli, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models for American Sign Language (ASL) could make language
technologies substantially more accessible to those who sign. To train models
on tasks such as isolated sign recognition (ISR) and ASL-to-English
translation, datasets provide annotated video examples of ASL signs. To
facilitate the generalizability and explainability of these models, we
introduce the American Sign Language Knowledge Graph (ASLKG), compiled from
twelve sources of expert linguistic knowledge. We use the ASLKG to train
neuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of
91% on ISR, 14% for predicting the semantic features of unseen signs, and 36%
for classifying the topic of Youtube-ASL videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unfair Alignment: Examining Safety Alignment Across Vision Encoder
  Layers in Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saketh Bachu, Erfan Shayegani, Trishna Chakraborty, Rohit Lal, Arindam Dutta, Chengyu Song, Yue Dong, Nael Abu-Ghazaleh, Amit K. Roy-Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have improved significantly in multi-modal
tasks, but their more complex architecture makes their safety alignment more
challenging than the alignment of large language models (LLMs). In this paper,
we reveal an unfair distribution of safety across the layers of VLM's vision
encoder, with earlier and middle layers being disproportionately vulnerable to
malicious inputs compared to the more robust final layers. This 'cross-layer'
vulnerability stems from the model's inability to generalize its safety
training from the default architectural settings used during training to unseen
or out-of-distribution scenarios, leaving certain layers exposed. We conduct a
comprehensive analysis by projecting activations from various intermediate
layers and demonstrate that these layers are more likely to generate harmful
outputs when exposed to malicious inputs. Our experiments with LLaVA-1.5 and
Llama 3.2 show discrepancies in attack success rates and toxicity scores across
layers, indicating that current safety alignment strategies focused on a single
default layer are insufficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Increasing the scalability of graph convolution for FPGA-implemented
  event-based vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Wzorek, Kamil Jeziorek, Tomasz Kryjak, Andrea Pinna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras are becoming increasingly popular as an alternative to
traditional frame-based vision sensors, especially in mobile robotics. Taking
full advantage of their high temporal resolution, high dynamic range, low power
consumption and sparsity of event data, which only reflects changes in the
observed scene, requires both an efficient algorithm and a specialised hardware
platform. A recent trend involves using Graph Convolutional Neural Networks
(GCNNs) implemented on a heterogeneous SoC FPGA. In this paper we focus on
optimising hardware modules for graph convolution to allow flexible selection
of the FPGA resource (BlockRAM, DSP and LUT) for their implementation. We
propose a ''two-step convolution'' approach that utilises additional BRAM
buffers in order to reduce up to 94% of LUT usage for multiplications. This
method significantly improves the scalability of GCNNs, enabling the deployment
of models with more layers, larger graphs sizes and their application for more
dynamic scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the PhD forum during FPT 2024 (International Conference
  on Field Programmable Technology), 10-12 December 2024, Sydney, Australia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object Recognition in Human Computer Interaction:- A Comparative
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaushik Ranade, Tanmay Khule, Riddhi More
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-computer interaction (HCI) has been a widely researched area for many
years, with continuous advancements in technology leading to the development of
new techniques that change the way we interact with computers. With the recent
advent of powerful computers, we recognize human actions and interact
accordingly, thus revolutionizing the way we interact with computers. The
purpose of this paper is to provide a comparative analysis of various
algorithms used for recognizing user faces and gestures in the context of
computer vision and HCI. This study aims to explore and evaluate the
performance of different algorithms in terms of accuracy, robustness, and
efficiency. This study aims to provide a comprehensive analysis of algorithms
for face and gesture recognition in the context of computer vision and HCI,
with the goal of improving the design and development of interactive systems
that are more intuitive, efficient, and user-friendly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pose-Transformation and Radial Distance Clustering for Unsupervised
  Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Seth, Akash Sonth, Anirban Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person re-identification (re-ID) aims to tackle the problem of matching
identities across non-overlapping cameras. Supervised approaches require
identity information that may be difficult to obtain and are inherently biased
towards the dataset they are trained on, making them unscalable across domains.
To overcome these challenges, we propose an unsupervised approach to the person
re-ID setup. Having zero knowledge of true labels, our proposed method enhances
the discriminating ability of the learned features via a novel two-stage
training strategy. The first stage involves training a deep network on an
expertly designed pose-transformed dataset obtained by generating multiple
perturbations for each original image in the pose space. Next, the network
learns to map similar features closer in the feature space using the proposed
discriminative clustering algorithm. We introduce a novel radial distance loss,
that attends to the fundamental aspects of feature learning - compact clusters
with low intra-cluster and high inter-cluster variation. Extensive experiments
on several large-scale re-ID datasets demonstrate the superiority of our method
compared to state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PocoLoco: A Point Cloud <span class="highlight-title">Diffusion</span> Model of Human Shape in Loose Clothing <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Seth, Rishabh Dabral, Diogo Luvizon, Marc Habermann, Ming-Hsuan Yang, Christian Theobalt, Adam Kortylewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling a human avatar that can plausibly deform to articulations is an
active area of research. We present PocoLoco -- the first template-free,
point-based, pose-conditioned generative model for 3D humans in loose clothing.
We motivate our work by noting that most methods require a parametric model of
the human body to ground pose-dependent deformations. Consequently, they are
restricted to modeling clothing that is topologically similar to the naked body
and do not extend well to loose clothing. The few methods that attempt to model
loose clothing typically require either canonicalization or a
UV-parameterization and need to address the challenging problem of explicitly
estimating correspondences for the deforming clothes. In this work, we
formulate avatar clothing deformation as a conditional point-cloud generation
task within the denoising diffusion framework. Crucially, our framework
operates directly on unordered point clouds, eliminating the need for a
parametric model or a clothing template. This also enables a variety of
practical applications, such as point-cloud completion and pose-based editing
-- important features for virtual human animation. As current datasets for
human avatars in loose clothing are far too small for training diffusion
models, we release a dataset of two subjects performing various poses in loose
clothing with a total of 75K point clouds. By contributing towards tackling the
challenging task of effectively modeling loose clothing and expanding the
available data for training these models, we aim to set the stage for further
innovation in digital humans. The source code is available at
https://github.com/sidsunny/pocoloco .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WiFlexFormer: Efficient WiFi-Based Person-Centric Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Strohmayer, Matthias Wödlinger, Martin Kampel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose WiFlexFormer, a highly efficient Transformer-based architecture
designed for WiFi Channel State Information (CSI)-based person-centric sensing.
We benchmark WiFlexFormer against state-of-the-art vision and specialized
architectures for processing radio frequency data and demonstrate that it
achieves comparable Human Activity Recognition (HAR) performance while offering
a significantly lower parameter count and faster inference times. With an
inference time of just 10 ms on an Nvidia Jetson Orin Nano, WiFlexFormer is
optimized for real-time inference. Additionally, its low parameter count
contributes to improved cross-domain generalization, where it often outperforms
larger models. Our comprehensive evaluation shows that WiFlexFormer is a
potential solution for efficient, scalable WiFi-based sensing applications. The
PyTorch implementation of WiFlexFormer is publicly available at:
https://github.com/StrohmayerJ/WiFlexFormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiMSUM: <span class="highlight-title">Diffusion</span> Mamba -- A Scalable and Unified Spatial-Frequency
  Method for Image Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Phung, Quan Dao, Trung Dao, Hoang Phan, Dimitris Metaxas, Anh Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel state-space architecture for diffusion models,
effectively harnessing spatial and frequency information to enhance the
inductive bias towards local features in input images for image generation
tasks. While state-space networks, including Mamba, a revolutionary advancement
in recurrent neural networks, typically scan input sequences from left to
right, they face difficulties in designing effective scanning strategies,
especially in the processing of image data. Our method demonstrates that
integrating wavelet transformation into Mamba enhances the local structure
awareness of visual inputs and better captures long-range relations of
frequencies by disentangling them into wavelet subbands, representing both low-
and high-frequency components. These wavelet-based outputs are then processed
and seamlessly fused with the original Mamba outputs through a cross-attention
fusion layer, combining both spatial and frequency information to optimize the
order awareness of state-space models which is essential for the details and
overall quality of image generation. Besides, we introduce a globally-shared
transformer to supercharge the performance of Mamba, harnessing its exceptional
power to capture global relationships. Through extensive experiments on
standard benchmarks, our method demonstrates superior results compared to DiT
and DIFFUSSM, achieving faster training convergence and delivering high-quality
outputs. The codes and pretrained models are released at
https://github.com/VinAIResearch/DiMSUM.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024. Project page:
  https://hao-pt.github.io/dimsum/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen
  Representations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walter Simoncini, Spyros Gidaris, Andrei Bursuc, Yuki M. Asano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces FUNGI, Features from UNsupervised GradIents, a method
to enhance the features of transformer encoders by leveraging self-supervised
gradients. Our method is simple: given any pretrained model, we first compute
gradients from various self-supervised objectives for each input. These
gradients are projected to a lower dimension and then concatenated with the
model's output embedding. The resulting features are evaluated on k-nearest
neighbor classification over 11 datasets from vision, 5 from natural language
processing, and 2 from audio. Across backbones spanning various sizes and
pretraining strategies, FUNGI features provide consistent performance
improvements over the embeddings. We also show that using FUNGI features can
benefit linear classification, clustering and image retrieval, and that they
significantly improve the retrieval-based in-context scene understanding
abilities of pretrained models, for example improving upon DINO by +17% for
semantic segmentation - without any training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code available at
  https://github.com/WalterSimoncini/fungivision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeNetDM: Debiasing by Network Depth Modulation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19863v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19863v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silpa Vadakkeeveetil Sreelatha, Adarsh Kappiyath, Abhra Chaudhuri, Anjan Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks trained on biased datasets tend to inadvertently learn
spurious correlations, hindering generalization. We formally prove that (1)
samples that exhibit spurious correlations lie on a lower rank manifold
relative to the ones that do not; and (2) the depth of a network acts as an
implicit regularizer on the rank of the attribute subspace that is encoded in
its representations. Leveraging these insights, we present DeNetDM, a novel
debiasing method that uses network depth modulation as a way of developing
robustness to spurious correlations. Using a training paradigm derived from
Product of Experts, we create both biased and debiased branches with deep and
shallow architectures and then distill knowledge to produce the target debiased
model. Our method requires no bias annotations or explicit data augmentation
while performing on par with approaches that require either or both. We
demonstrate that DeNetDM outperforms existing debiasing techniques on both
synthetic and real-world datasets by 5\%. The project page is available at
https://vssilpa.github.io/denetdm/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version : NeurIPS 2024, * indicates these authors
  contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with
  Enhanced Generalization and Personalization Abilities <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16147v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16147v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D Gaussian Splatting (3DGS) have unlocked significant
potential for modeling 3D head avatars, providing greater flexibility than
mesh-based methods and more efficient rendering compared to NeRF-based
approaches. Despite these advancements, the creation of controllable 3DGS-based
head avatars remains time-intensive, often requiring tens of minutes to hours.
To expedite this process, we here introduce the "Gaussian Deja-vu" framework,
which first obtains a generalized model of the head avatar and then
personalizes the result. The generalized model is trained on large 2D
(synthetic and real) image datasets. This model provides a well-initialized 3D
Gaussian head that is further refined using a monocular video to achieve the
personalized head avatar. For personalizing, we propose learnable
expression-aware rectification blendmaps to correct the initial 3D Gaussians,
ensuring rapid convergence without the reliance on neural networks. Experiments
demonstrate that the proposed method meets its objectives. It outperforms
state-of-the-art 3D Gaussian head avatars in terms of photorealistic quality as
well as reduces training time consumption to at least a quarter of the existing
methods, producing the avatar in minutes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, Accepted by WACV 2025 in Round 1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ bit2bit: 1-bit quanta video reconstruction via self-supervised photon
  prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehe Liu, Alexander Krull, Hector Basevi, Ales Leonardis, Michael W. Jenkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quanta image sensors, such as SPAD arrays, are an emerging sensor technology,
producing 1-bit arrays representing photon detection events over exposures as
short as a few nanoseconds. In practice, raw data are post-processed using
heavy spatiotemporal binning to create more useful and interpretable images at
the cost of degrading spatiotemporal resolution. In this work, we propose
bit2bit, a new method for reconstructing high-quality image stacks at the
original spatiotemporal resolution from sparse binary quanta image data.
Inspired by recent work on Poisson denoising, we developed an algorithm that
creates a dense image sequence from sparse binary photon data by predicting the
photon arrival location probability distribution. However, due to the binary
nature of the data, we show that the assumption of a Poisson distribution is
inadequate. Instead, we model the process with a Bernoulli lattice process from
the truncated Poisson. This leads to the proposal of a novel self-supervised
solution based on a masked loss function. We evaluate our method using both
simulated and real data. On simulated data from a conventional video, we
achieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06
photons per pixel per frame). We also present a novel dataset containing a wide
range of real SPAD high-speed videos under various challenging imaging
conditions. The scenes cover strong/weak ambient light, strong motion,
ultra-fast events, etc., which will be made available to the community, on
which we demonstrate the promise of our approach. Both reconstruction quality
and throughput substantially surpass the state-of-the-art methods (e.g., Quanta
Burst Photography (QBP)). Our approach significantly enhances the visualization
and usability of the data, enabling the application of existing analysis
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LDTrack: Dynamic People Tracking by Service Robots using <span class="highlight-title">Diffusion</span>
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angus Fung, Beno Benhabib, Goldie Nejat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tracking of dynamic people in cluttered and crowded human-centered
environments is a challenging robotics problem due to the presence of
intraclass variations including occlusions, pose deformations, and lighting
variations. This paper introduces a novel deep learning architecture, using
conditional latent diffusion models, the Latent Diffusion Track (LDTrack), for
tracking multiple dynamic people under intraclass variations. By uniquely
utilizing conditional latent diffusion models to capture temporal person
embeddings, our architecture can adapt to appearance changes of people over
time. We incorporated a latent feature encoder network which enables the
diffusion process to operate within a high-dimensional latent space to allow
for the extraction and spatial-temporal refinement of such rich features as
person appearance, motion, location, identity, and contextual information.
Extensive experiments demonstrate the effectiveness of LDTrack over other
state-of-the-art tracking methods in cluttered and crowded human-centered
environments under intraclass variations. Namely, the results show our method
outperforms existing deep learning robotic people tracking methods in both
tracking accuracy and tracking precision with statistical significance.
Additionally, a comprehensive multi-object tracking comparison study was
performed against the state-of-the-art methods in urban environments,
demonstrating the generalizability of LDTrack. An ablation study was performed
to validate the design choices of LDTrack.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DMPlug: A Plug-in Method for Solving Inverse Problems with <span class="highlight-title">Diffusion</span>
  Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengkang Wang, Xu Zhang, Taihui Li, Yuxiang Wan, Tiancong Chen, Ju Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained diffusion models (DMs) have recently been popularly used in
solving inverse problems (IPs). The existing methods mostly interleave
iterative steps in the reverse diffusion process and iterative steps to bring
the iterates closer to satisfying the measurement constraint. However, such
interleaving methods struggle to produce final results that look like natural
objects of interest (i.e., manifold feasibility) and fit the measurement (i.e.,
measurement feasibility), especially for nonlinear IPs. Moreover, their
capabilities to deal with noisy IPs with unknown types and levels of
measurement noise are unknown. In this paper, we advocate viewing the reverse
process in DMs as a function and propose a novel plug-in method for solving IPs
using pretrained DMs, dubbed DMPlug. DMPlug addresses the issues of manifold
feasibility and measurement feasibility in a principled manner, and also shows
great potential for being robust to unknown types and levels of noise. Through
extensive experiments across various IP tasks, including two linear and three
nonlinear IPs, we demonstrate that DMPlug consistently outperforms
state-of-the-art methods, often by large margins especially for nonlinear IPs.
The code is available at https://github.com/sun-umn/DMPlug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024
  (https://openreview.net/forum?id=81IFFsfQUj)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep neural network-based detection of counterfeit products from
  smartphone images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05969v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05969v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Garcia-Cotte, Dorra Mellouli, Abdul Rehman, Li Wang, David G. Stork
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfeit products such as drugs and vaccines as well as luxury items such
as high-fashion handbags, watches, jewelry, garments, and cosmetics, represent
significant direct losses of revenue to legitimate manufacturers and vendors,
as well as indirect costs to societies at large. We present the world's first
purely computer-vision-based system to combat such counterfeiting-one that does
not require special security tags or other alterations to the products or
modifications to supply chain tracking. Our deep neural network system shows
high accuracy on branded garments from our first manufacturer tested (99.71%
after 3.06% rejections) using images captured under natural, weakly controlled
conditions, such as in retail stores, customs checkpoints, warehouses, and
outdoors. Our system, suitably transfer trained on a small number of fake and
genuine articles, should find application in additional product categories as
well, for example fashion accessories, perfume boxes, medicines, and more.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ZeMing Gong, Austin T. Wang, Xiaoliang Huo, Joakim Bruslund Haurum, Scott C. Lowe, Graham W. Taylor, Angel X. Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring biodiversity is crucial for understanding ecosystem health. While
prior works have developed machine learning models for taxonomic classification
of photographic images and DNA separately, in this work, we introduce a
multimodal approach combining both, using CLIP-style contrastive learning to
align images, barcode DNA, and text-based representations of taxonomic labels
in a unified embedding space. This allows for accurate classification of both
known and unknown insect species without task-specific fine-tuning, leveraging
contrastive learning for the first time to fuse DNA and image data. Our method
surpasses previous single-modality approaches in accuracy by over 8% on
zero-shot learning tasks, showcasing its effectiveness in biodiversity studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages with 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BetterDepth: Plug-and-Play <span class="highlight-title">Diffusion</span> Refiner for Zero-Shot Monocular
  Depth Estimation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Zhang, Bingxin Ke, Hayko Riemenschneider, Nando Metzger, Anton Obukhov, Markus Gross, Konrad Schindler, Christopher Schroers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By training over large-scale datasets, zero-shot monocular depth estimation
(MDE) methods show robust performance in the wild but often suffer from
insufficient detail. Although recent diffusion-based MDE approaches exhibit a
superior ability to extract details, they struggle in geometrically complex
scenes that challenge their geometry prior, trained on less diverse 3D data. To
leverage the complementary merits of both worlds, we propose BetterDepth to
achieve geometrically correct affine-invariant MDE while capturing fine
details. Specifically, BetterDepth is a conditional diffusion-based refiner
that takes the prediction from pre-trained MDE models as depth conditioning, in
which the global depth layout is well-captured, and iteratively refines details
based on the input image. For the training of such a refiner, we propose global
pre-alignment and local patch masking methods to ensure BetterDepth remains
faithful to the depth conditioning while learning to add fine-grained scene
details. With efficient training on small-scale synthetic datasets, BetterDepth
achieves state-of-the-art zero-shot MDE performance on diverse public datasets
and on in-the-wild scenes. Moreover, BetterDepth can improve the performance of
other MDE models in a plug-and-play manner without further re-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Virchow2: Scaling Self-Supervised Mixed Magnification Models in
  Pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00738v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00738v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Zimmermann, Eugene Vorontsov, Julian Viret, Adam Casson, Michal Zelechowski, George Shaikovski, Neil Tenenholtz, James Hall, David Klimstra, Razik Yousfi, Thomas Fuchs, Nicolo Fusi, Siqi Liu, Kristen Severson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models are rapidly being developed for computational pathology
applications. However, it remains an open question which factors are most
important for downstream performance with data scale and diversity, model size,
and training algorithm all playing a role. In this work, we propose algorithmic
modifications, tailored for pathology, and we present the result of scaling
both data and model size, surpassing previous studies in both dimensions. We
introduce three new models: Virchow2, a 632 million parameter vision
transformer, Virchow2G, a 1.9 billion parameter vision transformer, and
Virchow2G Mini, a 22 million parameter distillation of Virchow2G, each trained
with 3.1 million histopathology whole slide images, with diverse tissues,
originating institutions, and stains. We achieve state of the art performance
on 12 tile-level tasks, as compared to the top performing competing models. Our
results suggest that data diversity and domain-specific methods can outperform
models that only scale in the number of parameters, but, on average,
performance benefits from the combination of domain-specific methods, data
scale, and model scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applying Guidance in a Limited Interval Improves Sample and Distribution
  Quality in <span class="highlight-title">Diffusion</span> Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, Jaakko Lehtinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Guidance is a crucial technique for extracting the best performance out of
image-generating diffusion models. Traditionally, a constant guidance weight
has been applied throughout the sampling chain of an image. We show that
guidance is clearly harmful toward the beginning of the chain (high noise
levels), largely unnecessary toward the end (low noise levels), and only
beneficial in the middle. We thus restrict it to a specific range of noise
levels, improving both the inference speed and result quality. This limited
guidance interval improves the record FID in ImageNet-512 significantly, from
1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial
across different sampler parameters, network architectures, and datasets,
including the large-scale setting of Stable Diffusion XL. We thus suggest
exposing the guidance interval as a hyperparameter in all diffusion models that
use guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-supervised 3D Point Cloud Completion via Multi-view Adversarial
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09786v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09786v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lintai Wu, Xianjing Cheng, Yong Xu, Huanqiang Zeng, Junhui Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, scanned point clouds are often incomplete due to
occlusion issues. The task of self-supervised point cloud completion involves
reconstructing missing regions of these incomplete objects without the
supervision of complete ground truth. Current self-supervised methods either
rely on multiple views of partial observations for supervision or overlook the
intrinsic geometric similarity that can be identified and utilized from the
given partial point clouds. In this paper, we propose MAL-SPC, a framework that
effectively leverages both object-level and category-specific geometric
similarities to complete missing structures. Our MAL-SPC does not require any
3D complete supervision and only necessitates a single partial point cloud for
each object. Specifically, we first introduce a Pattern Retrieval Network to
retrieve similar position and curvature patterns between the partial input and
the predicted shape, then leverage these similarities to densify and refine the
reconstructed results. Additionally, we render the reconstructed complete shape
into multi-view depth maps and design an adversarial learning module to learn
the geometry of the target shape from category-specific single-view depth
images. To achieve anisotropic rendering, we design a density-aware radius
estimation algorithm to improve the quality of the rendered images. Our MAL-SPC
yields the best results compared to current state-of-the-art methods.We will
make the source code publicly available at \url{https://github.com/ltwu6/malspc
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages,10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChartInsights: Evaluating Multimodal Large Language Models for Low-Level
  Chart Question Answering <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07001v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07001v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Wu, Lutao Yan, Leixian Shen, Yunhai Wang, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chart question answering (ChartQA) tasks play a critical role in interpreting
and extracting insights from visualization charts. While recent advancements in
multimodal large language models (MLLMs) like GPT-4o have shown promise in
high-level ChartQA tasks, such as chart captioning, their effectiveness in
low-level ChartQA tasks (e.g., identifying correlations) remains underexplored.
In this paper, we address this gap by evaluating MLLMs on low-level ChartQA
using a newly curated dataset, ChartInsights, which consists of 22,347 (chart,
task, query, answer) covering 10 data analysis tasks across 7 chart types. We
systematically evaluate 19 advanced MLLMs, including 12 open-source and 7
closed-source models. The average accuracy rate across these models is 39.8%,
with GPT-4o achieving the highest accuracy at 69.17%. To further explore the
limitations of MLLMs in low-level ChartQA, we conduct experiments that alter
visual elements of charts (e.g., changing color schemes, adding image noise) to
assess their impact on the task effectiveness. Furthermore, we propose a new
textual prompt strategy, Chain-of-Charts, tailored for low-level ChartQA tasks,
which boosts performance by 14.41%, achieving an accuracy of 83.58%. Finally,
incorporating a visual prompt strategy that directs attention to relevant
visual elements further improves accuracy to 84.32%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EViT: An Eagle Vision <span class="highlight-title">Transformer</span> with Bi-Fovea Self-Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06629v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06629v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulong Shi, Mingwei Sun, Yongshuai Wang, Jiahao Ma, Zengqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to advancements in deep learning technology, Vision Transformers (ViTs)
have demonstrated impressive performance in various computer vision tasks.
Nonetheless, ViTs still face some challenges, such as high computational
complexity and the absence of desirable inductive biases. To alleviate these
issues, {the potential advantages of combining eagle vision with ViTs are
explored. We summarize a Bi-Fovea Visual Interaction (BFVI) structure inspired
by the unique physiological and visual characteristics of eagle eyes. A novel
Bi-Fovea Self-Attention (BFSA) mechanism and Bi-Fovea Feedforward Network
(BFFN) are proposed based on this structural design approach, which can be used
to mimic the hierarchical and parallel information processing scheme of the
biological visual cortex, enabling networks to learn feature representations of
targets in a coarse-to-fine manner. Furthermore, a Bionic Eagle Vision (BEV)
block is designed as the basic building unit based on the BFSA mechanism and
BFFN. By stacking BEV blocks, a unified and efficient family of pyramid
backbone networks called Eagle Vision Transformers (EViTs) is developed.
Experimental results show that EViTs exhibit highly competitive performance in
various computer vision tasks, such as image classification, object detection
and semantic segmentation. Compared with other approaches, EViTs have
significant advantages, especially in terms of performance and computational
efficiency. Code is available at https://github.com/nkusyl/EViT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advantages of Neural Population Coding for Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00393v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00393v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heiko Hoffmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scalar variables, e.g., the orientation of a shape in an image, are commonly
predicted using a single output neuron in a neural network. In contrast, the
mammalian cortex represents variables with a population of neurons. In this
population code, each neuron is most active at its preferred value and shows
partial activity for other values. Here, we investigate the benefit of using a
population code for the output layer of a neural network. We compare population
codes against single-neuron outputs and one-hot vectors. First, we show
theoretically and in experiments with synthetic data that population codes
improve robustness to input noise in networks of stacked linear layers. Second,
we demonstrate the benefit of using population codes to encode ambiguous
outputs, such as the pose of symmetric objects. Using the T-LESS dataset of
feature-less real-world objects, we show that population codes improve the
accuracy of predicting 3D object orientation from image input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ATM: Improving Model Merging by Alternating Tuning and Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Zhou, Daniele Solombrino, Donato Crisostomi, Maria Sofia Bucarelli, Fabrizio Silvestri, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging has recently emerged as a cost-efficient paradigm for
multi-task learning. Among current approaches, task arithmetic stands out for
its simplicity and effectiveness. In this paper, we motivate the effectiveness
of task vectors by linking them to multi-task gradients. We show that in a
single-epoch scenario, task vectors are mathematically equivalent to the
gradients obtained via gradient descent in a multi-task setting, and still
approximate these gradients in subsequent epochs. Furthermore, we show that
task vectors perform optimally when equality is maintained, and their
effectiveness is largely driven by the first epoch's gradient. Building on this
insight, we propose viewing model merging as a single step in an iterative
process that Alternates between Tuning and Merging (ATM). This method acts as a
bridge between model merging and multi-task gradient descent, achieving
state-of-the-art results with the same data and computational requirements. We
extensively evaluate ATM across diverse settings, achieving up to 20% higher
accuracy in computer vision and NLP tasks, compared to the best baselines.
Finally, we provide both empirical and theoretical support for its
effectiveness, demonstrating increased orthogonality between task vectors and
proving that ATM minimizes an upper bound on the loss obtained by jointly
finetuning all tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 10 Pages, 11 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IFAdapter: Instance Feature Control for Grounded Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08240v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08240v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinwei Wu, Xianpan Zhou, Bing Ma, Xuefeng Su, Kai Ma, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Text-to-Image (T2I) diffusion models excel at generating visually
appealing images of individual instances, they struggle to accurately position
and control the features generation of multiple instances. The Layout-to-Image
(L2I) task was introduced to address the positioning challenges by
incorporating bounding boxes as spatial control signals, but it still falls
short in generating precise instance features. In response, we propose the
Instance Feature Generation (IFG) task, which aims to ensure both positional
accuracy and feature fidelity in generated instances. To address the IFG task,
we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances
feature depiction by incorporating additional appearance tokens and utilizing
an Instance Semantic Map to align instance-level features with spatial
locations. The IFAdapter guides the diffusion process as a plug-and-play
module, making it adaptable to various community models. For evaluation, we
contribute an IFG benchmark and develop a verification pipeline to objectively
compare models' abilities to generate instances with accurate positioning and
features. Experimental results demonstrate that IFAdapter outperforms other
models in both quantitative and qualitative evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface
  Reconstruction in Open Scenes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaochao Song, Chong Cheng, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present a novel method for efficient and effective 3D
surface reconstruction in open scenes. Existing Neural Radiance Fields (NeRF)
based works typically require extensive training and rendering time due to the
adopted implicit representations. In contrast, 3D Gaussian splatting (3DGS)
uses an explicit and discrete representation, hence the reconstructed surface
is built by the huge number of Gaussian primitives, which leads to excessive
memory consumption and rough surface details in sparse Gaussian areas. To
address these issues, we propose Gaussian Voxel Kernel Functions (GVKF), which
establish a continuous scene representation based on discrete 3DGS through
kernel regression. The GVKF integrates fast 3DGS rasterization and highly
effective scene implicit representations, achieving high-fidelity open scene
surface reconstruction. Experiments on challenging scene datasets demonstrate
the efficiency and effectiveness of our proposed GVKF, featuring with high
reconstruction quality, real-time rendering speed, significant savings in
storage and training memory consumption.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BCDNet: A Fast Residual Neural Network For Invasive Ductal Carcinoma
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Lin, Aiwei Lian, Mingyu Liao, Shuangjie Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is of great significance to diagnose Invasive Ductal Carcinoma (IDC) in
early stage, which is the most common subtype of breast cancer. Although the
powerful models in the Computer-Aided Diagnosis (CAD) systems provide promising
results, it is still difficult to integrate them into other medical devices or
use them without sufficient computation resource. In this paper, we propose
BCDNet, which firstly upsamples the input image by the residual block and use
smaller convolutional block and a special MLP to learn features. BCDNet is
proofed to effectively detect IDC in histopathological RGB images with an
average accuracy of 91.6% and reduce training consumption effectively compared
to ResNet 50 and ViT-B-16.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification Done Right for Vision-Language Pre-Training <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilong Huang, Qinghao Ye, Bingyi Kang, Jiashi Feng, Haoqi Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SuperClass, a super simple classification method for
vision-language pre-training on image-text data. Unlike its contrastive
counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes
tokenized raw text as supervised classification labels, without the need for
additional text filtering or selection. Due to the absence of the text encoding
as contrastive target, SuperClass does not require a text encoder and does not
need to maintain a large batch size as CLIP does. SuperClass demonstrated
superior performance on various downstream tasks, including classic computer
vision benchmarks and vision language downstream tasks. We further explored the
scaling behavior of SuperClass on model size, training length, or data size,
and reported encouraging results and comparisons to CLIP.
https://github.com/x-cls/superclass
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlin Hou, Jilan Xu, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The black-box nature of deep learning models has raised concerns about their
interpretability for successful deployment in real-world clinical applications.
To address the concerns, eXplainable Artificial Intelligence (XAI) aims to
provide clear and understandable explanations of the decision-making process.
In the medical domain, concepts such as attributes of lesions or abnormalities
serve as key evidence for deriving diagnostic results. Existing concept-based
models mainly depend on concepts that appear independently and require
fine-grained concept annotations such as bounding boxes. However, a medical
image usually contains multiple concepts, and the fine-grained concept
annotations are difficult to acquire. In this paper, we aim to interpret
representations in deep neural networks by aligning the axes of the latent
space with known concepts of interest. We propose a novel Concept-Attention
Whitening (CAW) framework for interpretable skin lesion diagnosis. CAW is
comprised of a disease diagnosis branch and a concept alignment branch. In the
former branch, we train a convolutional neural network (CNN) with an inserted
CAW layer to perform skin lesion diagnosis. The CAW layer decorrelates features
and aligns image features to conceptual meanings via an orthogonal matrix. In
the latter branch, the orthogonal matrix is calculated under the guidance of
the concept attention mask. We particularly introduce a weakly-supervised
concept mask generator that only leverages coarse concept labels for filtering
local regions that are relevant to certain concepts, improving the optimization
of the orthogonal matrix. Extensive experiments on two public skin lesion
diagnosis datasets demonstrated that CAW not only enhanced interpretability but
also maintained a state-of-the-art diagnostic performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Degradation Oriented and Regularized Network for Blind Depth
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11666v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11666v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxue Wang, Zhiqiang Yan, Jinshan Pan, Guangwei Gao, Kai Zhang, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent RGB-guided depth super-resolution methods have achieved impressive
performance under the assumption of fixed and known degradation (e.g., bicubic
downsampling). However, in real-world scenarios, captured depth data often
suffer from unconventional and unknown degradation due to sensor limitations
and complex imaging environments (e.g., low reflective surfaces, varying
illumination). Consequently, the performance of these methods significantly
declines when real-world degradation deviate from their assumptions. In this
paper, we propose the Degradation Oriented and Regularized Network (DORNet), a
novel framework designed to adaptively address unknown degradation in
real-world scenes through implicit degradation representations. Our approach
begins with the development of a self-supervised degradation learning strategy,
which models the degradation representations of low-resolution depth data using
routing selection-based degradation regularization. To facilitate effective
RGB-D fusion, we further introduce a degradation-oriented feature
transformation module that selectively propagates RGB content into the depth
data based on the learned degradation priors. Extensive experimental results on
both real and synthetic datasets demonstrate the superiority of our DORNet in
handling unknown degradation, outperforming existing methods. The code is
available at https://github.com/yanzq95/DORNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Task Affinity Learning for Multitask Dense Scene Predictions <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Sinodinos, Narges Armanfard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multitask learning (MTL) has become prominent for its ability to predict
multiple tasks jointly, achieving better per-task performance with fewer
parameters than single-task learning. Recently, decoder-focused architectures
have significantly improved multitask performance by refining task predictions
using features from related tasks. However, most refinement methods struggle to
efficiently capture both local and long-range dependencies between
task-specific representations and cross-task patterns. In this paper, we
introduce the Cross-Task Affinity Learning (CTAL) module, a lightweight
framework that enhances task refinement in multitask networks. CTAL effectively
captures local and long-range cross-task interactions by optimizing task
affinity matrices for parameter-efficient grouped convolutions without concern
for information loss. Our results demonstrate state-of-the-art MTL performance
for both CNN and transformer backbones, using significantly fewer parameters
than single-task learning. Our code is publicly available at
https://github.com/Armanfard-Lab/EMA-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the IEEE Winter Conference on
  Applications of Computer Vision (WACV) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic
  Scene <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandika Biswas, Qianyi Wu, Biplab Banerjee, Hamid Rezatofighi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advancements in Neural Implicit models for 3D surface reconstruction,
handling dynamic environments with arbitrary rigid, non-rigid, or deformable
entities remains challenging. Many template-based methods are entity-specific,
focusing on humans, while generic reconstruction methods adaptable to such
dynamic scenes often require additional inputs like depth or optical flow or
rely on pre-trained image features for reasonable outcomes. These methods
typically use latent codes to capture frame-by-frame deformations. In contrast,
some template-free methods bypass these requirements and adopt traditional LBS
(Linear Blend Skinning) weights for a detailed representation of deformable
object motions, although they involve complex optimizations leading to lengthy
training times. To this end, as a remedy, this paper introduces TFS-NeRF, a
template-free 3D semantic NeRF for dynamic scenes captured from sparse or
single-view RGB videos, featuring interactions among various entities and more
time-efficient than other LBS-based approaches. Our framework uses an
Invertible Neural Network (INN) for LBS prediction, simplifying the training
process. By disentangling the motions of multiple entities and optimizing
per-entity skinning weights, our method efficiently generates accurate,
semantically separable geometries. Extensive experiments demonstrate that our
approach produces high-quality reconstructions of both deformable and
non-deformable objects in complex interactions, with improved training
efficiency compared to existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeTikZify: Synthesizing Graphics Programs for Scientific Figures and
  Sketches with TikZ <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Belouadi, Simone Paolo Ponzetto, Steffen Eger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating high-quality scientific figures can be time-consuming and
challenging, even though sketching ideas on paper is relatively easy.
Furthermore, recreating existing figures that are not stored in formats
preserving semantic information is equally complex. To tackle this problem, we
introduce DeTikZify, a novel multimodal language model that automatically
synthesizes scientific figures as semantics-preserving TikZ graphics programs
based on sketches and existing figures. To achieve this, we create three new
datasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k
human-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn
sketches with their corresponding scientific figures; and MetaFig, a collection
of diverse scientific figures and associated metadata. We train DeTikZify on
MetaFig and DaTikZv2, along with synthetically generated sketches learned from
SketchFig. We also introduce an MCTS-based inference algorithm that enables
DeTikZify to iteratively refine its outputs without the need for additional
training. Through both automatic and human evaluation, we demonstrate that
DeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ
programs, with the MCTS algorithm effectively boosting its performance. We make
our code, models, and datasets publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 (spotlight); Project page:
  https://github.com/potamides/DeTikZify</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OmniGS: Fast Radiance Field Reconstruction using Omnidirectional
  Gaussian Splatting <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03202v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03202v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photorealistic reconstruction relying on 3D Gaussian Splatting has shown
promising potential in various domains. However, the current 3D Gaussian
Splatting system only supports radiance field reconstruction using undistorted
perspective images. In this paper, we present OmniGS, a novel omnidirectional
Gaussian splatting system, to take advantage of omnidirectional images for fast
radiance field reconstruction. Specifically, we conduct a theoretical analysis
of spherical camera model derivatives in 3D Gaussian Splatting. According to
the derivatives, we then implement a new GPU-accelerated omnidirectional
rasterizer that directly splats 3D Gaussians onto the equirectangular screen
space for omnidirectional image rendering. We realize differentiable
optimization of the omnidirectional radiance field without the requirement of
cube-map rectification or tangent-plane approximation. Extensive experiments
conducted in egocentric and roaming scenarios demonstrate that our method
achieves state-of-the-art reconstruction quality and high rendering speed using
omnidirectional images. The code will be publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, accepted by WACV 2025, project page:
  https://liquorleaf.github.io/research/OmniGS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Zhao, Zhaiyu Chen, Zhitong Xiong, Yilei Shi, Sudipan Saha, Xiao Xiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earth Observation (EO) data analysis has been significantly revolutionized by
deep learning (DL), with applications typically limited to grid-like data
structures. Graph Neural Networks (GNNs) emerge as an important innovation,
propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively
tackle the challenges posed by diverse modalities, multiple sensors, and the
heterogeneous nature of EO data. To introduce GNNs in the related domains, our
review begins by offering fundamental knowledge on GNNs. Then, we summarize the
generic problems in EO, to which GNNs can offer potential solutions. Following
this, we explore a broad spectrum of GNNs' applications to scientific problems
in Earth systems, covering areas such as weather and climate analysis, disaster
management, air quality monitoring, agriculture, land cover classification,
hydrological process modeling, and urban modeling. The rationale behind
adopting GNNs in these fields is explained, alongside methodologies for
organizing graphs and designing favorable architectures for various tasks.
Furthermore, we highlight methodological challenges of implementing GNNs in
these domains and possible solutions that could guide future research. While
acknowledging that GNNs are not a universal solution, we conclude the paper by
comparing them with other popular architectures like transformers and analyzing
their potential synergies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Geoscience and Remote Sensing Magazine
  (GRSM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Translation: Towards Unifying Image Recognition, Processing,
  and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09633v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09633v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Xue, Qianru Sun, Li Song, Wenjun Zhang, Zhiwu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose In-Context Translation (ICT), a general learning framework to
unify visual recognition (e.g., semantic segmentation), low-level image
processing (e.g., denoising), and conditional image generation (e.g.,
edge-to-image synthesis). Thanks to unification, ICT significantly reduces the
inherent inductive bias that comes with designing models for specific tasks,
and it maximizes mutual enhancement across similar tasks. However, the
unification across a large number of tasks is non-trivial due to various data
formats and training pipelines. To this end, ICT introduces two designs.
Firstly, it standardizes input-output data of different tasks into RGB image
pairs, e.g., semantic segmentation data pairs an RGB image with its
segmentation mask in the same RGB format. This turns different tasks into a
general translation task between two RGB images. Secondly, it standardizes the
training of different tasks into a general in-context learning, where
"in-context" means the input comprises an example input-output pair of the
target task and a query image. The learning objective is to generate the
"missing" data paired with the query. The implicit translation process is thus
between the query and the generated image. In experiments, ICT unifies ten
vision tasks and showcases impressive performance on their respective
benchmarks. Notably, ICT performs well across three major categories of
computer vision tasks, while its two competitors (Painter and PromptDiffusion)
are only effective in at most two of these task categories. In addition,
compared to its competitors, ICT trained on only 4 RTX 3090 GPUs is shown to be
more efficient and less costly in training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing Webpage UIs for Text-Rich Visual Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13824v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13824v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, Xiang Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich visual understanding-the ability to process environments where
dense textual content is integrated with visuals-is crucial for multimodal
large language models (MLLMs) to interact effectively with structured
environments. To enhance this capability, we propose synthesizing general
multimodal instructions from webpage UIs using text-based large language models
(LLMs). Despite lacking direct visual input, text-based LLMs are able to
process structured text representations from webpage accessibility trees. These
instructions are then paired with UI screenshots to train multimodal models. We
introduce MultiUI, a dataset containing 7.3 million samples from 1 million
websites, covering diverse multimodal tasks and UI layouts. Models trained on
MultiUI not only excel in web UI tasks-achieving up to a 48% improvement on
VisualWebBench and a 19.1% boost in element accuracy on a web agent dataset
Mind2Web-but also generalize surprisingly well to non-web UI tasks and even to
non-UI domains, such as document understanding, OCR, and chart interpretation.
These results highlight the broad applicability of web UI data for advancing
text-rich visual understanding across various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-label Cluster Discrimination for Visual Representation Learning <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang An, Kaicheng Yang, Xiangzi Dai, Ziyong Feng, Jiankang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language Image Pre-training (CLIP) has recently demonstrated
success across various tasks due to superior feature representation empowered
by image-text contrastive learning. However, the instance discrimination method
used by CLIP can hardly encode the semantic structure of training data. To
handle this limitation, cluster discrimination has been proposed through
iterative cluster assignment and classification. Nevertheless, most cluster
discrimination approaches only define a single pseudo-label for each image,
neglecting multi-label signals in the image. In this paper, we propose a novel
Multi-Label Cluster Discrimination method named MLCD to enhance representation
learning. In the clustering step, we first cluster the large-scale LAION-400M
dataset into one million centers based on off-the-shelf embedding features.
Considering that natural images frequently contain multiple visual objects or
attributes, we select the multiple closest centers as auxiliary class labels.
In the discrimination step, we design a novel multi-label classification loss,
which elegantly separates losses from positive classes and negative classes,
and alleviates ambiguity on decision boundary. We validate the proposed
multi-label cluster discrimination method with experiments on different scales
of models and pre-training datasets. Experimental results show that our method
achieves state-of-the-art performance on multiple downstream tasks including
linear probe, zero-shot classification, and image-text retrieval. Code and
models have been released at https://github.com/deepglint/unicom .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CPnP: Consistent Pose Estimator for Perspective-n-Point Problem with
  Bias Elimination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyang Zeng, Shiyu Chen, Biqiang Mu, Guodong Shi, Junfeng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Perspective-n-Point (PnP) problem has been widely studied in both
computer vision and photogrammetry societies. With the development of feature
extraction techniques, a large number of feature points might be available in a
single shot. It is promising to devise a consistent estimator, i.e., the
estimate can converge to the true camera pose as the number of points
increases. To this end, we propose a consistent PnP solver, named \emph{CPnP},
with bias elimination. Specifically, linear equations are constructed from the
original projection model via measurement model modification and variable
elimination, based on which a closed-form least-squares solution is obtained.
We then analyze and subtract the asymptotic bias of this solution, resulting in
a consistent estimate. Additionally, Gauss-Newton (GN) iterations are executed
to refine the consistent solution. Our proposed estimator is efficient in terms
of computations -- it has $O(n)$ computational complexity. Experimental tests
on both synthetic data and real images show that our proposed estimator is
superior to some well-known ones for images with dense visual features, in
terms of estimation precision and computing time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VHM: Versatile and Honest Vision Language Model for Remote Sensing Image
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.20213v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.20213v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Pang, Xingxing Weng, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Shuai Wang, Litong Feng, Gui-Song Xia, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a Versatile and Honest vision language Model (VHM) for
remote sensing image analysis. VHM is built on a large-scale remote sensing
image-text dataset with rich-content captions (VersaD), and an honest
instruction dataset comprising both factual and deceptive questions (HnstD).
Unlike prevailing remote sensing image-text datasets, in which image captions
focus on a few prominent objects and their relationships, VersaD captions
provide detailed information about image properties, object attributes, and the
overall scene. This comprehensive captioning enables VHM to thoroughly
understand remote sensing images and perform diverse remote sensing tasks.
Moreover, different from existing remote sensing instruction datasets that only
include factual questions, HnstD contains additional deceptive questions
stemming from the non-existence of objects. This feature prevents VHM from
producing affirmative answers to nonsense queries, thereby ensuring its
honesty. In our experiments, VHM significantly outperforms various vision
language models on common tasks of scene classification, visual question
answering, and visual grounding. Additionally, VHM achieves competent
performance on several unexplored tasks, such as building vectorizing,
multi-label classification and honest question answering. We will release the
code, data and model weights at https://github.com/opendatalab/VHM .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contribution: Chao Pang, Xingxing Weng, Jiang Wu; Corresponding
  author: Gui-Song Xia, Conghui He</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedFMS: Exploring Federated Foundation Models for Medical Image
  Segmentation <span class="chip">MICCAI'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxi Liu, Guibo Luo, Yuesheng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical image segmentation is crucial for clinical diagnosis. The
Segmentation Anything Model (SAM) serves as a powerful foundation model for
visual segmentation and can be adapted for medical image segmentation. However,
medical imaging data typically contain privacy-sensitive information, making it
challenging to train foundation models with centralized storage and sharing. To
date, there are few foundation models tailored for medical image deployment
within the federated learning framework, and the segmentation performance, as
well as the efficiency of communication and training, remain unexplored. In
response to these issues, we developed Federated Foundation models for Medical
image Segmentation (FedFMS), which includes the Federated SAM (FedSAM) and a
communication and training-efficient Federated SAM with Medical SAM Adapter
(FedMSA). Comprehensive experiments on diverse datasets are conducted to
investigate the performance disparities between centralized training and
federated learning across various configurations of FedFMS. The experiments
revealed that FedFMS could achieve performance comparable to models trained via
centralized training methods while maintaining privacy. Furthermore, FedMSA
demonstrated the potential to enhance communication and training efficiency.
Our model implementation codes are available at
https://github.com/LIU-YUXI/FedFMS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICCAI'2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition
  via Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02188v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02188v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjith George, Sebastien Marcel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accuracy of face recognition systems has improved significantly in the
past few years, thanks to the large amount of data collected and the
advancement in neural network architectures. However, these large-scale
datasets are often collected without explicit consent, raising ethical and
privacy concerns. To address this, there have been proposals to use synthetic
datasets for training face recognition models. Yet, such models still rely on
real data to train the generative models and generally exhibit inferior
performance compared to those trained on real datasets. One of these datasets,
DigiFace, uses a graphics pipeline to generate different identities and
different intra-class variations without using real data in training the
models. However, the performance of this approach is poor on face recognition
benchmarks, possibly due to the lack of realism in the images generated from
the graphics pipeline. In this work, we introduce a novel framework for realism
transfer aimed at enhancing the realism of synthetically generated face images.
Our method leverages the large-scale face foundation model, and we adapt the
pipeline for realism enhancement. By integrating the controllable aspects of
the graphics pipeline with our realism enhancement technique, we generate a
large amount of realistic variations-combining the advantages of both
approaches. Our empirical evaluations demonstrate that models trained using our
enhanced dataset significantly improve the performance of face recognition
systems over the baseline. The source code and datasets will be made available
publicly: https://www.idiap.ch/paper/digi2real
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The dataset would be available here:
  https://www.idiap.ch/paper/digi2real</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Data Perspective on Enhanced Identity Preservation for <span class="highlight-title">Diffusion</span>
  Personalization <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04315v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04315v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingzhe He, Zhiwen Cao, Nicholas Kolkin, Lantao Yu, Kun Wan, Helge Rhodin, Ratheesh Kalarot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large text-to-image models have revolutionized the ability to generate
imagery using natural language. However, particularly unique or personal visual
concepts, such as pets and furniture, will not be captured by the original
model. This has led to interest in how to personalize a text-to-image model.
Despite significant progress, this task remains a formidable challenge,
particularly in preserving the subject's identity. Most researchers attempt to
address this issue by modifying model architectures. These methods are capable
of keeping the subject structure and color but fail to preserve identity
details. Towards this issue, our approach takes a data-centric perspective. We
introduce a novel regularization dataset generation strategy on both the text
and image level. This strategy enables the model to preserve fine details of
the desired subjects, such as text and logos. Our method is
architecture-agnostic and can be flexibly applied on various text-to-image
models. We show on established benchmarks that our data-centric approach forms
the new state of the art in terms of identity preservation and text alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Utilizing Large Language Models in an iterative paradigm with Domain
  feedback for Zero-shot Molecule optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13147v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13147v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khiem Le, Nitesh V. Chawla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecule optimization is a critical task in drug discovery to optimize
desired properties of a given molecule through chemical modification. Despite
Large Language Models (LLMs) holding the potential to efficiently simulate this
task by using natural language to direct the optimization, straightforwardly
utilizing shows limited performance. In this work, we facilitate utilizing LLMs
in an iterative paradigm by proposing a simple yet highly effective domain
feedback provider, namely $\text{Re}^3$DF. In detail, $\text{Re}^3$DF harnesses
an external toolkit, RDKit, to handle the molecule hallucination, if the
modified molecule is chemically invalid. Otherwise, its desired properties are
computed and compared to the original one, establishing reliable domain
feedback with correct direction and distance towards the objective, followed by
a retrieved example, to explicitly guide the LLM to refine the modified
molecule. We conduct experiments across both single- and multi-property
objectives with 2 thresholds, where $\text{Re}^3$DF shows significant
improvements. Particularly, for 20 single-property objectives, $\text{Re}^3$DF
enhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,
respectively. For 32 multi-property objectives, $\text{Re}^3$DF enhances Hit
ratio by 6.04% and 5.25%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalizing Alignment Paradigm of Text-to-Image Generation with
  Preferences through $f$-divergence Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Sun, Bo Xia, Yongzhe Chang, Xueqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has recently expanded its successful
application from aligning large language models (LLMs) to aligning
text-to-image models with human preferences, which has generated considerable
interest within the community. However, we have observed that these approaches
rely solely on minimizing the reverse Kullback-Leibler divergence during
alignment process between the fine-tuned model and the reference model,
neglecting the incorporation of other divergence constraints. In this study, we
focus on extending reverse Kullback-Leibler divergence in the alignment
paradigm of text-to-image models to $f$-divergence, which aims to garner better
alignment performance as well as good generation diversity. We provide the
generalized formula of the alignment paradigm under the $f$-divergence
condition and thoroughly analyze the impact of different divergence constraints
on alignment process from the perspective of gradient fields. We conduct
comprehensive evaluation on image-text alignment performance, human value
alignment performance and generation diversity performance under different
divergence constraints, and the results indicate that alignment based on
Jensen-Shannon divergence achieves the best trade-off among them. The option of
divergence employed for aligning text-to-image models significantly impacts the
trade-off between alignment performance (especially human value alignment) and
generation diversity, which highlights the necessity of selecting an
appropriate divergence for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Temporal Context Learning for Camera-based Semantic Scene
  Completion <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02077v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02077v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Li, Jiajun Deng, Wenyao Zhang, Zhujin Liang, Dalong Du, Xin Jin, Wenjun Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera-based 3D semantic scene completion (SSC) is pivotal for predicting
complicated 3D layouts with limited 2D image observations. The existing
mainstream solutions generally leverage temporal information by roughly
stacking history frames to supplement the current frame, such straightforward
temporal modeling inevitably diminishes valid clues and increases learning
difficulty. To address this problem, we present HTCL, a novel Hierarchical
Temporal Context Learning paradigm for improving camera-based semantic scene
completion. The primary innovation of this work involves decomposing temporal
context learning into two hierarchical steps: (a) cross-frame affinity
measurement and (b) affinity-based dynamic refinement. Firstly, to separate
critical relevant context from redundant information, we introduce the pattern
affinity with scale-aware isolation and multiple independent learners for
fine-grained contextual correspondence modeling. Subsequently, to dynamically
compensate for incomplete observations, we adaptively refine the feature
sampling locations based on initially identified locations with high affinity
and their neighboring relevant regions. Our method ranks $1^{st}$ on the
SemanticKITTI benchmark and even surpasses LiDAR-based methods in terms of mIoU
on the OpenOccupancy benchmark. Our code is available on
https://github.com/Arlo0o/HTCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TalkMosaic: Interactive PhotoMosaic with Multi-modal LLM Q&A
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Li, Fulu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We use images of cars of a wide range of varieties to compose an image of an
animal such as a bird or a lion for the theme of environmental protection to
maximize the information about cars in a single composed image and to raise the
awareness about environmental challenges. We present a novel way of image
interaction with an artistically-composed photomosaic image, in which a simple
operation of "click and display" is used to demonstrate the interactive switch
between a tile image in a photomosaic image and the corresponding original car
image, which will be automatically saved on the Desktop. We build a multimodal
custom GPT named TalkMosaic by incorporating car images information and the
related knowledge to ChatGPT. By uploading the original car image to
TalkMosaic, we can ask questions about the given car image and get the
corresponding answers efficiently and effectively such as where to buy the tire
in the car image that satisfies high environmental standards. We give an
in-depth analysis on how to speed up the inference of multimodal LLM using
sparse attention and quantization techniques with presented probabilistic
FlashAttention (PrFlashAttention) and Staircase Adaptive Quantization (SAQ)
methods. The implemented prototype demonstrates the feasibility and
effectiveness of the presented approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Surgical Instrument Segmentation Without Human Intervention:
  A Graph Partitioning View <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Sheng, Jianan Fan, Dongnan Liu, Ron Kikinis, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical instrument segmentation (SIS) on endoscopic images stands as a
long-standing and essential task in the context of computer-assisted
interventions for boosting minimally invasive surgery. Given the recent surge
of deep learning methodologies and their data-hungry nature, training a neural
predictive model based on massive expert-curated annotations has been
dominating and served as an off-the-shelf approach in the field, which could,
however, impose prohibitive burden to clinicians for preparing fine-grained
pixel-wise labels corresponding to the collected surgical video frames. In this
work, we propose an unsupervised method by reframing the video frame
segmentation as a graph partitioning problem and regarding image pixels as
graph nodes, which is significantly different from the previous efforts. A
self-supervised pre-trained model is firstly leveraged as a feature extractor
to capture high-level semantic features. Then, Laplacian matrixs are computed
from the features and are eigendecomposed for graph partitioning. On the "deep"
eigenvectors, a surgical video frame is meaningfully segmented into different
modules such as tools and tissues, providing distinguishable semantic
information like locations, classes, and relations. The segmentation problem
can then be naturally tackled by applying clustering or threshold on the
eigenvectors. Extensive experiments are conducted on various datasets (e.g.,
EndoVis2017, EndoVis2018, UCL, etc.) for different clinical endpoints. Across
all the challenging scenarios, our method demonstrates outstanding performance
and robustness higher than unsupervised state-of-the-art (SOTA) methods. The
code is released at https://github.com/MingyuShengSMY/GraphClusteringSIS.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by The 32nd ACM International Conference on
  Multimedia (ACM MM 2024) Workshop on Multimedia Computing for Health and
  Medicine (MCHM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Identifying and Solving Conditional Image Leakage in Image-to-Video
  <span class="highlight-title">Diffusion</span> Model <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Zhao, Hongzhou Zhu, Chendong Xiang, Kaiwen Zheng, Chongxuan Li, <span class="highlight-author">Jun Zhu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have obtained substantial progress in image-to-video
generation. However, in this paper, we find that these models tend to generate
videos with less motion than expected. We attribute this to the issue called
conditional image leakage, where the image-to-video diffusion models (I2V-DMs)
tend to over-rely on the conditional image at large time steps. We further
address this challenge from both inference and training aspects. First, we
propose to start the generation process from an earlier time step to avoid the
unreliable large-time steps of I2V-DMs, as well as an initial noise
distribution with optimal analytic expressions (Analytic-Init) by minimizing
the KL divergence between it and the actual marginal distribution to bridge the
training-inference gap. Second, we design a time-dependent noise distribution
(TimeNoise) for the conditional image during training, applying higher noise
levels at larger time steps to disrupt it and reduce the model's dependency on
it. We validate these general strategies on various I2V-DMs on our collected
open-domain image benchmark and the UCF101 dataset. Extensive results show that
our methods outperform baselines by producing higher motion scores with lower
errors while maintaining image alignment and temporal consistency, thereby
yielding superior overall performance and enabling more accurate motion
control. The project page: \url{https://cond-image-leak.github.io/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Project page: https://cond-image-leak.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIWR: Aerial Image Water Resource <span class="highlight-title">Dataset</span> for Segmentation Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01797v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01797v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangdaow Noppitak, Emmanuel Okafor, Olarik Surinta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective water resource management is crucial in agricultural regions like
northeastern Thailand, where limited water retention in sandy soils poses
significant challenges. In response to this issue, the Aerial Image Water
Resource (AIWR) dataset was developed, comprising 800 aerial images focused on
natural and artificial water bodies in this region. The dataset was created
using Bing Maps and follows the standards of the Fundamental Geographic Data
Set (FGDS). It includes ground truth annotations validated by experts in remote
sensing, making it an invaluable resource for researchers in geoinformatics,
computer vision, and artificial intelligence. The AIWR dataset presents
considerable challenges, such as segmentation due to variations in the size,
color, shape, and similarity of water bodies, which often resemble other land
use categories. The objective of the proposed dataset is to explore advanced
AI-driven methods for water body segmentation, addressing the unique challenges
posed by the dataset complexity and limited size. This dataset and related
research contribute to the development of novel algorithms for water
management, supporting sustainable agricultural practices in regions facing
similar challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Efficient Brain Tumor Multi-Class Classification -- New
  Insights from the Vision Mamba Model in Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinyi Lai, Anbo Cao, Yuan Gao, Jiaqi Shang, Zongyu Li, Jia Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early and accurate diagnosis of brain tumors is crucial for improving patient
survival rates. However, the detection and classification of brain tumors are
challenging due to their diverse types and complex morphological
characteristics. This study investigates the application of pre-trained models
for brain tumor classification, with a particular focus on deploying the Mamba
model. We fine-tuned several mainstream transfer learning models and applied
them to the multi-class classification of brain tumors. By comparing these
models to those trained from scratch, we demonstrated the significant
advantages of transfer learning, especially in the medical imaging field, where
annotated data is often limited. Notably, we introduced the Vision Mamba (Vim),
a novel network architecture, and applied it for the first time in brain tumor
classification, achieving exceptional classification accuracy. Experimental
results indicate that the Vim model achieved 100% classification accuracy on an
independent test set, emphasizing its potential for tumor classification tasks.
These findings underscore the effectiveness of transfer learning in brain tumor
classification and reveal that, compared to existing state-of-the-art models,
the Vim model is lightweight, efficient, and highly accurate, offering a new
perspective for clinical applications. Furthermore, the framework proposed in
this study for brain tumor classification, based on transfer learning and the
Vision Mamba model, is broadly applicable to other medical imaging
classification problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D2SP: Dynamic Dual-Stage Purification Framework for Dual Noise
  Mitigation in Vision-based Affective Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Wang, Xinji Mai, Zeng Tao, Xuan Tong, Junxiong Lin, Yan Wang, Jiawen Yu, Boyang Wang, Shaoqi Yan, Qing Zhao, Ziheng Zhou, Shuyong Gao, Wenqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The contemporary state-of-the-art of Dynamic Facial Expression Recognition
(DFER) technology facilitates remarkable progress by deriving emotional
mappings of facial expressions from video content, underpinned by training on
voluminous datasets. Yet, the DFER datasets encompass a substantial volume of
noise data. Noise arises from low-quality captures that defy logical labeling,
and instances that suffer from mislabeling due to annotation bias, engendering
two principal types of uncertainty: the uncertainty regarding data usability
and the uncertainty concerning label reliability. Addressing the two types of
uncertainty, we have meticulously crafted a two-stage framework aiming at
\textbf{S}eeking \textbf{C}ertain data \textbf{I}n extensive \textbf{U}ncertain
data (SCIU). This initiative aims to purge the DFER datasets of these
uncertainties, thereby ensuring that only clean, verified data is employed in
training processes. To mitigate the issue of low-quality samples, we introduce
the Coarse-Grained Pruning (CGP) stage, which assesses sample weights and
prunes those deemed unusable due to their low weight. For samples with
incorrect annotations, the Fine-Grained Correction (FGC) stage evaluates
prediction stability to rectify mislabeled data. Moreover, SCIU is conceived as
a universally compatible, plug-and-play framework, tailored to integrate
seamlessly with prevailing DFER methodologies. Rigorous experiments across
prevalent DFER datasets and against numerous benchmark methods substantiates
SCIU's capacity to markedly elevate performance metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Document Parsing Unveiled: Techniques, Challenges, and Prospects for
  Structured Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qintong Zhang, Victor Shea-Jay Huang, Bin Wang, Junyuan Zhang, Zhengren Wang, Hao Liang, Shawn Wang, Matthieu Lin, Conghui He, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document parsing is essential for converting unstructured and semi-structured
documents-such as contracts, academic papers, and invoices-into structured,
machine-readable data. Document parsing extract reliable structured data from
unstructured inputs, providing huge convenience for numerous applications.
Especially with recent achievements in Large Language Models, document parsing
plays an indispensable role in both knowledge base construction and training
data generation. This survey presents a comprehensive review of the current
state of document parsing, covering key methodologies, from modular pipeline
systems to end-to-end models driven by large vision-language models. Core
components such as layout detection, content extraction (including text,
tables, and mathematical expressions), and multi-modal data integration are
examined in detail. Additionally, this paper discusses the challenges faced by
modular document parsing systems and vision-language models in handling complex
layouts, integrating multiple modules, and recognizing high-density text. It
emphasizes the importance of developing larger and more diverse datasets and
outlines future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Task-Specific Strategies for Accelerated MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.12507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.12507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihui Wu, Tianwei Yin, Yu Sun, Robert Frost, Andre van der Kouwe, Adrian V. Dalca, Katherine L. Bouman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compressed sensing magnetic resonance imaging (CS-MRI) seeks to recover
visual information from subsampled measurements for diagnostic tasks.
Traditional CS-MRI methods often separately address measurement subsampling,
image reconstruction, and task prediction, resulting in a suboptimal end-to-end
performance. In this work, we propose TACKLE as a unified co-design framework
for jointly optimizing subsampling, reconstruction, and prediction strategies
for the performance on downstream tasks. The na\"ive approach of simply
appending a task prediction module and training with a task-specific loss leads
to suboptimal downstream performance. Instead, we develop a training procedure
where a backbone architecture is first trained for a generic pre-training task
(image reconstruction in our case), and then fine-tuned for different
downstream tasks with a prediction head. Experimental results on multiple
public MRI datasets show that TACKLE achieves an improved performance on
various tasks over traditional CS-MRI methods. We also demonstrate that TACKLE
is robust to distribution shifts by showing that it generalizes to a new
dataset we experimentally collected using different acquisition setups from the
training data. Without additional fine-tuning, TACKLE leads to both numerical
and visual improvements compared to existing baselines. We have further
implemented a learned 4$\times$-accelerated sequence on a Siemens 3T MRI Skyra
scanner. Compared to the fully-sampling scan that takes 335 seconds, our
optimized sequence only takes 84 seconds, achieving a four-fold time reduction
as desired, while maintaining high performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/zihuiwu/TACKLE. More
  information can be found at http://imaging.cms.caltech.edu/tackle/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiently Collecting Training <span class="highlight-title">Dataset</span> for 2D Object Detection by
  Online Visual Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuya Kiyokawa, Naoki Shirakura, Hiroki Katayama, Keita Tomochika, Jun Takamatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep-learning-based vision systems require the manual annotation of
a significant number of images. Such manual annotation is highly time-consuming
and labor-intensive. Although previous studies have attempted to eliminate the
effort required for annotation, the effort required for image collection was
retained. To address this, we propose a human-in-the-loop dataset collection
method that uses a web application. To counterbalance the workload and
performance by encouraging the collection of multi-view object image datasets
in an enjoyable manner, thereby amplifying motivation, we propose three types
of online visual feedback features to track the progress of the collection
status. Our experiments thoroughly investigated the impact of each feature on
collection performance and quality of operation. The results suggested the
feasibility of annotation and object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating Epistemic and Aleatoric Uncertainty with a Single Model <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew A. Chan, Maria J. Molina, Christopher A. Metzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating and disentangling epistemic uncertainty, uncertainty that is
reducible with more training data, and aleatoric uncertainty, uncertainty that
is inherent to the task at hand, is critically important when applying machine
learning to high-stakes applications such as medical imaging and weather
forecasting. Conditional diffusion models' breakthrough ability to accurately
and efficiently sample from the posterior distribution of a dataset now makes
uncertainty estimation conceptually straightforward: One need only train and
sample from a large ensemble of diffusion models. Unfortunately, training such
an ensemble becomes computationally intractable as the complexity of the model
architecture grows. In this work we introduce a new approach to ensembling,
hyper-diffusion models (HyperDM), which allows one to accurately estimate both
epistemic and aleatoric uncertainty with a single model. Unlike existing
single-model uncertainty methods like Monte-Carlo dropout and Bayesian neural
networks, HyperDM offers prediction accuracy on par with, and in some cases
superior to, multi-model ensembles. Furthermore, our proposed approach scales
to modern network architectures such as Attention U-Net and yields more
accurate uncertainty estimates compared to existing methods. We validate our
method on two distinct real-world tasks: x-ray computed tomography
reconstruction and weather temperature forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 11 figures. To be published in Conference on Neural
  Information Processing Systems (NeurIPS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FocalPose++: Focal Length and Object Pose Estimation via Render and
  Compare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02985v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02985v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Cífka, Georgy Ponimatkin, Yann Labbé, Bryan Russell, Mathieu Aubry, Vladimir Petrik, Josef Sivic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce FocalPose++, a neural render-and-compare method for jointly
estimating the camera-object 6D pose and camera focal length given a single RGB
input image depicting a known object. The contributions of this work are
threefold. First, we derive a focal length update rule that extends an existing
state-of-the-art render-and-compare 6D pose estimator to address the joint
estimation task. Second, we investigate several different loss functions for
jointly estimating the object pose and focal length. We find that a combination
of direct focal length regression with a reprojection loss disentangling the
contribution of translation, rotation, and focal length leads to improved
results. Third, we explore the effect of different synthetic training data on
the performance of our method. Specifically, we investigate different
distributions used for sampling object's 6D pose and camera's focal length when
rendering the synthetic images, and show that parametric distribution fitted on
real training data works the best. We show results on three challenging
benchmark datasets that depict known 3D models in uncontrolled settings. We
demonstrate that our focal length and 6D pose estimates have lower error than
the existing state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 22 figures. IEEE TPAMI, 2024. Extended version of the
  conference paper arXiv:2204.05145</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferable Learned Image Compression-Resistant Adversarial
  Perturbations <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Sui, Zhuohang Li, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, Zhenzhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks can readily disrupt the image classification system,
revealing the vulnerability of DNN-based recognition tasks. While existing
adversarial perturbations are primarily applied to uncompressed images or
compressed images by the traditional image compression method, i.e., JPEG,
limited studies have investigated the robustness of models for image
classification in the context of DNN-based image compression. With the rapid
evolution of advanced image compression, DNN-based learned image compression
has emerged as the promising approach for transmitting images in many
security-critical applications, such as cloud-based face recognition and
autonomous driving, due to its superior performance over traditional
compression. Therefore, there is a pressing need to fully investigate the
robustness of a classification system post-processed by learned image
compression. To bridge this research gap, we explore the adversarial attack on
a new pipeline that targets image classification models that utilize learned
image compressors as pre-processing modules. Furthermore, to enhance the
transferability of perturbations across various quality levels and
architectures of learned image compression models, we introduce a saliency
score-based sampling method to enable the fast generation of transferable
perturbation. Extensive experiments with popular attack methods demonstrate the
enhanced transferability of our proposed method when attacking images that have
been post-processed with different learned image compression models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by BMVC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine learning approach to brain tumor detection and classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12692v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12692v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Oh, Inyoung Noh, Jian Choo, Jihoo Lee, Justin Park, Kate Hwang, Sanghyeon Kim, Soo Min Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumor detection and classification are critical tasks in medical image
analysis, particularly in early-stage diagnosis, where accurate and timely
detection can significantly improve treatment outcomes. In this study, we apply
various statistical and machine learning models to detect and classify brain
tumors using brain MRI images. We explore a variety of statistical models
including linear, logistic, and Bayesian regressions, and the machine learning
models including decision tree, random forest, single-layer perceptron,
multi-layer perceptron, convolutional neural network (CNN), recurrent neural
network, and long short-term memory. Our findings show that CNN outperforms
other models, achieving the best performance. Additionally, we confirm that the
CNN model can also work for multi-class classification, distinguishing between
four categories of brain MRI images such as normal, glioma, meningioma, and
pituitary tumor images. This study demonstrates that machine learning
approaches are suitable for brain tumor detection and classification,
facilitating real-world medical applications in assisting radiologists with
early and accurate diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ContextIQ: A Multimodal Expert-Based Video Retrieval System for
  Contextual Advertising <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashutosh Chaubey, Anoubhav Agarwaal, Sartaki Sinha Roy, Aayush Agrawal, Susmita Ghose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual advertising serves ads that are aligned to the content that the
user is viewing. The rapid growth of video content on social platforms and
streaming services, along with privacy concerns, has increased the need for
contextual advertising. Placing the right ad in the right context creates a
seamless and pleasant ad viewing experience, resulting in higher audience
engagement and, ultimately, better ad monetization. From a technology
standpoint, effective contextual advertising requires a video retrieval system
capable of understanding complex video content at a very granular level.
Current text-to-video retrieval models based on joint multimodal training
demand large datasets and computational resources, limiting their practicality
and lacking the key functionalities required for ad ecosystem integration. We
introduce ContextIQ, a multimodal expert-based video retrieval system designed
specifically for contextual advertising. ContextIQ utilizes modality-specific
experts-video, audio, transcript (captions), and metadata such as objects,
actions, emotion, etc.-to create semantically rich video representations. We
show that our system, without joint training, achieves better or comparable
results to state-of-the-art models and commercial solutions on multiple
text-to-video retrieval benchmarks. Our ablation studies highlight the benefits
of leveraging multiple modalities for enhanced video retrieval accuracy instead
of using a vision-language model alone. Furthermore, we show how video
retrieval systems such as ContextIQ can be used for contextual advertising in
an ad ecosystem while also addressing concerns related to brand safety and
filtering inappropriate content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INQUIRE: A Natural World Text-to-Image Retrieval Benchmark <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara Beery, Grant Van Horn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce INQUIRE, a text-to-image retrieval benchmark designed to
challenge multimodal vision-language models on expert-level queries. INQUIRE
includes iNaturalist 2024 (iNat24), a new dataset of five million natural world
images, along with 250 expert-level retrieval queries. These queries are paired
with all relevant images comprehensively labeled within iNat24, comprising
33,000 total matches. Queries span categories such as species identification,
context, behavior, and appearance, emphasizing tasks that require nuanced image
understanding and domain expertise. Our benchmark evaluates two core retrieval
tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)
INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed
evaluation of a range of recent multimodal models demonstrates that INQUIRE
poses a significant challenge, with the best models failing to achieve an
mAP@50 above 50%. In addition, we show that reranking with more powerful
multimodal models can enhance retrieval performance, yet there remains a
significant margin for improvement. By focusing on scientifically-motivated
ecological challenges, INQUIRE aims to bridge the gap between AI capabilities
and the needs of real-world scientific inquiry, encouraging the development of
retrieval systems that can assist with accelerating ecological and biodiversity
research. Our dataset and code are available at
https://inquire-benchmark.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reproducible Hybrid Time-Travel Retrieval in Evolving Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Staudinger, Florina Piroi, Andreas Rauber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are settings in which reproducibility of ranked lists is desirable,
such as when extracting a subset of an evolving document corpus for downstream
research tasks or in domains such as patent retrieval or in medical systematic
reviews, with high reproducibility expectations. However, as global term
statistics change when documents change or are added to a corpus, queries using
typical ranked retrieval models are not even reproducible for the parts of the
document corpus that have not changed. Thus, Boolean retrieval frequently
remains the mechanism of choice in such settings.
  We present a hybrid retrieval system combining Lucene for fast retrieval with
a column-store-based retrieval system maintaining a versioned and time-stamped
index. The latter component allows re-execution of previously posed queries
resulting in the same ranked list and further allows for time-travel queries
over evolving collection, as web archives, while maintaining the original
ranking. Thus, retrieval results in evolving document collections are fully
reproducible even when document collections and thus term statistics change.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Guidance for Retrievers: Leveraging LLMs' Feedback in
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Liu, Xueyu Hu, Shengyu Zhang, Jingyuan Chen, Fan Wu, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has proven to be an effective method for
mitigating hallucination issues inherent in large language models (LLMs).
Previous approaches typically train retrievers based on semantic similarity,
lacking optimization for RAG. More recent works have proposed aligning
retrievers with the preference signals of LLMs. However, these preference
signals are often difficult for dense retrievers, which typically have weaker
language capabilities, to understand and learn effectively. Drawing inspiration
from pedagogical theories like Guided Discovery Learning, we propose a novel
framework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the
language capabilities of LLMs to construct examples from a more granular,
information-centric perspective to guide the learning of retrievers.
Specifically, our method utilizes LLMs to construct easy-to-understand examples
from samples where the retriever performs poorly, focusing on three learning
objectives highly relevant to the RAG scenario: relevance, comprehensiveness,
and purity. These examples serve as scaffolding to ultimately align the
retriever with the LLM's preferences. Furthermore, we employ a dual curriculum
learning strategy and leverage the reciprocal feedback between LLM and
retriever to further enhance the performance of the RAG system. A series of
experiments demonstrate that our proposed framework enhances the performance of
RAG systems equipped with different retrievers and is applicable to various
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lexicalization Is All You Need: Examining the Impact of Lexical
  Knowledge in a Compositional QALD System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Maria Schmidt, Mohammad Fazleh Elahi, Philipp Cimiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we examine the impact of lexicalization on Question Answering
over Linked Data (QALD). It is well known that one of the key challenges in
interpreting natural language questions with respect to SPARQL lies in bridging
the lexical gap, that is mapping the words in the query to the correct
vocabulary elements. We argue in this paper that lexicalization, that is
explicit knowledge about the potential interpretations of a word with respect
to the given vocabulary, significantly eases the task and increases the
performance of QA systems. Towards this goal, we present a compositional QA
system that can leverage explicit lexical knowledge in a compositional manner
to infer the meaning of a question in terms of a SPARQL query. We show that
such a system, given lexical knowledge, has a performance well beyond current
QA systems, achieving up to a $35.8\%$ increase in the micro $F_1$ score
compared to the best QA system on QALD-9. This shows the importance and
potential of including explicit lexical knowledge. In contrast, we show that
LLMs have limited abilities to exploit lexical knowledge, with only marginal
improvements compared to a version without lexical knowledge. This shows that
LLMs have no ability to compositionally interpret a question on the basis of
the meaning of its parts, a key feature of compositional approaches. Taken
together, our work shows new avenues for QALD research, emphasizing the
importance of lexicalization and compositionality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24th International Conference on Knowledge Engineering and Knowledge
  Management (EKAW 2024), November 26-28, 2024, Amsterdam, The Netherlands</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Fusion of Synthetic Query Variants With Generative Large Language
  Models <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timo Breuer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering query variance in information retrieval (IR) experiments is
beneficial for retrieval effectiveness. Especially ranking ensembles based on
different topically related queries retrieve better results than rankings based
on a single query alone. Recently, generative instruction-tuned Large Language
Models (LLMs) improved on a variety of different tasks in capturing human
language. To this end, this work explores the feasibility of using synthetic
query variants generated by instruction-tuned LLMs in data fusion experiments.
More specifically, we introduce a lightweight, unsupervised, and cost-efficient
approach that exploits principled prompting and data fusion techniques. In our
experiments, LLMs produce more effective queries when provided with additional
context information on the topic. Furthermore, our analysis based on four TREC
newswire benchmarks shows that data fusion based on synthetic query variants is
significantly better than baselines with single queries and also outperforms
pseudo-relevance feedback methods. We publicly share the code and query
datasets with the community as resources for follow-up studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The definitive version of record was published in SIGIR-AP '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Essence of the Essence from the Web:The Metasearch Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajender Nath, Satinder Bal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of information source on the web and in turn
continuing technological progress of searching the information by using tools
like Search Engines gives rise to many problems for the user to know which tool
is best for their query and which tool is not. At this time Metasearch Engine
comes into play by reducing the user burden by dispatching queries to multiple
search engines in parallel and refining the results of these search engines to
give the best out of best by doing superior job on their side. These engines do
not own a database of Web pages rather they send search terms to the databases
maintained by the search engine companies, get back results from all the search
engines queried and then compile the results to be presented to the user. In
this paper, we describe the working of a typical metasearch engine and then
present a comparative study of traditional search engines and metasearch
engines on the basis of different parameters and show how metasearch engines
are better than the other search engines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEGMN: A Structure-Enhanced Graph Matching Network for Graph Similarity
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Wang, Jiacheng Lu, Kejia Chen, Zheng Liu, Shilong Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph similarity computation (GSC) aims to quantify the similarity score
between two graphs. Although recent GSC methods based on graph neural networks
(GNNs) take advantage of intra-graph structures in message passing, few of them
fully utilize the structures presented by edges to boost the representation of
their connected nodes. Moreover, previous cross-graph node embedding matching
lacks the perception of the overall structure of the graph pair, due to the
fact that the node representations from GNNs are confined to the intra-graph
structure, causing the unreasonable similarity score. Intuitively, the
cross-graph structure represented in the assignment graph is helpful to rectify
the inappropriate matching. Therefore, we propose a structure-enhanced graph
matching network (SEGMN). Equipped with a dual embedding learning module and a
structure perception matching module, SEGMN achieves structure enhancement in
both embedding learning and cross-graph matching. The dual embedding learning
module incorporates adjacent edge representation into each node to achieve a
structure-enhanced representation. The structure perception matching module
achieves cross-graph structure enhancement through assignment graph
convolution. The similarity score of each cross-graph node pair can be
rectified by aggregating messages from structurally relevant node pairs.
Experimental results on benchmark datasets demonstrate that SEGMN outperforms
the state-of-the-art GSC methods in the GED regression task, and the structure
perception matching module is plug-and-play, which can further improve the
performance of the baselines by up to 25%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge
  Reasoning and Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Dong, Shuo Wang, Hongye Zheng, Jiajing Chen, Zhenhong Zhang, Chihang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aims to optimize the existing retrieval-augmented generation model
(RAG) by introducing a graph structure to improve the performance of the model
in dealing with complex knowledge reasoning tasks. The traditional RAG model
has the problem of insufficient processing efficiency when facing complex graph
structure information (such as knowledge graphs, hierarchical relationships,
etc.), which affects the quality and consistency of the generated results. This
study proposes a scheme to process graph structure data by combining graph
neural network (GNN), so that the model can capture the complex relationship
between entities, thereby improving the knowledge consistency and reasoning
ability of the generated text. The experiment used the Natural Questions (NQ)
dataset and compared it with multiple existing generation models. The results
show that the graph-based RAG model proposed in this paper is superior to the
traditional generation model in terms of quality, knowledge consistency, and
reasoning ability, especially when dealing with tasks that require
multi-dimensional reasoning. Through the combination of the enhancement of the
retrieval module and the graph neural network, the model in this study can
better handle complex knowledge background information and has broad potential
value in multiple practical application scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PersianRAG: A Retrieval-Augmented Generation System for Persian Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Hosseini, Mohammad Sobhan Zare, Amir Hossein Mohammadi, Arefeh Kazemi, Zahra Zojaji, Mohammad Ali Nematbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmented generation (RAG) models, which integrate large-scale
pre-trained generative models with external retrieval mechanisms, have shown
significant success in various natural language processing (NLP) tasks.
However, applying RAG models in Persian language as a low-resource language,
poses distinct challenges. These challenges primarily involve the
preprocessing, embedding, retrieval, prompt construction, language modeling,
and response evaluation of the system. In this paper, we address the challenges
towards implementing a real-world RAG system for Persian language called
PersianRAG. We propose novel solutions to overcome these obstacles and evaluate
our approach using several Persian benchmark datasets. Our experimental results
demonstrate the capability of the PersianRAG framework to enhance question
answering task in Persian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Compositional Data Augmentation for Scientific Keyphrase Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03039v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03039v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mael Houbre, Florian Boudin, Beatrice Daille, Akiko Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art models for keyphrase generation require large amounts of
training data to achieve good performance. However, obtaining keyphrase-labeled
documents can be challenging and costly. To address this issue, we present a
self-compositional data augmentation method. More specifically, we measure the
relatedness of training documents based on their shared keyphrases, and combine
similar documents to generate synthetic samples. The advantage of our method
lies in its ability to create additional training samples that keep domain
coherence, without relying on external data or resources. Our results on
multiple datasets spanning three different domains, demonstrate that our method
consistently improves keyphrase generation. A qualitative analysis of the
generated keyphrases for the Computer Science domain confirms this improvement
towards their representativity property.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to JCDL 2024. This is the author's version of the work. It
  is posted here for your personal use. Not for redistribution. The definitive
  version was published in the proceedings of the 2024 ACM/IEEE Joint
  Conference on Digital Libraries (JCDL 24)
  https://doi.org/10.1145/3677389.3702504</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CheX-<span class="highlight-title">GPT</span>: Harnessing Large Language Models for Enhanced Chest X-ray
  Report Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jawook Gu, Kihyun You, Han-Cheol Cho, Jiho Kim, Eun Kyoung Hong, Byungseok Roh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Free-text radiology reports present a rich data source for various medical
tasks, but effectively labeling these texts remains challenging. Traditional
rule-based labeling methods fall short of capturing the nuances of diverse
free-text patterns. Moreover, models using expert-annotated data are limited by
data scarcity and pre-defined classes, impacting their performance, flexibility
and scalability. To address these issues, our study offers three main
contributions: 1) We demonstrate the potential of GPT as an adept labeler using
carefully designed prompts. 2) Utilizing only the data labeled by GPT, we
trained a BERT-based labeler, CheX-GPT, which operates faster and more
efficiently than its GPT counterpart. 3) To benchmark labeler performance, we
introduced a publicly available expert-annotated test set, MIMIC-500,
comprising 500 cases from the MIMIC validation set. Our findings demonstrate
that CheX-GPT not only excels in labeling accuracy over existing models, but
also showcases superior efficiency, flexibility, and scalability, supported by
our introduction of the MIMIC-500 dataset for robust benchmarking. Code and
models are available at https://github.com/Soombit-ai/CheXGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Real-Time Adaptive Multi-Stream GPU System for Online Approximate
  Nearest Neighborhood Search <span class="chip">CIKM'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiping Sun, Yang Shi, Jiaolong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Approximate Nearest Neighbor Search (ANNS) has played a
pivotal role in modern search and recommendation systems, especially in
emerging LLM applications like Retrieval-Augmented Generation. There is a
growing exploration into harnessing the parallel computing capabilities of GPUs
to meet the substantial demands of ANNS. However, existing systems primarily
focus on offline scenarios, overlooking the distinct requirements of online
applications that necessitate real-time insertion of new vectors. This
limitation renders such systems inefficient for real-world scenarios. Moreover,
previous architectures struggled to effectively support real-time insertion due
to their reliance on serial execution streams. In this paper, we introduce a
novel Real-Time Adaptive Multi-Stream GPU ANNS System (RTAMS-GANNS). Our
architecture achieves its objectives through three key advancements: 1) We
initially examined the real-time insertion mechanisms in existing GPU ANNS
systems and discovered their reliance on repetitive copying and memory
allocation, which significantly hinders real-time effectiveness on GPUs. As a
solution, we introduce a dynamic vector insertion algorithm based on memory
blocks, which includes in-place rearrangement. 2) To enable real-time vector
insertion in parallel, we introduce a multi-stream parallel execution mode,
which differs from existing systems that operate serially within a single
stream. Our system utilizes a dynamic resource pool, allowing multiple streams
to execute concurrently without additional execution blocking. 3) Through
extensive experiments and comparisons, our approach effectively handles varying
QPS levels across different datasets, reducing latency by up to 40%-80%. The
proposed system has also been deployed in real-world industrial search and
recommendation systems, serving hundreds of millions of users daily, and has
achieved good results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CIKM'24, V2 fixes some typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELASTIC: Efficient Linear Attention for Sequential Interest Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09380v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09380v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Deng, Shiyao Wang, Song Lu, Yinfeng Li, Xinchen Luo, Yuanjun Liu, Peixing Xu, Guorui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art sequential recommendation models heavily rely on
transformer's attention mechanism. However, the quadratic computational and
memory complexities of self attention have limited its scalability for modeling
users' long range behaviour sequences. To address this problem, we propose
ELASTIC, an Efficient Linear Attention for SequenTial Interest Compression,
requiring only linear time complexity and decoupling model capacity from
computational cost. Specifically, ELASTIC introduces a fixed length interest
experts with linear dispatcher attention mechanism which compresses the
long-term behaviour sequences to a significantly more compact representation
which reduces up to 90% GPU memory usage with x2.7 inference speed up. The
proposed linear dispatcher attention mechanism significantly reduces the
quadratic complexity and makes the model feasible for adequately modeling
extremely long sequences. Moreover, in order to retain the capacity for
modeling various user interests, ELASTIC initializes a vast learnable interest
memory bank and sparsely retrieves compressed user's interests from the memory
with a negligible computational overhead. The proposed interest memory
retrieval technique significantly expands the cardinality of available interest
space while keeping the same computational cost, thereby striking a trade-off
between recommendation accuracy and efficiency. To validate the effectiveness
of our proposed ELASTIC, we conduct extensive experiments on various public
datasets and compare it with several strong sequential recommenders.
Experimental results demonstrate that ELASTIC consistently outperforms
baselines by a significant margin and also highlight the computational
efficiency of ELASTIC when modeling long sequences. We will make our
implementation code publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We hereby withdraw this paper from arXiv due to incomplete
  experiments. Upon further review, we have determined that additional
  experimental work is necessary to fully validate our findings and conclusions</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">201</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Medical Adaptation of Large Language and Vision-Language Models: Are We
  Making Progress? <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel P. Jeong, Saurabh Garg, Zachary C. Lipton, Michael Oberst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several recent works seek to develop foundation models specifically for
medical applications, adapting general-purpose large language models (LLMs) and
vision-language models (VLMs) via continued pretraining on publicly available
biomedical corpora. These works typically claim that such domain-adaptive
pretraining (DAPT) improves performance on downstream medical tasks, such as
answering medical licensing exam questions. In this paper, we compare seven
public "medical" LLMs and two VLMs against their corresponding base models,
arriving at a different conclusion: all medical VLMs and nearly all medical
LLMs fail to consistently improve over their base models in the zero-/few-shot
prompting regime for medical question-answering (QA) tasks. For instance,
across the tasks and model pairs we consider in the 3-shot setting, medical
LLMs only outperform their base models in 12.1% of cases, reach a (statistical)
tie in 49.8% of cases, and are significantly worse than their base models in
the remaining 38.2% of cases. Our conclusions are based on (i) comparing each
medical model head-to-head, directly against the corresponding base model; (ii)
optimizing the prompts for each model separately; and (iii) accounting for
statistical uncertainty in comparisons. While these basic practices are not
consistently adopted in the literature, our ablations show that they
substantially impact conclusions. Our findings suggest that state-of-the-art
general-domain models may already exhibit strong medical knowledge and
reasoning capabilities, and offer recommendations to strengthen the conclusions
of future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 Main Conference as Long Paper (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Consistency Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Archiki Prasad, Weizhe Yuan, Richard Yuanzhe Pang, Jing Xu, Maryam Fazel-Zarandi, Mohit Bansal, Sainbayar Sukhbaatar, Jason Weston, Jane Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-alignment, whereby models learn to improve themselves without human
annotation, is a rapidly growing research area. However, existing techniques
often fail to improve complex reasoning tasks due to the difficulty of
assigning correct rewards. An orthogonal approach that is known to improve
correctness is self-consistency, a method applied at inference time based on
multiple sampling in order to find the most consistent answer. In this work, we
extend the self-consistency concept to help train models. We thus introduce
self-consistency preference optimization (ScPO), which iteratively trains
consistent answers to be preferred over inconsistent ones on unsupervised new
problems. We show ScPO leads to large improvements over conventional reward
model training on reasoning tasks such as GSM8K and MATH, closing the gap with
supervised training with gold answers or preferences, and that combining ScPO
with standard supervised learning improves results even further. On ZebraLogic,
ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and
Claude-3 Haiku.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weighted Sobolev Approximation Rates for Neural Networks on Unbounded
  Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Abdeljawad, Thomas Dittrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider the approximation capabilities of shallow neural
networks in weighted Sobolev spaces for functions in the spectral Barron space.
The existing literature already covers several cases, in which the spectral
Barron space can be approximated well, i.e., without curse of dimensionality,
by shallow networks and several different classes of activation function. The
limitations of the existing results are mostly on the error measures that were
considered, in which the results are restricted to Sobolev spaces over a
bounded domain. We will here treat two cases that extend upon the existing
results. Namely, we treat the case with bounded domain and Muckenhoupt weights
and the case, where the domain is allowed to be unbounded and the weights are
required to decay. We first present embedding results for the more general
weighted Fourier-Lebesgue spaces in the weighted Sobolev spaces and then we
establish asymptotic approximation rates for shallow neural networks that come
without curse of dimensionality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Deep Reinforcement Learning for Crop Production
  Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Balderas, Dong Chen, Yanbo Huang, Li Wang, Ren-Cang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crop production management is essential for optimizing yield and minimizing a
field's environmental impact to crop fields, yet it remains challenging due to
the complex and stochastic processes involved. Recently, researchers have
turned to machine learning to address these complexities. Specifically,
reinforcement learning (RL), a cutting-edge approach designed to learn optimal
decision-making strategies through trial and error in dynamic environments, has
emerged as a promising tool for developing adaptive crop management policies.
RL models aim to optimize long-term rewards by continuously interacting with
the environment, making them well-suited for tackling the uncertainties and
variability inherent in crop management. Studies have shown that RL can
generate crop management policies that compete with, and even outperform,
expert-designed policies within simulation-based crop models. In the gym-DSSAT
crop model environment, one of the most widely used simulators for crop
management, proximal policy optimization (PPO) and deep Q-networks (DQN) have
shown promising results. However, these methods have not yet been
systematically evaluated under identical conditions. In this study, we
evaluated PPO and DQN against static baseline policies across three different
RL tasks, fertilization, irrigation, and mixed management, provided by the
gym-DSSAT environment. To ensure a fair comparison, we used consistent default
parameters, identical reward functions, and the same environment settings. Our
results indicate that PPO outperforms DQN in fertilization and irrigation
tasks, while DQN excels in the mixed management task. This comparative analysis
provides critical insights into the strengths and limitations of each approach,
advancing the development of more effective RL-based crop management
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How <span class="highlight-title">Transformer</span>s Solve Propositional Logic Problems: A Mechanistic
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guan Zhe Hong, Nishanth Dikkala, Enming Luo, Cyrus Rashtchian, Rina Panigrahy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown amazing performance on tasks that
require planning and reasoning. Motivated by this, we investigate the internal
mechanisms that underpin a network's ability to perform complex logical
reasoning. We first construct a synthetic propositional logic problem that
serves as a concrete test-bed for network training and evaluation. Crucially,
this problem demands nontrivial planning to solve, but we can train a small
transformer to achieve perfect accuracy. Building on our set-up, we then pursue
an understanding of precisely how a three-layer transformer, trained from
scratch, solves this problem. We are able to identify certain "planning" and
"reasoning" circuits in the network that necessitate cooperation between the
attention blocks to implement the desired logic. To expand our findings, we
then study a larger model, Mistral 7B. Using activation patching, we
characterize internal components that are critical in solving our logic
problem. Overall, our work systemically uncovers novel aspects of small and
large transformers, and continues the study of how they plan and reason.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable and Efficient Data-driven Discovery and Control of
  Distributed Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Wolf, Nicolò Botteghi, Urban Fasel, Andrea Manzoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively controlling systems governed by Partial Differential Equations
(PDEs) is crucial in several fields of Applied Sciences and Engineering. These
systems usually yield significant challenges to conventional control schemes
due to their nonlinear dynamics, partial observability, high-dimensionality
once discretized, distributed nature, and the requirement for low-latency
feedback control. Reinforcement Learning (RL), particularly Deep RL (DRL), has
recently emerged as a promising control paradigm for such systems,
demonstrating exceptional capabilities in managing high-dimensional, nonlinear
dynamics. However, DRL faces challenges including sample inefficiency,
robustness issues, and an overall lack of interpretability. To address these
issues, we propose a data-efficient, interpretable, and scalable Dyna-style
Model-Based RL framework for PDE control, combining the Sparse Identification
of Nonlinear Dynamics with Control (SINDy-C) algorithm and an autoencoder (AE)
framework for the sake of dimensionality reduction of PDE states and actions.
This novel approach enables fast rollouts, reducing the need for extensive
environment interactions, and provides an interpretable latent space
representation of the PDE forward dynamics. We validate our method on two PDE
problems describing fluid flows - namely, the 1D Burgers equation and 2D
Navier-Stokes equations - comparing it against a model-free baseline, and
carrying out an extensive analysis of the learned dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Problem Space Transformations for Generalisation in Behavioural Cloning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Doshi, Marco Bagatella, Stelian Coros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The combination of behavioural cloning and neural networks has driven
significant progress in robotic manipulation. As these algorithms may require a
large number of demonstrations for each task of interest, they remain
fundamentally inefficient in complex scenarios. This issue is aggravated when
the system is treated as a black-box, ignoring its physical properties. This
work characterises widespread properties of robotic manipulation, such as pose
equivariance and locality. We empirically demonstrate that transformations
arising from each of these properties allow neural policies trained with
behavioural cloning to better generalise to out-of-distribution problem
instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-branch Spatio-Temporal Graph Neural Network For Efficient Ice
  Layer Thickness Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zesheng Liu, Maryam Rahnemoonfar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding spatio-temporal patterns in polar ice layers is essential for
tracking changes in ice sheet balance and assessing ice dynamics. While
convolutional neural networks are widely used in learning ice layer patterns
from raw echogram images captured by airborne snow radar sensors, noise in the
echogram images prevents researchers from getting high-quality results.
Instead, we focus on geometric deep learning using graph neural networks,
aiming to build a spatio-temporal graph neural network that learns from
thickness information of the top ice layers and predicts for deeper layers. In
this paper, we developed a novel multi-branch spatio-temporal graph neural
network that used the GraphSAGE framework for spatio features learning and a
temporal convolution operation to capture temporal changes, enabling different
branches of the network to be more specialized and focusing on a single
learning task. We found that our proposed multi-branch network can consistently
outperform the current fused spatio-temporal graph neural network in both
accuracy and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partial Structure Discovery is Sufficient for No-regret Learning in
  Causal Bandits <span class="chip">NeurIPS 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Qasim Elahi, Mahsa Ghasemi, Murat Kocaoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal knowledge about the relationships among decision variables and a
reward variable in a bandit setting can accelerate the learning of an optimal
decision. Current works often assume the causal graph is known, which may not
always be available a priori. Motivated by this challenge, we focus on the
causal bandit problem in scenarios where the underlying causal graph is unknown
and may include latent confounders. While intervention on the parents of the
reward node is optimal in the absence of latent confounders, this is not
necessarily the case in general. Instead, one must consider a set of possibly
optimal arms/interventions, each being a special subset of the ancestors of the
reward node, making causal discovery beyond the parents of the reward node
essential. For regret minimization, we identify that discovering the full
causal structure is unnecessary; however, no existing work provides the
necessary and sufficient components of the causal graph. We formally
characterize the set of necessary and sufficient latent confounders one needs
to detect or learn to ensure that all possibly optimal arms are identified
correctly. We also propose a randomized algorithm for learning the causal graph
with a limited number of samples, providing a sample complexity guarantee for
any desired confidence level. In the causal bandit setup, we propose a
two-stage approach. In the first stage, we learn the induced subgraph on
ancestors of the reward, along with a necessary and sufficient subset of latent
confounders, to construct the set of possibly optimal arms. The regret incurred
during this phase scales polynomially with respect to the number of nodes in
the causal graph. The second phase involves the application of a standard
bandit algorithm, such as the UCB algorithm. We also establish a regret bound
for our two-phase approach, which is sublinear in the number of rounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Proceedings of NeurIPS 24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stepping Forward on the Last Mile 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Feng, Shaojie Zhuo, Xiaopeng Zhang, Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Andrew Zou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuously adapting pre-trained models to local data on resource
constrained edge devices is the $\emph{last mile}$ for model deployment.
However, as models increase in size and depth, backpropagation requires a large
amount of memory, which becomes prohibitive for edge devices. In addition, most
existing low power neural processing engines (e.g., NPUs, DSPs, MCUs, etc.) are
designed as fixed-point inference accelerators, without training capabilities.
Forward gradients, solely based on directional derivatives computed from two
forward calls, have been recently used for model training, with substantial
savings in computation and memory. However, the performance of quantized
training with fixed-point forward gradients remains unclear. In this paper, we
investigate the feasibility of on-device training using fixed-point forward
gradients, by conducting comprehensive experiments across a variety of deep
learning benchmark tasks in both vision and audio domains. We propose a series
of algorithm enhancements that further reduce the memory footprint, and the
accuracy gap compared to backpropagation. An empirical study on how training
with forward gradients navigates in the loss landscape is further explored. Our
results demonstrate that on the last mile of model customization on edge
devices, training with fixed-point forward gradients is a feasible and
practical approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Stationary Learning of Neural Networks with Automatic Soft Parameter
  Reset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Galashov, Michalis K. Titsias, András György, Clare Lyle, Razvan Pascanu, Yee Whye Teh, Maneesh Sahani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are traditionally trained under the assumption that data come
from a stationary distribution. However, settings which violate this assumption
are becoming more popular; examples include supervised learning under
distributional shifts, reinforcement learning, continual learning and
non-stationary contextual bandits. In this work we introduce a novel learning
approach that automatically models and adapts to non-stationarity, via an
Ornstein-Uhlenbeck process with an adaptive drift parameter. The adaptive drift
tends to draw the parameters towards the initialisation distribution, so the
approach can be understood as a form of soft parameter reset. We show
empirically that our approach performs well in non-stationary supervised and
off-policy reinforcement learning settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Scale and Multimodal Species Distribution Modeling <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nina van Tiel, Robin Zbinden, Emanuele Dalsasso, Benjamin Kellenberger, Loïc Pellissier, Devis Tuia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Species distribution models (SDMs) aim to predict the distribution of species
by relating occurrence data with environmental variables. Recent applications
of deep learning to SDMs have enabled new avenues, specifically the inclusion
of spatial data (environmental rasters, satellite images) as model predictors,
allowing the model to consider the spatial context around each species'
observations. However, the appropriate spatial extent of the images is not
straightforward to determine and may affect the performance of the model, as
scale is recognized as an important factor in SDMs. We develop a modular
structure for SDMs that allows us to test the effect of scale in both single-
and multi-scale settings. Furthermore, our model enables different scales to be
considered for different modalities, using a late fusion approach. Results on
the GeoLifeCLEF 2023 benchmark indicate that considering multimodal data and
learning multi-scale representations leads to more accurate models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the CV4Ecology workshop at ECCV 2024
  (https://cv4e.netlify.app/papers/06.pdf)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $k$NN Attention Demystified: A Theoretical Exp<span class="highlight-title">lora</span>tion for Scalable
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Themistoklis Haris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their power, Transformers face challenges with long sequences due to
the quadratic complexity of self-attention. To address this limitation, methods
like $k$-Nearest-Neighbor ($k$NN) attention have been introduced [Roy, Saffar,
Vaswani, Grangier, 2021] enabling each token to attend to only its $k$ closest
tokens. While $k$NN attention has shown empirical success in making
Transformers more efficient, its exact approximation guarantees have not been
theoretically analyzed. In this work, we establish a theoretical framework for
$k$NN attention, reformulating self-attention as expectations over softmax
distributions and leveraging lazy Gumbel sampling [Mussmann, Levy, Ermon, 2017]
with $k$NN indices for efficient approximation. Building on this framework, we
also propose novel sub-quadratic algorithms that approximate self-attention
gradients by leveraging efficient sampling techniques, such as Markov
Chain-based estimation. Finally, we demonstrate the practical effectiveness of
these algorithms through empirical experiments, showcasing their benefits in
both training and inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting and Publishing Accurate Imbalance Prices Using Monte Carlo
  Tree Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Pavirani, Jonas Van Gompel, Seyed Soroush Karimi Madahi, Bert Claessens, Chris Develder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing reliance on renewable energy sources, particularly solar and
wind, has introduced challenges due to their uncontrollable production. This
complicates maintaining the electrical grid balance, prompting some
transmission system operators in Western Europe to implement imbalance tariffs
that penalize unsustainable power deviations. These tariffs create an implicit
demand response framework to mitigate grid instability. Yet, several challenges
limit active participation. In Belgium, for example, imbalance prices are only
calculated at the end of each 15-minute settlement period, creating high risk
due to price uncertainty. This risk is further amplified by the inherent
volatility of imbalance prices, discouraging participation. Although
transmission system operators provide minute-based price predictions, the
system imbalance volatility makes accurate price predictions challenging to
obtain and requires sophisticated techniques. Moreover, publishing price
estimates can prompt participants to adjust their schedules, potentially
affecting the system balance and the final price, adding further complexity. To
address these challenges, we propose a Monte Carlo Tree Search method that
publishes accurate imbalance prices while accounting for potential response
actions. Our approach models the system dynamics using a neural network
forecaster and a cluster of virtual batteries controlled by reinforcement
learning agents. Compared to Belgium's current publication method, our
technique improves price accuracy by 20.4% under ideal conditions and by 12.8%
in more realistic scenarios. This research addresses an unexplored, yet crucial
problem, positioning this paper as a pioneering work in analyzing the potential
of more advanced imbalance price publishing techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Resource-Efficient Federated Learning in Industrial IoT for
  Multivariate Time Series Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandros Gkillas, Aris Lalos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly and missing data constitute a thorny problem in industrial
applications. In recent years, deep learning enabled anomaly detection has
emerged as a critical direction, however the improved detection accuracy is
achieved with the utilization of large neural networks, increasing their
storage and computational cost. Moreover, the data collected in edge devices
contain user privacy, introducing challenges that can be successfully addressed
by the privacy-preserving distributed paradigm, known as federated learning
(FL). This framework allows edge devices to train and exchange models
increasing also the communication cost. Thus, to deal with the increased
communication, processing and storage challenges of the FL based deep anomaly
detection NN pruning is expected to have significant benefits towards reducing
the processing, storage and communication complexity. With this focus, a novel
compression-based optimization problem is proposed at the server-side of a FL
paradigm that fusses the received local models broadcast and performs pruning
generating a more compressed model. Experiments in the context of anomaly
detection and missing value imputation demonstrate that the proposed FL
scenario along with the proposed compressed-based method are able to achieve
high compression rates (more than $99.7\%$) with negligible performance losses
(less than $1.18\%$ ) as compared to the centralized solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ET-SEED: Efficient Trajectory-Level SE(3) Equivariant <span class="highlight-title">Diffusion</span> Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenrui Tie, Yue Chen, Ruihai Wu, Boxuan Dong, Zeyi Li, Chongkai Gao, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning, e.g., diffusion policy, has been proven effective in
various robotic manipulation tasks. However, extensive demonstrations are
required for policy robustness and generalization. To reduce the demonstration
reliance, we leverage spatial symmetry and propose ET-SEED, an efficient
trajectory-level SE(3) equivariant diffusion model for generating action
sequences in complex robot manipulation tasks. Further, previous equivariant
diffusion models require the per-step equivariance in the Markov process,
making it difficult to learn policy under such strong constraints. We
theoretically extend equivariant Markov kernels and simplify the condition of
equivariant diffusion process, thereby significantly improving training
efficiency for trajectory-level SE(3) equivariant diffusion policy in an
end-to-end manner. We evaluate ET-SEED on representative robotic manipulation
tasks, involving rigid body, articulated and deformable object. Experiments
demonstrate superior data efficiency and manipulation proficiency of our
proposed method, as well as its ability to generalize to unseen configurations
with only a few demonstrations. Website: https://et-seed.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to CoRL 2024 Workshop on X-Embodiment Robot Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Yao, Qi Qian, Juhua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple clustering aims to discover various latent structures of data from
different aspects. Deep multiple clustering methods have achieved remarkable
performance by exploiting complex patterns and relationships in data. However,
existing works struggle to flexibly adapt to diverse user-specific needs in
data grouping, which may require manual understanding of each clustering. To
address these limitations, we introduce Multi-Sub, a novel end-to-end multiple
clustering approach that incorporates a multi-modal subspace proxy learning
framework in this work. Utilizing the synergistic capabilities of CLIP and
GPT-4, Multi-Sub aligns textual prompts expressing user preferences with their
corresponding visual representations. This is achieved by automatically
generating proxy words from large language models that act as subspace bases,
thus allowing for the customized representation of data in terms specific to
the user's interests. Our method consistently outperforms existing baselines
across a broad set of datasets in visual multiple clustering tasks. Our code is
available at https://github.com/Alexander-Yao/Multi-Sub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian algorithmic perfumery: A Hierarchical Relevance Vector Machine
  for the Estimation of Personalized Fragrance Preferences based on Three
  Sensory Layers and Jungian Personality Archetypes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rolando Gonzales Martinez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores a Bayesian algorithmic approach to personalized fragrance
recommendation by integrating hierarchical Relevance Vector Machines (RVM) and
Jungian personality archetypes. The paper proposes a structured model that
links individual scent preferences for top, middle, and base notes to
personality traits derived from Jungian archetypes, such as the Hero,
Caregiver, and Explorer, among others. The algorithm utilizes Bayesian updating
to dynamically refine predictions as users interact with each fragrance note.
This iterative process allows for the personalization of fragrance experiences
based on prior data and personality assessments, leading to adaptive and
interpretable recommendations. By combining psychological theory with Bayesian
machine learning, this approach addresses the complexity of modeling individual
preferences while capturing user-specific and population-level trends. The
study highlights the potential of hierarchical Bayesian frameworks in creating
customized olfactory experiences, informed by psychological and demographic
factors, contributing to advancements in personalized product design and
machine learning applications in sensory-based industries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 0 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Custom Models Learn In-Context? An Exp<span class="highlight-title">lora</span>tion of Hybrid
  Architecture Performance on In-Context Learning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Campbell, Nelson Lojo, Kesava Viswanadha, Christoffer Grondal Tryggestad, Derrick Han Sun, Sriteja Vijapurapu, August Rolfsen, Anant Sahai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-Context Learning (ICL) is a phenomenon where task learning occurs through
a prompt sequence without the necessity of parameter updates. ICL in
Multi-Headed Attention (MHA) with absolute positional embedding has been the
focus of more study than other sequence model varieties. We examine
implications of architectural differences between GPT-2 and LLaMa as well as
LlaMa and Mamba. We extend work done by Garg et al. (2022) and Park et al.
(2024) to GPT-2/LLaMa hybrid and LLaMa/Mamba hybrid models - examining the
interplay between sequence transformation blocks and regressive performance
in-context. We note that certain architectural changes cause degraded training
efficiency/ICL accuracy by converging to suboptimal predictors or converging
slower. We also find certain hybrids showing optimistic performance
improvements, informing potential future ICL-focused architecture
modifications. Additionally, we propose the "ICL regression score", a scalar
metric describing a model's whole performance on a specific task. Compute
limitations impose restrictions on our architecture-space, training duration,
number of training runs, function class complexity, and benchmark complexity.
To foster reproducible and extensible research, we provide a typed, modular,
and extensible Python package on which we run all experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-tuning -- a Transfer Learning approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Arul Raj, Linglong Qian, Zina Ibrahim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Secondary research use of Electronic Health Records (EHRs) is often hampered
by the abundance of missing data in this valuable resource. Missingness in EHRs
occurs naturally as a result of the data recording practices during routine
clinical care, but handling it is crucial to the precision of medical analysis
and the decision-making that follows. The literature contains a variety of
imputation methodologies based on deep neural networks. Those aim to overcome
the dynamic, heterogeneous and multivariate missingness patterns of EHRs, which
cannot be handled by classical and statistical imputation methods. However, all
existing deep imputation methods rely on end-to-end pipelines that incorporate
both imputation and downstream analyses, e.g. classification. This coupling
makes it difficult to assess the quality of imputation and takes away the
flexibility of re-using the imputer for a different task. Furthermore, most
end-to-end deep architectures tend to use complex networks to perform the
downstream task, in addition to the already sophisticated deep imputation
network. We, therefore ask if the high performance reported in the literature
is due to the imputer or the classifier and further ask if an optimised
state-of-the-art imputer is used, a simpler classifier can achieve comparable
performance. This paper explores the development of a modular, deep
learning-based imputation and classification pipeline, specifically built to
leverage the capabilities of state-of-the-art imputation models for downstream
classification tasks. Such a modular approach enables a) objective assessment
of the quality of the imputer and classifier independently, and b) enables the
exploration of the performance of simpler classification architectures using an
optimised imputer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GUIDE-VAE: Advancing Data Generation with User Information and Pattern
  Dictionaries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kutay Bölat, Simon Tindemans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative modelling of multi-user datasets has become prominent in science
and engineering. Generating a data point for a given user requires employing
user information, and conventional generative models, including variational
autoencoders (VAEs), often ignore that. This paper introduces GUIDE-VAE, a
novel conditional generative model that leverages user embeddings to generate
user-guided data. By allowing the model to benefit from shared patterns across
users, GUIDE-VAE enhances performance in multi-user settings, even under
significant data imbalance. In addition to integrating user information,
GUIDE-VAE incorporates a pattern dictionary-based covariance composition (PDCC)
to improve the realism of generated samples by capturing complex feature
dependencies. While user embeddings drive performance gains, PDCC addresses
common issues such as noise and over-smoothing typically seen in VAEs.
  The proposed GUIDE-VAE was evaluated on a multi-user smart meter dataset
characterized by substantial data imbalance across users. Quantitative results
show that GUIDE-VAE performs effectively in both synthetic data generation and
missing record imputation tasks, while qualitative evaluations reveal that
GUIDE-VAE produces more plausible and less noisy data. These results establish
GUIDE-VAE as a promising tool for controlled, realistic data generation in
multi-user datasets, with potential applications across various domains
requiring user-informed modelling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactions Across Blocks in Post-Training Quantization of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khasmamad Shabanovi, Lukas Wiest, Vladimir Golkov, Daniel Cremers, Thomas Pfeil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization is widely employed to reduce the computational
demands of neural networks. Typically, individual substructures, such as layers
or blocks of layers, are quantized with the objective of minimizing
quantization errors in their pre-activations by fine-tuning the corresponding
weights. Deriving this local objective from the global objective of minimizing
task loss involves two key simplifications: assuming substructures are mutually
independent and ignoring the knowledge of subsequent substructures as well as
the task loss. In this work, we assess the effects of these simplifications on
weight-only quantization of large language models. We introduce two multi-block
fine-tuning strategies and compare them against the baseline of fine-tuning
single transformer blocks. The first captures correlations of weights across
blocks by jointly optimizing multiple quantized blocks. The second incorporates
knowledge of subsequent blocks by minimizing the error in downstream
pre-activations rather than focusing solely on the quantized block. Our
findings indicate that the effectiveness of these methods depends on the
specific network model, with no impact on some models but demonstrating
significant benefits for others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Regret of Linear Ensemble Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harin Lee, Min-hwan Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we close the fundamental gap of theory and practice by
providing an improved regret bound for linear ensemble sampling. We prove that
with an ensemble size logarithmic in $T$, linear ensemble sampling can achieve
a frequentist regret bound of $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$, matching
state-of-the-art results for randomized linear bandit algorithms, where $d$ and
$T$ are the dimension of the parameter and the time horizon respectively. Our
approach introduces a general regret analysis framework for linear bandit
algorithms. Additionally, we reveal a significant relationship between linear
ensemble sampling and Linear Perturbed-History Exploration (LinPHE), showing
that LinPHE is a special case of linear ensemble sampling when the ensemble
size equals $T$. This insight allows us to derive a new regret bound of
$\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ for LinPHE, independent of the number of
arms. Our contributions advance the theoretical foundation of ensemble
sampling, bringing its regret bounds in line with the best known bounds for
other randomized exploration algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Algorithm for Sparse Online Learning with Truncated Gradient
  Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debbie Lim, Yixian Qiu, Patrick Rebentrost, Qisheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logistic regression, the Support Vector Machine (SVM), and least squares are
well-studied methods in the statistical and computer science community, with
various practical applications. High-dimensional data arriving on a real-time
basis makes the design of online learning algorithms that produce sparse
solutions essential. The seminal work of
\hyperlink{cite.langford2009sparse}{Langford, Li, and Zhang (2009)} developed a
method to obtain sparsity via truncated gradient descent, showing a
near-optimal online regret bound. Based on this method, we develop a quantum
sparse online learning algorithm for logistic regression, the SVM, and least
squares. Given efficient quantum access to the inputs, we show that a quadratic
speedup in the time complexity with respect to the dimension of the problem is
achievable, while maintaining a regret of $O(1/\sqrt{T})$, where $T$ is the
number of iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 1 table, 4 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Causal Framework for Precision Rehabilitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. James Cotton, Bryant A. Seamon, Richard L. Segal, Randal D. Davis, Amrita Sahu, Michelle M. McLeod, Pablo Celnik, Sharon L. Ramey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precision rehabilitation offers the promise of an evidence-based approach for
optimizing individual rehabilitation to improve long-term functional outcomes.
Emerging techniques, including those driven by artificial intelligence, are
rapidly expanding our ability to quantify the different domains of function
during rehabilitation, other encounters with healthcare, and in the community.
While this seems poised to usher rehabilitation into the era of big data and
should be a powerful driver of precision rehabilitation, our field lacks a
coherent framework to utilize these data and deliver on this promise. We
propose a framework that builds upon multiple existing pillars to fill this
gap. Our framework aims to identify the Optimal Dynamic Treatment Regimens
(ODTR), or the decision-making strategy that takes in the range of available
measurements and biomarkers to identify interventions likely to maximize
long-term function. This is achieved by designing and fitting causal models,
which extend the Computational Neurorehabilitation framework using tools from
causal inference. These causal models can learn from heterogeneous data from
different silos, which must include detailed documentation of interventions,
such as using the Rehabilitation Treatment Specification System. The models
then serve as digital twins of patient recovery trajectories, which can be used
to learn the ODTR. Our causal modeling framework also emphasizes quantitatively
linking changes across levels of the functioning to ensure that interventions
can be precisely selected based on careful measurement of impairments while
also being selected to maximize outcomes that are meaningful to patients and
stakeholders. We believe this approach can provide a unifying framework to
leverage growing big rehabilitation data and AI-powered measurements to produce
precision rehabilitation treatments that can improve clinical outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>keywords: rehabilitation; precision rehabilitation; causal inference;
  international classification of functioning; rehabilitation treatment
  specification system; computational neurorehabilitation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Game-Theoretic Machine Unlearning: Mitigating Extra Privacy Leakage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengzhu Liu, Tianqing Zhu, Lefeng Zhang, Ping Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the extensive use of machine learning technologies, data providers
encounter increasing privacy risks. Recent legislation, such as GDPR, obligates
organizations to remove requested data and its influence from a trained model.
Machine unlearning is an emerging technique designed to enable machine learning
models to erase users' private information. Although several efficient machine
unlearning schemes have been proposed, these methods still have limitations.
First, removing the contributions of partial data may lead to model performance
degradation. Second, discrepancies between the original and generated unlearned
models can be exploited by attackers to obtain target sample's information,
resulting in additional privacy leakage risks. To address above challenges, we
proposed a game-theoretic machine unlearning algorithm that simulates the
competitive relationship between unlearning performance and privacy protection.
This algorithm comprises unlearning and privacy modules. The unlearning module
possesses a loss function composed of model distance and classification error,
which is used to derive the optimal strategy. The privacy module aims to make
it difficult for an attacker to infer membership information from the unlearned
data, thereby reducing the privacy leakage risk during the unlearning process.
Additionally, the experimental results on real-world datasets demonstrate that
this game-theoretic unlearning algorithm's effectiveness and its ability to
generate an unlearned model with a performance similar to that of the retrained
one while mitigating extra privacy leakage risks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retentive Neural Quantum States: Efficient Ansätze for Ab Initio
  Quantum Chemistry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Knitter, Dan Zhao, James Stokes, Martin Ganahl, Stefan Leichenauer, Shravan Veerapaneni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural-network quantum states (NQS) has emerged as a powerful application of
quantum-inspired deep learning for variational Monte Carlo methods, offering a
competitive alternative to existing techniques for identifying ground states of
quantum problems. A significant advancement toward improving the practical
scalability of NQS has been the incorporation of autoregressive models, most
recently transformers, as variational ansatze. Transformers learn sequence
information with greater expressiveness than recurrent models, but at the cost
of increased time complexity with respect to sequence length. We explore the
use of the retentive network (RetNet), a recurrent alternative to transformers,
as an ansatz for solving electronic ground state problems in $\textit{ab
initio}$ quantum chemistry. Unlike transformers, RetNets overcome this time
complexity bottleneck by processing data in parallel during training, and
recurrently during inference. We give a simple computational cost estimate of
the RetNet and directly compare it with similar estimates for transformers,
establishing a clear threshold ratio of problem-to-model size past which the
RetNet's time complexity outperforms that of the transformer. Though this
efficiency can comes at the expense of decreased expressiveness relative to the
transformer, we overcome this gap through training strategies that leverage the
autoregressive structure of the model -- namely, variational neural annealing.
Our findings support the RetNet as a means of improving the time complexity of
NQS without sacrificing accuracy. We provide further evidence that the ablative
improvements of neural annealing extend beyond the RetNet architecture,
suggesting it would serve as an effective general training strategy for
autoregressive NQS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 1 figure, to be submitted for peer-reviewed publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibrating for the Future:Enhancing Calorimeter Longevity with Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Ali, A. S. Ryzhikov, D. A. Derkach, F. D. Ratnikov, V. O. Bocharnikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of high-energy physics, the longevity of calorimeters is
paramount. Our research introduces a deep learning strategy to refine the
calibration process of calorimeters used in particle physics experiments. We
develop a Wasserstein GAN inspired methodology that adeptly calibrates the
misalignment in calorimeter data due to aging or other factors. Leveraging the
Wasserstein distance for loss calculation, this innovative approach requires a
significantly lower number of events and resources to achieve high precision,
minimizing absolute errors effectively. Our work extends the operational
lifespan of calorimeters, thereby ensuring the accuracy and reliability of data
in the long term, and is particularly beneficial for experiments where data
integrity is crucial for scientific discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Polynomial Composition Activations: Unleashing the Dynamics of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Zhuo, Ya Wang, Yutao Zeng, Xiaoqing Li, Xun Zhou, Jinwen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have found extensive applications across various domains due to
the powerful fitting capabilities. This success can be partially attributed to
their inherent nonlinearity. Thus, in addition to the ReLU function employed in
the original transformer architecture, researchers have explored alternative
modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment
representational capacity. In this paper, we propose a novel category of
polynomial composition activations (PolyCom), designed to optimize the dynamics
of transformers. Theoretically, we provide a comprehensive mathematical
analysis of PolyCom, highlighting its enhanced expressivity and efficacy
relative to other activation functions. Notably, we demonstrate that networks
incorporating PolyCom achieve the $\textbf{optimal approximation rate}$,
indicating that PolyCom networks require minimal parameters to approximate
general smooth functions in Sobolev spaces. We conduct empirical experiments on
the pre-training configurations of large language models (LLMs), including both
dense and sparse architectures. By substituting conventional activation
functions with PolyCom, we enable LLMs to capture higher-order interactions
within the data, thus improving performance metrics in terms of accuracy and
convergence rates. Extensive experimental results demonstrate the effectiveness
of our method, showing substantial improvements over other activation
functions. Code is available at https://github.com/BryceZhuo/PolyCom.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEG: Medical Knowledge-Augmented Large Language Models for Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Cabello, Carmen Martin-Turrero, Uchenna Akujuobi, Anders Søgaard, Carlos Bobed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering is a natural language understanding task that involves
reasoning over both explicit context and unstated, relevant domain knowledge.
Large language models (LLMs), which underpin most contemporary question
answering systems, struggle to induce how concepts relate in specialized
domains such as medicine. Existing medical LLMs are also costly to train. In
this work, we present MEG, a parameter-efficient approach for medical
knowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate
graph embeddings into the LLM, enabling it to leverage external knowledge in a
cost-effective way. We evaluate our method on four popular medical
multiple-choice datasets and show that LLMs greatly benefit from the factual
grounding provided by knowledge graph embeddings. MEG attains an average of
+10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized
models like BioMistral. We also show results based on Llama-3. Finally, we show
that MEG's performance remains robust to the choice of graph encoder.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EXP<span class="highlight-title">LORA</span>: Efficient Exemplar Subset Selection for Complex Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Purohit, Venktesh V, Raghuram Devalla, Krishna Mohan Yerragorla, Sourangshu Bhattacharya, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering reasoning-based complex questions over text and hybrid sources,
including tables, is a challenging task. Recent advances in large language
models (LLMs) have enabled in-context learning (ICL), allowing LLMs to acquire
proficiency in a specific task using only a few demonstration samples
(exemplars). A critical challenge in ICL is the selection of optimal exemplars,
which can be either task-specific (static) or test-example-specific (dynamic).
Static exemplars provide faster inference times and increased robustness across
a distribution of test examples. In this paper, we propose an algorithm for
static exemplar subset selection for complex reasoning tasks. We introduce
EXPLORA, a novel exploration method designed to estimate the parameters of the
scoring function, which evaluates exemplar subsets without incorporating
confidence information. EXPLORA significantly reduces the number of LLM calls
to ~11% of those required by state-of-the-art methods and achieves a
substantial performance improvement of 12.24%. We open-source our code and data
(https://github.com/kiranpurohit/EXPLORA).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Generative Model-assisted Talking-face Semantic Communication
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feibo Jiang, Siwei Tu, Li Dong, Cunhua Pan, Jiangzhou Wang, Xiaohu You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of generative Artificial Intelligence (AI) continually
unveils the potential of Semantic Communication (SemCom). However, current
talking-face SemCom systems still encounter challenges such as low bandwidth
utilization, semantic ambiguity, and diminished Quality of Experience (QoE).
This study introduces a Large Generative Model-assisted Talking-face Semantic
Communication (LGM-TSC) System tailored for the talking-face video
communication. Firstly, we introduce a Generative Semantic Extractor (GSE) at
the transmitter based on the FunASR model to convert semantically sparse
talking-face videos into texts with high information density. Secondly, we
establish a private Knowledge Base (KB) based on the Large Language Model (LLM)
for semantic disambiguation and correction, complemented by a joint knowledge
base-semantic-channel coding scheme. Finally, at the receiver, we propose a
Generative Semantic Reconstructor (GSR) that utilizes BERT-VITS2 and SadTalker
models to transform text back into a high-QoE talking-face video matching the
user's timbre. Simulation results demonstrate the feasibility and effectiveness
of the proposed LGM-TSC system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaSociety: An Adaptive Environment with Social Structures for
  Multi-Agent Decision-Making <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Huang, Xingbo Wang, Hao Liu, Fanqi Kong, Aoyang Qin, Min Tang, Xiaoxi Wang, Song-Chun Zhu, Mingjie Bi, Siyuan Qi, Xue Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional interactive environments limit agents' intelligence growth with
fixed tasks. Recently, single-agent environments address this by generating new
tasks based on agent actions, enhancing task diversity. We consider the
decision-making problem in multi-agent settings, where tasks are further
influenced by social connections, affecting rewards and information access.
However, existing multi-agent environments lack a combination of adaptive
physical surroundings and social connections, hindering the learning of
intelligent behaviors. To address this, we introduce AdaSociety, a customizable
multi-agent environment featuring expanding state and action spaces, alongside
explicit and alterable social structures. As agents progress, the environment
adaptively generates new tasks with social structures for agents to undertake.
In AdaSociety, we develop three mini-games showcasing distinct social
structures and tasks. Initial results demonstrate that specific social
structures can promote both individual and collective benefits, though current
reinforcement learning and LLM-based algorithms show limited effectiveness in
leveraging social structures to enhance performance. Overall, AdaSociety serves
as a valuable research platform for exploring intelligence in diverse physical
and social settings. The code is available at
https://github.com/bigai-ai/AdaSociety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS D&B 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UniTraj: Universal Human Trajectory Modeling from Billion-Scale
  Worldwide Traces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanshao Zhu, James Jianqiao Yu, Xiangyu Zhao, Xuetao Wei, Yuxuan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human trajectory modeling is essential for deciphering movement patterns and
supporting advanced applications across various domains. However, existing
methods are often tailored to specific tasks and regions, resulting in
limitations related to task specificity, regional dependency, and data quality
sensitivity. Addressing these challenges requires a universal human trajectory
foundation model capable of generalizing and scaling across diverse tasks and
geographic contexts. To this end, we propose UniTraj, a Universal human
Trajectory foundation model that is task-adaptive, region-independent, and
highly generalizable. To further enhance performance, we construct WorldTrace,
the first large-scale, high-quality, globally distributed dataset sourced from
open web platforms, encompassing 2.45 million trajectories with billions of
points across 70 countries. Through multiple resampling and masking strategies
designed for pre-training, UniTraj effectively overcomes geographic and task
constraints, adapting to heterogeneous data quality. Extensive experiments
across multiple trajectory analysis tasks and real-world datasets demonstrate
that UniTraj consistently outperforms existing approaches in terms of
scalability and adaptability. These results underscore the potential of UniTraj
as a versatile, robust solution for a wide range of trajectory analysis
applications, with WorldTrace serving as an ideal but non-exclusive foundation
for training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Message Passing Architecture for GCN Training on HBM-based
  FPGAs with Orthogonal Topology On-Chip Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qizhe Wu, Letian Zhao, Yuchen Gui, Huawen Liang Xiaotian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Convolutional Networks (GCNs) are state-of-the-art deep learning models
for representation learning on graphs. However, the efficient training of GCNs
is hampered by constraints in memory capacity and bandwidth, compounded by the
irregular data flow that results in communication bottlenecks. To address these
challenges, we propose a message-passing architecture that leverages NUMA-based
memory access properties and employs a parallel multicast routing algorithm
based on a 4-D hypercube network within the accelerator for efficient message
passing in graphs. Additionally, we have re-engineered the backpropagation
algorithm specific to GCNs within our proposed accelerator. This redesign
strategically mitigates the memory demands prevalent during the training phase
and diminishes the computational overhead associated with the transposition of
extensive matrices. Compared to the state-of-the-art HP-GNN architecture we
achieved a performance improvement of $1.03\times \sim 1.81\times$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for 2024 ACM/SIGDA International
  Symposium on Field Programmable Gate Arrays(FPGA'24) as poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mamba<span class="highlight-title">PEFT</span>: Exploring <span class="highlight-title">Parameter-Efficient Fine-Tuning</span> for Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masakazu Yoshimura, Teruaki Hayashi, Yota Maeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An ecosystem of Transformer-based models has been established by building
large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a
crucial technology for deploying these models to downstream tasks with minimal
cost while achieving effective performance. Recently, Mamba, a State Space
Model (SSM)-based model, has attracted attention as a potential alternative to
Transformers. While many large-scale Mamba-based models have been proposed,
efficiently adapting pre-trained Mamba-based models to downstream tasks remains
unexplored. In this paper, we conduct an exploratory analysis of PEFT methods
for Mamba. We investigate the effectiveness of existing PEFT methods for
Transformers when applied to Mamba. We also modify these methods to better
align with the Mamba architecture. Additionally, we propose new Mamba-specific
PEFT methods that leverage the distinctive structure of Mamba. Our experiments
indicate that PEFT performs more effectively for Mamba than Transformers.
Lastly, we demonstrate how to effectively combine multiple PEFT methods and
provide a framework that outperforms previous works. To ensure reproducibility,
we will release the code after publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconsidering the Performance of GAE in Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weishuo Ma, Yanbo Wang, Xiyuan Wang, Muhan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various graph neural networks (GNNs) with advanced training techniques and
model designs have been proposed for link prediction tasks. However, outdated
baseline models may lead to an overestimation of the benefits provided by these
novel approaches. To address this, we systematically investigate the potential
of Graph Autoencoders (GAE) by meticulously tuning hyperparameters and
utilizing the trick of orthogonal embedding and linear propagation. Our
findings reveal that a well-optimized GAE can match the performance of more
complex models while offering greater computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flexible task abstractions emerge in linear networks with fast and
  bounded units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Sandbrink, Jan P. Bauer, Alexandra M. Proca, Andrew M. Saxe, Christopher Summerfield, Ali Hummos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animals survive in dynamic environments changing at arbitrary timescales, but
such data distribution shifts are a challenge to neural networks. To adapt to
change, neural systems may change a large number of parameters, which is a slow
process involving forgetting past information. In contrast, animals leverage
distribution changes to segment their stream of experience into tasks and
associate them with internal task abstracts. Animals can then respond flexibly
by selecting the appropriate task abstraction. However, how such flexible task
abstractions may arise in neural systems remains unknown. Here, we analyze a
linear gated network where the weights and gates are jointly optimized via
gradient descent, but with neuron-like constraints on the gates including a
faster timescale, nonnegativity, and bounded activity. We observe that the
weights self-organize into modules specialized for tasks or sub-tasks
encountered, while the gates layer forms unique representations that switch the
appropriate weight modules (task abstractions). We analytically reduce the
learning dynamics to an effective eigenspace, revealing a virtuous cycle: fast
adapting gates drive weight specialization by protecting previous knowledge,
while weight specialization in turn increases the update rate of the gating
layer. Task switching in the gating layer accelerates as a function of
curriculum block size and task training, mirroring key findings in cognitive
neuroscience. We show that the discovered task abstractions support
generalization through both task and subtask composition, and we extend our
findings to a non-linear network switching between two tasks. Overall, our work
offers a theory of cognitive flexibility in animals as arising from joint
gradient descent on synaptic and neural gating in a neural network
architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond The Rainbow: High Performance Deep Reinforcement Learning On A
  Desktop PC <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Clark, Mark Towers, Christine Evers, Jonathon Hare
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rainbow Deep Q-Network (DQN) demonstrated combining multiple independent
enhancements could significantly boost a reinforcement learning (RL) agent's
performance. In this paper, we present "Beyond The Rainbow" (BTR), a novel
algorithm that integrates six improvements from across the RL literature to
Rainbow DQN, establishing a new state-of-the-art for RL using a desktop PC,
with a human-normalized interquartile mean (IQM) of 7.4 on atari-60. Beyond
Atari, we demonstrate BTR's capability to handle complex 3D games, successfully
training agents to play Super Mario Galaxy, Mario Kart, and Mortal Kombat with
minimal algorithmic changes. Designing BTR with computational efficiency in
mind, agents can be trained using a desktop PC on 200 million Atari frames
within 12 hours. Additionally, we conduct detailed ablation studies of each
component, analzying the performance and impact using numerous measures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 main pages, 26 total. Currently under review at ICLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Transfer Reinforcement Learning: Provable Sample Efficiency from
  Shifted-Dynamics Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengrui Qu, Laixi Shi, Kishan Panaganti, Pengcheng You, Adam Wierman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online Reinforcement learning (RL) typically requires high-stakes online
interaction data to learn a policy for a target task. This prompts interest in
leveraging historical data to improve sample efficiency. The historical data
may come from outdated or related source environments with different dynamics.
It remains unclear how to effectively use such data in the target task to
provably enhance learning and sample efficiency. To address this, we propose a
hybrid transfer RL (HTRL) setting, where an agent learns in a target
environment while accessing offline data from a source environment with shifted
dynamics. We show that -- without information on the dynamics shift -- general
shifted-dynamics data, even with subtle shifts, does not reduce sample
complexity in the target environment. However, with prior information on the
degree of the dynamics shift, we design HySRL, a transfer algorithm that
achieves problem-dependent sample complexity and outperforms pure online RL.
Finally, our experimental results demonstrate that HySRL surpasses
state-of-the-art online RL baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Decomposition of Differential Game 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nanxiang Zhou, Jing Dong, Yutian Li, Baoxiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To understand the complexity of the dynamic of learning in differential
games, we decompose the game into components where the dynamic is well
understood. One of the possible tools is Helmholtz's theorem, which can
decompose a vector field into a potential and a harmonic component. This has
been shown to be effective in finite and normal-form games. However, applying
Helmholtz's theorem by connecting it with the Hodge theorem on $\mathbb{R}^n$
(which is the strategy space of differential game) is non-trivial due to the
non-compactness of $\mathbb{R}^n$. Bridging the dynamic-strategic disconnect
through Hodge/Helmoltz's theorem in differential games is then left as an open
problem \cite{letcher2019differentiable}. In this work, we provide two
decompositions of differential games to answer this question: the first as an
exact scalar potential part, a near vector potential part, and a non-strategic
part; the second as a near scalar potential part, an exact vector potential
part, and a non-strategic part. We show that scalar potential games coincide
with potential games proposed by \cite{monderer1996potential}, where the
gradient descent dynamic can successfully find the Nash equilibrium. For the
vector potential game, we show that the individual gradient field is
divergence-free, in which case the gradient descent dynamic may either be
divergent or recurrent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overcoming label shift in targeted federated learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edvin Listo Zec, Adam Breitholtz, Fredrik D. Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning enables multiple actors to collaboratively train models
without sharing private data. This unlocks the potential for scaling machine
learning to diverse applications. Existing algorithms for this task are
well-justified when clients and the intended target domain share the same
distribution of features and labels, but this assumption is often violated in
real-world scenarios. One common violation is label shift, where the label
distributions differ across clients or between clients and the target domain,
which can significantly degrade model performance. To address this problem, we
propose FedPALS, a novel model aggregation scheme that adapts to label shifts
by leveraging knowledge of the target label distribution at the central server.
Our approach ensures unbiased updates under stochastic gradient descent,
ensuring robust generalization across clients with diverse, label-shifted data.
Extensive experiments on image classification demonstrate that FedPALS
consistently outperforms standard baselines by aligning model aggregation with
the target domain. Our findings reveal that conventional federated learning
methods suffer severely in cases of extreme client sparsity, highlighting the
critical need for target-aware aggregation. FedPALS offers a principled and
practical solution to mitigate label distribution mismatch, ensuring models
trained in federated settings can generalize effectively to label-shifted
target domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The N-Grammys: Accelerating Autoregressive Inference with Learning-Free
  Batched Speculation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lawrence Stewart, Matthew Trager, Sujan Kumar Gonugondla, Stefano Soatto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding aims to speed up autoregressive generation of a language
model by verifying in parallel the tokens generated by a smaller draft model.In
this work, we explore the effectiveness of learning-free, negligible-cost draft
strategies, namely $N$-grams obtained from the model weights and the context.
While the predicted next token of the base model is rarely the top prediction
of these simple strategies, we observe that it is often within their top-$k$
predictions for small $k$. Based on this, we show that combinations of simple
strategies can achieve significant inference speedups over different tasks. The
overall performance is comparable to more complex methods, yet does not require
expensive preprocessing or modification of the base model, and allows for
seamless `plug-and-play' integration into pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating the landscape of multimodal AI in medicine: a scoping review
  on technical challenges and clinical applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daan Schouten, Giulia Nicoletti, Bas Dille, Catherine Chia, Pierpaolo Vendittelli, Megan Schuurmans, Geert Litjens, Nadieh Khalili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent technological advances in healthcare have led to unprecedented growth
in patient data quantity and diversity. While artificial intelligence (AI)
models have shown promising results in analyzing individual data modalities,
there is increasing recognition that models integrating multiple complementary
data sources, so-called multimodal AI, could enhance clinical decision-making.
This scoping review examines the landscape of deep learning-based multimodal AI
applications across the medical domain, analyzing 432 papers published between
2018 and 2024. We provide an extensive overview of multimodal AI development
across different medical disciplines, examining various architectural
approaches, fusion strategies, and common application areas. Our analysis
reveals that multimodal AI models consistently outperform their unimodal
counterparts, with an average improvement of 6.2 percentage points in AUC.
However, several challenges persist, including cross-departmental coordination,
heterogeneous data characteristics, and incomplete datasets. We critically
assess the technical and practical challenges in developing multimodal AI
systems and discuss potential strategies for their clinical implementation,
including a brief overview of commercially available multimodal AI models for
clinical decision-making. Additionally, we identify key factors driving
multimodal AI development and propose recommendations to accelerate the field's
maturation. This review provides researchers and clinicians with a thorough
understanding of the current state, challenges, and future directions of
multimodal AI in medicine.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with
  Captions in 28 Languages <span class="chip">EMNLP 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Mohamed, Runjia Li, Ibrahim Said Ahmad, Kilichbek Haydarov, Philip Torr, Kenneth Ward Church, Mohamed Elhoseiny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in vision and language has made considerable progress thanks to
benchmarks such as COCO. COCO captions focused on unambiguous facts in English;
ArtEmis introduced subjective emotions and ArtELingo introduced some
multilinguality (Chinese and Arabic). However we believe there should be more
multilinguality. Hence, we present ArtELingo-28, a vision-language benchmark
that spans $\textbf{28}$ languages and encompasses approximately
$\textbf{200,000}$ annotations ($\textbf{140}$ annotations per image).
Traditionally, vision research focused on unambiguous class labels, whereas
ArtELingo-28 emphasizes diversity of opinions over languages and cultures. The
challenge is to build machine learning systems that assign emotional captions
to images. Baseline results will be presented for three novel conditions:
Zero-Shot, Few-Shot and One-vs-All Zero-Shot. We find that cross-lingual
transfer is more successful for culturally-related languages. Data and code are
provided at www.artelingo.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Accepted at EMNLP 24, for more details see www.artelingo.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Bayesian Approach to Data Point Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinnuo Xu, Minyoung Kim, Royson Lee, Brais Martinez, Timothy Hospedales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data point selection (DPS) is becoming a critical topic in deep learning due
to the ease of acquiring uncurated training data compared to the difficulty of
obtaining curated or processed data. Existing approaches to DPS are
predominantly based on a bi-level optimisation (BLO) formulation, which is
demanding in terms of memory and computation, and exhibits some theoretical
defects regarding minibatches. Thus, we propose a novel Bayesian approach to
DPS. We view the DPS problem as posterior inference in a novel Bayesian model
where the posterior distributions of the instance-wise weights and the main
neural network parameters are inferred under a reasonable prior and likelihood
model. We employ stochastic gradient Langevin MCMC sampling to learn the main
network and instance-wise weights jointly, ensuring convergence even with
minibatches. Our update equation is comparable to the widely used SGD and much
more efficient than existing BLO-based methods. Through controlled experiments
in both the vision and language domains, we present the proof-of-concept.
Additionally, we demonstrate that our method scales effectively to large
language models and facilitates automated per-task optimization for instruction
fine-tuning datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variational Inference on the Boolean Hypercube with the Quantum Entropy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliot Beyler, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we derive variational inference upper-bounds on the
log-partition function of pairwise Markov random fields on the Boolean
hypercube, based on quantum relaxations of the Kullback-Leibler divergence. We
then propose an efficient algorithm to compute these bounds based on
primal-dual optimization. An improvement of these bounds through the use of
''hierarchies,'' similar to sum-of-squares (SoS) hierarchies is proposed, and
we present a greedy algorithm to select among these relaxations. We carry
extensive numerical experiments and compare with state-of-the-art methods for
this inference problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Content-Style Learning from Unaligned Domains: Identifiability under
  Unknown Latent Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagar Shrestha, Xiao Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding identifiability of latent content and style variables from
unaligned multi-domain data is essential for tasks such as domain translation
and data generation. Existing works on content-style identification were often
developed under somewhat stringent conditions, e.g., that all latent components
are mutually independent and that the dimensions of the content and style
variables are known. We introduce a new analytical framework via cross-domain
\textit{latent distribution matching} (LDM), which establishes content-style
identifiability under substantially more relaxed conditions. Specifically, we
show that restrictive assumptions such as component-wise independence of the
latent variables can be removed. Most notably, we prove that prior knowledge of
the content and style dimensions is not necessary for ensuring identifiability,
if sparsity constraints are properly imposed onto the learned latent
representations. Bypassing the knowledge of the exact latent dimension has been
a longstanding aspiration in unsupervised representation learning -- our
analysis is the first to underpin its theoretical and practical viability. On
the implementation side, we recast the LDM formulation into a regularized
multi-domain GAN loss with coupled latent variables. We show that the
reformulation is equivalent to LDM under mild conditions -- yet requiring
considerably less computational resource. Experiments corroborate with our
theoretical claims.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolic regression via MDLformer-guided search: from minimizing
  prediction error to minimizing description length 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Yu, Jingtao Ding, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic regression, a task discovering the formula best fitting the given
data, is typically based on the heuristical search. These methods usually
update candidate formulas to obtain new ones with lower prediction errors
iteratively. However, since formulas with similar function shapes may have
completely different symbolic forms, the prediction error does not decrease
monotonously as the search approaches the target formula, causing the low
recovery rate of existing methods. To solve this problem, we propose a novel
search objective based on the minimum description length, which reflects the
distance from the target and decreases monotonically as the search approaches
the correct form of the target formula. To estimate the minimum description
length of any input data, we design a neural network, MDLformer, which enables
robust and scalable estimation through large-scale training. With the
MDLformer's output as the search objective, we implement a symbolic regression
method, SR4MDL, that can effectively recover the correct mathematical form of
the formula. Extensive experiments illustrate its excellent performance in
recovering formulas from data. Our method successfully recovers around 50
formulas across two benchmark datasets comprising 133 problems, outperforming
state-of-the-art methods by 43.92%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deferred Poisoning: Making the Model More Vulnerable via Hessian
  Singularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao He, Jinyu Tian, Xianwei Zheng, Li Dong, Yuanman Li, Leo Yu Zhang, Jiantao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that deep learning models are very vulnerable to
poisoning attacks. Many defense methods have been proposed to address this
issue. However, traditional poisoning attacks are not as threatening as
commonly believed. This is because they often cause differences in how the
model performs on the training set compared to the validation set. Such
inconsistency can alert defenders that their data has been poisoned, allowing
them to take the necessary defensive actions. In this paper, we introduce a
more threatening type of poisoning attack called the Deferred Poisoning Attack.
This new attack allows the model to function normally during the training and
validation phases but makes it very sensitive to evasion attacks or even
natural noise. We achieve this by ensuring the poisoned model's loss function
has a similar value as a normally trained model at each input sample but with a
large local curvature. A similar model loss ensures that there is no obvious
inconsistency between the training and validation accuracy, demonstrating high
stealthiness. On the other hand, the large curvature implies that a small
perturbation may cause a significant increase in model loss, leading to
substantial performance degradation, which reflects a worse robustness. We
fulfill this purpose by making the model have singular Hessian information at
the optimal point via our proposed Singularization Regularization term. We have
conducted both theoretical and empirical analyses of the proposed method and
validated its effectiveness through experiments on image classification tasks.
Furthermore, we have confirmed the hazards of this form of poisoning attack
under more general scenarios using natural noise, offering a new perspective
for research in the field of security.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Defenses Against Gradient Reconstruction Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiao Chen, Gamze Gürsoy, Qi Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is designed to prevent data leakage through
collaborative model training without centralized data storage. However, it
remains vulnerable to gradient reconstruction attacks that recover original
training data from shared gradients. To optimize the trade-off between data
leakage and utility loss, we first derive a theoretical lower bound of
reconstruction error (among all attackers) for the two standard methods: adding
noise, and gradient pruning. We then customize these two defenses to be
parameter- and model-specific and achieve the optimal trade-off between our
obtained reconstruction lower bound and model utility. Experimental results
validate that our methods outperform Gradient Noise and Gradient Pruning by
protecting the training data better while also achieving better utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code for this project is available at
  https://github.com/cyx78/Optimal_Defenses_Against_Gradient_Reconstruction_Attacks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Networks with Coarse- and Fine-Grained Division for
  Mitigating Label Sparsity and Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangjie Li, Baoming Zhang, Jianqing Song, Gaoli Ruan, Chongjun Wang, Junyuan Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have gained considerable prominence in
semi-supervised learning tasks in processing graph-structured data, primarily
owing to their message-passing mechanism, which largely relies on the
availability of clean labels. However, in real-world scenarios, labels on nodes
of graphs are inevitably noisy and sparsely labeled, significantly degrading
the performance of GNNs. Exploring robust GNNs for semi-supervised node
classification in the presence of noisy and sparse labels remains a critical
challenge. Therefore, we propose a novel \textbf{G}raph \textbf{N}eural
\textbf{N}etwork with \textbf{C}oarse- and \textbf{F}ine-\textbf{G}rained
\textbf{D}ivision for mitigating label sparsity and noise, namely GNN-CFGD. The
key idea of GNN-CFGD is reducing the negative impact of noisy labels via
coarse- and fine-grained division, along with graph reconstruction.
Specifically, we first investigate the effectiveness of linking unlabeled nodes
to cleanly labeled nodes, demonstrating that this approach is more effective in
combating labeling noise than linking to potentially noisy labeled nodes. Based
on this observation, we introduce a Gaussian Mixture Model (GMM) based on the
memory effect to perform a coarse-grained division of the given labels into
clean and noisy labels. Next, we propose a clean labels oriented link that
connects unlabeled nodes to cleanly labeled nodes, aimed at mitigating label
sparsity and promoting supervision propagation. Furthermore, to provide refined
supervision for noisy labeled nodes and additional supervision for unlabeled
nodes, we fine-grain the noisy labeled and unlabeled nodes into two candidate
sets based on confidence, respectively. Extensive experiments on various
datasets demonstrate the superior effectiveness and robustness of GNN-CFGD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Consensus Gradients Aggregation for Scaled Distributed Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoni Choukroun, Shlomi Azoulay, Pavel Kisilev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed machine learning has recently become a critical paradigm for
training large models on vast datasets. We examine the stochastic optimization
problem for deep learning within synchronous parallel computing environments
under communication constraints. While averaging distributed gradients is the
most widely used method for gradient estimation, whether this is the optimal
strategy remains an open question. In this work, we analyze the distributed
gradient aggregation process through the lens of subspace optimization. By
formulating the aggregation problem as an objective-aware subspace optimization
problem, we derive an efficient weighting scheme for gradients, guided by
subspace coefficients. We further introduce subspace momentum to accelerate
convergence while maintaining statistical unbiasedness in the aggregation. Our
method demonstrates improved performance over the ubiquitous gradient averaging
on multiple MLPerf tasks while remaining extremely efficient in both
communicational and computational complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-in-the-Loop Feature Selection Using Interpretable
  Kolmogorov-Arnold Network-based Double Deep Q-Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Abrar Jahin, M. F. Mridha, Nilanjan Dey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature selection is critical for improving the performance and
interpretability of machine learning models, particularly in high-dimensional
spaces where complex feature interactions can reduce accuracy and increase
computational demands. Existing approaches often rely on static feature subsets
or manual intervention, limiting adaptability and scalability. However,
dynamic, per-instance feature selection methods and model-specific
interpretability in reinforcement learning remain underexplored. This study
proposes a human-in-the-loop (HITL) feature selection framework integrated into
a Double Deep Q-Network (DDQN) using a Kolmogorov-Arnold Network (KAN). Our
novel approach leverages simulated human feedback and stochastic
distribution-based sampling, specifically Beta, to iteratively refine feature
subsets per data instance, improving flexibility in feature selection. The
KAN-DDQN achieved notable test accuracies of 93% on MNIST and 83% on
FashionMNIST, outperforming conventional MLP-DDQN models by up to 9%. The
KAN-based model provided high interpretability via symbolic representation
while using 4 times fewer neurons in the hidden layer than MLPs did.
Comparatively, the models without feature selection achieved test accuracies of
only 58% on MNIST and 64% on FashionMNIST, highlighting significant gains with
our framework. Pruning and visualization further enhanced model transparency by
elucidating decision pathways. These findings present a scalable, interpretable
solution for feature selection that is suitable for applications requiring
real-time, adaptive decision-making with minimal human oversight.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to a journal under IEEE Transactions series</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model
  Training Pipelines via Memoization-Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelmajid Essofi, Ridwan Salahuddeen, Munachiso Nwadike, Elnura Zhalieva, Kun Zhang, Eric Xing, Willie Neiswanger, Qirong Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The training or fine-tuning of machine learning, vision, and language models
is often implemented as a pipeline: a sequence of stages encompassing data
preparation, model training and evaluation. In this paper, we exploit pipeline
structures to reduce the cost of hyperparameter tuning for model
training/fine-tuning, which is particularly valuable for language models given
their high costs in GPU-days. We propose a "memoization-aware" Bayesian
Optimization (BO) algorithm, EEIPU, that works in tandem with a pipeline
caching system, allowing it to evaluate significantly more hyperparameter
candidates per GPU-day than other tuning algorithms. The result is
better-quality hyperparameters in the same amount of search time, or
equivalently, reduced search time to reach the same hyperparameter quality. In
our benchmarks on machine learning (model ensembles), vision (convolutional
architecture) and language (T5 architecture) pipelines, we compare EEIPU
against recent BO algorithms: EEIPU produces an average of $103\%$ more
hyperparameter candidates (within the same budget), and increases the
validation metric by an average of $108\%$ more than other algorithms (where
the increase is measured starting from the end of warm-up iterations).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document
  VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marlon Tobaben, Mohamed Ali Souibgui, Rubèn Tito, Khanh Nguyen, Raouf Kerkouche, Kangsoo Jung, Joonas Jälkö, Lei Kang, Andrey Barsky, Vincent Poulain d'Andecy, Aurélie Joseph, Aashiq Muhamed, Kevin Kuo, Virginia Smith, Yusuke Yamasaki, Takumi Fukami, Kenta Niwa, Iifan Tyou, Hiro Ishii, Rio Yokota, Ragul N, Rintu Kutum, Josep Llados, Ernest Valveny, Antti Honkela, Mario Fritz, Dimosthenis Karatzas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA)
competition challenged the community to develop provably private and
communication-efficient solutions in a federated setting for a real-life use
case: invoice processing. The competition introduced a dataset of real invoice
documents, along with associated questions and answers requiring information
extraction and reasoning over the document images. Thereby, it brings together
researchers and expertise from the document analysis, privacy, and federated
learning communities. Participants fine-tuned a pre-trained, state-of-the-art
Document Visual Question Answering model provided by the organizers for this
new domain, mimicking a typical federated invoice processing setup. The base
model is a multi-modal generative language model, and sensitive information
could be exposed through either the visual or textual input modality.
Participants proposed elegant solutions to reduce communication costs while
maintaining a minimum utility threshold in track 1 and to protect all
information from each document provider using differential privacy in track 2.
The competition served as a new testbed for developing and testing private
federated learning methods, simultaneously raising awareness about privacy
within the document image analysis and recognition community. Ultimately, the
competition analysis provides best practices and recommendations for
successfully running privacy-focused federated learning challenges in the
future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PropNEAT -- Efficient GPU-Compatible Backpropagation over
  NeuroEvolutionary Augmenting Topology Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Merry, Patricia Riddle, Jim Warren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PropNEAT, a fast backpropagation implementation of NEAT that
uses a bidirectional mapping of the genome graph to a layer-based architecture
that preserves the NEAT genomes whilst enabling efficient GPU backpropagation.
We test PropNEAT on 58 binary classification datasets from the Penn Machine
Learning Benchmarks database, comparing the performance against logistic
regression, dense neural networks and random forests, as well as a densely
retrained variant of the final PropNEAT model. PropNEAT had the second best
overall performance, behind Random Forest, though the difference between the
models was not statistically significant apart from between Random Forest in
comparison with logistic regression and the PropNEAT retrain models. PropNEAT
was substantially faster than a naive backpropagation method, and both were
substantially faster and had better performance than the original NEAT
implementation. We demonstrate that the per-epoch training time for PropNEAT
scales linearly with network depth, and is efficient on GPU implementations for
backpropagation. This implementation could be extended to support reinforcement
learning or convolutional networks, and is able to find sparser and smaller
networks with potential for applications in low-power contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Trusted Multi-view Classification Framework with
  Hierarchical Opinion Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Shi, Chuanqing Tang, Huangyi Deng, Cai Xu, Lei Xing, Badong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, multi-view learning has witnessed a considerable interest on the
research of trusted decision-making. Previous methods are mainly inspired from
an important paper published by Han et al. in 2021, which formulates a Trusted
Multi-view Classification (TMC) framework that aggregates evidence from
different views based on Dempster's combination rule. All these methods only
consider inter-view aggregation, yet lacking exploitation of intra-view
information. In this paper, we propose a generalized trusted multi-view
classification framework with hierarchical opinion aggregation. This
hierarchical framework includes a two-phase aggregation process: the intra-view
and inter-view aggregation hierarchies. In the intra aggregation, we assume
that each view is comprised of common information shared with other views, as
well as its specific information. We then aggregate both the common and
specific information. This aggregation phase is useful to eliminate the feature
noise inherent to view itself, thereby improving the view quality. In the
inter-view aggregation, we design an attention mechanism at the evidence level
to facilitate opinion aggregation from different views. To the best of our
knowledge, this is one of the pioneering efforts to formulate a hierarchical
aggregation framework in the trusted multi-view learning domain. Extensive
experiments show that our model outperforms some state-of-art trust-related
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Model Adaptation at Test Time: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehao Xiao, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning algorithms have achieved remarkable success across various
disciplines, use cases and applications, under the prevailing assumption that
training and test samples are drawn from the same distribution. Consequently,
these algorithms struggle and become brittle even when samples in the test
distribution start to deviate from the ones observed during training. Domain
adaptation and domain generalization have been studied extensively as
approaches to address distribution shifts across test and train domains, but
each has its limitations. Test-time adaptation, a recently emerging learning
paradigm, combines the benefits of domain adaptation and domain generalization
by training models only on source data and adapting them to target data during
test-time inference. In this survey, we provide a comprehensive and systematic
review on test-time adaptation, covering more than 400 recent papers. We
structure our review by categorizing existing methods into five distinct
categories based on what component of the method is adjusted for test-time
adaptation: the model, the inference, the normalization, the sample, or the
prompt, providing detailed analysis of each. We further discuss the various
preparation and adaptation settings for methods within these categories,
offering deeper insights into the effective deployment for the evaluation of
distribution shifts and their real-world application in understanding images,
video and 3D, as well as modalities beyond vision. We close the survey with an
outlook on emerging research opportunities for test-time adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-model Ensemble Conformal Prediction in Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erfan Hajihashemi, Yanning Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal prediction is an uncertainty quantification method that constructs
a prediction set for a previously unseen datum, ensuring the true label is
included with a predetermined coverage probability. Adaptive conformal
prediction has been developed to address data distribution shifts in dynamic
environments. However, the efficiency of prediction sets varies depending on
the learning model used. Employing a single fixed model may not consistently
offer the best performance in dynamic environments with unknown data
distribution shifts. To address this issue, we introduce a novel adaptive
conformal prediction framework, where the model used for creating prediction
sets is selected on the fly from multiple candidate models. The proposed
algorithm is proven to achieve strongly adaptive regret over all intervals
while maintaining valid coverage. Experiments on real and synthetic datasets
corroborate that the proposed approach consistently yields more efficient
prediction sets while maintaining valid coverage, outperforming alternative
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-based physics-informed neural network for frictionless contact
  problems under large deformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinshuai Bai, Zhongya Lin, Yizheng Wang, Jiancong Wen, Yinghua Liu, Timon Rabczuk, YuanTong Gu, Xi-Qiao Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerical methods for contact mechanics are of great importance in
engineering applications, enabling the prediction and analysis of complex
surface interactions under various conditions. In this work, we propose an
energy-based physics-informed neural network (PINNs) framework for solving
frictionless contact problems under large deformation. Inspired by microscopic
Lennard-Jones potential, a surface contact energy is used to describe the
contact phenomena. To ensure the robustness of the proposed PINN framework,
relaxation, gradual loading and output scaling techniques are introduced. In
the numerical examples, the well-known Hertz contact benchmark problem is
conducted, demonstrating the effectiveness and robustness of the proposed PINNs
framework. Moreover, challenging contact problems with the consideration of
geometrical and material nonlinearities are tested. It has been shown that the
proposed PINNs framework provides a reliable and powerful tool for nonlinear
contact mechanics. More importantly, the proposed PINNs framework exhibits
competitive computational efficiency to the commercial FEM software when
dealing with those complex contact problems. The codes used in this manuscript
are available at https://github.com/JinshuaiBai/energy_PINN_Contact.(The code
will be available after acceptance)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Graph Neural Networks Expose Training Data Properties? An Efficient
  Risk Assessment Approach <span class="chip">NeurIPS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyang Yuan, Jiarong Xu, Renhong Huang, Mingli Song, Chunping Wang, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have attracted considerable attention due to
their diverse applications. However, the scarcity and quality limitations of
graph data present challenges to their training process in practical settings.
To facilitate the development of effective GNNs, companies and researchers
often seek external collaboration. Yet, directly sharing data raises privacy
concerns, motivating data owners to train GNNs on their private graphs and
share the trained models. Unfortunately, these models may still inadvertently
disclose sensitive properties of their training graphs (e.g., average default
rate in a transaction network), leading to severe consequences for data owners.
In this work, we study graph property inference attack to identify the risk of
sensitive property information leakage from shared models. Existing approaches
typically train numerous shadow models for developing such attack, which is
computationally intensive and impractical. To address this issue, we propose an
efficient graph property inference attack by leveraging model approximation
techniques. Our method only requires training a small set of models on graphs,
while generating a sufficient number of approximated shadow models for attacks.
To enhance diversity while reducing errors in the approximated models, we apply
edit distance to quantify the diversity within a group of approximated models
and introduce a theoretically guaranteed criterion to evaluate each model's
error. Subsequently, we propose a novel selection mechanism to ensure that the
retained approximated models achieve high diversity and low error. Extensive
experiments across six real-world scenarios demonstrate our method's
substantial improvement, with average increases of 2.7% in attack accuracy and
4.1% in ROC-AUC, while being 6.5$\times$ faster compared to the best baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In NeurIPS'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Requirements Engineering for Older Adult Digital Health Software: A
  Systematic Literature Review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqing Xiao, John Grundy, Anuradha Madugalla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Growth of the older adult population has led to an increasing interest in
technology-supported aged care. However, the area has some challenges such as a
lack of caregivers and limitations in understanding the emotional, social,
physical, and mental well-being needs of seniors. Furthermore, there is a gap
in the understanding between developers and ageing people of their
requirements. Digital health can be important in supporting older adults
wellbeing, emotional requirements, and social needs. Requirements Engineering
(RE) is a major software engineering field, which can help to identify, elicit
and prioritize the requirements of stakeholders and ensure that the systems
meet standards for performance, reliability, and usability. We carried out a
systematic review of the literature on RE for older adult digital health
software. This was necessary to show the representatives of the current stage
of understanding the needs of older adults in aged care digital health. Using
established guidelines outlined by the Kitchenham method, the PRISMA and the
PICO guideline, we developed a protocol, followed by the systematic exploration
of eight databases. This resulted in 69 primary studies of high relevance,
which were subsequently subjected to data extraction, synthesis, and reporting.
We highlight key RE processes in digital health software for ageing people. It
explored the utilization of technology for older user well-being and care, and
the evaluations of such solutions. The review also identified key limitations
found in existing primary studies that inspire future research opportunities.
The results indicate that requirement gathering and understanding have a
significant variation between different studies. The differences are in the
quality, depth, and techniques adopted for requirement gathering and these
differences are largely due to uneven adoption of RE methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arxiv version of SLR on RE for Older Adult Digital Health Software</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Policy Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parand A. Alamdari, Soroush Ebadian, Ariel D. Procaccia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the challenge of AI value alignment with multiple individuals
that have different reward functions and optimal policies in an underlying
Markov decision process. We formalize this problem as one of policy
aggregation, where the goal is to identify a desirable collective policy. We
argue that an approach informed by social choice theory is especially suitable.
Our key insight is that social choice methods can be reinterpreted by
identifying ordinal preferences with volumes of subsets of the state-action
occupancy polytope. Building on this insight, we demonstrate that a variety of
methods--including approval voting, Borda count, the proportional veto core,
and quantile fairness--can be practically applied to policy aggregation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Multi-objective Bayesian Optimization through Optimistic
  Constraints Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diantong Li, Fengxue Zhang, Chong Liu, Yuxin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective Bayesian optimization has been widely adopted in scientific
experiment design, including drug discovery and hyperparameter optimization. In
practice, regulatory or safety concerns often impose additional thresholds on
certain attributes of the experimental outcomes. Previous work has primarily
focused on constrained single-objective optimization tasks or active search
under constraints. We propose CMOBO, a sample-efficient constrained
multi-objective Bayesian optimization algorithm that balances learning of the
feasible region (defined on multiple unknowns) with multi-objective
optimization within the feasible region in a principled manner. We provide both
theoretical justification and empirical evidence, demonstrating the efficacy of
our approach on various synthetic benchmarks and real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEGMN: A Structure-Enhanced Graph Matching Network for Graph Similarity
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Wang, Jiacheng Lu, Kejia Chen, Zheng Liu, Shilong Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph similarity computation (GSC) aims to quantify the similarity score
between two graphs. Although recent GSC methods based on graph neural networks
(GNNs) take advantage of intra-graph structures in message passing, few of them
fully utilize the structures presented by edges to boost the representation of
their connected nodes. Moreover, previous cross-graph node embedding matching
lacks the perception of the overall structure of the graph pair, due to the
fact that the node representations from GNNs are confined to the intra-graph
structure, causing the unreasonable similarity score. Intuitively, the
cross-graph structure represented in the assignment graph is helpful to rectify
the inappropriate matching. Therefore, we propose a structure-enhanced graph
matching network (SEGMN). Equipped with a dual embedding learning module and a
structure perception matching module, SEGMN achieves structure enhancement in
both embedding learning and cross-graph matching. The dual embedding learning
module incorporates adjacent edge representation into each node to achieve a
structure-enhanced representation. The structure perception matching module
achieves cross-graph structure enhancement through assignment graph
convolution. The similarity score of each cross-graph node pair can be
rectified by aggregating messages from structurally relevant node pairs.
Experimental results on benchmark datasets demonstrate that SEGMN outperforms
the state-of-the-art GSC methods in the GED regression task, and the structure
perception matching module is plug-and-play, which can further improve the
performance of the baselines by up to 25%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully Hyperbolic Rotation for Knowledge Graph Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuyu Liang, Weihua Wang, Feilong Bao, Guanglai Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperbolic rotation is commonly used to effectively model knowledge graphs
and their inherent hierarchies. However, existing hyperbolic rotation models
rely on logarithmic and exponential mappings for feature transformation. These
models only project data features into hyperbolic space for rotation, limiting
their ability to fully exploit the hyperbolic space. To address this problem,
we propose a novel fully hyperbolic model designed for knowledge graph
embedding. Instead of feature mappings, we define the model directly in
hyperbolic space with the Lorentz model. Our model considers each relation in
knowledge graphs as a Lorentz rotation from the head entity to the tail entity.
We adopt the Lorentzian version distance as the scoring function for measuring
the plausibility of triplets. Extensive results on standard knowledge graph
completion benchmarks demonstrated that our model achieves competitive results
with fewer parameters. In addition, our model get the state-of-the-art
performance on datasets of CoDEx-s and CoDEx-m, which are more diverse and
challenging than before. Our code is available at
https://github.com/llqy123/FHRE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Subsampling Based Neural Network for Spatial Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debjoy Thakur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of deep neural networks in geospatial data has become a
trending research problem in the present day. A significant amount of
statistical research has already been introduced, such as generalized least
square optimization by incorporating spatial variance-covariance matrix,
considering basis functions in the input nodes of the neural networks, and so
on. However, for lattice data, there is no available literature about the
utilization of asymptotic analysis of neural networks in regression for spatial
data. This article proposes a consistent localized two-layer deep neural
network-based regression for spatial data. We have proved the consistency of
this deep neural network for bounded and unbounded spatial domains under a
fixed sampling design of mixed-increasing spatial regions. We have proved that
its asymptotic convergence rate is faster than that of \cite{zhan2024neural}'s
neural network and an improved generalization of \cite{shen2023asymptotic}'s
neural network structure. We empirically observe the rate of convergence of
discrepancy measures between the empirical probability distribution of observed
and predicted data, which will become faster for a less smooth spatial surface.
We have applied our asymptotic analysis of deep neural networks to the
estimation of the monthly average temperature of major cities in the USA from
its satellite image. This application is an effective showcase of non-linear
spatial regression. We demonstrate our methodology with simulated lattice data
in various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing a Linearized Potential Function in Neural Network Optimization
  Using Csiszár Type of Tsallis Entropy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keito Akiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, learning for neural networks can be viewed as optimization
in the space of probability measures. To obtain the exponential convergence to
the optimizer, the regularizing term based on Shannon entropy plays an
important role. Even though an entropy function heavily affects convergence
results, there is almost no result on its generalization, because of the
following two technical difficulties: one is the lack of sufficient condition
for generalized logarithmic Sobolev inequality, and the other is the
distributional dependence of the potential function within the gradient flow
equation. In this paper, we establish a framework that utilizes a linearized
potential function via Csisz\'{a}r type of Tsallis entropy, which is one of the
generalized entropies. We also show that our new framework enable us to derive
an exponential convergence result.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal-Difference Learning Using Distributed Error Signals <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Guan, Shon Eduard Verch, Claas Voelcker, Ethan C. Jackson, Nicolas Papernot, William A. Cunningham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A computational problem in biological reward-based learning is how credit
assignment is performed in the nucleus accumbens (NAc). Much research suggests
that NAc dopamine encodes temporal-difference (TD) errors for learning value
predictions. However, dopamine is synchronously distributed in regionally
homogeneous concentrations, which does not support explicit credit assignment
(like used by backpropagation). It is unclear whether distributed errors alone
are sufficient for synapses to make coordinated updates to learn complex,
nonlinear reward-based learning tasks. We design a new deep Q-learning
algorithm, Artificial Dopamine, to computationally demonstrate that
synchronously distributed, per-layer TD errors may be sufficient to learn
surprisingly complex RL tasks. We empirically evaluate our algorithm on
MinAtar, the DeepMind Control Suite, and classic control tasks, and show it
often achieves comparable performance to deep RL algorithms that use
backpropagation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, to be published at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Source High-Speed Flight Surrogate Modeling Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler E. Korenyi-Both, Nathan J. Falkiewicz, Matthew C. Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-speed flight vehicles, which travel much faster than the speed of sound,
are crucial for national defense and space exploration. However, accurately
predicting their behavior under numerous, varied flight conditions is a
challenge and often prohibitively expensive. The proposed approach involves
creating smarter, more efficient machine learning models (also known as
surrogate models or meta models) that can fuse data generated from a variety of
fidelity levels -- to include engineering methods, simulation, wind tunnel, and
flight test data -- to make more accurate predictions. These models are able to
move the bulk of the computation from high performance computing (HPC) to
single user machines (laptop, desktop, etc.). The project builds upon previous
work but introduces code improvements and an informed perspective on the
direction of the field. The new surrogate modeling framework is now modular
and, by design, broadly applicable to many modeling problems. The new framework
also has a more robust automatic hyperparameter tuning capability and abstracts
away most of the pre- and post-processing tasks. The Gaussian process
regression and deep neural network-based models included in the presented
framework were able to model two datasets with high accuracy (R^2>0.99). The
primary conclusion is that the framework is effective and has been delivered to
the Air Force for integration into real-world projects. For future work,
significant and immediate investment in continued research is crucial. The
author recommends further testing and refining modeling methods that explicitly
incorporate physical laws and are robust enough to handle simulation and test
data from varying resolutions and sources, including coarse meshes, fine
meshes, unstructured meshes, and limited experimental test points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing the Expressivity of Temporal Graph Networks through
  Source-Target Identification <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedict Aaron Tjandra, Federico Barbero, Michael Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the successful application of Temporal Graph Networks (TGNs) for
tasks such as dynamic node classification and link prediction, they still
perform poorly on the task of dynamic node affinity prediction -- where the
goal is to predict `how much' two nodes will interact in the future. In fact,
simple heuristic approaches such as persistent forecasts and moving averages
over \emph{ground-truth labels} significantly and consistently outperform TGNs.
Building on this observation, we find that computing heuristics \textit{over
messages} is an equally competitive approach, outperforming TGN and all current
temporal graph (TG) models on dynamic node affinity prediction. In this paper,
we prove that no formulation of TGN can represent persistent forecasting or
moving averages over messages, and propose to enhance the expressivity of TGNs
by adding source-target identification to each interaction event message. We
show that this modification is required to represent persistent forecasting,
moving averages, and the broader class of autoregressive models over messages.
Our proposed method, TGNv2, significantly outperforms TGN and all current TG
models on all Temporal Graph Benchmark (TGB) dynamic node affinity prediction
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS Symmetry and Geometry in Neural Representations
  Workshop 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Experimental Study on Decomposition-Based Deep Ensemble Learning for
  Traffic Flow Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Zhu, A. K. Qin, Hussein Dia, Adriana-Simona Mihaita, Hanna Grzybowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic flow forecasting is a crucial task in intelligent transport systems.
Deep learning offers an effective solution, capturing complex patterns in
time-series traffic flow data to enable the accurate prediction. However, deep
learning models are prone to overfitting the intricate details of flow data,
leading to poor generalisation. Recent studies suggest that decomposition-based
deep ensemble learning methods may address this issue by breaking down a time
series into multiple simpler signals, upon which deep learning models are built
and ensembled to generate the final prediction. However, few studies have
compared the performance of decomposition-based ensemble methods with
non-decomposition-based ones which directly utilise raw time-series data. This
work compares several decomposition-based and non-decomposition-based deep
ensemble learning methods. Experimental results on three traffic datasets
demonstrate the superiority of decomposition-based ensemble methods, while also
revealing their sensitivity to aggregation strategies and forecasting horizons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by the 2024 Australasian Joint Conference
  on Artificial Intelligence (AJCAI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Constant-Depth Circuits in Malicious Noise Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam R. Klivans, Konstantinos Stavropoulos, Arsen Vasilyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The seminal work of Linial, Mansour, and Nisan gave a quasipolynomial-time
algorithm for learning constant-depth circuits ($\mathsf{AC}^0$) with respect
to the uniform distribution on the hypercube. Extending their algorithm to the
setting of malicious noise, where both covariates and labels can be
adversarially corrupted, has remained open. Here we achieve such a result,
inspired by recent work on learning with distribution shift. Our running time
essentially matches their algorithm, which is known to be optimal assuming
various cryptographic primitives.
  Our proof uses a simple outlier-removal method combined with Braverman's
theorem for fooling constant-depth circuits. We attain the best possible
dependence on the noise rate and succeed in the harshest possible noise model
(i.e., contamination or so-called "nasty noise").
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Personalized Federated Learning via Comprehensive Knowledge
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengju Wang, Bochao Liu, Weijia Guo, Yong Li, Shiming Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a distributed machine learning paradigm designed to
protect data privacy. However, data heterogeneity across various clients
results in catastrophic forgetting, where the model rapidly forgets previous
knowledge while acquiring new knowledge. To address this challenge,
personalized federated learning has emerged to customize a personalized model
for each client. However, the inherent limitation of this mechanism is its
excessive focus on personalization, potentially hindering the generalization of
those models. In this paper, we present a novel personalized federated learning
method that uses global and historical models as teachers and the local model
as the student to facilitate comprehensive knowledge distillation. The
historical model represents the local model from the last round of client
training, containing historical personalized knowledge, while the global model
represents the aggregated model from the last round of server aggregation,
containing global generalized knowledge. By applying knowledge distillation, we
effectively transfer global generalized knowledge and historical personalized
knowledge to the local model, thus mitigating catastrophic forgetting and
enhancing the general performance of personalized models. Extensive
experimental results demonstrate the significant advantages of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE SMC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gradient Boosting Trees and Large Language Models for Tabular Data
  Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Huertas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) have brought numerous of new applications to
Machine Learning (ML). In the context of tabular data (TD), recent studies show
that TabLLM is a very powerful mechanism for few-shot-learning (FSL)
applications, even if gradient boosting decisions trees (GBDT) have
historically dominated the TD field. In this work we demonstrate that although
LLMs are a viable alternative, the evidence suggests that baselines used to
gauge performance can be improved. We replicated public benchmarks and our
methodology improves LightGBM by 290%, this is mainly driven by forcing node
splitting with few samples, a critical step in FSL with GBDT. Our results show
an advantage to TabLLM for 8 or fewer shots, but as the number of samples
increases GBDT provides competitive performance at a fraction of runtime. For
other real-life applications with vast number of samples, we found FSL still
useful to improve model diversity, and when combined with ExtraTrees it
provides strong resilience to overfitting, our proposal was validated in a ML
competition setting ranking first place.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>FedCSIS 2024 - Data Mining Competition - 1st Place Winner</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Symmetry-Aware Materials Generation via Hierarchical
  Generative Flow Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tri Minh Nguyen, Sherif Abdulkader Tawfik, Truyen Tran, Sunil Gupta, Santu Rana, Svetha Venkatesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering new solid-state materials requires rapidly exploring the vast
space of crystal structures and locating stable regions. Generating stable
materials with desired properties and compositions is extremely difficult as we
search for very small isolated pockets in the exponentially many possibilities,
considering elements from the periodic table and their 3D arrangements in
crystal lattices. Materials discovery necessitates both optimized solution
structures and diversity in the generated material structures. Existing methods
struggle to explore large material spaces and generate diverse samples with
desired properties and requirements. We propose the Symmetry-aware Hierarchical
Architecture for Flow-based Traversal (SHAFT), a novel generative model
employing a hierarchical exploration strategy to efficiently exploit the
symmetry of the materials space to generate crystal structures given desired
properties. In particular, our model decomposes the exponentially large
materials space into a hierarchy of subspaces consisting of symmetric space
groups, lattice parameters, and atoms. We demonstrate that SHAFT significantly
outperforms state-of-the-art iterative generative methods, such as Generative
Flow Networks (GFlowNets) and Crystal Diffusion Variational AutoEncoders
(CDVAE), in crystal structure generation tasks, achieving higher validity,
diversity, and stability of generated structures optimized for target
properties and requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Optimizing SQL Generation via LLM Routing <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadhossein Malekpour, Nour Shaheen, Foutse Khomh, Amine Mhedhbi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL enables users to interact with databases through natural
language, simplifying access to structured data. Although highly capable large
language models (LLMs) achieve strong accuracy for complex queries, they incur
unnecessary latency and dollar cost for simpler ones. In this paper, we
introduce the first LLM routing approach for Text-to-SQL, which dynamically
selects the most cost-effective LLM capable of generating accurate SQL for each
query. We present two routing strategies (score- and classification-based) that
achieve accuracy comparable to the most capable LLM while reducing costs. We
design the routers for ease of training and efficient inference. In our
experiments, we highlight a practical and explainable accuracy-cost trade-off
on the BIRD dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Table Representation Learning Workshop at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multilingual Sentiment Lexicon for Low-Resource Language Translation
  using Large Languages Models and Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Melusi Malinga, Isaac Lupanda, Mike Wa Nkongolo, Phil van Deventer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  South Africa and the Democratic Republic of Congo (DRC) present a complex
linguistic landscape with languages such as Zulu, Sepedi, Afrikaans, French,
English, and Tshiluba (Ciluba), which creates unique challenges for AI-driven
translation and sentiment analysis systems due to a lack of accurately labeled
data. This study seeks to address these challenges by developing a multilingual
lexicon designed for French and Tshiluba, now expanded to include translations
in English, Afrikaans, Sepedi, and Zulu. The lexicon enhances cultural
relevance in sentiment classification by integrating language-specific
sentiment scores. A comprehensive testing corpus is created to support
translation and sentiment analysis tasks, with machine learning models such as
Random Forest, Support Vector Machine (SVM), Decision Trees, and Gaussian Naive
Bayes (GNB) trained to predict sentiment across low resource languages (LRLs).
Among them, the Random Forest model performed particularly well, capturing
sentiment polarity and handling language-specific nuances effectively.
Furthermore, Bidirectional Encoder Representations from Transformers (BERT), a
Large Language Model (LLM), is applied to predict context-based sentiment with
high accuracy, achieving 99% accuracy and 98% precision, outperforming other
models. The BERT predictions were clarified using Explainable AI (XAI),
improving transparency and fostering confidence in sentiment classification.
Overall, findings demonstrate that the proposed lexicon and machine learning
models significantly enhance translation and sentiment analysis for LRLs in
South Africa and the DRC, laying a foundation for future AI models that support
underrepresented languages, with applications across education, governance, and
business in multilingual contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is part of a PhD proposal in Information Technology at the
  University of Pretoria, supervised by Dr. Mike Wa Nkongolo and co-supervised
  by Dr. Phil van Deventer, under the Low-Resource Language Processing Lab in
  the Department of Informatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretically informed selection of latent activation in autoencoder
  based recommender systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aviad Susman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoencoders may lend themselves to the design of more accurate and
computationally efficient recommender systems by distilling sparse
high-dimensional data into dense lower-dimensional latent representations.
However, designing these systems remains challenging due to the lack of
theoretical guidance. This work addresses this by identifying three key
mathematical properties that the encoder in an autoencoder should exhibit to
improve recommendation accuracy: (1) dimensionality reduction, (2) preservation
of similarity ordering in dot product comparisons, and (3) preservation of
non-zero vectors. Through theoretical analysis, we demonstrate that common
activation functions, such as ReLU and tanh, cannot fulfill these properties
jointly within a generalizable framework. In contrast, sigmoid-like activations
emerge as suitable choices for latent activations. This theoretically informed
approach offers a more systematic method for hyperparameter selection,
enhancing the efficiency of model design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Exp<span class="highlight-title">lora</span>tion and Exploitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Pasteris, Chris Hicks, Vasilios Mavroudis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we consider the contextual bandit problem with a finite (or
infinite and clustered) context set. We consider the fully adversarial problem
in which, apart from having bounded losses, there are no assumptions whatsoever
on the generation of the contexts and losses. In our problem we assume that the
context set is partitioned into a set of protected groups. At the start of each
trial we are given a probability distribution over the context set and are
required (on that trial) to be fair with respect to that distribution, in that
if the context (for that trial) was drawn from the distribution then our choice
of action would be unbiased towards any protected group. We develop an
algorithm FexEx for this problem which has remarkable efficiency, having a
space and per-trial time complexity at most linear in the dimensionality of the
policy space. FexEx can handle non-stationarity, in that its regret can be
bounded with respect to any sequence of policies satisfying the fairness
constraints. For such a sequence the regret bound of FexEx is essentially the
same as that of running Exp3.S for each context independently (an approach that
does not satisfy the fairness constraints).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Real-Time Mortality Prediction in the Intensive Care Unit using
  Temporal Difference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Frost, Kezhi Li, Steve Harris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of predicting long-term patient outcomes using supervised machine
learning is a challenging one, in part because of the high variance of each
patient's trajectory, which can result in the model over-fitting to the
training data. Temporal difference (TD) learning, a common reinforcement
learning technique, may reduce variance by generalising learning to the pattern
of state transitions rather than terminal outcomes. However, in healthcare this
method requires several strong assumptions about patient states, and there
appears to be limited literature evaluating the performance of TD learning
against traditional supervised learning methods for long-term health outcome
prediction tasks. In this study, we define a framework for applying TD learning
to real-time irregularly sampled time series data using a Semi-Markov Reward
Process. We evaluate the model framework in predicting intensive care mortality
and show that TD learning under this framework can result in improved model
robustness compared to standard supervised learning methods. and that this
robustness is maintained even when validated on external datasets. This
approach may offer a more reliable method when learning to predict patient
outcomes using high-variance irregular time series data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the Proceedings of the 4th Machine Learning for
  Health symposium, Proceedings of Machine Learning Research (PMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Security Control Production With Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Ling, Mina Ghashami, Vianne Gao, Ali Torkamani, Ruslan Vaulin, Nivedita Mangam, Bhavya Jain, Farhan Diwan, Malini SS, Mingrui Cheng, Shreya Tarur Kumar, Felix Candelario
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Security controls are mechanisms or policies designed for cloud based
services to reduce risk, protect information, and ensure compliance with
security regulations. The development of security controls is traditionally a
labor-intensive and time-consuming process. This paper explores the use of
Generative AI to accelerate the generation of security controls. We
specifically focus on generating Gherkin codes which are the domain-specific
language used to define the behavior of security controls in a structured and
understandable format. By leveraging large language models and in-context
learning, we propose a structured framework that reduces the time required for
developing security controls from 2-3 days to less than one minute. Our
approach integrates detailed task descriptions, step-by-step instructions, and
retrieval-augmented generation to enhance the accuracy and efficiency of the
generated Gherkin code. Initial evaluations on AWS cloud services demonstrate
promising results, indicating that GenAI can effectively streamline the
security control development process, thus providing a robust and dynamic
safeguard for cloud-based infrastructures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models are Hidden Reasoners: Unlocking Latent Reasoning
  Capabilities via Self-Rewarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Chen, Yihao Feng, Zuxin Liu, Weiran Yao, Akshara Prabhakar, Shelby Heinecke, Ricky Ho, Phil Mui, Silvio Savarese, Caiming Xiong, Huan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown impressive capabilities, but still
struggle with complex reasoning tasks requiring multiple steps. While
prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at
inference time, optimizing reasoning capabilities during training remains
challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled
framework that formulates reasoning as sampling from a latent distribution and
optimizes it via variational approaches. LaTRO enables LLMs to concurrently
improve both their reasoning process and ability to evaluate reasoning quality,
without requiring external feedback or reward models. We validate LaTRO through
experiments on GSM8K and ARC-Challenge datasets using multiple model
architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of
12.5% over base models and 9.6% over supervised fine-tuning across
Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that
pre-trained LLMs possess latent reasoning capabilities that can be unlocked and
enhanced through our proposed optimization approach in a self-improvement
manner. The code of LaTRO is available at
\url{https://github.com/SalesforceAIResearch/LaTRO}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Synthetic Electronic Health Record (EHR) Data: A Review with
  Benchmarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingran Chen, Zhenke Wu, Xu Shi, Hyunghoon Cho, Bhramar Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We conduct a scoping review of existing approaches for synthetic EHR data
generation, and benchmark major methods with proposed open-source software to
offer recommendations for practitioners. We search three academic databases for
our scoping review. Methods are benchmarked on open-source EHR datasets,
MIMIC-III/IV. Seven existing methods covering major categories and two baseline
methods are implemented and compared. Evaluation metrics concern data fidelity,
downstream utility, privacy protection, and computational cost. 42 studies are
identified and classified into five categories. Seven open-source methods
covering all categories are selected, trained on MIMIC-III, and evaluated on
MIMIC-III or MIMIC-IV for transportability considerations. Among them,
GAN-based methods demonstrate competitive performance in fidelity and utility
on MIMIC-III; rule-based methods excel in privacy protection. Similar findings
are observed on MIMIC-IV, except that GAN-based methods further outperform the
baseline methods in preserving fidelity. A Python package, ``SynthEHRella'', is
provided to integrate various choices of approaches and evaluation metrics,
enabling more streamlined exploration and evaluation of multiple methods. We
found that method choice is governed by the relative importance of the
evaluation metrics in downstream use cases. We provide a decision tree to guide
the choice among the benchmarked methods. Based on the decision tree, GAN-based
methods excel when distributional shifts exist between the training and testing
populations. Otherwise, CorGAN and MedGAN are most suitable for association
modeling and predictive modeling, respectively. Future research should
prioritize enhancing fidelity of the synthetic data while controlling privacy
exposure, and comprehensive benchmarking of longitudinal or conditional
generation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Inference in Recurrent Explicit Duration Switching Linear
  Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikołaj Słupiński, Piotr Lipiński
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel model called Recurrent Explicit Duration
Switching Linear Dynamical Systems (REDSLDS) that incorporates recurrent
explicit duration variables into the rSLDS model. We also propose an inference
and learning scheme that involves the use of P\'olya-gamma augmentation. We
demonstrate the improved segmentation capabilities of our model on three
benchmark datasets, including two quantitative datasets and one qualitative
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Recurrent Sticky Hierarchical Dirichlet Process Hidden Markov Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikołaj Słupiński, Piotr Lipiński
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) is a natural
Bayesian nonparametric extension of the classical Hidden Markov Model for
learning from (spatio-)temporal data. A sticky HDP-HMM has been proposed to
strengthen the self-persistence probability in the HDP-HMM. Then, disentangled
sticky HDP-HMM has been proposed to disentangle the strength of the
self-persistence prior and transition prior. However, the sticky HDP-HMM
assumes that the self-persistence probability is stationary, limiting its
expressiveness. Here, we build on previous work on sticky HDP-HMM and
disentangled sticky HDP-HMM, developing a more general model: the recurrent
sticky HDP-HMM (RS-HDP-HMM). We develop a novel Gibbs sampling strategy for
efficient inference in this model. We show that RS-HDP-HMM outperforms
disentangled sticky HDP-HMM, sticky HDP-HMM, and HDP-HMM in both synthetic and
real data segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Labels in Extremes: How Well Calibrated are Extreme Multi-label
  Classifiers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasib Ullah, Erik Schultheis, Jinbin Zhang, Rohit Babbar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extreme multilabel classification (XMLC) problems occur in settings such as
related product recommendation, large-scale document tagging, or ad prediction,
and are characterized by a label space that can span millions of possible
labels. There are two implicit tasks that the classifier performs:
\emph{Evaluating} each potential label for its expected worth, and then
\emph{selecting} the best candidates. For the latter task, only the relative
order of scores matters, and this is what is captured by the standard
evaluation procedure in the XMLC literature. However, in many practical
applications, it is important to have a good estimate of the actual probability
of a label being relevant, e.g., to decide whether to pay the fee to be allowed
to display the corresponding ad. To judge whether an extreme classifier is
indeed suited to this task, one can look, for example, to whether it returns
\emph{calibrated} probabilities, which has hitherto not been done in this
field. Therefore, this paper aims to establish the current status quo of
calibration in XMLC by providing a systematic evaluation, comprising nine
models from four different model families across seven benchmark datasets. As
naive application of Expected Calibration Error (ECE) leads to meaningless
results in long-tailed XMC datasets, we instead introduce the notion of
\emph{calibration@k} (e.g., ECE@k), which focusses on the top-$k$ probability
mass, offering a more appropriate measure for evaluating probability
calibration in XMLC scenarios. While we find that different models can exhibit
widely varying reliability plots, we also show that post-training calibration
via a computationally efficient isotonic regression method enhances model
calibration without sacrificing prediction accuracy. Thus, the practitioner can
choose the model family based on accuracy considerations, and leave calibration
to isotonic regression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Discrete Event Process Simulation for Hidden Markov Models to
  Predict Competitor Time-to-Market 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nandakishore Santhi, Stephan Eidenbenz, Brian Key, George Tompkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the challenge of predicting the time at which a competitor product,
such as a novel high-capacity EV battery or a new car model, will be available
to customers; as new information is obtained, this time-to-market estimate is
revised. Our scenario is as follows: We assume that the product is under
development at a Firm B, which is a competitor to Firm A; as they are in the
same industry, Firm A has a relatively good understanding of the processes and
steps required to produce the product. While Firm B tries to keep its
activities hidden (think of stealth-mode for start-ups), Firm A is nevertheless
able to gain periodic insights by observing what type of resources Firm B is
using. We show how Firm A can build a model that predicts when Firm B will be
ready to sell its product; the model leverages knowledge of the underlying
processes and required resources to build a Parallel Discrete Simulation
(PDES)-based process model that it then uses as a generative model to train a
Hidden Markov Model (HMM). We study the question of how many resource
observations Firm A requires in order to accurately assess the current state of
development at Firm B. In order to gain general insights into the capabilities
of this approach, we study the effect of different process graph densities,
different densities of the resource-activity maps, etc., and also scaling
properties as we increase the number resource counts. We find that in most
cases, the HMM achieves a prediction accuracy of 70 to 80 percent after 20
(daily) observations of a production process that lasts 150 days on average and
we characterize the effects of different problem instance densities on this
prediction accuracy. Our results give insight into the level of market
knowledge required for accurate and early time-to-market prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph neural networks and non-commuting operators <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mauricio Velasco, Kaiying O'Hare, Bernardo Rychtenberg, Soledad Villar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) provide state-of-the-art results in a wide
variety of tasks which typically involve predicting features at the vertices of
a graph. They are built from layers of graph convolutions which serve as a
powerful inductive bias for describing the flow of information among the
vertices. Often, more than one data modality is available. This work considers
a setting in which several graphs have the same vertex set and a common
vertex-level learning task. This generalizes standard GNN models to GNNs with
several graph operators that do not commute. We may call this model graph-tuple
neural networks (GtNN).
  In this work, we develop the mathematical theory to address the stability and
transferability of GtNNs using properties of non-commuting non-expansive
operators. We develop a limit theory of graphon-tuple neural networks and use
it to prove a universal transferability theorem that guarantees that all
graph-tuple neural networks are transferable on convergent graph-tuple
sequences. In particular, there is no non-transferable energy under the
convergence we consider here. Our theoretical results extend well-known
transferability theorems for GNNs to the case of several simultaneous graphs
(GtNNs) and provide a strict improvement on what is currently known even in the
GNN case.
  We illustrate our theoretical results with simple experiments on synthetic
and real-world data. To this end, we derive a training procedure that provably
enforces the stability of the resulting model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object Recognition in Human Computer Interaction:- A Comparative
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaushik Ranade, Tanmay Khule, Riddhi More
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-computer interaction (HCI) has been a widely researched area for many
years, with continuous advancements in technology leading to the development of
new techniques that change the way we interact with computers. With the recent
advent of powerful computers, we recognize human actions and interact
accordingly, thus revolutionizing the way we interact with computers. The
purpose of this paper is to provide a comparative analysis of various
algorithms used for recognizing user faces and gestures in the context of
computer vision and HCI. This study aims to explore and evaluate the
performance of different algorithms in terms of accuracy, robustness, and
efficiency. This study aims to provide a comprehensive analysis of algorithms
for face and gesture recognition in the context of computer vision and HCI,
with the goal of improving the design and development of interactive systems
that are more intuitive, efficient, and user-friendly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSHBloom: Memory-efficient, Extreme-scale Document Deduplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arham Khan, Robert Underwood, Carlo Siebenschuh, Yadu Babuji, Aswathy Ajith, Kyle Hippe, Ozan Gokdemir, Alexander Brace, Kyle Chard, Ian Foster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deduplication is a major focus for assembling and curating training datasets
for large language models (LLM) -- detecting and eliminating additional
instances of the same content -- in large collections of technical documents.
Unrestrained, duplicates in the training dataset increase training costs and
lead to undesirable properties such as memorization in trained models or
cheating on evaluation. Contemporary approaches to document-level deduplication
are often extremely expensive in both runtime and memory. We propose LSHBloom,
an extension to MinhashLSH, which replaces the expensive LSHIndex with
lightweight Bloom filters. LSHBloom demonstrates the same deduplication
performance as MinhashLSH with only a marginal increase in false positives (as
low as 1e-5 in our experiments); demonstrates competitive runtime (270\% faster
than MinhashLSH on peS2o); and, crucially, uses just 0.6\% of the disk space
required by MinhashLSH to deduplicate peS2o. We demonstrate that this space
advantage scales with increased dataset size -- at the extreme scale of several
billion documents, LSHBloom promises a 250\% speedup and a 54$\times$ space
advantage over traditional MinHashLSH scaling deduplication of text datasets to
many billions of documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ION-C: Integration of Overlapping Networks via Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praveen Nair, Payal Bhandari, Mohammadsajad Abavisani, Sergey Plis, David Danks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many causal learning problems, variables of interest are often not all
measured over the same observations, but are instead distributed across
multiple datasets with overlapping variables. Tillman et al. (2008) presented
the first algorithm for enumerating the minimal equivalence class of
ground-truth DAGs consistent with all input graphs by exploiting local
independence relations, called ION. In this paper, this problem is formulated
as a more computationally efficient answer set programming (ASP) problem, which
we call ION-C, and solved with the ASP system clingo. The ION-C algorithm was
run on random synthetic graphs with varying sizes, densities, and degrees of
overlap between subgraphs, with overlap having the largest impact on runtime,
number of solution graphs, and agreement within the output set. To validate
ION-C on real-world data, we ran the algorithm on overlapping graphs learned
from data from two successive iterations of the European Social Survey (ESS),
using a procedure for conducting joint independence tests to prevent
inconsistencies in the input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Structure-Aware Quantum Data Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hala Hawashin, Mehrnoosh Sadrzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have advanced the field of natural
language processing (NLP), their ``black box'' nature obscures their
decision-making processes. To address this, researchers developed structured
approaches using higher order tensors. These are able to model linguistic
relations, but stall when training on classical computers due to their
excessive size. Tensors are natural inhabitants of quantum systems and training
on quantum computers provides a solution by translating text to variational
quantum circuits. In this paper, we develop MultiQ-NLP: a framework for
structure-aware data processing with multimodal text+image data. Here,
``structure'' refers to syntactic and grammatical relationships in language, as
well as the hierarchical organization of visual elements in images. We enrich
the translation with new types and type homomorphisms and develop novel
architectures to represent structure. When tested on a main stream image
classification task (SVO Probes), our best model showed a par performance with
the state of the art classical models; moreover the best model was fully
structured.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 Pages, 16 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ dsld: A Socially Relevant Tool for Teaching Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Abdullah, Arjun Ashok, Brandon Estrada, Norman Matloff, Aditya Mittal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing power of data science can play a crucial role in addressing
social discrimination, necessitating nuanced understanding and effective
mitigation strategies of potential biases. Data Science Looks At Discrimination
(dsld) is an R and Python package designed to provide users with a
comprehensive toolkit of statistical and graphical methods for assessing
possible discrimination related to protected groups, such as race, gender, and
age. Our software offers techniques for discrimination analysis by identifying
and mitigating confounding variables, along with methods for reducing bias in
predictive models.
  In educational settings, dsld offers instructors powerful tools to teach
important statistical principles through motivating real world examples of
discrimination analysis. The inclusion of an 80-page Quarto book further
supports users, from statistics educators to legal professionals, in
effectively applying these analytical tools to real world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be submitted to the Journal of Statistics and Data Science
  Education</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximate Equivariance in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jung Yeon Park, Sujay Bhatt, Sihan Zeng, Lawson L. S. Wong, Alec Koppel, Sumitra Ganesh, Robin Walters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equivariant neural networks have shown great success in reinforcement
learning, improving sample efficiency and generalization when there is symmetry
in the task. However, in many problems, only approximate symmetry is present,
which makes imposing exact symmetry inappropriate. Recently, approximately
equivariant networks have been proposed for supervised classification and
modeling physical systems. In this work, we develop approximately equivariant
algorithms in reinforcement learning (RL). We define approximately equivariant
MDPs and theoretically characterize the effect of approximate equivariance on
the optimal Q function. We propose novel RL architectures using relaxed group
convolutions and experiment on several continuous control domains and stock
trading with real financial data. Our results demonstrate that approximate
equivariance matches prior work when exact symmetries are present, and
outperforms them when domains exhibit approximate symmetry. As an added
byproduct of these techniques, we observe increased robustness to noise at test
time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WiFlexFormer: Efficient WiFi-Based Person-Centric Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Strohmayer, Matthias Wödlinger, Martin Kampel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose WiFlexFormer, a highly efficient Transformer-based architecture
designed for WiFi Channel State Information (CSI)-based person-centric sensing.
We benchmark WiFlexFormer against state-of-the-art vision and specialized
architectures for processing radio frequency data and demonstrate that it
achieves comparable Human Activity Recognition (HAR) performance while offering
a significantly lower parameter count and faster inference times. With an
inference time of just 10 ms on an Nvidia Jetson Orin Nano, WiFlexFormer is
optimized for real-time inference. Additionally, its low parameter count
contributes to improved cross-domain generalization, where it often outperforms
larger models. Our comprehensive evaluation shows that WiFlexFormer is a
potential solution for efficient, scalable WiFi-based sensing applications. The
PyTorch implementation of WiFlexFormer is publicly available at:
https://github.com/StrohmayerJ/WiFlexFormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equivariant Graph Network Approximations of High-Degree Polynomials for
  Force Field Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Xu, Haiyang Yu, Montgomery Bohde, Shuiwang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in equivariant deep models have shown promise in
accurately predicting atomic potentials and force fields in molecular dynamics
simulations. Using spherical harmonics (SH) and tensor products (TP), these
equivariant networks gain enhanced physical understanding, like symmetries and
many-body interactions. Beyond encoding physical insights, SH and TP are also
crucial to represent equivariant polynomial functions. In this work, we analyze
the equivariant polynomial functions for the equivariant architecture, and
introduce a novel equivariant network, named PACE. The proposed PACE utilizes
edge booster and the Atomic Cluster Expansion (ACE) technique to approximate a
greater number of $SE(3) \times S_n$ equivariant polynomial functions with
enhanced degrees. As experimented in commonly used benchmarks, PACE
demonstrates state-of-the-art performance in predicting atomic energy and force
fields, with robust generalization capability across various geometric
distributions under molecular dynamics (MD) across different temperature
conditions. Our code is publicly available as part of the AIRS library
https://github.com/divelab/AIRS/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum <span class="highlight-title">Diffusion</span> Models for Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruhan Wang, Ye Wang, Jing Liu, Toshiaki Koike-Akino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern quantum machine learning (QML) methods involve the variational
optimization of parameterized quantum circuits on training datasets, followed
by predictions on testing datasets. Most state-of-the-art QML algorithms
currently lack practical advantages due to their limited learning capabilities,
especially in few-shot learning tasks. In this work, we propose three new
frameworks employing quantum diffusion model (QDM) as a solution for the
few-shot learning: label-guided generation inference (LGGI); label-guided
denoising inference (LGDI); and label-guided noise addition inference (LGNAI).
Experimental results demonstrate that our proposed algorithms significantly
outperform existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiasing Synthetic Data Generated by Deep Generative Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Decruyenaere, Heidelinde Dehaene, Paloma Rabaey, Christiaan Polet, Johan Decruyenaere, Thomas Demeester, Stijn Vansteelandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While synthetic data hold great promise for privacy protection, their
statistical analysis poses significant challenges that necessitate innovative
solutions. The use of deep generative models (DGMs) for synthetic data
generation is known to induce considerable bias and imprecision into synthetic
data analyses, compromising their inferential utility as opposed to original
data analyses. This bias and uncertainty can be substantial enough to impede
statistical convergence rates, even in seemingly straightforward analyses like
mean calculation. The standard errors of such estimators then exhibit slower
shrinkage with sample size than the typical 1 over root-$n$ rate. This
complicates fundamental calculations like p-values and confidence intervals,
with no straightforward remedy currently available. In response to these
challenges, we propose a new strategy that targets synthetic data created by
DGMs for specific data analyses. Drawing insights from debiased and targeted
machine learning, our approach accounts for biases, enhances convergence rates,
and facilitates the calculation of estimators with easily approximated large
sample variances. We exemplify our proposal through a simulation study on toy
data and two case studies on real-world data, highlighting the importance of
tailoring DGMs for targeted data analysis. This debiasing strategy contributes
to advancing the reliability and applicability of synthetic data in statistical
inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024), joint first authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Mutation-Acyclicity of Quivers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kymani T. K. Armstrong-Williams, Edward Hirst, Blake Jackson, Kyu-Hwan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) has emerged as a powerful tool in mathematical research
in recent years. This paper applies ML techniques to the study of quivers--a
type of directed multigraph with significant relevance in algebra,
combinatorics, computer science, and mathematical physics. Specifically, we
focus on the challenging problem of determining the mutation-acyclicity of a
quiver on 4 vertices, a property that is pivotal since mutation-acyclicity is
often a necessary condition for theorems involving path algebras and cluster
algebras. Although this classification is known for quivers with at most 3
vertices, little is known about quivers on more than 3 vertices. We give a
computer-assisted proof of a theorem to prove that mutation-acyclicity is
decidable for quivers on 4 vertices with edge weight at most 2. By leveraging
neural networks (NNs) and support vector machines (SVMs), we then accurately
classify more general 4-vertex quivers as mutation-acyclic or
non-mutation-acyclic. Our results demonstrate that ML models can efficiently
detect mutation-acyclicity, providing a promising computational approach to
this combinatorial problem, from which the trained SVM equation provides a
starting point to guide future theoretical development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 14 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable DP-SGD: Shuffling vs. Poisson Subsampling <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chiyuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide new lower bounds on the privacy guarantee of the multi-epoch
Adaptive Batch Linear Queries (ABLQ) mechanism with shuffled batch sampling,
demonstrating substantial gaps when compared to Poisson subsampling; prior
analysis was limited to a single epoch. Since the privacy analysis of
Differentially Private Stochastic Gradient Descent (DP-SGD) is obtained by
analyzing the ABLQ mechanism, this brings into serious question the common
practice of implementing shuffling-based DP-SGD, but reporting privacy
parameters as if Poisson subsampling was used. To understand the impact of this
gap on the utility of trained machine learning models, we introduce a practical
approach to implement Poisson subsampling at scale using massively parallel
computation, and efficiently train models with the same. We compare the utility
of models trained with Poisson-subsampling-based DP-SGD, and the optimistic
estimates of utility when using shuffling, via our new lower bounds on the
privacy guarantee of ABLQ with shuffling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Budgeted Matching with General Bids <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyi Yang, Pengfei Li, Adam Wierman, Shaolei Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online Budgeted Matching (OBM) is a classic problem with important
applications in online advertising, online service matching, revenue
management, and beyond. Traditional online algorithms typically assume a small
bid setting, where the maximum bid-to-budget ratio (\kappa) is infinitesimally
small. While recent algorithms have tried to address scenarios with non-small
or general bids, they often rely on the Fractional Last Matching (FLM)
assumption, which allows for accepting partial bids when the remaining budget
is insufficient. This assumption, however, does not hold for many applications
with indivisible bids. In this paper, we remove the FLM assumption and tackle
the open problem of OBM with general bids. We first establish an upper bound of
1-\kappa on the competitive ratio for any deterministic online algorithm. We
then propose a novel meta algorithm, called MetaAd, which reduces to different
algorithms with first known provable competitive ratios parameterized by the
maximum bid-to-budget ratio \kappa \in [0, 1]. As a by-product, we extend
MetaAd to the FLM setting and get provable competitive algorithms. Finally, we
apply our competitive analysis to the design learning-augmented algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bio-xLSTM: Generative modeling, representation and in-context learning
  of biological and chemical sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Schmidinger, Lisa Schneckenreiter, Philipp Seidl, Johannes Schimunek, Pieter-Jan Hoedt, Johannes Brandstetter, Andreas Mayr, Sohvi Luukkonen, Sepp Hochreiter, Günter Klambauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models for biological and chemical sequences enable crucial
applications such as drug discovery, protein engineering, and precision
medicine. Currently, these language models are predominantly based on
Transformer architectures. While Transformers have yielded impressive results,
their quadratic runtime dependency on the sequence length complicates their use
for long genomic sequences and in-context learning on proteins and chemical
sequences. Recently, the recurrent xLSTM architecture has been shown to perform
favorably compared to Transformers and modern state-space model (SSM)
architectures in the natural language domain. Similar to SSMs, xLSTMs have a
linear runtime dependency on the sequence length and allow for constant-memory
decoding at inference time, which makes them prime candidates for modeling
long-range dependencies in biological and chemical sequences. In this work, we
tailor xLSTM towards these domains and propose a suite of architectural
variants called Bio-xLSTM. Extensive experiments in three large domains,
genomics, proteins, and chemistry, were performed to assess xLSTM's ability to
model biological and chemical sequences. The results show that models based on
Bio-xLSTM a) can serve as proficient generative models for DNA, protein, and
chemical sequences, b) learn rich representations for those modalities, and c)
can perform in-context learning for proteins and small molecules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen
  Representations <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walter Simoncini, Spyros Gidaris, Andrei Bursuc, Yuki M. Asano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces FUNGI, Features from UNsupervised GradIents, a method
to enhance the features of transformer encoders by leveraging self-supervised
gradients. Our method is simple: given any pretrained model, we first compute
gradients from various self-supervised objectives for each input. These
gradients are projected to a lower dimension and then concatenated with the
model's output embedding. The resulting features are evaluated on k-nearest
neighbor classification over 11 datasets from vision, 5 from natural language
processing, and 2 from audio. Across backbones spanning various sizes and
pretraining strategies, FUNGI features provide consistent performance
improvements over the embeddings. We also show that using FUNGI features can
benefit linear classification, clustering and image retrieval, and that they
significantly improve the retrieval-based in-context scene understanding
abilities of pretrained models, for example improving upon DINO by +17% for
semantic segmentation - without any training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Code available at
  https://github.com/WalterSimoncini/fungivision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Federated Learning to Quantum Federated Learning for
  Space-Air-Ground Integrated Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vu Khanh Quy, Nguyen Minh Quy, Tran Thi Hoai, Shaba Shaon, Md Raihan Uddin, Tien Nguyen, Dinh C. Nguyen, Aryan Kaushik, Periklis Chatzimisios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  6G wireless networks are expected to provide seamless and data-based
connections that cover space-air-ground and underwater networks. As a core
partition of future 6G networks, Space-Air-Ground Integrated Networks (SAGIN)
have been envisioned to provide countless real-time intelligent applications.
To realize this, promoting AI techniques into SAGIN is an inevitable trend. Due
to the distributed and heterogeneous architecture of SAGIN, federated learning
(FL) and then quantum FL are emerging AI model training techniques for enabling
future privacy-enhanced and computation-efficient SAGINs. In this work, we
explore the vision of using FL/QFL in SAGINs. We present a few representative
applications enabled by the integration of FL and QFL in SAGINs. A case study
of QFL over UAV networks is also given, showing the merit of quantum-enabled
training approach over the conventional FL benchmark. Research challenges along
with standardization for QFL adoption in future SAGINs are also highlighted.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by IEEE Conference on Standards for
  Communications and Networking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ False Data Injection Attack Detection in Edge-based Smart Metering
  Networks with Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Raihan Uddin, Ratun Rahman, Dinh C. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smart metering networks are increasingly susceptible to cyber threats, where
false data injection (FDI) appears as a critical attack. Data-driven-based
machine learning (ML) methods have shown immense benefits in detecting FDI
attacks via data learning and prediction abilities. Literature works have
mostly focused on centralized learning and deploying FDI attack detection
models at the control center, which requires data collection from local
utilities like meters and transformers. However, this data sharing may raise
privacy concerns due to the potential disclosure of household information like
energy usage patterns. This paper proposes a new privacy-preserved FDI attack
detection by developing an efficient federated learning (FL) framework in the
smart meter network with edge computing. Distributed edge servers located at
the network edge run an ML-based FDI attack detection model and share the
trained model with the grid operator, aiming to build a strong FDI attack
detection model without data sharing. Simulation results demonstrate the
efficiency of our proposed FL method over the conventional method without
collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by IEEE Consumer Communications &
  Networking Conference (CCNC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeNetDM: Debiasing by Network Depth Modulation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19863v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19863v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silpa Vadakkeeveetil Sreelatha, Adarsh Kappiyath, Abhra Chaudhuri, Anjan Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks trained on biased datasets tend to inadvertently learn
spurious correlations, hindering generalization. We formally prove that (1)
samples that exhibit spurious correlations lie on a lower rank manifold
relative to the ones that do not; and (2) the depth of a network acts as an
implicit regularizer on the rank of the attribute subspace that is encoded in
its representations. Leveraging these insights, we present DeNetDM, a novel
debiasing method that uses network depth modulation as a way of developing
robustness to spurious correlations. Using a training paradigm derived from
Product of Experts, we create both biased and debiased branches with deep and
shallow architectures and then distill knowledge to produce the target debiased
model. Our method requires no bias annotations or explicit data augmentation
while performing on par with approaches that require either or both. We
demonstrate that DeNetDM outperforms existing debiasing techniques on both
synthetic and real-world datasets by 5\%. The project page is available at
https://vssilpa.github.io/denetdm/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version : NeurIPS 2024, * indicates these authors
  contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynCode: LLM Generation with Grammar Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01632v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01632v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, Gagandeep Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are widely used in complex AI applications. These applications
underscore the need for LLM outputs to adhere to a specific format, for their
integration with other components in the systems. Typically the format rules
e.g., for data serialization formats such as JSON, YAML, or Code in Programming
Language are expressed as context-free grammar (CFG). Due to the hallucinations
and unreliability of LLMs, instructing LLMs to adhere to specified syntax
becomes an increasingly important challenge.
  We present SynCode, a novel framework for efficient and general syntactical
decoding with LLMs, to address this challenge. SynCode ensures soundness and
completeness with respect to the CFG of a formal language, effectively
retaining valid tokens while filtering out invalid ones. SynCode uses an
offline-constructed, efficient lookup table, the DFA mask store, derived from
the DFA of the language's grammar for efficient generation. SynCode seamlessly
integrates with any language defined by CFG, as evidenced by experiments
focusing on generating JSON, Python, and Go outputs. Our experiments evaluating
the effectiveness of SynCode for JSON generation demonstrate that SynCode
eliminates all syntax errors and significantly outperforms state-of-the-art
baselines. Furthermore, our results underscore how SynCode significantly
reduces 96.07% of syntax errors in generated Python and Go code, showcasing its
substantial impact on enhancing syntactical precision in LLM generation. Our
code is available at https://github.com/uiuc-focal-lab/syncode
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Token Generation in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivi Chatzi, Nina Corvelo Benz, Eleni Straitouri, Stratis Tsirtsis, Manuel Gomez-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  "Sure, I am happy to generate a story for you: Captain Lyra stood at the helm
of her trusty ship, the Maelstrom's Fury, gazing out at the endless sea. [...]
Lyra's eyes welled up with tears as she realized the bitter truth - she had
sacrificed everything for fleeting riches, and lost the love of her crew, her
family, and herself." Although this story, generated by a large language model,
is captivating, one may wonder -- how would the story have unfolded if the
model had chosen "Captain Maeve" as the protagonist instead? We cannot know.
State-of-the-art large language models are stateless -- they maintain no
internal memory or state. Given a prompt, they generate a sequence of tokens as
an output using an autoregressive process. As a consequence, they cannot reason
about counterfactual alternatives to tokens they have generated in the past. In
this work, our goal is to enhance them with this functionality. To this end, we
develop a causal model of token generation that builds upon the Gumbel-Max
structural causal model. Our model allows any large language model to perform
counterfactual token generation at almost no cost in comparison with vanilla
token generation, it is embarrassingly simple to implement, and it does not
require any fine-tuning nor prompt engineering. We implement our model on Llama
3 8B-Instruct and Ministral-8B-Instruct and conduct a qualitative and a
quantitative analysis of counterfactually generated text. We conclude with a
demonstrative application of counterfactual token generation for bias
detection, unveiling interesting insights about the model of the world
constructed by large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Probabilistic Perspective on Unlearning and Alignment for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03523v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03523v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Scholten, Stephan Günnemann, Leo Schwinn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comprehensive evaluation of Large Language Models (LLMs) is an open research
problem. Existing evaluations rely on deterministic point estimates generated
via greedy decoding. However, we find that deterministic evaluations fail to
capture the whole output distribution of a model, yielding inaccurate
estimations of model capabilities. This is particularly problematic in critical
contexts such as unlearning and alignment, where precise model evaluations are
crucial. To remedy this, we introduce the first formal probabilistic evaluation
framework in LLMs. Namely, we derive novel metrics with high-probability
guarantees concerning the output distribution of a model. Our metrics are
application-independent and allow practitioners to make more reliable estimates
about model capabilities before deployment. Through a case study focused on
unlearning, we reveal that deterministic evaluations falsely indicate
successful unlearning, whereas our probabilistic evaluations demonstrate that
most if not all of the supposedly unlearned information remains accessible in
these models. Additionally, we propose a novel unlearning loss based on entropy
optimization and adaptive temperature scaling, which significantly improves
unlearning in probabilistic settings on recent benchmarks. Our proposed shift
from point estimates to probabilistic evaluations of output distributions
represents an important step toward comprehensive evaluations of LLMs. Code
available at https://github.com/yascho/probabilistic-unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating Extremes: Dynamic Sparsity in Large Output Space <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasib Ullah, Erik Schultheis, Mike Lasby, Yani Ioannou, Rohit Babbar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Dynamic Sparse Training (DST) has emerged as an alternative
to post-training pruning for generating efficient models. In principle, DST
allows for a more memory efficient training process, as it maintains sparsity
throughout the entire training run. However, current DST implementations fail
to capitalize on this in practice. Because sparse matrix multiplication is much
less efficient than dense matrix multiplication on GPUs, most implementations
simulate sparsity by masking weights. In this paper, we leverage recent
advances in semi-structured sparse training to apply DST in the domain of
classification with large output spaces, where memory-efficiency is paramount.
With a label space of possibly millions of candidates, the classification layer
alone will consume several gigabytes of memory. Switching from a dense to a
fixed fan-in sparse layer updated with sparse evolutionary training (SET);
however, severely hampers training convergence, especially at the largest label
spaces. We find that poor gradient flow from the sparse classifier to the dense
text encoder make it difficult to learn good input representations. By
employing an intermediate layer or adding an auxiliary training objective, we
recover most of the generalisation performance of the dense model. Overall, we
demonstrate the applicability and practical benefits of DST in a challenging
domain -- characterized by a highly skewed label distribution that differs
substantially from typical DST benchmark datasets -- which enables end-to-end
training with millions of labels on commodity hardware.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cartan moving frames and the data manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliot Tron, Rita Fioresi, Nicolas Couellan, Stéphane Puechmorel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The purpose of this paper is to employ the language of Cartan moving frames
to study the geometry of the data manifolds and its Riemannian structure, via
the data information metric and its curvature at data points. Using this
framework and through experiments, explanations on the response of a neural
network are given by pointing out the output classes that are easily reachable
from a given input. This emphasizes how the proposed mathematical relationship
between the output of the network and the geometry of its inputs can be
exploited as an explainable artificial intelligence tool.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ bit2bit: 1-bit quanta video reconstruction via self-supervised photon
  prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yehe Liu, Alexander Krull, Hector Basevi, Ales Leonardis, Michael W. Jenkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quanta image sensors, such as SPAD arrays, are an emerging sensor technology,
producing 1-bit arrays representing photon detection events over exposures as
short as a few nanoseconds. In practice, raw data are post-processed using
heavy spatiotemporal binning to create more useful and interpretable images at
the cost of degrading spatiotemporal resolution. In this work, we propose
bit2bit, a new method for reconstructing high-quality image stacks at the
original spatiotemporal resolution from sparse binary quanta image data.
Inspired by recent work on Poisson denoising, we developed an algorithm that
creates a dense image sequence from sparse binary photon data by predicting the
photon arrival location probability distribution. However, due to the binary
nature of the data, we show that the assumption of a Poisson distribution is
inadequate. Instead, we model the process with a Bernoulli lattice process from
the truncated Poisson. This leads to the proposal of a novel self-supervised
solution based on a masked loss function. We evaluate our method using both
simulated and real data. On simulated data from a conventional video, we
achieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06
photons per pixel per frame). We also present a novel dataset containing a wide
range of real SPAD high-speed videos under various challenging imaging
conditions. The scenes cover strong/weak ambient light, strong motion,
ultra-fast events, etc., which will be made available to the community, on
which we demonstrate the promise of our approach. Both reconstruction quality
and throughput substantially surpass the state-of-the-art methods (e.g., Quanta
Burst Photography (QBP)). Our approach significantly enhances the visualization
and usability of the data, enabling the application of existing analysis
techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DMPlug: A Plug-in Method for Solving Inverse Problems with <span class="highlight-title">Diffusion</span>
  Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengkang Wang, Xu Zhang, Taihui Li, Yuxiang Wan, Tiancong Chen, Ju Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained diffusion models (DMs) have recently been popularly used in
solving inverse problems (IPs). The existing methods mostly interleave
iterative steps in the reverse diffusion process and iterative steps to bring
the iterates closer to satisfying the measurement constraint. However, such
interleaving methods struggle to produce final results that look like natural
objects of interest (i.e., manifold feasibility) and fit the measurement (i.e.,
measurement feasibility), especially for nonlinear IPs. Moreover, their
capabilities to deal with noisy IPs with unknown types and levels of
measurement noise are unknown. In this paper, we advocate viewing the reverse
process in DMs as a function and propose a novel plug-in method for solving IPs
using pretrained DMs, dubbed DMPlug. DMPlug addresses the issues of manifold
feasibility and measurement feasibility in a principled manner, and also shows
great potential for being robust to unknown types and levels of noise. Through
extensive experiments across various IP tasks, including two linear and three
nonlinear IPs, we demonstrate that DMPlug consistently outperforms
state-of-the-art methods, often by large margins especially for nonlinear IPs.
The code is available at https://github.com/sun-umn/DMPlug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024
  (https://openreview.net/forum?id=81IFFsfQUj)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Landscape for Generative Sequence Models for Specialized
  Data Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Zbeeb, Mohammad Ghorayeb, Mariam Salman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) research often aims to develop models that can
generalize reliably across complex datasets, yet this remains challenging in
fields where data is scarce, intricate, or inaccessible. This paper introduces
a novel approach that leverages three generative models of varying complexity
to synthesize one of the most demanding structured datasets: Malicious Network
Traffic. Our approach uniquely transforms numerical data into text, re-framing
data generation as a language modeling task, which not only enhances data
regularization but also significantly improves generalization and the quality
of the synthetic data. Extensive statistical analyses demonstrate that our
method surpasses state-of-the-art generative models in producing high-fidelity
synthetic data. Additionally, we conduct a comprehensive study on synthetic
data applications, effectiveness, and evaluation strategies, offering valuable
insights into its role across various domains. Our code and pre-trained models
are openly accessible at Github, enabling further exploration and application
of our methodology. Index Terms: Data synthesis, machine learning, traffic
generation, privacy preserving data, generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 7 figures, 3 tables, 1 algorithm. code @
  https://github.com/Moe-Zbeeb/Exploring-the-landscape-for-generative-models-for-specialized-data-generation.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DexDiffuser: Generating Dexterous Grasps with <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02989v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02989v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehang Weng, Haofei Lu, Danica Kragic, Jens Lundell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DexDiffuser, a novel dexterous grasping method that generates,
evaluates, and refines grasps on partial object point clouds. DexDiffuser
includes the conditional diffusion-based grasp sampler DexSampler and the
dexterous grasp evaluator DexEvaluator. DexSampler generates high-quality
grasps conditioned on object point clouds by iterative denoising of randomly
sampled grasps. We also introduce two grasp refinement strategies:
Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR).
The experiment results demonstrate that DexDiffuser consistently outperforms
the state-of-the-art multi-finger grasp generation method FFHNet with an, on
average, 9.12% and 19.44% higher grasp success rate in simulation and real
robot experiments, respectively. Supplementary materials are available at
https://yulihn.github.io/DexDiffuser_page/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Context-Aware Preference Modeling for Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silviu Pitis, Ziang Xiao, Nicolas Le Roux, Alessandro Sordoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While finetuning language models from pairwise preferences has proven
remarkably effective, the underspecified nature of natural language presents
critical challenges. Direct preference feedback is uninterpretable, difficult
to provide where multidimensional criteria may apply, and often inconsistent,
either because it is based on incomplete instructions or provided by diverse
principals. To address these challenges, we consider the two-step preference
modeling procedure that first resolves the under-specification by selecting a
context, and then evaluates preference with respect to the chosen context. We
decompose reward modeling error according to these two steps, which suggests
that supervising context in addition to context-specific preference may be a
viable approach to aligning models with diverse human preferences. For this to
work, the ability of models to evaluate context-specific preference is
critical. To this end, we contribute context-conditioned preference datasets
and accompanying experiments that investigate the ability of language models to
evaluate context-specific preference. We use our datasets to (1) show that
existing preference models benefit from, but fail to fully consider, added
context, (2) finetune a context-aware reward model with context-specific
performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3)
investigate the value of context-aware preference modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. 10 pages (29 with references and appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SHyPar: A Spectral Coarsening Approach to Hypergraph Partitioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Sajadinia, Ali Aghdaei, Zhuo Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art hypergraph partitioners utilize a multilevel paradigm to
construct progressively coarser hypergraphs across multiple layers, guiding cut
refinements at each level of the hierarchy. Traditionally, these partitioners
employ heuristic methods for coarsening and do not consider the structural
features of hypergraphs. In this work, we introduce a multilevel spectral
framework, SHyPar, for partitioning large-scale hypergraphs by leveraging
hyperedge effective resistances and flow-based community detection techniques.
Inspired by the latest theoretical spectral clustering frameworks, such as
HyperEF and HyperSF, SHyPar aims to decompose large hypergraphs into multiple
subgraphs with few inter-partition hyperedges (cut size). A key component of
SHyPar is a flow-based local clustering scheme for hypergraph coarsening, which
incorporates a max-flow-based algorithm to produce clusters with substantially
improved conductance. Additionally, SHyPar utilizes an effective
resistance-based rating function for merging nodes that are strongly connected
(coupled). Compared with existing state-of-the-art hypergraph partitioning
methods, our extensive experimental results on real-world VLSI designs
demonstrate that SHyPar can more effectively partition hypergraphs, achieving
state-of-the-art solution quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Compute Gröbner Bases <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12904v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12904v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroshi Kera, Yuki Ishihara, Yuta Kambe, Tristan Vaccon, Kazuhiro Yokoyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving a polynomial system, or computing an associated Gr\"obner basis, has
been a fundamental task in computational algebra. However, it is also known for
its notorious doubly exponential time complexity in the number of variables in
the worst case. This paper is the first to address the learning of Gr\"obner
basis computation with Transformers. The training requires many pairs of a
polynomial system and the associated Gr\"obner basis, raising two novel
algebraic problems: random generation of Gr\"obner bases and transforming them
into non-Gr\"obner ones, termed as backward Gr\"obner problem. We resolve these
problems with 0-dimensional radical ideals, the ideals appearing in various
applications. Further, we propose a hybrid input embedding to handle
coefficient tokens with continuity bias and avoid the growth of the vocabulary
set. The experiments show that our dataset generation method is a few orders of
magnitude faster than a naive approach, overcoming a crucial challenge in
learning to compute Gr\"obner bases, and Gr\"obner computation is learnable in
a particular class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Table<span class="highlight-title">GPT</span>2: A Large Multimodal Model with Tabular Data Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aofeng Su, Aowen Wang, Chao Ye, Chen Zhou, Ga Zhang, Guangcheng Zhu, Haobo Wang, Haokai Xu, Hao Chen, Haoze Li, Haoxuan Lan, Jiaming Tian, Jing Yuan, Junbo Zhao, Junlin Zhou, Kaizhe Shou, Liangyu Zha, Lin Long, Liyao Li, Pengzuo Wu, Qi Zhang, Qingyi Huang, Saisai Yang, Tao Zhang, Wentao Ye, Wufang Zhu, Xiaomeng Hu, Xijun Gu, Xinjie Sun, Xiang Li, Yuhang Yang, Zhiqing Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI
applications, presenting vast new opportunities across industries. Yet, the
integration of tabular data remains notably underdeveloped, despite its
foundational role in numerous real-world domains.
  This gap is critical for three main reasons. First, database or data
warehouse data integration is essential for advanced applications; second, the
vast and largely untapped resource of tabular data offers immense potential for
analysis; and third, the business intelligence domain specifically demands
adaptable, precise solutions that many current LLMs may struggle to provide.
  In response, we introduce TableGPT2, a model rigorously pre-trained and
fine-tuned with over 593.8K tables and 2.36M high-quality query-table-output
tuples, a scale of table-related data unprecedented in prior research. This
extensive training enables TableGPT2 to excel in table-centric tasks while
maintaining strong general language and coding abilities.
  One of TableGPT2's key innovations is its novel table encoder, specifically
designed to capture schema-level and cell-level information. This encoder
strengthens the model's ability to handle ambiguous queries, missing column
names, and irregular tables commonly encountered in real-world applications.
Similar to visual language models, this pioneering approach integrates with the
decoder to form a robust large multimodal model.
  We believe the results are compelling: over 23 benchmarking metrics,
TableGPT2 achieves an average performance improvement of 35.20% in the 7B model
and 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust
general-purpose capabilities intact.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topology-guided Hypergraph <span class="highlight-title">Transformer</span> Network: Unveiling Structural
  Insights for Improved Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09657v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09657v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaled Mohammed Saifuddin, Mehmet Emin Aktas, Esra Akbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypergraphs, with their capacity to depict high-order relationships, have
emerged as a significant extension of traditional graphs. Although Graph Neural
Networks (GNNs) have remarkable performance in graph representation learning,
their extension to hypergraphs encounters challenges due to their intricate
structures. Furthermore, current hypergraph transformers, a special variant of
GNN, utilize semantic feature-based self-attention, ignoring topological
attributes of nodes and hyperedges. To address these challenges, we propose a
Topology-guided Hypergraph Transformer Network (THTN). In this model, we first
formulate a hypergraph from a graph while retaining its structural essence to
learn higher-order relations within the graph. Then, we design a simple yet
effective structural and spatial encoding module to incorporate the topological
and spatial information of the nodes into their representation. Further, we
present a structure-aware self-attention mechanism that discovers the important
nodes and hyperedges from both semantic and structural viewpoints. By
leveraging these two modules, THTN crafts an improved node representation,
capturing both local and global topological expressions. Extensive experiments
conducted on node classification tasks demonstrate that the performance of the
proposed model consistently exceeds that of the existing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lecture Notes on Linear Neural Networks: A Tale of Optimization and
  Generalization in Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Cohen, Noam Razin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  These notes are based on a lecture delivered by NC on March 2021, as part of
an advanced course in Princeton University on the mathematical understanding of
deep learning. They present a theory (developed by NC, NR and collaborators) of
linear neural networks -- a fundamental model in the study of optimization and
generalization in deep learning. Practical applications born from the presented
theory are also discussed. The theory is based on mathematical tools that are
dynamical in nature. It showcases the potential of such tools to push the
envelope of our understanding of optimization and generalization in deep
learning. The text assumes familiarity with the basics of statistical learning
theory. Exercises (without solutions) are included.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Lecture notes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversity Progress for Goal Selection in Discriminability-Motivated RL <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik M. Lintunen, Nadia M. Ady, Christian Guckelsberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-uniform goal selection has the potential to improve the reinforcement
learning (RL) of skills over uniform-random selection. In this paper, we
introduce a method for learning a goal-selection policy in
intrinsically-motivated goal-conditioned RL: "Diversity Progress" (DP). The
learner forms a curriculum based on observed improvement in discriminability
over its set of goals. Our proposed method is applicable to the class of
discriminability-motivated agents, where the intrinsic reward is computed as a
function of the agent's certainty of following the true goal being pursued.
This reward can motivate the agent to learn a set of diverse skills without
extrinsic rewards. We demonstrate empirically that a DP-motivated agent can
learn a set of distinguishable skills faster than previous approaches, and do
so without suffering from a collapse of the goal distribution -- a known issue
with some prior approaches. We end with plans to take this proof-of-concept
forward.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages including appendices, full-track paper at the Intrinsically
  Motivated Open-ended Learning workshop at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Aware Matrix Completion via Convexized $\ell_0$-Norm
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niclas Führling, Kengo Ando, Giuseppe Thadeu Freitas de Abreu, David González G., Osvaldo Gonsa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a novel algorithm, for the completion of partially observed
low-rank matrices in a structured setting where each entry can be chosen from a
finite discrete alphabet set, such as in common recommender systems. The
proposed low-rank matrix completion (MC) method is an improved variation of
state-of-the-art (SotA) discrete aware matrix completion method which we
previously proposed, in which discreteness is enforced by an $\ell_0$-norm
regularizer, not by replaced with the $\ell_1$-norm, but instead approximated
by a continuous and differentiable function normalized via fractional
programming (FP) under a proximal gradient (PG) framework. Simulation results
demonstrate the superior performance of the new method compared to the SotA
techniques as well as the earlier $\ell_1$-norm-based discrete-aware matrix
completion approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the IEEE 2024 Asilomar Conference on Signals, Systems,
  and Computers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific
  Energy-Based Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16118v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16118v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrei Margeloiu, Xiangjian Jiang, Nikola Simidjievski, Mateja Jamnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data collection is often difficult in critical fields such as medicine,
physics, and chemistry. As a result, classification methods usually perform
poorly with these small datasets, leading to weak predictive performance.
Increasing the training set with additional synthetic data, similar to data
augmentation in images, is commonly believed to improve downstream
classification performance. However, current tabular generative methods that
learn either the joint distribution $ p(\mathbf{x}, y) $ or the
class-conditional distribution $ p(\mathbf{x} \mid y) $ often overfit on small
datasets, resulting in poor-quality synthetic data, usually worsening
classification performance compared to using real data alone. To solve these
challenges, we introduce TabEBM, a novel class-conditional generative method
using Energy-Based Models (EBMs). Unlike existing methods that use a shared
model to approximate all class-conditional densities, our key innovation is to
create distinct EBM generative models for each class, each modelling its
class-specific data distribution individually. This approach creates robust
energy landscapes, even in ambiguous class distributions. Our experiments show
that TabEBM generates synthetic data with higher quality and better statistical
fidelity than existing methods. When used for data augmentation, our synthetic
data consistently improves the classification performance across diverse
datasets of various sizes, especially small ones. Code is available at
https://github.com/andreimargeloiu/TabEBM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the Thirty-Eighth Annual Conference on Neural Information
  Processing Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applying Guidance in a Limited Interval Improves Sample and Distribution
  Quality in <span class="highlight-title">Diffusion</span> Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, Jaakko Lehtinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Guidance is a crucial technique for extracting the best performance out of
image-generating diffusion models. Traditionally, a constant guidance weight
has been applied throughout the sampling chain of an image. We show that
guidance is clearly harmful toward the beginning of the chain (high noise
levels), largely unnecessary toward the end (low noise levels), and only
beneficial in the middle. We thus restrict it to a specific range of noise
levels, improving both the inference speed and result quality. This limited
guidance interval improves the record FID in ImageNet-512 significantly, from
1.81 to 1.40. We show that it is quantitatively and qualitatively beneficial
across different sampler parameters, network architectures, and datasets,
including the large-scale setting of Stable Diffusion XL. We thus suggest
exposing the guidance interval as a hyperparameter in all diffusion models that
use guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Mirror Descent with Lookahead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14156v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14156v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kimon Protopapas, Anas Barakat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy Mirror Descent (PMD) stands as a versatile algorithmic framework
encompassing several seminal policy gradient algorithms such as natural policy
gradient, with connections with state-of-the-art reinforcement learning (RL)
algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration
algorithm implementing regularized 1-step greedy policy improvement. However,
1-step greedy policies might not be the best choice and recent remarkable
empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that
greedy approaches with respect to multiple steps outperform their 1-step
counterpart. In this work, we propose a new class of PMD algorithms called
$h$-PMD which incorporates multi-step greedy policy improvement with lookahead
depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov
Decision Processes with discount factor $\gamma$, we show that $h$-PMD which
generalizes the standard PMD enjoys a faster dimension-free $\gamma^h$-linear
convergence rate, contingent on the computation of multi-step greedy policies.
We propose an inexact version of $h$-PMD where lookahead action values are
estimated. Under a generative model, we establish a sample complexity for
$h$-PMD which improves over prior work. Finally, we extend our result to linear
function approximation to scale to large state spaces. Under suitable
assumptions, our sample complexity only involves dependence on the dimension of
the feature map space instead of the state space size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous Management of Machine Learning-Based Application Behavior <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12686v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12686v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Anisetti, Claudio A. Ardagna, Nicola Bena, Ernesto Damiani, Paolo G. Panero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern applications are increasingly driven by Machine Learning (ML) models
whose non-deterministic behavior is affecting the entire application life cycle
from design to operation. The pervasive adoption of ML is urgently calling for
approaches that guarantee a stable non-functional behavior of ML-based
applications over time and across model changes. To this aim, non-functional
properties of ML models, such as privacy, confidentiality, fairness, and
explainability, must be monitored, verified, and maintained. Existing
approaches mostly focus on i) implementing solutions for classifier selection
according to the functional behavior of ML models, ii) finding new algorithmic
solutions, such as continuous re-training. In this paper, we propose a
multi-model approach that aims to guarantee a stable non-functional behavior of
ML-based applications. An architectural and methodological approach is provided
to compare multiple ML models showing similar non-functional properties and
select the model supporting stable non-functional behavior over time according
to (dynamic and unpredictable) contextual changes. Our approach goes beyond the
state of the art by providing a solution that continuously guarantees a stable
non-functional behavior of ML-based applications, is ML algorithm-agnostic, and
is driven by non-functional properties assessed on the ML models themselves. It
consists of a two-step process working during application operation, where
model assessment verifies non-functional properties of ML models trained and
selected at development time, and model substitution guarantees continuous and
stable support of non-functional properties. We experimentally evaluate our
solution in a real-world scenario focusing on non-functional property fairness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Transactions on Services Computing;
  DOI: 10.1109/TSC.2024.3486226</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speaker Emotion Recognition: Leveraging Self-Supervised Models for
  Feature Extraction Using Wav2Vec2 and HuBERT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pourya Jafarzadeh, Amir Mohammad Rostami, Padideh Choobdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech is the most natural way of expressing ourselves as humans. Identifying
emotion from speech is a nontrivial task due to the ambiguous definition of
emotion itself. Speaker Emotion Recognition (SER) is essential for
understanding human emotional behavior. The SER task is challenging due to the
variety of speakers, background noise, complexity of emotions, and speaking
styles. It has many applications in education, healthcare, customer service,
and Human-Computer Interaction (HCI). Previously, conventional machine learning
methods such as SVM, HMM, and KNN have been used for the SER task. In recent
years, deep learning methods have become popular, with convolutional neural
networks and recurrent neural networks being used for SER tasks. The input of
these methods is mostly spectrograms and hand-crafted features. In this work,
we study the use of self-supervised transformer-based models, Wav2Vec2 and
HuBERT, to determine the emotion of speakers from their voice. The models
automatically extract features from raw audio signals, which are then used for
the classification task. The proposed solution is evaluated on reputable
datasets, including RAVDESS, SHEMO, SAVEE, AESDD, and Emo-DB. The results show
the effectiveness of the proposed method on different datasets. Moreover, the
model has been used for real-world applications like call center conversations,
and the results demonstrate that the model accurately predicts emotions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open Problem: Active Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikola Milosevic, Gesine Müller, Jan Huisken, Nico Scherf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce the concept of Active Representation Learning, a
novel class of problems that intertwines exploration and representation
learning within partially observable environments. We extend ideas from Active
Simultaneous Localization and Mapping (active SLAM), and translate them to
scientific discovery problems, exemplified by adaptive microscopy. We explore
the need for a framework that derives exploration skills from representations
that are in some sense actionable, aiming to enhance the efficiency and
effectiveness of data collection and model building in the natural sciences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Root Cause Analysis of Outliers with Missing Structural Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nastaran Okati, Sergio Hernan Garrido Mejia, William Roy Orchard, Patrick Blöbaum, Dominik Janzing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work conceptualized root cause analysis (RCA) of anomalies via
quantitative contribution analysis using causal counterfactuals in structural
causal models (SCMs).The framework comes with three practical challenges: (1)
it requires the causal directed acyclic graph (DAG), together with an SCM, (2)
it is statistically ill-posed since it probes regression models in regions of
low probability density, (3) it relies on Shapley values which are
computationally expensive to find.
  In this paper, we propose simplified, efficient methods of root cause
analysis when the task is to identify a unique root cause instead of
quantitative contribution analysis. Our proposed methods run in linear order of
SCM nodes and they require only the causal DAG without counterfactuals.
Furthermore, for those use cases where the causal DAG is unknown, we justify
the heuristic of identifying root causes as the variables with the highest
anomaly score. To this end, we prove that anomalies with small scores are
unlikely to cause those with large scores and show upper bounds for the
likelihood of causal pathways with non-monotonic anomaly scores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Copyright-Aware Incentive Scheme for Generative Art Models Using
  Hierarchical Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuan Shi, Yifei Song, Xiaoli Tang, Lingjuan Lyu, Boi Faltings
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative art using Diffusion models has achieved remarkable performance in
image generation and text-to-image tasks. However, the increasing demand for
training data in generative art raises significant concerns about copyright
infringement, as models can produce images highly similar to copyrighted works.
Existing solutions attempt to mitigate this by perturbing Diffusion models to
reduce the likelihood of generating such images, but this often compromises
model performance. Another approach focuses on economically compensating data
holders for their contributions, yet it fails to address copyright loss
adequately. Our approach begin with the introduction of a novel copyright
metric grounded in copyright law and court precedents on infringement. We then
employ the TRAK method to estimate the contribution of data holders. To
accommodate the continuous data collection process, we divide the training into
multiple rounds. Finally, We designed a hierarchical budget allocation method
based on reinforcement learning to determine the budget for each round and the
remuneration of the data holder based on the data holder's contribution and
copyright loss in each round. Extensive experiments across three datasets show
that our method outperforms all eight benchmarks, demonstrating its
effectiveness in optimizing budget distribution in a copyright-aware manner. To
the best of our knowledge, this is the first technical work that introduces to
incentive contributors and protect their copyrights by compensating them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Selective G-Bispectrum and its Inversion: Applications to
  G-Invariant Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07655v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07655v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Mataigne, Johan Mathe, Sophia Sanborn, Christopher Hillar, Nina Miolane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important problem in signal processing and deep learning is to achieve
\textit{invariance} to nuisance factors not relevant for the task. Since many
of these factors are describable as the action of a group $G$ (e.g. rotations,
translations, scalings), we want methods to be $G$-invariant. The
$G$-Bispectrum extracts every characteristic of a given signal up to group
action: for example, the shape of an object in an image, but not its
orientation. Consequently, the $G$-Bispectrum has been incorporated into deep
neural network architectures as a computational primitive for
$G$-invariance\textemdash akin to a pooling mechanism, but with greater
selectivity and robustness. However, the computational cost of the
$G$-Bispectrum ($\mathcal{O}(|G|^2)$, with $|G|$ the size of the group) has
limited its widespread adoption. Here, we show that the $G$-Bispectrum
computation contains redundancies that can be reduced into a \textit{selective
$G$-Bispectrum} with $\mathcal{O}(|G|)$ complexity. We prove desirable
mathematical properties of the selective $G$-Bispectrum and demonstrate how its
integration in neural networks enhances accuracy and robustness compared to
traditional approaches, while enjoying considerable speeds-up compared to the
full $G$-Bispectrum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advantages of Neural Population Coding for Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00393v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00393v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heiko Hoffmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scalar variables, e.g., the orientation of a shape in an image, are commonly
predicted using a single output neuron in a neural network. In contrast, the
mammalian cortex represents variables with a population of neurons. In this
population code, each neuron is most active at its preferred value and shows
partial activity for other values. Here, we investigate the benefit of using a
population code for the output layer of a neural network. We compare population
codes against single-neuron outputs and one-hot vectors. First, we show
theoretically and in experiments with synthetic data that population codes
improve robustness to input noise in networks of stacked linear layers. Second,
we demonstrate the benefit of using population codes to encode ambiguous
outputs, such as the pose of symmetric objects. Using the T-LESS dataset of
feature-less real-world objects, we show that population codes improve the
accuracy of predicting 3D object orientation from image input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skill-aware Mutual Information Optimisation for Generalisation in
  Reinforcement Learning <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04815v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04815v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuehui Yu, Mhairi Dunion, Xin Li, Stefano V. Albrecht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta-Reinforcement Learning (Meta-RL) agents can struggle to operate across
tasks with varying environmental features that require different optimal skills
(i.e., different modes of behaviour). Using context encoders based on
contrastive learning to enhance the generalisability of Meta-RL agents is now
widely studied but faces challenges such as the requirement for a large sample
size, also referred to as the $\log$-$K$ curse. To improve RL generalisation to
different tasks, we first introduce Skill-aware Mutual Information (SaMI), an
optimisation objective that aids in distinguishing context embeddings according
to skills, thereby equipping RL agents with the ability to identify and execute
different skills across tasks. We then propose Skill-aware Noise Contrastive
Estimation (SaNCE), a $K$-sample estimator used to optimise the SaMI objective.
We provide a framework for equipping an RL agent with SaNCE in practice and
conduct experimental validation on modified MuJoCo and Panda-gym benchmarks. We
empirically find that RL agents that learn by maximising SaMI achieve
substantially improved zero-shot generalisation to unseen tasks. Additionally,
the context encoder trained with SaNCE demonstrates greater robustness to a
reduction in the number of available samples, thus possessing the potential to
overcome the $\log$-$K$ curse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The Thirty-eighth Annual Conference on Neural Information Processing
  Systems (NeurIPS), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ATM: Improving Model Merging by Alternating Tuning and Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Zhou, Daniele Solombrino, Donato Crisostomi, Maria Sofia Bucarelli, Fabrizio Silvestri, Emanuele Rodolà
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging has recently emerged as a cost-efficient paradigm for
multi-task learning. Among current approaches, task arithmetic stands out for
its simplicity and effectiveness. In this paper, we motivate the effectiveness
of task vectors by linking them to multi-task gradients. We show that in a
single-epoch scenario, task vectors are mathematically equivalent to the
gradients obtained via gradient descent in a multi-task setting, and still
approximate these gradients in subsequent epochs. Furthermore, we show that
task vectors perform optimally when equality is maintained, and their
effectiveness is largely driven by the first epoch's gradient. Building on this
insight, we propose viewing model merging as a single step in an iterative
process that Alternates between Tuning and Merging (ATM). This method acts as a
bridge between model merging and multi-task gradient descent, achieving
state-of-the-art results with the same data and computational requirements. We
extensively evaluate ATM across diverse settings, achieving up to 20% higher
accuracy in computer vision and NLP tasks, compared to the best baselines.
Finally, we provide both empirical and theoretical support for its
effectiveness, demonstrating increased orthogonality between task vectors and
proving that ATM minimizes an upper bound on the loss obtained by jointly
finetuning all tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 10 Pages, 11 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Yet Another Representation of Binary Decision Trees: A Mathematical
  Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.07077v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.07077v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinxiong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A decision tree looks like a simple directed acyclic computational graph,
where only the leaf nodes specify the output values and the non-terminals
specify their tests or split conditions. From the numerical perspective, we
express decision trees in the language of computational graph. We explicitly
parameterize the test phase, traversal phase and prediction phase of decision
trees based on the bitvectors of non-terminal nodes. As shown, the decision
tree is a shallow binary network in some sense. Especially, we introduce the
bitvector matrix to implement the tree traversal in numerical approach, where
the core is to convert the logical `AND' operation to arithmetic operations.
And we apply this numerical representation to extend and unify diverse decision
trees in concept.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Dynamic Brain Functional Connectivity Based on Random
  Convolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16619v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16619v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongjie Duan, Vince D. Calhoun, Zhiying Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic functional connectivity (DFC) analysis has been widely applied to
functional magnetic resonance imaging (fMRI) data to reveal time-varying
dynamic changes of brain states. The sliding window method is by far the most
popular DFC analysis method due to its simplicity. However, the sliding window
method comes with some assumptions, namely the typically approach uses a single
window which captures dynamics only within a specific frequency range. In this
study, we propose a generalized approach to dynamics via a multi-dimensional
random convolution (RandCon) DFC method that is able to effectively capture
time-varying DFC at arbitrary time scales by extracting different local
features from fMRI time series using a number of multi-dimensional random
convolution kernels without the need for learning kernel weights. Compared to a
standard sliding window method, multiplication of temporal derivatives (MTD)
and phase synchrony methods, RandCon with the smallest kernel size (3 time
points) showed notable improvements in performance on simulated data,
particularly in terms of DFC temporal and spatial estimation in very short
window/kernel size under different noise levels. Results from real fMRI data
indicated that RandCon was more sensitive to gender differences than competing
methods. Furthermore, we show that the sliding window method can be considered
a special case of the proposed multi-dimensional convolution framework. The
proposed method is simple and efficient significantly broadens the scope of
dynamic functional connectivity research and offer theoretical and practical
potential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Testably Learning Polynomial Threshold Functions <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Slot, Stefan Tiegel, Manuel Wiedmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rubinfeld & Vasilyan recently introduced the framework of testable learning
as an extension of the classical agnostic model. It relaxes distributional
assumptions which are difficult to verify by conditions that can be checked
efficiently by a tester. The tester has to accept whenever the data truly
satisfies the original assumptions, and the learner has to succeed whenever the
tester accepts. We focus on the setting where the tester has to accept standard
Gaussian data. There, it is known that basic concept classes such as halfspaces
can be learned testably with the same time complexity as in the
(distribution-specific) agnostic model. In this work, we ask whether there is a
price to pay for testably learning more complex concept classes. In particular,
we consider polynomial threshold functions (PTFs), which naturally generalize
halfspaces. We show that PTFs of arbitrary constant degree can be testably
learned up to excess error $\varepsilon > 0$ in time
$n^{\mathrm{poly}(1/\varepsilon)}$. This qualitatively matches the best known
guarantees in the agnostic model. Our results build on a connection between
testable learning and fooling. In particular, we show that distributions that
approximately match at least $\mathrm{poly}(1/\varepsilon)$ moments of the
standard Gaussian fool constant-degree PTFs (up to error $\varepsilon$). As a
secondary result, we prove that a direct approach to show testable learning
(without fooling), which was successfully used for halfspaces, cannot work for
PTFs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024. v2: Minor updates of exposition. 53 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DistriBlock: Identifying adversarial audio samples by leveraging
  characteristics of the output distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17000v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17000v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matías Pizarro, Dorothea Kolossa, Asja Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks can mislead automatic speech recognition (ASR) systems
into predicting an arbitrary target text, thus posing a clear security threat.
To prevent such attacks, we propose DistriBlock, an efficient detection
strategy applicable to any ASR system that predicts a probability distribution
over output tokens in each time step. We measure a set of characteristics of
this distribution: the median, maximum, and minimum over the output
probabilities, the entropy of the distribution, as well as the Kullback-Leibler
and the Jensen-Shannon divergence with respect to the distributions of the
subsequent time step. Then, by leveraging the characteristics observed for both
benign and adversarial data, we apply binary classifiers, including simple
threshold-based classification, ensembles of such classifiers, and neural
networks. Through extensive analysis across different state-of-the-art ASR
systems and language data sets, we demonstrate the supreme performance of this
approach, with a mean area under the receiver operating characteristic curve
for distinguishing target adversarial examples against clean and noisy data of
99% and 97%, respectively. To assess the robustness of our method, we show that
adaptive adversarial examples that can circumvent DistriBlock are much noisier,
which makes them easier to detect through filtering and creates another avenue
for preserving the system's robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Available at: https://proceedings.mlr.press/v244/pizarro24a.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reassessing Noise Augmentation Methods in the Context of Adversarial
  Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01813v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01813v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karla Pizzi, Matías Pizarro, Asja Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we investigate if noise-augmented training can concurrently
improve adversarial robustness in automatic speech recognition (ASR) systems.
We conduct a comparative analysis of the adversarial robustness of four
different state-of-the-art ASR architectures, where each of the ASR
architectures is trained under three different augmentation conditions: one
subject to background noise, speed variations, and reverberations, another
subject to speed variations only, and a third without any form of data
augmentation. The results demonstrate that noise augmentation not only improves
model performance on noisy speech but also the model's robustness to
adversarial attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Robert, Mher Safaryan, Ionut-Vlad Modoranu, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LDAdam, a memory-efficient optimizer for training large models,
that performs adaptive optimization steps within lower dimensional subspaces,
while consistently exploring the full parameter space during training. This
strategy keeps the optimizer's memory footprint to a fraction of the model
size. LDAdam relies on a new projection-aware update rule for the optimizer
states that allows for transitioning between subspaces, i.e., estimation of the
statistics of the projected gradients. To mitigate the errors due to low-rank
projection, LDAdam integrates a new generalized error feedback mechanism, which
explicitly accounts for both gradient and optimizer state compression. We prove
the convergence of LDAdam under standard assumptions, and show that LDAdam
allows for accurate and efficient fine-tuning and pre-training of language
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Task Affinity Learning for Multitask Dense Scene Predictions <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Sinodinos, Narges Armanfard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multitask learning (MTL) has become prominent for its ability to predict
multiple tasks jointly, achieving better per-task performance with fewer
parameters than single-task learning. Recently, decoder-focused architectures
have significantly improved multitask performance by refining task predictions
using features from related tasks. However, most refinement methods struggle to
efficiently capture both local and long-range dependencies between
task-specific representations and cross-task patterns. In this paper, we
introduce the Cross-Task Affinity Learning (CTAL) module, a lightweight
framework that enhances task refinement in multitask networks. CTAL effectively
captures local and long-range cross-task interactions by optimizing task
affinity matrices for parameter-efficient grouped convolutions without concern
for information loss. Our results demonstrate state-of-the-art MTL performance
for both CNN and transformer backbones, using significantly fewer parameters
than single-task learning. Our code is publicly available at
https://github.com/Armanfard-Lab/EMA-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the IEEE Winter Conference on
  Applications of Computer Vision (WACV) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustifying automatic speech recognition by extracting slowly varying
  features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.07400v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.07400v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matías Pizarro, Dorothea Kolossa, Asja Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past few years, it has been shown that deep learning systems are
highly vulnerable under attacks with adversarial examples. Neural-network-based
automatic speech recognition (ASR) systems are no exception. Targeted and
untargeted attacks can modify an audio input signal in such a way that humans
still recognise the same words, while ASR systems are steered to predict a
different transcription. In this paper, we propose a defense mechanism against
targeted adversarial attacks consisting in removing fast-changing features from
the audio signals, either by applying slow feature analysis, a low-pass filter,
or both, before feeding the input to the ASR system. We perform an empirical
analysis of hybrid ASR models trained on data pre-processed in such a way.
While the resulting models perform quite well on benign data, they are
significantly more robust against targeted adversarial attacks: Our final,
proposed model shows a performance on clean data similar to the baseline model,
while being more than four times more robust.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble-Based Annealed Importance Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan Chen, Lexing Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling from a multimodal distribution is a fundamental and challenging
problem in computational science and statistics. Among various approaches
proposed for this task, one popular method is Annealed Importance Sampling
(AIS). In this paper, we propose an ensemble-based version of AIS by combining
it with population-based Monte Carlo methods to improve its efficiency. By
keeping track of an ensemble instead of a single particle along some
continuation path between the starting distribution and the target
distribution, we take advantage of the interaction within the ensemble to
encourage the exploration of undiscovered modes. Specifically, our main idea is
to utilize either the snooker algorithm or the genetic algorithm used in
Evolutionary Monte Carlo. We discuss how the proposed algorithm can be
implemented and derive a partial differential equation governing the evolution
of the ensemble under the continuous time and mean-field limit. We also test
the efficiency of the proposed algorithm on various continuous and discrete
distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient Descent Finds Over-Parameterized Neural Networks with Sharp
  Generalization for Nonparametric Regression: A Distribution-Free Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingzhen Yang, Ping Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study nonparametric regression by an over-parameterized two-layer neural
network trained by gradient descent (GD) in this paper. We show that, if the
neural network is trained by GD with early stopping, then the trained network
renders a sharp rate of the nonparametric regression risk of $\cO(\eps_n^2)$,
which is the same rate as that for the classical kernel regression trained by
GD with early stopping, where $\eps_n$ is the critical population rate of the
Neural Tangent Kernel (NTK) associated with the network and $n$ is the size of
the training data. It is remarked that our result does not require
distributional assumptions on the training data, in a strong contrast with many
existing results which rely on specific distributions such as the spherical
uniform data distribution or distributions satisfying certain restrictive
conditions. The rate $\cO(\eps_n^2)$ is known to be minimax optimal for
specific cases, such as the case that the NTK has a polynomial eigenvalue decay
rate which happens under certain distributional assumptions. Our result
formally fills the gap between training a classical kernel regression model and
training an over-parameterized but finite-width neural network by GD for
nonparametric regression without distributional assumptions. We also provide
confirmative answers to certain open questions or address particular concerns
in the literature of training over-parameterized neural networks by GD with
early stopping for nonparametric regression, including the characterization of
the stopping time, the lower bound for the network width, and the constant
learning rate used in GD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article draws results with revisions from the first author's
  other work in arXiv:2407.11353</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Opening the Black-Box: A Systematic Review on Explainable AI in Remote
  Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Höhl, Ivica Obadic, Miguel Ángel Fernández Torres, Hiba Najjar, Dario Oliveira, Zeynep Akata, Andreas Dengel, Xiao Xiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, black-box machine learning approaches have become a dominant
modeling paradigm for knowledge extraction in remote sensing. Despite the
potential benefits of uncovering the inner workings of these models with
explainable AI, a comprehensive overview summarizing the explainable AI methods
used and their objectives, findings, and challenges in remote sensing
applications is still missing. In this paper, we address this gap by performing
a systematic review to identify the key trends in the field and shed light on
novel explainable AI approaches and emerging directions that tackle specific
remote sensing challenges. We also reveal the common patterns of explanation
interpretation, discuss the extracted scientific insights, and reflect on the
approaches used for the evaluation of explainable AI methods. As such, our
review provides a complete summary of the state-of-the-art of explainable AI in
remote sensing. Further, we give a detailed outlook on the challenges and
promising research directions, representing a basis for novel methodological
development and a useful starting point for new researchers in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffBatt: A <span class="highlight-title">Diffusion</span> Model for Battery Degradation Prediction and
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Eivazi, André Hebenbrock, Raphael Ginster, Steffen Blömeke, Stefan Wittek, Christoph Herrmann, Thomas S. Spengler, Thomas Turek, Andreas Rausch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Battery degradation remains a critical challenge in the pursuit of green
technologies and sustainable energy solutions. Despite significant research
efforts, predicting battery capacity loss accurately remains a formidable task
due to its complex nature, influenced by both aging and cycling behaviors. To
address this challenge, we introduce a novel general-purpose model for battery
degradation prediction and synthesis, DiffBatt. Leveraging an innovative
combination of conditional and unconditional diffusion models with
classifier-free guidance and transformer architecture, DiffBatt achieves high
expressivity and scalability. DiffBatt operates as a probabilistic model to
capture uncertainty in aging behaviors and a generative model to simulate
battery degradation. The performance of the model excels in prediction tasks
while also enabling the generation of synthetic degradation curves,
facilitating enhanced model training by data augmentation. In the remaining
useful life prediction task, DiffBatt provides accurate results with a mean
RMSE of 196 cycles across all datasets, outperforming all other models and
demonstrating superior generalizability. This work represents an important step
towards developing foundational models for battery degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Reinforcement Learning on the Constraint Manifold: Theory and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09080v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09080v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puze Liu, Haitham Bou-Ammar, Jan Peters, Davide Tateo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating learning-based techniques, especially reinforcement learning,
into robotics is promising for solving complex problems in unstructured
environments. However, most existing approaches are trained in well-tuned
simulators and subsequently deployed on real robots without online fine-tuning.
In this setting, extensive engineering is required to mitigate the sim-to-real
gap, which can be challenging for complex systems. Instead, learning with
real-world interaction data offers a promising alternative: it not only
eliminates the need for a fine-tuned simulator but also applies to a broader
range of tasks where accurate modeling is unfeasible. One major problem for
on-robot reinforcement learning is ensuring safety, as uncontrolled exploration
can cause catastrophic damage to the robot or the environment. Indeed, safety
specifications, often represented as constraints, can be complex and
non-linear, making safety challenging to guarantee in learning systems. In this
paper, we show how we can impose complex safety constraints on learning-based
robotics systems in a principled manner, both from theoretical and practical
points of view. Our approach is based on the concept of the Constraint
Manifold, representing the set of safe robot configurations. Exploiting
differential geometry techniques, i.e., the tangent space, we can construct a
safe action space, allowing learning agents to sample arbitrary actions while
ensuring safety. We demonstrate the method's effectiveness in a real-world
Robot Air Hockey task, showing that our method can handle high-dimensional
tasks with complex constraints. Videos of the real robot experiments are
available on the project website (https://puzeliu.github.io/TRO-ATACOM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages; sumitted to IEEE Transactions on Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Whispers in the Machine: Confidentiality in LLM-integrated Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Evertz, Merlin Chlosta, Lea Schönherr, Thorsten Eisenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly augmented with external tools
and commercial services into LLM-integrated systems. While these interfaces can
significantly enhance the capabilities of the models, they also introduce a new
attack surface. Manipulated integrations, for example, can exploit the model
and compromise sensitive data accessed through other interfaces. While previous
work primarily focused on attacks targeting a model's alignment or the leakage
of training data, the security of data that is only available during inference
has escaped scrutiny so far. In this work, we demonstrate the vulnerabilities
associated with external components and introduce a systematic approach to
evaluate confidentiality risks in LLM-integrated systems. We identify two
specific attack scenarios unique to these systems and formalize these into a
tool-robustness framework designed to measure a model's ability to protect
sensitive information. Our findings show that all examined models are highly
vulnerable to confidentiality attacks, with the risk increasing significantly
when models are used together with external tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exponential convergence rates for momentum stochastic gradient descent
  in the overparametrized setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Gess, Sebastian Kassing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove explicit bounds on the exponential rate of convergence for the
momentum stochastic gradient descent scheme (MSGD) for arbitrary, fixed
hyperparameters (learning rate, friction parameter) and its continuous-in-time
counterpart in the context of non-convex optimization. In the small step-size
regime and in the case of flat minima or large noise intensities, these bounds
prove faster convergence of MSGD compared to plain stochastic gradient descent
(SGD). The results are shown for objective functions satisfying a local
Polyak-Lojasiewicz inequality and under assumptions on the variance of MSGD
that are satisfied in overparametrized settings. Moreover, we analyze the
optimal choice of the friction parameter and show that the MSGD process almost
surely converges to a local minimum.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MT2ST: Adaptive Multi-Task to Single-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional training approaches often face challenges in balancing the
breadth of multi-task learning (MTL) with the depth of single-task learning
(STL). To address this issue, we introduce the Multi-Task to Single-Task
(MT2ST) framework, a groundbreaking approach that can combine the
generalizability of MTL with the precision of STL. Our work include two
strategies: 'Diminish' and 'Switch'. 'Diminish' Strategy will gradually reduce
the influence of auxiliary tasks, while the 'Switch' strategy involves a shift
from multi-tasking to single-tasking at a specific timepoint at the training
process.
  In this paper, we propose the Multi-Task to Single-Task (MT2ST) framework, a
novel approach that significantly enhances the efficiency and accuracy of word
embedding training while concurrently addressing prevalent issues such as
overfitting. Our empirical studies demonstrate that MT2ST can reduce training
time by 67% when contrasted with single-task learning approaches, and by 13%
compared to traditional multi-task learning methods. These findings underscore
MT2ST's potential to be a powerful tools for word embedding training
acceleration. The code implementation is can be found at:
https://github.com/NoakLiu/MT2ST-Word-Embeddings-Acceleration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Confidence Calibration of Classifiers with Many Classes <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrien LeCoz, Stéphane Herbin, Faouzi Adjed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For classification models based on neural networks, the maximum predicted
class probability is often used as a confidence score. This score rarely
predicts well the probability of making a correct prediction and requires a
post-processing calibration step. However, many confidence calibration methods
fail for problems with many classes. To address this issue, we transform the
problem of calibrating a multiclass classifier into calibrating a single
surrogate binary classifier. This approach allows for more efficient use of
standard calibration methods. We evaluate our approach on numerous neural
networks used for image or text classification and show that it significantly
enhances existing calibration methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024; code available at
  https://github.com/allglc/tva-calibration</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoldMark: Protecting Protein Generative Models with Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20354v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20354v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaixi Zhang, Ruofan Jin, Kaidi Fu, Le Cong, Marinka Zitnik, Mengdi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein structure is key to understanding protein function and is essential
for progress in bioengineering, drug discovery, and molecular biology.
Recently, with the incorporation of generative AI, the power and accuracy of
computational protein structure prediction/design have been improved
significantly. However, ethical concerns such as copyright protection and
harmful content generation (biosecurity) pose challenges to the wide
implementation of protein generative models. Here, we investigate whether it is
possible to embed watermarks into protein generative models and their outputs
for copyright authentication and the tracking of generated structures. As a
proof of concept, we propose a two-stage method FoldMark as a generalized
watermarking strategy for protein generative models. FoldMark first pretrain
watermark encoder and decoder, which can minorly adjust protein structures to
embed user-specific information and faithfully recover the information from the
encoded structure. In the second step, protein generative models are fine-tuned
with watermark Low-Rank Adaptation (LoRA) modules to preserve generation
quality while learning to generate watermarked structures with high recovery
rates. Extensive experiments are conducted on open-source protein structure
prediction models (e.g., ESMFold and MultiFlow) and de novo structure design
models (e.g., FrameDiff and FoldFlow) and we demonstrate that our method is
effective across all these generative models. Meanwhile, our watermarking
framework only exerts a negligible impact on the original protein structure
quality and is robust under potential post-processing and adaptive attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind the spikes: Benign overfitting of kernels and neural networks in
  fixed dimension <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14077v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14077v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Haas, David Holzmüller, Ulrike von Luxburg, Ingo Steinwart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of over-parameterized neural networks trained to near-zero
training error has caused great interest in the phenomenon of benign
overfitting, where estimators are statistically consistent even though they
interpolate noisy training data. While benign overfitting in fixed dimension
has been established for some learning methods, current literature suggests
that for regression with typical kernel methods and wide neural networks,
benign overfitting requires a high-dimensional setting where the dimension
grows with the sample size. In this paper, we show that the smoothness of the
estimators, and not the dimension, is the key: benign overfitting is possible
if and only if the estimator's derivatives are large enough. We generalize
existing inconsistency results to non-interpolating models and more kernels to
show that benign overfitting with moderate derivatives is impossible in fixed
dimension. Conversely, we show that rate-optimal benign overfitting is possible
for regression with a sequence of spiky-smooth kernels with large derivatives.
Using neural tangent kernels, we translate our results to wide neural networks.
We prove that while infinite-width networks do not overfit benignly with the
ReLU activation, this can be fixed by adding small high-frequency fluctuations
to the activation function. Our experiments verify that such neural networks,
while overfitting, can indeed generalize well even on low-dimensional data
sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Compared to the NeurIPS version (v2), this version strengthens
  Assumption (K) from d/2<s<=3d/4 to d/2<s<3d/4 and corrects Lemma B.2 by
  posing additional assumptions. This does not affect any other statements. We
  provide Python code to reproduce all of our experimental results at
  https://github.com/moritzhaas/mind-the-spikes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Zhao, Zhaiyu Chen, Zhitong Xiong, Yilei Shi, Sudipan Saha, Xiao Xiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earth Observation (EO) data analysis has been significantly revolutionized by
deep learning (DL), with applications typically limited to grid-like data
structures. Graph Neural Networks (GNNs) emerge as an important innovation,
propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively
tackle the challenges posed by diverse modalities, multiple sensors, and the
heterogeneous nature of EO data. To introduce GNNs in the related domains, our
review begins by offering fundamental knowledge on GNNs. Then, we summarize the
generic problems in EO, to which GNNs can offer potential solutions. Following
this, we explore a broad spectrum of GNNs' applications to scientific problems
in Earth systems, covering areas such as weather and climate analysis, disaster
management, air quality monitoring, agriculture, land cover classification,
hydrological process modeling, and urban modeling. The rationale behind
adopting GNNs in these fields is explained, alongside methodologies for
organizing graphs and designing favorable architectures for various tasks.
Furthermore, we highlight methodological challenges of implementing GNNs in
these domains and possible solutions that could guide future research. While
acknowledging that GNNs are not a universal solution, we conclude the paper by
comparing them with other popular architectures like transformers and analyzing
their potential synergies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Geoscience and Remote Sensing Magazine
  (GRSM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Timer-XL: Long-Context <span class="highlight-title">Transformer</span>s for Unified Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, Mingsheng Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Timer-XL, a generative Transformer for unified time series
forecasting. To uniformly predict 1D and 2D time series, we generalize next
token prediction, predominantly adopted for causal generation of 1D sequences,
to multivariate next token prediction. The proposed paradigm uniformly
formulates various forecasting scenarios as a long-context generation problem.
We opt for the generative Transformer, which can capture global-range and
causal dependencies while providing contextual flexibility, to implement
unified forecasting on univariate series characterized by non-stationarity,
multivariate time series with complicated dynamics and correlations, and
covariate-informed contexts that include both endogenous and exogenous
variables. Technically, we propose a universal TimeAttention to facilitate
generative Transformers on time series, which can effectively capture
fine-grained intra- and inter-series dependencies of flattened time series
tokens (patches) and is further strengthened by position embeddings in both
temporal and variable dimensions. Timer-XL achieves state-of-the-art
performance across challenging forecasting benchmarks through a unified
approach. As a large time series model, it demonstrates notable model
transferability by large-scale pre-training, as well as contextual flexibility
in token lengths, positioning it as a one-for-all forecaster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Getting By Goal Misgeneralization With a Little Help From a Mentor <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tu Trinh, Mohamad H. Danesh, Nguyen X. Khanh, Benjamin Plaut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While reinforcement learning (RL) agents often perform well during training,
they can struggle with distribution shift in real-world deployments. One
particularly severe risk of distribution shift is goal misgeneralization, where
the agent learns a proxy goal that coincides with the true goal during training
but not during deployment. In this paper, we explore whether allowing an agent
to ask for help from a supervisor in unfamiliar situations can mitigate this
issue. We focus on agents trained with PPO in the CoinRun environment, a
setting known to exhibit goal misgeneralization. We evaluate multiple methods
for determining when the agent should request help and find that asking for
help consistently improves performance. However, we also find that methods
based on the agent's internal state fail to proactively request help, instead
waiting until mistakes have already occurred. Further investigation suggests
that the agent's internal state does not represent the coin at all,
highlighting the importance of learning nuanced representations, the risks of
ignoring everything not immediately relevant to reward, and the necessity of
developing ask-for-help strategies tailored to the agent's training algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SATA Workshop @ NeurIPS 2024 (Towards Safe and Trustworthy Agents)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Neural Network Interpretability with Feature-Aligned Sparse
  Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Marks, Alasdair Paren, David Krueger, Fazl Barez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Autoencoders (SAEs) have shown promise in improving the
interpretability of neural network activations, but can learn features that are
not features of the input, limiting their effectiveness. We propose
\textsc{Mutual Feature Regularization} \textbf{(MFR)}, a regularization
technique for improving feature learning by encouraging SAEs trained in
parallel to learn similar features. We motivate \textsc{MFR} by showing that
features learned by multiple SAEs are more likely to correlate with features of
the input. By training on synthetic data with known features of the input, we
show that \textsc{MFR} can help SAEs learn those features, as we can directly
compare the features learned by the SAE with the input features for the
synthetic data. We then scale \textsc{MFR} to SAEs that are trained to denoise
electroencephalography (EEG) data and SAEs that are trained to reconstruct
GPT-2 Small activations. We show that \textsc{MFR} can improve the
reconstruction loss of SAEs by up to 21.21\% on GPT-2 Small, and 6.67\% on EEG
data. Our results suggest that the similarity between features learned by
different SAEs can be leveraged to improve SAE training, thereby enhancing
performance and the usefulness of SAEs for model interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ab-initio variational wave functions for the time-dependent
  many-electron Schrödinger equation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07447v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07447v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannes Nys, Gabriel Pescia, Alessandro Sinibaldi, Giuseppe Carleo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the real-time evolution of many-electron quantum systems is
essential for studying dynamical properties in condensed matter, quantum
chemistry, and complex materials, yet it poses a significant theoretical and
computational challenge. Our work introduces a variational approach for
fermionic time-dependent wave functions, surpassing mean-field approximations
by accurately capturing many-body correlations. Therefore, we employ
time-dependent Jastrow factors and backflow transformations, which are enhanced
through neural networks parameterizations. To compute the optimal
time-dependent parameters, we utilize the time-dependent variational Monte
Carlo technique and a new method based on Taylor-root expansions of the
propagator, enhancing the accuracy of our simulations. The approach is
demonstrated in three distinct systems. In all cases, we show clear signatures
of many-body correlations in the dynamics. The results showcase the ability of
our variational approach to accurately capture the time evolution, providing
insight into the quantum dynamics of interacting electronic systems, beyond the
capabilities of mean-field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Inference on the Final-Layer Output of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02420v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02420v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yadi Wei, Roni Khardon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional neural networks are simple to train but they typically produce
overconfident predictions. In contrast, Bayesian neural networks provide good
uncertainty quantification but optimizing them is time consuming due to the
large parameter space. This paper proposes to combine the advantages of both
approaches by performing Variational Inference in the Final layer Output space
(VIFO), because the output space is much smaller than the parameter space. We
use neural networks to learn the mean and the variance of the probabilistic
output. Using the Bayesian formulation we incorporate collapsed variational
inference with VIFO which significantly improves the performance in practice.
On the other hand, like standard, non-Bayesian models, VIFO enjoys simple
training and one can use Rademacher complexity to provide risk bounds for the
model. Experiments show that VIFO provides a good tradeoff in terms of run time
and uncertainty quantification, especially for out of distribution data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published to TMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Why Go Full? Elevating Federated Learning Through Partial Network
  Updates <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11559v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11559v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Wang, Xuefeng Liu, Jianwei Niu, Wenkai Guo, Shaojie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a distributed machine learning paradigm designed to
protect user data privacy, which has been successfully implemented across
various scenarios. In traditional federated learning, the entire parameter set
of local models is updated and averaged in each training round. Although this
full network update method maximizes knowledge acquisition and sharing for each
model layer, it prevents the layers of the global model from cooperating
effectively to complete the tasks of each client, a challenge we refer to as
layer mismatch. This mismatch problem recurs after every parameter averaging,
consequently slowing down model convergence and degrading overall performance.
To address the layer mismatch issue, we introduce the FedPart method, which
restricts model updates to either a single layer or a few layers during each
communication round. Furthermore, to maintain the efficiency of knowledge
acquisition and sharing, we develop several strategies to select trainable
layers in each round, including sequential updating and multi-round cycle
training. Through both theoretical analysis and experiments, our findings
demonstrate that the FedPart method significantly surpasses conventional full
network update strategies in terms of convergence speed and accuracy, while
also reducing communication and computational overheads.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures, accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Packing Analysis: Packing Is More Appropriate for Large Models or
  <span class="highlight-title">Dataset</span>s in Supervised Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhe Wang, Guoyin Wang, Yizhong Wang, Jiwei Li, Eduard Hovy, Chen Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Packing, initially utilized in the pre-training phase, is an optimization
technique designed to maximize hardware resource efficiency by combining
different training sequences to fit the model's maximum input length. Although
it has demonstrated effectiveness during pre-training, there remains a lack of
comprehensive analysis for the supervised fine-tuning (SFT) stage on the
following points: (1) whether packing can effectively enhance training
efficiency while maintaining performance, (2) the suitable size of the model
and dataset for fine-tuning with the packing method, and (3) whether packing
unrelated or related training samples might cause the model to either
excessively disregard or over-rely on the context.
  In this paper, we perform extensive comparisons between SFT methods using
padding and packing, covering SFT datasets ranging from 69K to 1.2M and models
from 8B to 70B. This provides the first comprehensive analysis of the
advantages and limitations of packing versus padding, as well as practical
considerations for implementing packing in various training scenarios. Our
analysis covers various benchmarks, including knowledge, reasoning, and coding,
as well as GPT-based evaluations, time efficiency, and other fine-tuning
parameters. We also open-source our code for fine-tuning and evaluation and
provide checkpoints fine-tuned on datasets of different sizes, aiming to
advance future research on packing methods. Code is available at:
https://github.com/ShuheWang1998/Packing-Analysis?tab=readme-ov-file.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EncodingNet: A Novel Encoding-based MAC Design for Efficient Neural
  Network Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18595v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18595v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Liu, Grace Li Zhang, Xunzhao Yin, Ulf Schlichtmann, Bing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) have achieved great breakthroughs in many fields
such as image classification and natural language processing. However, the
execution of DNNs needs to conduct massive numbers of multiply-accumulate (MAC)
operations on hardware and thus incurs a large power consumption. To address
this challenge, we propose a novel digital MAC design based on encoding. In
this new design, the multipliers are replaced by simple logic gates to
represent the results with a wide bit representation. The outputs of the new
multipliers are added by bit-wise weighted accumulation and the accumulation
results are compatible with existing computing platforms accelerating neural
networks. Since the multiplication function is replaced by a simple logic
representation, the critical paths in the resulting circuits become much
shorter. Correspondingly, pipelining stages and intermediate registers used to
store partial sums in the MAC array can be reduced, leading to a significantly
smaller area as well as better power efficiency. The proposed design has been
synthesized and verified by ResNet18- Cifar10, ResNet20-Cifar100,
ResNet50-ImageNet, MobileNetV2-Cifar10, MobileNetV2-Cifar100, and
EfficientNetB0-ImageNet. The experimental results confirmed the reduction of
circuit area by up to 48.79% and the reduction of power consumption of
executing DNNs by up to 64.41%, while the accuracy of the neural networks can
still be well maintained. The open source code of this work can be found on
GitHub with link https://github.com/Bo-Liu-TUM/EncodingNet/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A dynamical clipping approach with task feedback for Proximal Policy
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07624v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07624v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Zhang, Jingzehua Xu, Zifeng Zhuang, Hongyin Zhang, Jinxin Liu, Donglin wang, Shuai Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proximal Policy Optimization (PPO) has been broadly applied to robotics
learning, showcasing stable training performance. However, the fixed clipping
bound setting may limit the performance of PPO. Specifically, there is no
theoretical proof that the optimal clipping bound remains consistent throughout
the entire training process. Meanwhile, previous researches suggest that a
fixed clipping bound restricts the policy's ability to explore. Therefore, many
past studies have aimed to dynamically adjust the PPO clipping bound to enhance
PPO's performance. However, the objective of these approaches are not directly
aligned with the objective of reinforcement learning (RL) tasks, which is to
maximize the cumulative Return. Unlike previous clipping approaches, we propose
a bi-level proximal policy optimization objective that can dynamically adjust
the clipping bound to better reflect the preference (maximizing Return) of
these RL tasks. Based on this bi-level proximal policy optimization paradigm,
we introduce a new algorithm named Preference based Proximal Policy
Optimization (Pb-PPO). Pb-PPO utilizes a multi-armed bandit approach to
refelect RL preference, recommending the clipping bound for PPO that can
maximizes the current Return. Therefore, Pb-PPO results in greater stability
and improved performance compared to PPO with a fixed clipping bound. We test
Pb-PPO on locomotion benchmarks across multiple environments, including
Gym-Mujoco and legged-gym. Additionally, we validate Pb-PPO on customized
navigation tasks. Meanwhile, we conducted comparisons with PPO using various
fixed clipping bounds and various of clipping approaches. The experimental
results indicate that Pb-PPO demonstrates superior training performance
compared to PPO and its variants. Our codebase has been released at :
https://github.com/stevezhangzA/pb_ppo
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Roles of Generative Artificial Intelligence in Internet of Electric
  Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanwen Zhang, Dusit Niyato, Wei Zhang, Changyuan Zhao, Hongyang Du, Abbas Jamalipour, Sumei Sun, Yiyang Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancements of generative artificial intelligence (GenAI) models,
their capabilities are expanding significantly beyond content generation and
the models are increasingly being used across diverse applications.
Particularly, GenAI shows great potential in addressing challenges in the
electric vehicle (EV) ecosystem ranging from charging management to
cyber-attack prevention. In this paper, we specifically consider Internet of
electric vehicles (IoEV) and we categorize GenAI for IoEV into four different
layers namely, EV's battery layer, individual EV layer, smart grid layer, and
security layer. We introduce various GenAI techniques used in each layer of
IoEV applications. Subsequently, public datasets available for training the
GenAI models are summarized. Finally, we provide recommendations for future
directions. This survey not only categorizes the applications of GenAI in IoEV
across different layers but also serves as a valuable resource for researchers
and practitioners by highlighting the design and implementation challenges
within each layer. Furthermore, it provides a roadmap for future research
directions, enabling the development of more robust and efficient IoEV systems
through the integration of advanced GenAI techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Similarity to Superiority: Channel Clustering for Time Series
  Forecasting <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialin Chen, Jan Eric Lenssen, Aosong Feng, Weihua Hu, Matthias Fey, Leandros Tassiulas, Jure Leskovec, Rex Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting has attracted significant attention in recent
decades. Previous studies have demonstrated that the Channel-Independent (CI)
strategy improves forecasting performance by treating different channels
individually, while it leads to poor generalization on unseen instances and
ignores potentially necessary interactions between channels. Conversely, the
Channel-Dependent (CD) strategy mixes all channels with even irrelevant and
indiscriminate information, which, however, results in oversmoothing issues and
limits forecasting accuracy. There is a lack of channel strategy that
effectively balances individual channel treatment for improved forecasting
performance without overlooking essential interactions between channels.
Motivated by our observation of a correlation between the time series model's
performance boost against channel mixing and the intrinsic similarity on a pair
of channels, we developed a novel and adaptable Channel Clustering Module
(CCM). CCM dynamically groups channels characterized by intrinsic similarities
and leverages cluster information instead of individual channel identities,
combining the best of CD and CI worlds. Extensive experiments on real-world
datasets demonstrate that CCM can (1) boost the performance of CI and CD models
by an average margin of 2.4% and 7.2% on long-term and short-term forecasting,
respectively; (2) enable zero-shot forecasting with mainstream time series
forecasting models; (3) uncover intrinsic time series patterns among channels
and improve interpretability of complex time series models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping
  Backward Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunghyeon Woo, Baeseong Park, Byeongwook Kim, Minjung Jo, Se Jung Kwon, Dongsuk Jeon, Dongsoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved significant success across various
domains. However, training these LLMs typically involves substantial memory and
computational costs during both forward and backward propagation. While
parameter-efficient fine-tuning (PEFT) considerably reduces the training memory
associated with parameters, it does not address the significant computational
costs and activation memory. In this paper, we propose Dropping Backward
Propagation (DropBP), a novel approach designed to reduce computational costs
and activation memory while maintaining accuracy. DropBP randomly drops layers
during backward propagation, which is essentially equivalent to training
shallow submodules generated by undropped layers and residual connections.
Additionally, DropBP calculates the sensitivity of each layer to assign an
appropriate drop rate, thereby stabilizing the training process. DropBP is not
only applicable to full fine-tuning but can also be orthogonally integrated
with all types of PEFT by dropping layers during backward propagation.
Specifically, DropBP can reduce training time by 44% with comparable accuracy
to the baseline, accelerate convergence to the same perplexity by 1.5x, and
enable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.
Furthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100
GPU and 117% on an Intel Gaudi2 HPU. The code is available at
https://github.com/WooSunghyeon/dropbp.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Utilizing Large Language Models in an iterative paradigm with Domain
  feedback for Zero-shot Molecule optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13147v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13147v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khiem Le, Nitesh V. Chawla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecule optimization is a critical task in drug discovery to optimize
desired properties of a given molecule through chemical modification. Despite
Large Language Models (LLMs) holding the potential to efficiently simulate this
task by using natural language to direct the optimization, straightforwardly
utilizing shows limited performance. In this work, we facilitate utilizing LLMs
in an iterative paradigm by proposing a simple yet highly effective domain
feedback provider, namely $\text{Re}^3$DF. In detail, $\text{Re}^3$DF harnesses
an external toolkit, RDKit, to handle the molecule hallucination, if the
modified molecule is chemically invalid. Otherwise, its desired properties are
computed and compared to the original one, establishing reliable domain
feedback with correct direction and distance towards the objective, followed by
a retrieved example, to explicitly guide the LLM to refine the modified
molecule. We conduct experiments across both single- and multi-property
objectives with 2 thresholds, where $\text{Re}^3$DF shows significant
improvements. Particularly, for 20 single-property objectives, $\text{Re}^3$DF
enhances Hit ratio by 16.95% and 20.76% under loose and strict thresholds,
respectively. For 32 multi-property objectives, $\text{Re}^3$DF enhances Hit
ratio by 6.04% and 5.25%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Communication-Efficient Adaptive Batch Size Strategies for Distributed
  Local Gradient Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13936v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13936v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Tsz-Kit Lau, Weijian Li, Chenwei Xu, Han Liu, Mladen Kolar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern deep neural networks often require distributed training with many
workers due to their large size. As the number of workers increases,
communication overheads become the main bottleneck in data-parallel minibatch
stochastic gradient methods with per-iteration gradient synchronization. Local
gradient methods like Local SGD reduce communication by only synchronizing
model parameters and/or gradients after several local steps. Despite an
understanding of their convergence and the importance of batch sizes for
training efficiency and generalization, optimal batch sizes for local gradient
methods are difficult to determine. We introduce adaptive batch size strategies
for local gradient methods that increase batch sizes adaptively to reduce
minibatch gradient variance. We provide convergence guarantees under
homogeneous data conditions and support our claims with image classification
and language modeling experiments, demonstrating the effectiveness of our
strategies for both training efficiency and generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning with Geometry: Including Riemannian Geometric Features in
  Coefficient of Pressure Prediction on Aircraft Wings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liwei Hu, Wenyong Wang, Yu Xiang, Stefan Sommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to incorporate Riemannian geometric features from the geometry of
aircraft wing surfaces in the prediction of coefficient of pressure (CP) on the
aircraft wing. Contrary to existing approaches that treat the wing surface as a
flat object, we represent the wing as a piecewise smooth manifold and calculate
a set of Riemannian geometric features (Riemannian metric, connection, and
curvature) over points of the wing. Combining these features in neighborhoods
of points on the wing with coordinates and flight conditions gives inputs to a
deep learning model that predicts CP distributions. Experimental results show
that the method with incorporation of Riemannian geometric features, compared
to state-of-the-art Deep Attention Network (DAN), reduces the predicted mean
square error (MSE) of CP by an average of 15.00% for the DLR-F11 aircraft test
set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Virtual Human Generative Model: Masked Modeling Approach for Learning
  Human Characteristics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10656v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10656v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenta Oono, Nontawat Charoenphakdee, Kotatsu Bito, Zhengyan Gao, Hideyoshi Igata, Masashi Yoshikawa, Yoshiaki Ota, Hiroki Okui, Kei Akita, Shoichiro Yamaguchi, Yohei Sugawara, Shin-ichi Maeda, Kunihiko Miyoshi, Yuki Saito, Koki Tsuda, Hiroshi Maruyama, Kohei Hayashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying the relationship between healthcare attributes, lifestyles, and
personality is vital for understanding and improving physical and mental
well-being. Machine learning approaches are promising for modeling their
relationships and offering actionable suggestions. In this paper, we propose
Virtual Human Generative Model (VHGM), a machine learning model for estimating
healthcare, lifestyles, and personality attributes. VHGM is a deep generative
model trained with masked modeling to learn the joint distribution of
attributes conditioned on known ones. Using heterogeneous tabular datasets,
VHGM learns more than 2,000 attributes efficiently. We numerically evaluate the
performance of VHGM and its training techniques and have deployed VHGM as a Web
service, enabling various healthcare applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contraction Theory for Nonlinear Stability Analysis and Learning-based
  Control: A Tutorial Overview 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.00675v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.00675v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroyasu Tsukamoto, Soon-Jo Chung, Jean-Jacques E. Slotine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contraction theory is an analytical tool to study differential dynamics of a
non-autonomous (i.e., time-varying) nonlinear system under a contraction metric
defined with a uniformly positive definite matrix, the existence of which
results in a necessary and sufficient characterization of incremental
exponential stability of multiple solution trajectories with respect to each
other. By using a squared differential length as a Lyapunov-like function, its
nonlinear stability analysis boils down to finding a suitable contraction
metric that satisfies a stability condition expressed as a linear matrix
inequality, indicating that many parallels can be drawn between well-known
linear systems theory and contraction theory for nonlinear systems.
Furthermore, contraction theory takes advantage of a superior robustness
property of exponential stability used in conjunction with the comparison
lemma. This yields much-needed safety and stability guarantees for neural
network-based control and estimation schemes, without resorting to a more
involved method of using uniform asymptotic stability for input-to-state
stability. Such distinctive features permit systematic construction of a
contraction metric via convex optimization, thereby obtaining an explicit
exponential bound on the distance between a time-varying target trajectory and
solution trajectories perturbed externally due to disturbances and learning
errors. The objective of this paper is therefore to present a tutorial overview
of contraction theory and its advantages in nonlinear stability analysis of
deterministic and stochastic systems, with an emphasis on deriving formal
robustness and stability guarantees for various learning-based and data-driven
automatic control methods. In particular, we provide a detailed review of
techniques for finding contraction metrics and associated control and
estimation laws using deep neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Annual Reviews in Control, Accepted, Oct. 1st</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balanced Mixed-Type Tabular Data Synthesis with <span class="highlight-title">Diffusion</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Yang, Han Yu, Peikun Guo, Khadija Zanna, Xiaoxue Yang, Akane Sano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as a robust framework for various generative
tasks, including tabular data synthesis. However, current tabular diffusion
models tend to inherit bias in the training dataset and generate biased
synthetic data, which may influence discriminatory actions. In this research,
we introduce a novel tabular diffusion model that incorporates sensitive
guidance to generate fair synthetic data with balanced joint distributions of
the target label and sensitive attributes, such as sex and race. The empirical
results demonstrate that our method effectively mitigates bias in training data
while maintaining the quality of the generated samples. Furthermore, we provide
evidence that our approach outperforms existing methods for synthesizing
tabular data on fairness metrics such as demographic parity ratio and equalized
odds ratio, achieving improvements of over $10\%$. Our implementation is
available at https://github.com/comp-well-org/fair-tab-diffusion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dissecting the Failure of Invariant Learning on Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qixun Wang, Yifei Wang, Yisen Wang, Xianghua Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing node-level Out-Of-Distribution (OOD) generalization on graphs
remains a crucial area of research. In this paper, we develop a Structural
Causal Model (SCM) to theoretically dissect the performance of two prominent
invariant learning methods -- Invariant Risk Minimization (IRM) and
Variance-Risk Extrapolation (VREx) -- in node-level OOD settings. Our analysis
reveals a critical limitation: due to the lack of class-conditional invariance
constraints, these methods may struggle to accurately identify the structure of
the predictive invariant ego-graph and consequently rely on spurious features.
To address this, we propose Cross-environment Intra-class Alignment (CIA),
which explicitly eliminates spurious features by aligning cross-environment
representations conditioned on the same class, bypassing the need for explicit
knowledge of the causal pattern structure. To adapt CIA to node-level OOD
scenarios where environment labels are hard to obtain, we further propose
CIA-LRA (Localized Reweighting Alignment) that leverages the distribution of
neighboring labels to selectively align node representations, effectively
distinguishing and preserving invariant features while removing spurious ones,
all without relying on environment labels. We theoretically prove CIA-LRA's
effectiveness by deriving an OOD generalization error bound based on
PAC-Bayesian analysis. Experiments on graph OOD benchmarks validate the
superiority of CIA and CIA-LRA, marking a significant advancement in node-level
OOD generalization. The codes are available at
https://github.com/NOVAglow646/NeurIPS24-Invariant-Learning-on-Graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Multi-Stage Loss Dynamics in Neural Networks: Mechanisms of Plateau
  and Descent Stages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng-An Chen, Tao Luo, GuiHong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multi-stage phenomenon in the training loss curves of neural networks has
been widely observed, reflecting the non-linearity and complexity inherent in
the training process. In this work, we investigate the training dynamics of
neural networks (NNs), with particular emphasis on the small initialization
regime, identifying three distinct stages observed in the loss curve during
training: the initial plateau stage, the initial descent stage, and the
secondary plateau stage. Through rigorous analysis, we reveal the underlying
challenges contributing to slow training during the plateau stages. While the
proof and estimate for the emergence of the initial plateau were established in
our previous work, the behaviors of the initial descent and secondary plateau
stages had not been explored before. Here, we provide a more detailed proof for
the initial plateau, followed by a comprehensive analysis of the initial
descent stage dynamics. Furthermore, we examine the factors facilitating the
network's ability to overcome the prolonged secondary plateau, supported by
both experimental evidence and heuristic reasoning. Finally, to clarify the
link between global training trends and local parameter adjustments, we use the
Wasserstein distance to track the fine-scale evolution of weight amplitude
distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provably <span class="highlight-title">Transformer</span>s Harness Multi-Concept Word Semantics for Efficient
  In-Context Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02199v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02199v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Taiji Suzuki, Qingfu Zhang, Hau-San Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large language models (LLMs) have displayed remarkable
creative prowess and emergence capabilities. Existing empirical studies have
revealed a strong connection between these LLMs' impressive emergence abilities
and their in-context learning (ICL) capacity, allowing them to solve new tasks
using only task-specific prompts without further fine-tuning. On the other
hand, existing empirical and theoretical studies also show that there is a
linear regularity of the multi-concept encoded semantic representation behind
transformer-based LLMs. However, existing theoretical work fail to build up an
understanding of the connection between this regularity and the innovative
power of ICL. Additionally, prior work often focuses on simplified, unrealistic
scenarios involving linear transformers or unrealistic loss functions, and they
achieve only linear or sub-linear convergence rates. In contrast, this work
provides a fine-grained mathematical analysis to show how transformers leverage
the multi-concept semantics of words to enable powerful ICL and excellent
out-of-distribution ICL abilities, offering insights into how transformers
innovate solutions for certain unseen tasks encoded with multiple cross-concept
semantics. Inspired by empirical studies on the linear latent geometry of LLMs,
the analysis is based on a concept-based low-noise sparse coding prompt model.
Leveraging advanced techniques, this work showcases the exponential 0-1 loss
convergence over the highly non-convex training dynamics, which pioneeringly
incorporates the challenges of softmax self-attention, ReLU-activated MLPs, and
cross-entropy loss. Empirical simulations corroborate the theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 38th Conference on Neural Information Processing
  Systems (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LeanAgent: Lifelong Learning for Formal Theorem Proving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06209v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06209v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Kumarappan, Mo Tiwari, Peiyang Song, Robert Joseph George, Chaowei Xiao, Anima Anandkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been successful in mathematical reasoning
tasks such as formal theorem proving when integrated with interactive proof
assistants like Lean. Existing approaches involve training or fine-tuning an
LLM on a specific dataset to perform well on particular domains, such as
undergraduate-level mathematics. These methods struggle with generalizability
to advanced mathematics. A fundamental limitation is that these approaches
operate on static domains, failing to capture how mathematicians often work
across multiple domains and projects simultaneously or cyclically. We present
LeanAgent, a novel lifelong learning framework for theorem proving that
continuously generalizes to and improves on ever-expanding mathematical
knowledge without forgetting previously learned knowledge. LeanAgent introduces
several key innovations, including a curriculum learning strategy that
optimizes the learning trajectory in terms of mathematical difficulty, a
dynamic database for efficient management of evolving mathematical knowledge,
and progressive training to balance stability and plasticity. LeanAgent
successfully proves 162 theorems previously unproved by humans across 23
diverse Lean repositories, many from advanced mathematics. It performs
significantly better than the static LLM baseline, proving challenging theorems
in domains like abstract algebra and algebraic topology while showcasing a
clear progression of learning from basic concepts to advanced topics. In
addition, we analyze LeanAgent's superior performance on key lifelong learning
metrics. LeanAgent achieves exceptional scores in stability and backward
transfer, where learning new tasks improves performance on previously learned
tasks. This emphasizes LeanAgent's continuous generalizability and improvement,
explaining its superior theorem-proving performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01433v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01433v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Tang, Jiacheng Liu, Xiaofeng Hou, Yifei Pu, Jing Wang, Pheng-Ann Heng, Chao Li, Minyi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Mixture-of-Experts (MoE) architecture has demonstrated significant
advantages in the era of Large Language Models (LLMs), offering enhanced
capabilities with reduced inference costs. However, deploying MoE-based LLMs on
memoryconstrained edge devices remains challenging due to their substantial
memory requirements. While existing expertoffloading methods alleviate the
memory requirements, they often incur significant expert-loading costs or
compromise model accuracy. We present HOBBIT, a mixed precision expert
offloading system to enable flexible and efficient MoE inference. Our key
insight is that dynamically replacing less critical cache-miss experts with low
precision versions can substantially reduce expert-loading latency while
preserving model accuracy. HOBBIT introduces three innovative techniques that
map the natural hierarchy of MoE computation: (1) a token-level dynamic expert
loading mechanism, (2) a layer-level adaptive expert prefetching technique, and
(3) a sequence-level multidimensional expert caching policy. These innovations
fully leverage the benefits of mixedprecision expert inference. By implementing
HOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate
its performance across different edge devices with representative MoE models.
The results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding
compared to state-of-the-art MoE offloading systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatioformer: A Geo-encoded <span class="highlight-title">Transformer</span> for Large-Scale Plant Species
  Richness Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqing Guo, Karel Mokany, Shaun R. Levick, Jinyan Yang, Peyman Moghadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Earth observation data have shown promise in predicting species richness of
vascular plants ($\alpha$-diversity), but extending this approach to large
spatial scales is challenging because geographically distant regions may
exhibit different compositions of plant species ($\beta$-diversity), resulting
in a location-dependent relationship between richness and spectral
measurements. In order to handle such geolocation dependency, we propose
Spatioformer, where a novel geolocation encoder is coupled with the transformer
model to encode geolocation context into remote sensing imagery. The
Spatioformer model compares favourably to state-of-the-art models in richness
predictions on a large-scale ground-truth richness dataset (HAVPlot) that
consists of 68,170 in-situ richness samples covering diverse landscapes across
Australia. The results demonstrate that geolocational information is
advantageous in predicting species richness from satellite observations over
large spatial scales. With Spatioformer, plant species richness maps over
Australia are compiled from Landsat archive for the years from 2015 to 2023.
The richness maps produced in this study reveal the spatiotemporal dynamics of
plant species richness in Australia, providing supporting evidence to inform
effective planning and policy development for plant diversity conservation.
Regions of high richness prediction uncertainties are identified, highlighting
the need for future in-situ surveys to be conducted in these areas to enhance
the prediction accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Geoscience and Remote Sensing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating Epistemic and Aleatoric Uncertainty with a Single Model <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.03478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.03478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew A. Chan, Maria J. Molina, Christopher A. Metzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating and disentangling epistemic uncertainty, uncertainty that is
reducible with more training data, and aleatoric uncertainty, uncertainty that
is inherent to the task at hand, is critically important when applying machine
learning to high-stakes applications such as medical imaging and weather
forecasting. Conditional diffusion models' breakthrough ability to accurately
and efficiently sample from the posterior distribution of a dataset now makes
uncertainty estimation conceptually straightforward: One need only train and
sample from a large ensemble of diffusion models. Unfortunately, training such
an ensemble becomes computationally intractable as the complexity of the model
architecture grows. In this work we introduce a new approach to ensembling,
hyper-diffusion models (HyperDM), which allows one to accurately estimate both
epistemic and aleatoric uncertainty with a single model. Unlike existing
single-model uncertainty methods like Monte-Carlo dropout and Bayesian neural
networks, HyperDM offers prediction accuracy on par with, and in some cases
superior to, multi-model ensembles. Furthermore, our proposed approach scales
to modern network architectures such as Attention U-Net and yields more
accurate uncertainty estimates compared to existing methods. We validate our
method on two distinct real-world tasks: x-ray computed tomography
reconstruction and weather temperature forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 11 figures. To be published in Conference on Neural
  Information Processing Systems (NeurIPS) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding and Minimising Outlier Features in Neural Network Training <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bobby He, Lorenzo Noci, Daniele Paliotta, Imanol Schlag, Thomas Hofmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Outlier Features (OFs) are neurons whose activation magnitudes significantly
exceed the average over a neural network's (NN) width. They are well known to
emerge during standard transformer training and have the undesirable effect of
hindering quantisation in afflicted models. Despite their practical importance,
little is known behind why OFs emerge during training, nor how one can minimise
them.
  Our work focuses on the above questions, first identifying several
quantitative metrics, such as the kurtosis over neuron activation norms, to
measure OFs. With these metrics, we study how architectural and optimisation
choices influence OFs, and provide practical insights to minimise OFs during
training. As highlights, we introduce a novel unnormalised transformer block,
the Outlier Protected block, and present a previously unknown benefit of
non-diagonal preconditioning optimisers, finding both approaches to
significantly reduce OFs and improve quantisation without compromising
convergence speed, at scales of up to 7B parameters. Notably, our combination
of OP block and non-diagonal preconditioner (SOAP) achieves 14.87 int8
weight-and-activation perplexity (from 14.71 in standard precision), compared
to 63.4 int8 perplexity (from 16.00) with a default OF-prone combination of
Pre-Norm model and Adam, when quantising OPT-125m models post-training.
Overall, our findings shed new light on our understanding of, our ability to
prevent, and the complexity of this important aspect of NN training dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transcoders Find Interpretable LLM Feature Circuits <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Dunefsky, Philippe Chlenski, Neel Nanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key goal in mechanistic interpretability is circuit analysis: finding
sparse subgraphs of models corresponding to specific behaviors or capabilities.
However, MLP sublayers make fine-grained circuit analysis on transformer-based
language models difficult. In particular, interpretable features -- such as
those found by sparse autoencoders (SAEs) -- are typically linear combinations
of extremely many neurons, each with its own nonlinearity to account for.
Circuit analysis in this setting thus either yields intractably large circuits
or fails to disentangle local and global behavior. To address this we explore
transcoders, which seek to faithfully approximate a densely activating MLP
layer with a wider, sparsely-activating MLP layer. We introduce a novel method
for using transcoders to perform weights-based circuit analysis through MLP
sublayers. The resulting circuits neatly factorize into input-dependent and
input-invariant terms. We then successfully train transcoders on language
models with 120M, 410M, and 1.4B parameters, and find them to perform at least
on par with SAEs in terms of sparsity, faithfulness, and
human-interpretability. Finally, we apply transcoders to reverse-engineer
unknown circuits in the model, and we obtain novel insights regarding the
"greater-than circuit" in GPT2-small. Our results suggest that transcoders can
prove effective in decomposing model computations involving MLPs into
interpretable circuits. Code is available at
https://github.com/jacobdunefsky/transcoder_circuits/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 6 figures, 4 tables, 2 algorithms. NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anytime-valid t-tests and confidence sequences for Gaussian means with
  unknown variance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03722v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03722v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjian Wang, Aaditya Ramdas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$
of a Gaussian distribution with unknown variance $\sigma^2$. Curiously, he
employed both an improper (right Haar) mixture over $\sigma$ and an improper
(flat) mixture over $\mu$. Here, we elaborate carefully on the details of his
construction, which use generalized nonintegrable martingales and an extended
Ville's inequality. While this does yield a sequential t-test, it does not
yield an "e-process" (due to the nonintegrability of his martingale). In this
paper, we develop two new e-processes and confidence sequences for the same
setting: one is a test martingale in a reduced filtration, while the other is
an e-process in the canonical data filtration. These are respectively obtained
by swapping Lai's flat mixture for a Gaussian mixture, and swapping the right
Haar mixture over $\sigma$ with the maximum likelihood estimate under the null,
as done in universal inference. We also analyze the width of resulting
confidence sequences, which have a curious polynomial dependence on the error
probability $\alpha$ that we prove to be not only unavoidable, but (for
universal inference) even better than the classical fixed-sample t-test.
Numerical experiments are provided along the way to compare and contrast the
various approaches, including some recent suboptimal ones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Substantive revision in v3 (Apr 23 2024); Final revision in v4 (Nov 6
  2024) accepted by the journal Sequential Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Symmetry Discovery Beyond Affine Transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Shaw, Abram Magner, Kevin R. Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symmetry detection can improve various machine learning tasks. In the context
of continuous symmetry detection, current state of the art experiments are
limited to detecting affine transformations. Under the manifold assumption, we
outline a framework for discovering continuous symmetry in data beyond the
affine transformation group. We also provide a similar framework for
discovering discrete symmetry. We experimentally compare our method to an
existing method known as LieGAN and show that our method is competitive at
detecting affine symmetries for large sample sizes and superior than LieGAN for
small sample sizes. We also show our method is able to detect continuous
symmetries beyond the affine group and is generally more computationally
efficient than LieGAN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Explainability in Machine Learning Predictions through
  Explainer-Agnostic Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12094v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12094v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian Munoz, Kleyton da Costa, Bernardo Modenesi, Adriano Koshiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid integration of artificial intelligence (AI) into various industries
has introduced new challenges in governance and regulation, particularly
regarding the understanding of complex AI systems. A critical demand from
decision-makers is the ability to explain the results of machine learning
models, which is essential for fostering trust and ensuring ethical AI
practices. In this paper, we develop six distinct model-agnostic metrics
designed to quantify the extent to which model predictions can be explained.
These metrics measure different aspects of model explainability, ranging from
local importance, global importance, and surrogate predictions, allowing for a
comprehensive evaluation of how models generate their outputs. Furthermore, by
computing our metrics, we can rank models in terms of explainability criteria
such as importance concentration and consistency, prediction fluctuation, and
surrogate fidelity and stability, offering a valuable tool for selecting models
based not only on accuracy but also on transparency. We demonstrate the
practical utility of these metrics on classification and regression tasks, and
integrate these metrics into an existing Python package for public use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating Chemical Space with Latent Flows <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03987v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03987v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghao Wei, Yining Huang, Chenru Duan, Yue Song, Yuanqi Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress of deep generative models in the vision and language domain
has stimulated significant interest in more structured data generation such as
molecules. However, beyond generating new random molecules, efficient
exploration and a comprehensive understanding of the vast chemical space are of
great importance to molecular science and applications in drug design and
materials discovery. In this paper, we propose a new framework, ChemFlow, to
traverse chemical space through navigating the latent space learned by molecule
generative models through flows. We introduce a dynamical system perspective
that formulates the problem as learning a vector field that transports the mass
of the molecular distribution to the region with desired molecular properties
or structure diversity. Under this framework, we unify previous approaches on
molecule latent space traversal and optimization and propose alternative
competing methods incorporating different physical priors. We validate the
efficacy of ChemFlow on molecule manipulation and single- and multi-objective
molecule optimization tasks under both supervised and unsupervised molecular
discovery settings. Codes and demos are publicly available on GitHub at
https://github.com/garywei944/ChemFlow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teach Better or Show Smarter? On Instructions and Exemplars in Automatic
  Prompt Optimization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchen Wan, Ruoxi Sun, Hootan Nakhost, Sercan O. Arik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated remarkable capabilities, but their
performance is heavily reliant on effective prompt engineering. Automatic
prompt optimization (APO) methods are designed to automate this and can be
broadly categorized into those targeting instructions (instruction
optimization, IO) vs. those targeting exemplars (exemplar optimization, EO).
Despite their shared objective, these have evolved rather independently, with
IO receiving more research attention recently. This paper seeks to bridge this
gap by comprehensively comparing the performance of representative IO and EO
techniques both isolation and combination on a diverse set of challenging
tasks. Our findings reveal that intelligently reusing model-generated
input-output pairs obtained from evaluating prompts on the validation set as
exemplars, consistently improves performance on top of IO methods but is
currently under-investigated. We also find that despite the recent focus on IO,
how we select exemplars can outweigh how we optimize instructions, with EO
strategies as simple as random search outperforming state-of-the-art IO methods
with seed instructions without any optimization. Moreover, we observe a synergy
between EO and IO, with optimal combinations surpassing the individual
contributions. We conclude that studying exemplar optimization both as a
standalone method and its optimal combination with instruction optimization
remain a crucial aspect of APO and deserve greater consideration in future
research, even in the era of highly capable instruction-following models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Expanded version of the NeurIPS 2024 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Personal data Value at Risk Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Enriquez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What if the main data protection vulnerability is risk management? Data
Protection merges three disciplines: data protection law, information security,
and risk management. Nonetheless, very little research has been made on the
field of data protection risk management, where subjectivity and superficiality
are the dominant state of the art. Since the GDPR tells you what to do, but not
how to do it, the solution for approaching GDPR compliance is still a gray
zone, where the trend is using the rule of thumb. Considering that the most
important goal of risk management is to reduce uncertainty in order to take
informed decisions, risk management for the protection of the rights and
freedoms of the data subjects cannot be disconnected from the impact
materialization that data controllers and processors need to assess. This paper
proposes a quantitative approach to data protection risk-based compliance from
a data controllers perspective, with the aim of proposing a mindset change,
where data protection impact assessments can be improved by using data
protection analytics, quantitative risk analysis, and calibrating expert
opinions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Range Queries with Correlated Input Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prathamesh Dharangutte, Jie Gao, Ruobin Gong, Guanyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a class of differentially private mechanisms for linear
queries, in particular range queries, that leverages correlated input
perturbation to simultaneously achieve unbiasedness, consistency, statistical
transparency, and control over utility requirements in terms of accuracy
targets expressed either in certain query margins or as implied by the
hierarchical database structure. The proposed Cascade Sampling algorithm
instantiates the mechanism exactly and efficiently. Our theoretical and
empirical analysis demonstrates that we achieve near-optimal utility,
effectively compete with other methods, and retain all the favorable
statistical properties discussed earlier.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine learning approach to brain tumor detection and classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12692v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12692v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Oh, Inyoung Noh, Jian Choo, Jihoo Lee, Justin Park, Kate Hwang, Sanghyeon Kim, Soo Min Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumor detection and classification are critical tasks in medical image
analysis, particularly in early-stage diagnosis, where accurate and timely
detection can significantly improve treatment outcomes. In this study, we apply
various statistical and machine learning models to detect and classify brain
tumors using brain MRI images. We explore a variety of statistical models
including linear, logistic, and Bayesian regressions, and the machine learning
models including decision tree, random forest, single-layer perceptron,
multi-layer perceptron, convolutional neural network (CNN), recurrent neural
network, and long short-term memory. Our findings show that CNN outperforms
other models, achieving the best performance. Additionally, we confirm that the
CNN model can also work for multi-class classification, distinguishing between
four categories of brain MRI images such as normal, glioma, meningioma, and
pituitary tumor images. This study demonstrates that machine learning
approaches are suitable for brain tumor detection and classification,
facilitating real-world medical applications in assisting radiologists with
early and accurate diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Communicate and Collaborate in a Competitive Multi-Agent
  Setup to Clean the Ocean from Macroplastics <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Dominic Siedler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding a balance between collaboration and competition is crucial for
artificial agents in many real-world applications. We investigate this using a
Multi-Agent Reinforcement Learning (MARL) setup on the back of a high-impact
problem. The accumulation and yearly growth of plastic in the ocean cause
irreparable damage to many aspects of oceanic health and the marina system. To
prevent further damage, we need to find ways to reduce macroplastics from known
plastic patches in the ocean. Here we propose a Graph Neural Network (GNN)
based communication mechanism that increases the agents' observation space. In
our custom environment, agents control a plastic collecting vessel. The
communication mechanism enables agents to develop a communication protocol
using a binary signal. While the goal of the agent collective is to clean up as
much as possible, agents are rewarded for the individual amount of
macroplastics collected. Hence agents have to learn to communicate effectively
while maintaining high individual performance. We compare our proposed
communication mechanism with a multi-agent baseline without the ability to
communicate. Results show communication enables collaboration and increases
collective performance significantly. This means agents have learned the
importance of communication and found a balance between collaboration and
competition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tackling Climate Change with Machine Learning Workshop at the 11th
  International Conference on Learning Representations (ICLR 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> RGFN: Synthesizable Molecular Generation Using GFlowNets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michał Koziarski, Andrei Rekesh, Dmytro Shevchuk, Almer van der Sloot, Piotr Gaiński, <span class="highlight-author">Yoshua Bengio</span>, Cheng-Hao Liu, Mike Tyers, Robert A. Batey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models hold great promise for small molecule discovery,
significantly increasing the size of search space compared to traditional in
silico screening libraries. However, most existing machine learning methods for
small molecule generation suffer from poor synthesizability of candidate
compounds, making experimental validation difficult. In this paper we propose
Reaction-GFlowNet (RGFN), an extension of the GFlowNet framework that operates
directly in the space of chemical reactions, thereby allowing out-of-the-box
synthesizability while maintaining comparable quality of generated candidates.
We demonstrate that with the proposed set of reactions and building blocks, it
is possible to obtain a search space of molecules orders of magnitude larger
than existing screening libraries coupled with low cost of synthesis. We also
show that the approach scales to very large fragment libraries, further
increasing the number of potential molecules. We demonstrate the effectiveness
of the proposed approach across a range of oracle models, including pretrained
proxy models and GPU-accelerated docking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Fair Clustering with Group Membership Uncertainty Sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharmila Duppala, Juan Luque, John P. Dickerson, Seyed A. Esmaeili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the canonical fair clustering problem where each cluster is
constrained to have close to population-level representation of each group.
Despite significant attention, the salient issue of having incomplete knowledge
about the group membership of each point has been superficially addressed. In
this paper, we consider a setting where the assigned group memberships are
noisy. We introduce a simple noise model that requires a small number of
parameters to be given by the decision maker. We then present an algorithm for
fair clustering with provable \emph{robustness} guarantees. Our framework
enables the decision maker to trade off between the robustness and the
clustering quality. Unlike previous work, our algorithms are backed by
worst-case theoretical guarantees. Finally, we empirically verify the
performance of our algorithm on real world datasets and show its superior
performance over existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-Aware Dynamic Neural Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcello Bullo, Seifallah Jardak, Pietro Carnelli, Deniz Gündüz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing demand for intelligent applications beyond the network edge,
coupled with the need for sustainable operation, are driving the seamless
integration of deep learning (DL) algorithms into energy-limited, and even
energy-harvesting end-devices. However, the stochastic nature of ambient energy
sources often results in insufficient harvesting rates, failing to meet the
energy requirements for inference and causing significant performance
degradation in energy-agnostic systems. To address this problem, we consider an
on-device adaptive inference system equipped with an energy-harvester and
finite-capacity energy storage. We then allow the device to reduce the run-time
execution cost on-demand, by either switching between differently-sized neural
networks, referred to as multi-model selection (MMS), or by enabling earlier
predictions at intermediate layers, called early exiting (EE). The model to be
employed, or the exit point is then dynamically chosen based on the energy
storage and harvesting process states. We also study the efficacy of
integrating the prediction confidence into the decision-making process. We
derive a principled policy with theoretical guarantees for confidence-aware and
-agnostic controllers. Moreover, in multi-exit networks, we study the
advantages of taking decisions incrementally, exit-by-exit, by designing a
lightweight reinforcement learning-based controller. Experimental results show
that, as the rate of the ambient energy increases, energy- and confidence-aware
control schemes show approximately 5% improvement in accuracy compared to their
energy-aware confidence-agnostic counterparts. Incremental approaches achieve
even higher accuracy, particularly when the energy storage capacity is limited
relative to the energy consumption of the inference model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright}2024 IEEE. This work has been submitted to the IEEE for
  possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Persistent de Rham-Hodge Laplacians in Eulerian representation for
  manifold topological learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Su, Yiying Tong, Guo-Wei Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, topological data analysis has become a trending topic in data
science and engineering. However, the key technique of topological data
analysis, i.e., persistent homology, is defined on point cloud data, which does
not work directly for data on manifolds. Although earlier evolutionary de
Rham-Hodge theory deals with data on manifolds, it is inconvenient for machine
learning applications because of the numerical inconsistency caused by
remeshing the involving manifolds in the Lagrangian representation. In this
work, we introduce persistent de Rham-Hodge Laplacian, or persistent Hodge
Laplacian (PHL) as an abbreviation, for manifold topological learning. Our PHLs
are constructed in the Eulerian representation via structure-persevering
Cartesian grids, avoiding the numerical inconsistency over the multiscale
manifolds. To facilitate the manifold topological learning, we propose a
persistent Hodge Laplacian learning algorithm for data on manifolds or
volumetric data. As a proof-of-principle application of the proposed manifold
topological learning model, we consider the prediction of protein-ligand
binding affinities with two benchmark datasets. Our numerical experiments
highlight the power and promise of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Monitoring fairness in machine learning models that predict patient
  mortality in the ICU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tempest A. van Schaik, Xinggang Liu, Louis Atallah, Omar Badawi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a fairness monitoring approach for machine learning models
that predict patient mortality in the ICU. We investigate how well models
perform for patient groups with different race, sex and medical diagnoses. We
investigate Documentation bias in clinical measurement, showing how fairness
analysis provides a more detailed and insightful comparison of model
performance than traditional accuracy metrics alone.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic contextual bandits with graph feedback: from independence
  number to MAS number 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18591v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18591v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiao Wen, Yanjun Han, Zhengyuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider contextual bandits with graph feedback, a class of interactive
learning problems with richer structures than vanilla contextual bandits, where
taking an action reveals the rewards for all neighboring actions in the
feedback graph under all contexts. Unlike the multi-armed bandits setting where
a growing literature has painted a near-complete understanding of graph
feedback, much remains unexplored in the contextual bandits counterpart. In
this paper, we make inroads into this inquiry by establishing a regret lower
bound $\Omega(\sqrt{\beta_M(G) T})$, where $M$ is the number of contexts, $G$
is the feedback graph, and $\beta_M(G)$ is our proposed graph-theoretic
quantity that characterizes the fundamental learning limit for this class of
problems. Interestingly, $\beta_M(G)$ interpolates between $\alpha(G)$ (the
independence number of the graph) and $\mathsf{m}(G)$ (the maximum acyclic
subgraph (MAS) number of the graph) as the number of contexts $M$ varies. We
also provide algorithms that achieve near-optimal regret for important classes
of context sequences and/or feedback graphs, such as transitively closed graphs
that find applications in auctions and inventory control. In particular, with
many contexts, our results show that the MAS number essentially characterizes
the statistical complexity for contextual bandits, as opposed to the
independence number in multi-armed bandits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph
  Representational Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raffaele Paolino, Sohir Maskey, Pascal Welke, Gitta Kutyniok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce $r$-loopy Weisfeiler-Leman ($r$-$\ell{}$WL), a novel hierarchy
of graph isomorphism tests and a corresponding GNN framework, $r$-$\ell{}$MPNN,
that can count cycles up to length $r + 2$. Most notably, we show that
$r$-$\ell{}$WL can count homomorphisms of cactus graphs. This strictly extends
classical 1-WL, which can only count homomorphisms of trees and, in fact, is
incomparable to $k$-WL for any fixed $k$. We empirically validate the
expressive and counting power of the proposed $r$-$\ell{}$MPNN on several
synthetic datasets and present state-of-the-art predictive performance on
various real-world datasets. The code is available at
https://github.com/RPaolino/loopy
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (Oral). The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Depth Separations in Neural Networks: Separating the Dimension from the
  Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itay Safran, Daniel Reichman, Paul Valiant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove an exponential size separation between depth 2 and depth 3 neural
networks (with real inputs), when approximating a $\mathcal{O}(1)$-Lipschitz
target function to constant accuracy, with respect to a distribution with
support in the unit ball, under the mild assumption that the weights of the
depth 2 network are exponentially bounded. This resolves an open problem posed
in \citet{safran2019depth}, and proves that the curse of dimensionality
manifests itself in depth 2 approximation, even in cases where the target
function can be represented efficiently using a depth 3 network. Previously,
lower bounds that were used to separate depth 2 from depth 3 networks required
that at least one of the Lipschitz constant, target accuracy or (some measure
of) the size of the domain of approximation scale \emph{polynomially} with the
input dimension, whereas in our result these parameters are fixed to be
\emph{constants} independent of the input dimension: our parameters are
simultaneously optimal. Our lower bound holds for a wide variety of activation
functions, and is based on a novel application of a worst- to average-case
random self-reducibility argument, allowing us to leverage depth 2 threshold
circuits lower bounds in a new domain.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of
  Study in Tabletop Role-Playing Games Soundtracks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felipe Marra, Lucas N. Ferreira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the capabilities of text-to-audio music generation
models in producing long-form music with prompts that change over time,
focusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We
introduce Babel Bardo, a system that uses Large Language Models (LLMs) to
transform speech transcriptions into music descriptions for controlling a
text-to-music model. Four versions of Babel Bardo were compared in two TRPG
campaigns: a baseline using direct speech transcriptions, and three LLM-based
versions with varying approaches to music description generation. Evaluations
considered audio quality, story alignment, and transition smoothness. Results
indicate that detailed music descriptions improve audio quality while
maintaining consistency across consecutive descriptions enhances story
alignment and transition smoothness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at the LAMIR 2024 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inter-Frame Coding for Dynamic Meshes via Coarse-to-Fine Anchor Mesh
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Huang, Lizhi Hou, Qi Yang, Yiling Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the current Video-based Dynamic Mesh Coding (V-DMC) standard, inter-frame
coding is restricted to mesh frames with constant topology. Consequently,
temporal redundancy is not fully leveraged, resulting in suboptimal compression
efficacy. To address this limitation, this paper introduces a novel
coarse-to-fine scheme to generate anchor meshes for frames with time-varying
topology. Initially, we generate a coarse anchor mesh using an octree-based
nearest neighbor search. Motion estimation compensates for regions with
significant motion changes during this process. However, the quality of the
coarse mesh is low due to its suboptimal vertices. To enhance details, the fine
anchor mesh is further optimized using the Quadric Error Metrics (QEM)
algorithm to calculate more precise anchor points. The inter-frame anchor mesh
generated herein retains the connectivity of the reference base mesh, while
concurrently preserving superior quality. Experimental results show that our
method achieves 7.2% ~ 10.3% BD-rate gain compared to the existing V-DMC test
model version 7.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Conceptual Blending of a <span class="highlight-title">Diffusion</span> Model for Improving
  Nonword-to-Image Generation <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihaya Matsuhira, Marc A. Kastner, Takahiro Komamizu, Takatsugu Hirayama, Ichiro Ide
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models sometimes depict blended concepts in the
generated images. One promising use case of this effect would be the
nonword-to-image generation task which attempts to generate images intuitively
imaginable from a non-existing word (nonword). To realize nonword-to-image
generation, an existing study focused on associating nonwords with
similar-sounding words. Since each nonword can have multiple similar-sounding
words, generating images containing their blended concepts would increase
intuitiveness, facilitating creative activities and promoting computational
psycholinguistics. Nevertheless, no existing study has quantitatively evaluated
this effect in either diffusion models or the nonword-to-image generation
paradigm. Therefore, this paper first analyzes the conceptual blending in a
pretrained diffusion model, Stable Diffusion. The analysis reveals that a high
percentage of generated images depict blended concepts when inputting an
embedding interpolating between the text embeddings of two text prompts
referring to different concepts. Next, this paper explores the best text
embedding space conversion method of an existing nonword-to-image generation
framework to ensure both the occurrence of conceptual blending and image
generation quality. We compare the conventional direct prediction approach with
the proposed method that combines $k$-nearest neighbor search and linear
regression. Evaluation reveals that the enhanced accuracy of the embedding
space conversion by the proposed method improves the image generation quality,
while the emergence of conceptual blending could be attributed mainly to the
specific dimensions of the high-dimensional text embedding space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at ACM MM 2024 (doi: 10.1145/3664647.3681202) with
  supplementary materials concatenated</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM
  Data Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting dataset contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is sensitive to varying degrees of contamination and can
highlight significant performance improvements due to leakage of the training
set of multimodal benchmarks. Furthermore, We also explore the possibility of
contamination originating from the pre-training phase of LLMs used by MLLMs and
the fine-tuning phase of MLLMs, offering new insights into the stages at which
contamination may be introduced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Routing Experts: Learning to Route Dynamic Experts in Multi-modal Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Wu, Zhaoxi Ke, Yiyi Zhou, Gen Luo, Xiaoshuai Sun, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, mixture of experts (MoE) has become a popular paradigm for
achieving the trade-off between modal capacity and efficiency of multi-modal
large language models (MLLMs). Different from previous efforts, we are
dedicated to exploring the dynamic expert path in an already exist MLLM and
show that a standard MLLM can be also a mixture of experts. To approach this
target, we propose a novel dynamic expert scheme for MLLMs, termed Routing
Experts (RoE), which can achieve example-dependent optimal path routing without
obvious structure tweaks. Meanwhile, a new regularization of structure sparsity
is also introduced to enforce MLLMs to learn more short-cut inference, ensuring
the efficiency. In addition, we also realize the first attempt of aligning the
training and inference schemes of MLLMs in terms of network routing. To
validate RoE, we apply it to a set of latest MLLMs, including LLaVA-1.5,
LLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL
benchmarks. The experiment results not only show the great advantages of our
RoE in improving MLLMs' efficiency, but also yield obvious advantages than
MoE-LLaVA in both performance and speed, e.g., an average performance gain of
3.3% on 5 benchmarks while being faster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Single-Audio: Advancing Multi-Audio Processing in Audio Large
  Language Models <span class="chip">EMNLP24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18680v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18680v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Chen, Xianghu Yue, Xiaoxue Gao, Chen Zhang, Luis Fernando D'Haro, Robby T. Tan, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various audio-LLMs (ALLMs) have been explored recently for tackling different
audio tasks simultaneously using a single, unified model. While existing
evaluations of ALLMs primarily focus on single-audio tasks, real-world
applications often involve processing multiple audio streams simultaneously. To
bridge this gap, we propose the first multi-audio evaluation (MAE) benchmark
that consists of 20 datasets from 11 multi-audio tasks encompassing both speech
and sound scenarios. Comprehensive experiments on MAE demonstrate that the
existing ALLMs, while being powerful in comprehending primary audio elements in
individual audio inputs, struggling to handle multi-audio scenarios. To this
end, we propose a novel multi-audio-LLM (MALLM) to capture audio context among
multiple similar audios using discriminative learning on our proposed synthetic
data. The results demonstrate that the proposed MALLM outperforms all baselines
and achieves high data efficiency using synthetic data without requiring human
annotations. The proposed MALLM opens the door for ALLMs towards multi-audio
processing era and brings us closer to replicating human auditory capabilities
in machines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP24 Findings. Data available at
  https://github.com/MatthewCYM/MALLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Document Parsing Unveiled: Techniques, Challenges, and Prospects for
  Structured Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21169v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21169v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qintong Zhang, Victor Shea-Jay Huang, Bin Wang, Junyuan Zhang, Zhengren Wang, Hao Liang, Shawn Wang, Matthieu Lin, Conghui He, Wentao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document parsing is essential for converting unstructured and semi-structured
documents-such as contracts, academic papers, and invoices-into structured,
machine-readable data. Document parsing extract reliable structured data from
unstructured inputs, providing huge convenience for numerous applications.
Especially with recent achievements in Large Language Models, document parsing
plays an indispensable role in both knowledge base construction and training
data generation. This survey presents a comprehensive review of the current
state of document parsing, covering key methodologies, from modular pipeline
systems to end-to-end models driven by large vision-language models. Core
components such as layout detection, content extraction (including text,
tables, and mathematical expressions), and multi-modal data integration are
examined in detail. Additionally, this paper discusses the challenges faced by
modular document parsing systems and vision-language models in handling complex
layouts, integrating multiple modules, and recognizing high-density text. It
emphasizes the importance of developing larger and more diverse datasets and
outlines future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiently Collecting Training <span class="highlight-title">Dataset</span> for 2D Object Detection by
  Online Visual Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuya Kiyokawa, Naoki Shirakura, Hiroki Katayama, Keita Tomochika, Jun Takamatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep-learning-based vision systems require the manual annotation of
a significant number of images. Such manual annotation is highly time-consuming
and labor-intensive. Although previous studies have attempted to eliminate the
effort required for annotation, the effort required for image collection was
retained. To address this, we propose a human-in-the-loop dataset collection
method that uses a web application. To counterbalance the workload and
performance by encouraging the collection of multi-view object image datasets
in an enjoyable manner, thereby amplifying motivation, we propose three types
of online visual feedback features to track the progress of the collection
status. Our experiments thoroughly investigated the impact of each feature on
collection performance and quality of operation. The results suggested the
feasibility of annotation and object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferable Learned Image Compression-Resistant Adversarial
  Perturbations <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Sui, Zhuohang Li, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, Zhenzhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks can readily disrupt the image classification system,
revealing the vulnerability of DNN-based recognition tasks. While existing
adversarial perturbations are primarily applied to uncompressed images or
compressed images by the traditional image compression method, i.e., JPEG,
limited studies have investigated the robustness of models for image
classification in the context of DNN-based image compression. With the rapid
evolution of advanced image compression, DNN-based learned image compression
has emerged as the promising approach for transmitting images in many
security-critical applications, such as cloud-based face recognition and
autonomous driving, due to its superior performance over traditional
compression. Therefore, there is a pressing need to fully investigate the
robustness of a classification system post-processed by learned image
compression. To bridge this research gap, we explore the adversarial attack on
a new pipeline that targets image classification models that utilize learned
image compressors as pre-processing modules. Furthermore, to enhance the
transferability of perturbations across various quality levels and
architectures of learned image compression models, we introduce a saliency
score-based sampling method to enable the fast generation of transferable
perturbation. Extensive experiments with popular attack methods demonstrate the
enhanced transferability of our proposed method when attacking images that have
been post-processed with different learned image compression models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by BMVC 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-05T00:00:00Z">2024-11-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">76</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Write Rationally: How Information Is Distributed in
  Non-Native Speakers' Essays <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixin Tang, Janet G. van Hell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People tend to distribute information evenly in language production for
better and clearer communication. In this study, we compared essays written by
second language learners with various native language (L1) backgrounds to
investigate how they distribute information in their non-native language (L2)
production. Analyses of surprisal and constancy of entropy rate indicated that
writers with higher L2 proficiency can reduce the expected uncertainty of
language production while still conveying informative content. However, the
uniformity of information distribution showed less variability among different
groups of L2 speakers, suggesting that this feature may be universal in L2
essay writing and less affected by L2 writers' variability in L1 background and
L2 proficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in main of Conference on Empirical Methods in Natural
  Language Processing; EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Benefits of Domain-Pretraining of Generative Large
  Language Models for Chemistry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anurag Acharya, Shivam Sharma, Robin Cosbey, Megha Subramanian, Scott Howland, Maria Glenski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A proliferation of Large Language Models (the GPT series, BLOOM, LLaMA, and
more) are driving forward novel development of multipurpose AI for a variety of
tasks, particularly natural language processing (NLP) tasks. These models
demonstrate strong performance on a range of tasks; however, there has been
evidence of brittleness when applied to more niche or narrow domains where
hallucinations or fluent but incorrect responses reduce performance. Given the
complex nature of scientific domains, it is prudent to investigate the
trade-offs of leveraging off-the-shelf versus more targeted foundation models
for scientific domains. In this work, we examine the benefits of in-domain
pre-training for a given scientific domain, chemistry, and compare these to
open-source, off-the-shelf models with zero-shot and few-shot prompting. Our
results show that not only do in-domain base models perform reasonably well on
in-domain tasks in a zero-shot setting but that further adaptation using
instruction fine-tuning yields impressive performance on chemistry-specific
tasks such as named entity recognition and molecular formula generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long Context RAG Performance of Large Language Models <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, Michael Carbin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) has emerged as a crucial technique for
enhancing the accuracy of Large Language Models (LLMs) by incorporating
external information. With the advent of LLMs that support increasingly longer
context lengths, there is a growing interest in understanding how these models
perform in RAG scenarios. Can these new long context models improve RAG
performance? This paper presents a comprehensive study of the impact of
increased context length on RAG performance across 20 popular open source and
commercial LLMs. We ran RAG workflows while varying the total context length
from 2,000 to 128,000 tokens (and 2 million tokens when possible) on three
domain-specific datasets, and report key insights on the benefits and
limitations of long context in RAG applications. Our findings reveal that while
retrieving more documents can improve performance, only a handful of the most
recent state of the art LLMs can maintain consistent accuracy at long context
above 64k tokens. We also identify distinct failure modes in long context
scenarios, suggesting areas for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 NeurIPS workshop on Adaptive Foundation Models: Evolving AI for
  Personalized and Efficient Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Metric Bias in Minimum Bayes Risk Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geza Kovacs, Daniel Deutsch, Markus Freitag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Minimum Bayes Risk (MBR) decoding using metrics such as COMET or
MetricX has outperformed traditional decoding methods such as greedy or beam
search, it introduces a challenge we refer to as metric bias. As MBR decoding
aims to produce translations that score highly according to a specific utility
metric, this very process makes it impossible to use the same metric for both
decoding and evaluation, as improvements might simply be due to reward hacking
rather than reflecting real quality improvements. In this work we find that
compared to human ratings, neural metrics not only overestimate the quality of
MBR decoding when the same metric is used as the utility metric, but they also
overestimate the quality of MBR/QE decoding with other neural utility metrics
as well. We also show that the metric bias issue can be mitigated by using an
ensemble of utility metrics during MBR decoding: human evaluations show that
MBR decoding using an ensemble of utility metrics outperforms a single utility
metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at WMT2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Change Is the Only Constant: Dynamic LLM Slicing based on Layer
  Redundancy <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razvan-Gabriel Dumitru, Paul-Ioan Clotan, Vikas Yadav, Darius Peteleaza, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel model compression approach through dynamic
layer-specific pruning in Large Language Models (LLMs), enhancing the
traditional methodology established by SliceGPT. By transitioning from constant
to dynamic slicing, our method leverages the newly proposed Layer Redundancy
(LR) score, which assesses how much change each layer changes its input by
measuring the cosine similarity of the input to the output of the layer. We use
this score to prune parts of individual layers based on redundancy in such a
way that the average pruned percentage for all layers is a fixed value. We
conducted extensive experiments using models like Llama3-8B and Mistral-7B on
multiple datasets, evaluating different slicing bases and percentages to
determine optimal configurations that balance efficiency and performance. Our
findings show that our dynamic slicing approach not only maintains but, in many
cases, enhances model performance compared to the baseline established by
constant slicing methods. For instance, in several settings, we see performance
improvements of up to 5% over the SliceGPT baseline. Additionally, a perplexity
decrease by as much as 7% was observed across multiple benchmarks, validating
the effectiveness of our method. The code, model weights, and datasets are
open-sourced at https://github.com/RazvanDu/DynamicSlicing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Quantification for Clinical Outcome Predictions with (Large)
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhang Chen, Peizhao Li, Xiaomeng Dong, Pengyu Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To facilitate healthcare delivery, language models (LMs) have significant
potential for clinical prediction tasks using electronic health records (EHRs).
However, in these high-stakes applications, unreliable decisions can result in
high costs due to compromised patient safety and ethical concerns, thus
increasing the need for good uncertainty modeling of automated clinical
predictions. To address this, we consider the uncertainty quantification of LMs
for EHR tasks in white- and black-box settings. We first quantify uncertainty
in white-box models, where we can access model parameters and output logits. We
show that an effective reduction of model uncertainty can be achieved by using
the proposed multi-tasking and ensemble methods in EHRs. Continuing with this
idea, we extend our approach to black-box settings, including popular
proprietary LMs such as GPT-4. We validate our framework using longitudinal
clinical data from more than 6,000 patients in ten clinical prediction tasks.
Results show that ensembling methods and multi-task prediction prompts reduce
uncertainty across different scenarios. These findings increase the
transparency of the model in white-box and black-box settings, thus advancing
reliable AI healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Generation of Question Hints for Mathematics Problems using
  Large Language Models in Educational Technology <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junior Cedric Tonga, Benjamin Clement, Pierre-Yves Oudeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automatic generation of hints by Large Language Models (LLMs) within
Intelligent Tutoring Systems (ITSs) has shown potential to enhance student
learning. However, generating pedagogically sound hints that address student
misconceptions and adhere to specific educational objectives remains
challenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as
teachers to generate effective hints for students simulated through LLMs
(GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math
exercises designed for human high-school students, and designed using cognitive
science principles. We present here the study of several dimensions: 1)
identifying error patterns made by simulated students on secondary-level math
exercises; 2) developing various prompts for GPT-4o as a teacher and evaluating
their effectiveness in generating hints that enable simulated students to
self-correct; and 3) testing the best-performing prompts, based on their
ability to produce relevant hints and facilitate error correction, with
Llama-3-8B-Instruct as the teacher, allowing for a performance comparison with
GPT-4o. The results show that model errors increase with higher temperature
settings. Notably, when hints are generated by GPT-4o, the most effective
prompts include prompts tailored to specific errors as well as prompts
providing general hints based on common mathematical errors. Interestingly,
Llama-3-8B-Instruct as a teacher showed better overall performance than GPT-4o.
Also the problem-solving and response revision capabilities of the LLMs as
students, particularly GPT-3.5-turbo, improved significantly after receiving
hints, especially at lower temperature settings. However, models like
Mistral-7B-Instruct demonstrated a decline in performance as the temperature
increased.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024 Workshop on Large Foundation Models for
  Educational Assessment (FM-Assess)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LASER: Attention with Exponential Transformation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Surya Duvvuri, Inderjit S. Dhillon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have had tremendous impact for several sequence related tasks,
largely due to their ability to retrieve from any part of the sequence via
softmax based dot-product attention. This mechanism plays a crucial role in
Transformer's performance. We analyze the gradients backpropagated through the
softmax operation in the attention mechanism and observe that these gradients
can often be small. This poor gradient signal backpropagation can lead to
inefficient learning of parameters preceeding the attention operations. To this
end, we introduce a new attention mechanism called LASER, which we analytically
show to admit a larger gradient signal. We show that LASER Attention can be
implemented by making small modifications to existing attention
implementations. We conduct experiments on autoregressive large language models
(LLMs) with upto 2.2 billion parameters where we show upto 3.38% and an average
of ~1% improvement over standard attention on downstream evaluations. Using
LASER gives the following relative improvements in generalization performance
across a variety of tasks (vision, text and speech): 4.67% accuracy in Vision
Transformer (ViT) on Imagenet, 2.25% error rate in Conformer on the Librispeech
speech-to-text and 0.93% fraction of incorrect predictions in BERT with 2.2
billion parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, under review in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Generated Distribution-Based Prediction of US Electoral Results,
  Part I 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caleb Bradshaw, Caelen Miller, Sean Warnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces distribution-based prediction, a novel approach to
using Large Language Models (LLMs) as predictive tools by interpreting output
token probabilities as distributions representing the models' learned
representation of the world. This distribution-based nature offers an
alternative perspective for analyzing algorithmic fidelity, complementing the
approach used in silicon sampling. We demonstrate the use of distribution-based
prediction in the context of recent United States presidential election,
showing that this method can be used to determine task specific bias, prompt
noise, and algorithmic fidelity. This approach has significant implications for
assessing the reliability and increasing transparency of LLM-based predictions
across various domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 Figures, Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manar Abdelatty, Jingxiao Ma, Sherief Reda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been applied to various hardware design
tasks, including Verilog code generation, EDA tool scripting, and RTL bug
fixing. Despite this extensive exploration, LLMs are yet to be used for the
task of post-synthesis metric reasoning and estimation of HDL designs. In this
paper, we assess the ability of LLMs to reason about post-synthesis metrics of
Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868
Verilog HDL designs and their corresponding post-synthesis metrics, namely
area, delay, and static power. MetRex incorporates a Chain of Thought (CoT)
template to enhance LLMs' reasoning about these metrics. Extensive experiments
show that Supervised Fine-Tuning (SFT) boosts the LLM's reasoning capabilities
on average by 37.0\%, 25.3\%, and 25.7\% on the area, delay, and static power,
respectively. While SFT improves performance on our benchmark, it remains far
from achieving optimal results, especially on complex problems. Comparing to
state-of-the-art regression models, our approach delivers accurate
post-synthesis predictions for 17.4\% more designs (within a 5\% error margin),
in addition to offering a 1.7x speedup by eliminating the need for
pre-processing. This work lays the groundwork for advancing LLM-based Verilog
code metric reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Trojan Detection Competitions with Linear Weight Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Todd Huster, Peter Lin, Razvan Stefanescu, Emmanuel Ekwedike, Ritu Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks can conceal malicious Trojan backdoors that allow a trigger
to covertly change the model behavior. Detecting signs of these backdoors,
particularly without access to any triggered data, is the subject of ongoing
research and open challenges. In one common formulation of the problem, we are
given a set of clean and poisoned models and need to predict whether a given
test model is clean or poisoned. In this paper, we introduce a detector that
works remarkably well across many of the existing datasets and domains. It is
obtained by training a binary classifier on a large number of models' weights
after performing a few different pre-processing steps including feature
selection and standardization, reference model weights subtraction, and model
alignment prior to detection. We evaluate this algorithm on a diverse set of
Trojan detection benchmarks and domains and examine the cases where the
approach is most and least effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MME-Finance: A Multimodal Finance Benchmark for Expert-level
  Understanding and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziliang Gan, Yu Lu, Dong Zhang, Haohan Li, Che Liu, Jian Liu, Ji Liu, Haipang Wu, Chaoyou Fu, Zenglin Xu, Rongjunchen Zhang, Yong Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, multimodal benchmarks for general domains have guided the
rapid development of multimodal models on general tasks. However, the financial
field has its peculiarities. It features unique graphical images (e.g.,
candlestick charts, technical indicator charts) and possesses a wealth of
specialized financial knowledge (e.g., futures, turnover rate). Therefore,
benchmarks from general fields often fail to measure the performance of
multimodal models in the financial domain, and thus cannot effectively guide
the rapid development of large financial models. To promote the development of
large financial multimodal models, we propose MME-Finance, an bilingual
open-ended and practical usage-oriented Visual Question Answering (VQA)
benchmark. The characteristics of our benchmark are finance and expertise,
which include constructing charts that reflect the actual usage needs of users
(e.g., computer screenshots and mobile photography), creating questions
according to the preferences in financial domain inquiries, and annotating
questions by experts with 10+ years of experience in the financial industry.
Additionally, we have developed a custom-designed financial evaluation system
in which visual information is first introduced in the multi-modal evaluation
process. Extensive experimental evaluations of 19 mainstream MLLMs are
conducted to test their perception, reasoning, and cognition capabilities. The
results indicate that models performing well on general benchmarks cannot do
well on MME-Finance; for instance, the top-performing open-source and
closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o),
respectively. Their performance is particularly poor in categories most
relevant to finance, such as candlestick charts and technical indicator charts.
In addition, we propose a Chinese version, which helps compare performance of
MLLMs under a Chinese context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://hithink-research.github.io/MME-Finance/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Usefulness of LLMs as an Author Checklist Assistant for Scientific
  Papers: NeurIPS'24 Experiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Goldberg, Ihsan Ullah, Thanh Gia Hieu Khuong, Benedictus Kent Rachmat, Zhen Xu, Isabelle Guyon, Nihar B. Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) represent a promising, but controversial, tool
in aiding scientific peer review. This study evaluates the usefulness of LLMs
in a conference setting as a tool for vetting paper submissions against
submission standards. We conduct an experiment at the 2024 Neural Information
Processing Systems (NeurIPS) conference, where 234 papers were voluntarily
submitted to an "LLM-based Checklist Assistant." This assistant validates
whether papers adhere to the author checklist used by NeurIPS, which includes
questions to ensure compliance with research and manuscript preparation
standards. Evaluation of the assistant by NeurIPS paper authors suggests that
the LLM-based assistant was generally helpful in verifying checklist
completion. In post-usage surveys, over 70% of authors found the assistant
useful, and 70% indicate that they would revise their papers or checklist
responses based on its feedback. While causal attribution to the assistant is
not definitive, qualitative evidence suggests that the LLM contributed to
improving some submissions. Survey responses and analysis of re-submissions
indicate that authors made substantive revisions to their submissions in
response to specific feedback from the LLM. The experiment also highlights
common issues with LLMs: inaccuracy (20/52) and excessive strictness (14/52)
were the most frequent issues flagged by authors. We also conduct experiments
to understand potential gaming of the system, which reveal that the assistant
could be manipulated to enhance scores through fabricated justifications,
highlighting potential vulnerabilities of automated review tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAUCE: Synchronous and Asynchronous User-Customizable Environment for
  Multi-Agent LLM Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shlomo Neuberger, Niv Eckhaus, Uri Berger, Amir Taubenfeld, Gabriel Stanovsky, Ariel Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many human interactions, such as political debates, are carried out in group
settings, where there are arbitrarily many participants, each with different
views and agendas. To explore such complex social settings, we present SAUCE: a
customizable Python platform, allowing researchers to plug-and-play various
LLMs participating in discussions on any topic chosen by the user. Our platform
takes care of instantiating the models, scheduling their responses, managing
the discussion history, and producing a comprehensive output log, all
customizable through configuration files, requiring little to no coding skills.
A novel feature of SAUCE is our asynchronous communication feature, where
models decide when to speak in addition to what to say, thus modeling an
important facet of human communication. We show SAUCE's attractiveness in two
initial experiments, and invite the community to use it in simulating various
group simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/Deep-Cognition-Lab/SAUCE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Large Language Models for Specialist-level Oncology Care 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Palepu, Vikram Dhillon, Polly Niravath, Wei-Hung Weng, Preethi Prasad, Khaled Saab, Ryutaro Tanno, Yong Cheng, Hanh Mai, Ethan Burns, Zainub Ajmal, Kavita Kulkarni, Philip Mansfield, Dale Webster, Joelle Barral, Juraj Gottweis, Mike Schaekermann, S. Sara Mahdavi, Vivek Natarajan, Alan Karthikesalingam, Tao Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable progress in encoding
clinical knowledge and responding to complex medical queries with appropriate
clinical reasoning. However, their applicability in subspecialist or complex
medical settings remains underexplored. In this work, we probe the performance
of AMIE, a research conversational diagnostic AI system, in the subspecialist
domain of breast oncology care without specific fine-tuning to this challenging
domain. To perform this evaluation, we curated a set of 50 synthetic breast
cancer vignettes representing a range of treatment-naive and
treatment-refractory cases and mirroring the key information available to a
multidisciplinary tumor board for decision-making (openly released with this
work). We developed a detailed clinical rubric for evaluating management plans,
including axes such as the quality of case summarization, safety of the
proposed care plan, and recommendations for chemotherapy, radiotherapy, surgery
and hormonal therapy. To improve performance, we enhanced AMIE with the
inference-time ability to perform web search retrieval to gather relevant and
up-to-date clinical knowledge and refine its responses with a multi-stage
self-critique pipeline. We compare response quality of AMIE with internal
medicine trainees, oncology fellows, and general oncology attendings under both
automated and specialist clinician evaluations. In our evaluations, AMIE
outperformed trainees and fellows demonstrating the potential of the system in
this challenging and important domain. We further demonstrate through
qualitative examples, how systems such as AMIE might facilitate conversational
interactions to assist clinicians in their decision making. However, AMIE's
performance was overall inferior to attending oncologists suggesting that
further research is needed prior to consideration of prospective uses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs for Domain Generation Algorithm Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reynier Leyva La O, Carlos A. Catania, Tatiana Parlanti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work analyzes the use of large language models (LLMs) for detecting
domain generation algorithms (DGAs). We perform a detailed evaluation of two
important techniques: In-Context Learning (ICL) and Supervised Fine-Tuning
(SFT), showing how they can improve detection. SFT increases performance by
using domain-specific data, whereas ICL helps the detection model to quickly
adapt to new threats without requiring much retraining. We use Meta's Llama3 8B
model, on a custom dataset with 68 malware families and normal domains,
covering several hard-to-detect schemes, including recent word-based DGAs.
Results proved that LLM-based methods can achieve competitive results in DGA
detection. In particular, the SFT-based LLM DGA detector outperforms
state-of-the-art models using attention layers, achieving 94% accuracy with a
4% false positive rate (FPR) and excelling at detecting word-based DGA domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VERITAS: A Unified Approach to Reliability Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajkumar Ramamurthy, Meghana Arakkal Rajeev, Oliver Molenschot, James Zou, Nazneen Rajani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often fail to synthesize information from their
context to generate an accurate response. This renders them unreliable in
knowledge intensive settings where reliability of the output is key. A critical
component for reliable LLMs is the integration of a robust fact-checking system
that can detect hallucinations across various formats. While several
open-access fact-checking models are available, their functionality is often
limited to specific tasks, such as grounded question-answering or entailment
verification, and they perform less effectively in conversational settings. On
the other hand, closed-access models like GPT-4 and Claude offer greater
flexibility across different contexts, including grounded dialogue
verification, but are hindered by high costs and latency. In this work, we
introduce VERITAS, a family of hallucination detection models designed to
operate flexibly across diverse contexts while minimizing latency and costs.
VERITAS achieves state-of-the-art results considering average performance on
all major hallucination detection benchmarks, with $10\%$ increase in average
performance when compared to similar-sized models and get close to the
performance of GPT4 turbo with LLM-as-a-judge setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMoA: Improving Multi-agent Large Language Models with Sparse
  Mixture-of-Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Kumar Satvik Chaudhary, Lijie Hu, Jiayi Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-agent systems have been shown to significantly enhance the
performance of Large Language Models (LLMs) across various tasks and
applications, the dense interaction between scaling agents potentially hampers
their efficiency and diversity. To address these challenges, we draw
inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse
mixture-of-agents (SMoA) framework to improve the efficiency and diversity of
multi-agent LLMs. Unlike completely connected structures, SMoA introduces novel
Response Selection and Early Stopping mechanisms to sparsify information flows
among individual LLM agents, striking a balance between performance and
efficiency. Additionally, inspired by the expert diversity principle in SMoE
frameworks for workload balance between experts, we assign distinct role
descriptions to each LLM agent, fostering diverse and divergent thinking.
Extensive experiments on reasoning, alignment, and fairness benchmarks
demonstrate that SMoA achieves performance comparable to traditional
mixture-of-agents approaches but with significantly lower computational costs.
Further analysis reveals that SMoA is more stable, has a greater capacity to
scale, and offers considerable potential through hyper-parameter optimization.
Code and data will be available at: https://github.com/David-Li0406/SMoA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffLM: Controllable Synthetic Data Generation via <span class="highlight-title">Diffusion</span> Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Zhou, Xinyao Wang, Yulei Niu, Yaojie Shen, Lexin Tang, Fan Chen, Ben He, Le Sun, Longyin Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have significantly
enhanced their knowledge and generative capabilities, leading to a surge of
interest in leveraging LLMs for high-quality data synthesis. However, synthetic
data generation via prompting LLMs remains challenging due to LLMs' limited
understanding of target data distributions and the complexity of prompt
engineering, especially for structured formatted data. To address these issues,
we introduce DiffLM, a controllable data synthesis framework based on
variational autoencoder (VAE), which further (1) leverages diffusion models to
reserve more information of original distribution and format structure in the
learned latent distribution and (2) decouples the learning of target
distribution knowledge from the LLM's generative objectives via a plug-and-play
latent feature injection module. As we observed significant discrepancies
between the VAE's latent representations and the real data distribution, the
latent diffusion module is introduced into our framework to learn a fully
expressive latent distribution. Evaluations on seven real-world datasets with
structured formatted data (i.e., Tabular, Code and Tool data) demonstrate that
DiffLM generates high-quality data, with performance on downstream tasks
surpassing that of real data by 2-7 percent in certain cases. The data and code
will be publicly available upon completion of internal review.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictor-Corrector Enhanced <span class="highlight-title">Transformer</span>s with Exponential Moving
  Average Coefficient Learning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bei Li, Tong Zheng, Rui Wang, Jiahao Liu, Qingyan Guo, Junliang Guo, Xu Tan, Tong Xiao, Jingbo Zhu, Jingang Wang, Xunliang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Residual networks, as discrete approximations of Ordinary Differential
Equations (ODEs), have inspired significant advancements in neural network
design, including multistep methods, high-order methods, and multi-particle
dynamical systems. The precision of the solution to ODEs significantly affects
parameter optimization, thereby impacting model performance. In this work, we
present a series of advanced explorations of Transformer architecture design to
minimize the error compared to the true ``solution.'' First, we introduce a
predictor-corrector learning framework to minimize truncation errors, which
consists of a high-order predictor and a multistep corrector. Second, we
propose an exponential moving average-based coefficient learning method to
strengthen our higher-order predictor. Extensive experiments on large-scale
machine translation, abstractive summarization, language modeling, and natural
language understanding benchmarks demonstrate the superiority of our approach.
On the WMT'14 English-German and English-French tasks, our model achieved BLEU
scores of 30.95 and 44.27, respectively. Furthermore, on the OPUS multilingual
machine translation task, our model surpasses a robust 3.8B DeepNet by an
average of 2.9 SacreBLEU, using only 1/3 parameters. Notably, it also beats
LLama models by 5.7 accuracy points on the LM Harness Evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models in Code Question Answering: Baselines
  and Issues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgy Andryushchenko, Vladimir Ivanov, Vladimir Makharev, Elizaveta Tukhtina, Aidar Valeev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering over source code provides software engineers and project
managers with helpful information about the implemented features of a software
product. This paper presents a work devoted to using large language models for
question answering over source code in Python. The proposed method for
implementing a source code question answering system involves fine-tuning a
large language model on a unified dataset of questions and answers for Python
code. To achieve the highest quality answers, we tested various models trained
on datasets preprocessed in different ways: a dataset without grammar
correction, a dataset with grammar correction, and a dataset augmented with the
generated summaries. The model answers were also analyzed for errors manually.
We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along
with the conclusions from the manual error analysis. The obtained experimental
results highlight the current problems of the research area, such as poor
quality of the public genuine question-answering datasets. In addition, the
findings include the positive effect of the grammar correction of the training
data on the testing metric values. The addressed findings and issues could be
important for other researchers who attempt to improve the quality of source
code question answering solutions. The training and evaluation code is publicly
available at https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures, Accepted to NLP (CCIS) @ AIST'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Growing a Tail: Increasing Output Diversity in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Shur-Ofry, Bar Horowitz-Amsalem, Adir Rahamim, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How diverse are the outputs of large language models when diversity is
desired? We examine the diversity of responses of various models to questions
with multiple possible answers, comparing them with human responses. Our
findings suggest that models' outputs are highly concentrated, reflecting a
narrow, mainstream 'worldview', in comparison to humans, whose responses
exhibit a much longer-tail. We examine three ways to increase models' output
diversity: 1) increasing generation randomness via temperature sampling; 2)
prompting models to answer from diverse perspectives; 3) aggregating outputs
from several models. A combination of these measures significantly increases
models' output diversity, reaching that of humans. We discuss implications of
these findings for AI policy that wishes to preserve cultural diversity, an
essential building block of a democratic social fabric.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ [Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for
  Diabetic Retinopathy using Chatbots and Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maren Pielka, Tobias Schneider, Jan Terheyden, Rafet Sifa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an outline of the first large language model (LLM) based chatbot
application in the context of patient-reported outcome measures (PROMs) for
diabetic retinopathy. By utilizing the capabilities of current LLMs, we enable
patients to provide feedback about their quality of life and treatment progress
via an interactive application. The proposed framework offers significant
advantages over the current approach, which encompasses only qualitative
collection of survey data or a static survey with limited answer options. Using
the PROBot LLM-PROM application, patients will be asked tailored questions
about their individual challenges, and can give more detailed feedback on the
progress of their treatment. Based on this input, we will use machine learning
to infer conventional PROM scores, which can be used by clinicians to evaluate
the treatment status. The goal of the application is to improve adherence to
the healthcare system and treatments, and thus ultimately reduce cases of
subsequent vision impairment. The approach needs to be further validated using
a survey and a clinical study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grounding Natural Language to SQL Translation with Data-Based
  Self-Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuankai Fan, Tonghui Ren, Can Huang, Zhenying He, X. Sean Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Interfaces for Databases empower non-technical users to
interact with data using natural language (NL). Advanced approaches, utilizing
either neural sequence-to-sequence or more recent sophisticated large-scale
language models, typically implement NL to SQL (NL2SQL) translation in an
end-to-end fashion. However, like humans, these end-to-end translation models
may not always generate the best SQL output on their first try. In this paper,
we propose CycleSQL, an iterative framework designed for end-to-end translation
models to autonomously generate the best output through self-evaluation. The
main idea of CycleSQL is to introduce data-grounded NL explanations of query
results as self-provided feedback, and use the feedback to validate the
correctness of the translation iteratively, hence improving the overall
translation accuracy. Extensive experiments, including quantitative and
qualitative evaluations, are conducted to study CycleSQL by applying it to
seven existing translation models on five widely used benchmarks. The results
show that 1) the feedback loop introduced in CycleSQL can consistently improve
the performance of existing models, and in particular, by applying CycleSQL to
RESDSQL, obtains a translation accuracy of 82.0% (+2.6%) on the validation set,
and 81.6% (+3.2%) on the test set of Spider benchmark; 2) the generated NL
explanations can also provide insightful information for users, aiding in the
comprehension of translation results and consequently enhancing the
interpretability of NL2SQL translation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Capturing research literature attitude towards Sustainable Development
  Goals: an LLM-based topic modeling approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Invernici, Francesca Curati, Jelena Jakimov, Amirhossein Samavi, Anna Bernasconi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The world is facing a multitude of challenges that hinder the development of
human civilization and the well-being of humanity on the planet. The
Sustainable Development Goals (SDGs) were formulated by the United Nations in
2015 to address these global challenges by 2030. Natural language processing
techniques can help uncover discussions on SDGs within research literature. We
propose a completely automated pipeline to 1) fetch content from the Scopus
database and prepare datasets dedicated to five groups of SDGs; 2) perform
topic modeling, a statistical technique used to identify topics in large
collections of textual data; and 3) enable topic exploration through
keywords-based search and topic frequency time series extraction. For topic
modeling, we leverage the stack of BERTopic scaled up to be applied on large
corpora of textual documents (we find hundreds of topics on hundreds of
thousands of documents), introducing i) a novel LLM-based embeddings
computation for representing scientific abstracts in the continuous space and
ii) a hyperparameter optimizer to efficiently find the best configuration for
any new big datasets. We additionally produce the visualization of results on
interactive dashboards reporting topics' temporal evolution. Results are made
inspectable and explorable, contributing to the interpretability of the topic
modeling process. Our proposed LLM-based topic modeling pipeline for big-text
datasets allows users to capture insights on the evolution of the attitude
toward SDGs within scientific abstracts in the 2006-2023 time span. All the
results are reproducible by using our system; the workflow can be generalized
to be applied at any point in time to any big corpus of textual documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 8 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Post-Training Enhanced Optimization Approach for Small Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keke Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper delves into the continuous post-training optimization methods for
small language models, and proposes a continuous post-training alignment data
construction method for small language models. The core of this method is based
on the data guidance of large models, optimizing the diversity and accuracy of
alignment data. In addition, to verify the effectiveness of the methods in this
paper, we used Qwen2-0.5B-Instruct model as the baseline model for small
language models, using the alignment dataset constructed by our proposed
method, we trained and compared several groups of experiments, including SFT
(Supervised Fine Tuning) post-training experiment and KTO (Kahneman Tversky
optimization) post-training experiment, as well as SFT-KTO two-stage
post-training experiment and model weight fusion experiment. Finally, we
evaluated and analyzed the performance of post-training models, and confirmed
that the continuous post-training optimization method proposed by us can
significantly improve the performance of small language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Textual Aesthetics in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingjie Jiang, Shaohan Huang, Xun Wu, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image aesthetics is a crucial metric in the field of image generation.
However, textual aesthetics has not been sufficiently explored. With the
widespread application of large language models (LLMs), previous work has
primarily focused on the correctness of content and the helpfulness of
responses. Nonetheless, providing responses with textual aesthetics is also an
important factor for LLMs, which can offer a cleaner layout and ensure greater
consistency and coherence in content. In this work, we introduce a pipeline for
aesthetics polishing and help construct a textual aesthetics dataset named
TexAes. We propose a textual aesthetics-powered fine-tuning method based on
direct preference optimization, termed TAPO, which leverages textual aesthetics
without compromising content correctness. Additionally, we develop two
evaluation methods for textual aesthetics based on text and image analysis,
respectively. Our experiments demonstrate that using textual aesthetics data
and employing the TAPO fine-tuning method not only improves aesthetic scores
but also enhances performance on general evaluation datasets such as
AlpacalEval and Anera-hard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Membership Inference Attacks against Large Vision-Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Li, Yongtao Wu, Yihang Chen, Francesco Tonin, Elias Abad Rocamora, Volkan Cevher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (VLLMs) exhibit promising capabilities for
processing multi-modal tasks across various application scenarios. However,
their emergence also raises significant data security concerns, given the
potential inclusion of sensitive information, such as private photos and
medical records, in their training datasets. Detecting inappropriately used
data in VLLMs remains a critical and unresolved issue, mainly due to the lack
of standardized datasets and suitable methodologies. In this study, we
introduce the first membership inference attack (MIA) benchmark tailored for
various VLLMs to facilitate training data detection. Then, we propose a novel
MIA pipeline specifically designed for token-level image detection. Lastly, we
present a new metric called MaxR\'enyi-K%, which is based on the confidence of
the model output and applies to both text and image data. We believe that our
work can deepen the understanding and methodology of MIAs in the context of
VLLMs. Our code and datasets are available at
https://github.com/LIONS-EPFL/VL-MIA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Translation of Circumlocution in Arabic Short Stories into English 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dalal Waadallah Shehab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the translation of circumlocution from Arabic to
English in a corpus of short stories by renowned Arabic authors. By analyzing
the source and target texts, the study aims to identify and categorize
circumlocution instances in Arabic and their corresponding renditions in
English. The study employs Nida's (1964) translation theory as a framework to
assess the appropriateness of the translation strategies employed. It examines
the extent to which translators successfully rendered Arabic circumlocution
into English, identifying potential challenges and limitations in the
translation process. The findings reveal significant similarities between
Arabic circumlocution categories and English metadiscourse categories,
particularly in terms of textual and interpersonal functions. However, the
study also highlights instances where translators encountered difficulties in
accurately conveying the nuances of circumlocution, often resorting to
strategies like addition, subtraction, and alteration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TokenSelect: Efficient Long-Context Inference and Length Extrapolation
  for LLMs via Dynamic Token-Level KV Cache Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Kun Fu, Zheng Wang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of large language models (LLMs), the ability to handle
longer contexts has become a key capability for Web applications such as
cross-document understanding and LLM-powered search systems. However, this
progress faces two major challenges: performance degradation due to sequence
lengths out-of-distribution, and excessively long inference times caused by the
quadratic computational complexity of attention. These issues hinder the
application of LLMs in long-context scenarios. In this paper, we propose
Dynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,
training-free method for efficient and accurate long-context inference.
TokenSelect builds upon the observation of non-contiguous attention sparsity,
using Query-Key dot products to measure per-head KV Cache criticality at
token-level. By per-head soft voting mechanism, TokenSelect selectively
involves a small number of critical KV cache tokens in the attention
calculation without sacrificing accuracy. To further accelerate TokenSelect, we
designed the Selection Cache based on observations of consecutive Query
similarity and implemented efficient dot product kernel, significantly reducing
the overhead of token selection. A comprehensive evaluation of TokenSelect
demonstrates up to 23.84x speedup in attention computation and up to 2.28x
acceleration in end-to-end latency, while providing superior performance
compared to state-of-the-art long-context inference methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document
  Relation Extraction with Graph-of-Thoughts Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Zhang, Ning Yan, Masood Mortazavi, Hoang H. Nguyen, Zhongfen Deng, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) pre-trained on massive corpora have demonstrated
impressive few-shot learning capability on many NLP tasks. Recasting an NLP
task into a text-to-text generation task is a common practice so that
generative LLMs can be prompted to resolve it. However, performing
document-level relation extraction (DocRE) tasks with generative LLM models is
still challenging due to the structured output format of DocRE, which
complicates the conversion to plain text. Limited information available in
few-shot samples and prompt instructions induce further difficulties and
challenges in relation extraction for mentioned entities in a document. In this
paper, we represent the structured output as a graph-style triplet rather than
natural language expressions and leverage generative LLMs for the DocRE task.
Our approach, the Graph-DPEP framework is grounded in the reasoning behind
triplet explanation thoughts presented in natural language. In this framework,
we first introduce a ``decomposed-plug" method for performing the generation
from LLMs over prompts with type-space decomposition to alleviate the burden of
distinguishing all relation types. Second, we employ a verifier for calibrating
the generation and identifying overlooked query entity pairs. Third, we develop
"ensemble-play", reapplying generation on the entire type list by leveraging
the reasoning thoughts embedded in a sub-graph associated with the missing
query pair to address the missingness issue. Through extensive comparisons with
existing prompt techniques and alternative Language Models (LLMs), our
framework demonstrates superior performance on publicly available benchmarks in
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual
  Visual Answer Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibin Wen, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Multilingual Visual Answer Localization (MVAL) is to locate a
video segment that answers a given multilingual question. Existing methods
either focus solely on visual modality or integrate visual and subtitle
modalities. However, these methods neglect the audio modality in videos,
consequently leading to incomplete input information and poor performance in
the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span
Localization (AVTSL) method that incorporates audio modality to augment both
visual and textual representations for the MVAL task. Specifically, we
integrate features from three modalities and develop three predictors, each
tailored to the unique contributions of the fused modalities: an audio-visual
predictor, a visual predictor, and a textual predictor. Each predictor
generates predictions based on its respective modality. To maintain consistency
across the predicted results, we introduce an Audio-Visual-Textual Consistency
module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing
each modality's predictor to dynamically learn from the others. This
collaborative learning ensures that the model generates consistent and
comprehensive answers. Extensive experiments show that our proposed method
outperforms several state-of-the-art (SOTA) methods, which demonstrates the
effectiveness of the audio modality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixtures of In-Context Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giwon Hong, Emile van Krieken, Edoardo Ponti, Nikolay Malkin, Pasquale Minervini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) adapts LLMs by providing demonstrations without
fine-tuning the model parameters; however, it does not differentiate between
demonstrations and quadratically increases the complexity of Transformer LLMs,
exhausting the memory. As a solution, we propose Mixtures of In-Context
Learners (MoICL), a novel approach to treat subsets of demonstrations as
experts and learn a weighting function to merge their output distributions
based on a training set. In our experiments, we show performance improvements
on 5 out of 7 classification datasets compared to a set of strong baselines (up
to +13\% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of
ICL by reducing the inference time needed to achieve the same performance with
fewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to
+11\%), imbalanced (up to +49\%), or noisy demonstrations (up to +38\%) or can
filter these out from datasets. Overall, MoICL is a more expressive approach to
learning from demonstrations without exhausting the context window or memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DroidSpeak: Enhancing Cross-LLM Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Liu, Esha Choukse, Shan Lu, Junchen Jiang, Madan Musuvathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-agent systems utilizing Large Language Models (LLMs), communication
between agents traditionally relies on natural language. This communication
often includes the full context of the query so far, which can introduce
significant prefill-phase latency, especially with long contexts.
  We introduce DroidSpeak, a novel framework to target this cross-LLM
communication by leveraging the reuse of intermediate data, such as input
embeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the
need to reprocess entire contexts for fine-tuned versions of the same
foundational model. This approach allows faster context integration while
maintaining the quality of task performance. Experimental evaluations
demonstrate DroidSpeak's ability to significantly accelerate inter-agent
communication, achieving up to a 2.78x speedup in prefill latency with
negligible loss in accuracy. Our findings underscore the potential to create
more efficient and scalable multi-agent systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LADDER: Language Driven Slice Discovery and Error Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07832v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07832v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Shyam Visweswaran, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error slice discovery associates structured patterns with model errors.
Existing methods discover error slices by clustering the error-prone samples
with similar patterns or assigning discrete attributes to each sample for
post-hoc analysis. While these methods aim for interpretability and easier
mitigation through reweighting or rebalancing, they may not capture the full
complexity of error patterns due to incomplete or missing attributes. Contrary
to the existing approach, this paper utilizes the reasoning capabilities of the
Large Language Model (LLM) to analyze complex error patterns and generate
testable hypotheses. This paper proposes LADDER: Language Driven slice
Discovery and Error Rectification. It first projects the model's representation
into a language-aligned feature space (eg CLIP) to preserve semantics in the
original model feature space. This ensures the accurate retrieval of sentences
that highlight the model's errors. Next, the LLM utilizes the sentences and
generates hypotheses to discover error slices. Finally, we mitigate the error
by fine-tuning the classification head by creating a group-balanced dataset
using the hypotheses. Our entire method does not require any attribute
annotation, either explicitly or through external tagging models. We validate
our method with \textbf{five} image classification datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NutriBench: A <span class="highlight-title">Dataset</span> for Evaluating Large Language Models in
  Carbohydrate Estimation from Meal Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12843v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12843v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Hua, Mehak Preet Dhaliwal, Ryan Burke, Laya Pullela, Yao Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate nutrition estimation helps people make informed dietary choices and
is essential in the prevention of serious health complications. We present
NutriBench, the first publicly available natural language meal description
nutrition benchmark. NutriBench consists of 11,857 meal descriptions generated
from real-world global dietary intake data. The data is human-verified and
annotated with macro-nutrient labels, including carbohydrates, proteins, fats,
and calories. We conduct an extensive evaluation of NutriBench on the task of
carbohydrate estimation, testing twelve leading Large Language Models (LLMs),
including GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using
standard, Chain-of-Thought and Retrieval-Augmented Generation strategies.
Additionally, we present a study involving professional nutritionists, finding
that LLMs can provide more accurate and faster estimates. Finally, we perform a
real-world risk assessment by simulating the effect of carbohydrate predictions
on the blood glucose levels of individuals with diabetes. Our work highlights
the opportunities and challenges of using LLMs for nutrition estimation,
demonstrating their potential to aid professionals and laypersons and improve
health outcomes. Our benchmark is publicly available at:
https://mehak126.github.io/nutribench.html
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building on Efficient Foundations: Effectively Training LLMs with
  Structured Feedforward Layers <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiuying Wei, Skander Moalla, Razvan Pascanu, Caglar Gulcehre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art results in large language models (LLMs) often rely on scale,
which becomes computationally expensive. This has sparked a research agenda to
reduce these models' parameter counts and computational costs without
significantly impacting their performance. Our study focuses on
transformer-based LLMs, specifically targeting the computationally intensive
feedforward networks (FFNs), which are less studied than attention blocks. We
consider three structured linear parameterizations of the FFN using efficient
low-rank and block-diagonal matrices. In contrast to many previous works that
examined these approximations, our study i) explores these structures from a
training-from-scratch perspective, ii) scales up to 1.3B parameters, and iii)
is conducted within recent Transformer-based LLMs rather than convolutional
architectures. We demonstrate that these structures can lead to actual
computational gains in various scenarios, including online decoding when using
a pre-merge technique. Additionally, we propose a novel training regime, called
\textit{self-guided training}, aimed at improving the poor training dynamics
that these approximations exhibit when used from initialization. Interestingly,
the scaling performance of structured matrices is explored, revealing steeper
curves in scaling training FLOPs, along with a favorable scaling trend in the
overtraining regime. Specifically, we show that wide and structured networks
can utilize training FLOPs more efficiently, with fewer parameters and lower
loss than dense models at their optimal trade-off. Our code is available at
\url{https://github.com/CLAIRE-Labo/StructuredFFN/tree/main}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tree-Averaging Algorithms for Ensemble-Based Unsupervised Discontinuous
  Constituency Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behzad Shayegh, Yuqiao Wen, Lili Mou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address unsupervised discontinuous constituency parsing, where we observe
a high variance in the performance of the only previous model in the
literature. We propose to build an ensemble of different runs of the existing
discontinuous parser by averaging the predicted trees, to stabilize and boost
performance. To begin with, we provide comprehensive computational complexity
analysis (in terms of P and NP-complete) for tree averaging under different
setups of binarity and continuity. We then develop an efficient exact algorithm
to tackle the task, which runs in a reasonable time for all samples in our
experiments. Results on three datasets show our method outperforms all
baselines in all metrics; we also provide in-depth analyses of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoKaggle: A Multi-Agent Framework for Autonomous Data Science
  Competitions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20424v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20424v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, Wanjun Zhong, Wangchunshu Zhou, Wenhao Huang, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data science tasks involving tabular data present complex challenges that
require sophisticated problem-solving approaches. We propose AutoKaggle, a
powerful and user-centric framework that assists data scientists in completing
daily data pipelines through a collaborative multi-agent system. AutoKaggle
implements an iterative development process that combines code execution,
debugging, and comprehensive unit testing to ensure code correctness and logic
consistency. The framework offers highly customizable workflows, allowing users
to intervene at each phase, thus integrating automated intelligence with human
expertise. Our universal data science toolkit, comprising validated functions
for data cleaning, feature engineering, and modeling, forms the foundation of
this solution, enhancing productivity by streamlining common tasks. We selected
8 Kaggle competitions to simulate data processing workflows in real-world
application scenarios. Evaluation results demonstrate that AutoKaggle achieves
a validation submission rate of 0.85 and a comprehensive score of 0.82 in
typical data science pipelines, fully proving its effectiveness and
practicality in handling complex data science tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning Based Amharic Chatbot for FAQs in Universities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01720v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01720v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Goitom Ybrah Hailu, Hadush Hailu, Shishay Welay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  University students often spend a considerable amount of time seeking answers
to common questions from administrators or teachers. This can become tedious
for both parties, leading to a need for a solution. In response, this paper
proposes a chatbot model that utilizes natural language processing and deep
learning techniques to answer frequently asked questions (FAQs) in the Amharic
language. Chatbots are computer programs that simulate human conversation
through the use of artificial intelligence (AI), acting as a virtual assistant
to handle questions and other tasks. The proposed chatbot program employs
tokenization, normalization, stop word removal, and stemming to analyze and
categorize Amharic input sentences. Three machine learning model algorithms
were used to classify tokens and retrieve appropriate responses: Support Vector
Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented
through TensorFlow, Keras, and NLTK. The deep learning model achieved the best
results with 91.55% accuracy and a validation loss of 0.3548 using an Adam
optimizer and SoftMax activation function. The chatbot model was integrated
with Facebook Messenger and deployed on a Heroku server for 24-hour
accessibility. The experimental results demonstrate that the chatbot framework
achieved its objectives and effectively addressed challenges such as Amharic
Fidel variation, morphological variation, and lexical gaps. Future research
could explore the integration of Amharic WordNet to narrow the lexical gap and
support more complex questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LiveMind: Low-latency Large Language Models with Simultaneous Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuangtao Chen, Grace Li Zhang, Xunzhao Yin, Cheng Zhuo, Ulf Schlichtmann, Bing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce LiveMind, a novel low-latency inference framework
for large language model (LLM) inference which enables LLMs to perform
inferences with incomplete user input. By reallocating computational processes
to the input phase, a substantial reduction in latency is achieved, thereby
significantly enhancing the interactive experience for users of LLMs. The
framework adeptly manages the visibility of the streaming input to the model,
allowing it to infer from incomplete user input or await additional content.
Compared with traditional inference methods on complete user input, our
approach demonstrates an average reduction in response latency of 84.0% on the
MMLU dataset and 71.6% on the MMLU-Pro dataset, while maintaining comparable
accuracy. Additionally, our framework facilitates collaborative inference and
output across different models. By employing an large LLM for inference and a
small LLM for output, we achieve an average 37% reduction in response latency,
alongside a 4.30% improvement in accuracy on the MMLU-Pro dataset compared with
the baseline. The proposed LiveMind framework advances the field of human-AI
interaction by enabling more responsive and efficient communication between
users and AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Language Models via Token Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhili Feng, Tanya Marwah, Nicolo Fusi, David Alvarez-Melis, Lester Mackey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern large language models use a fixed tokenizer to effectively compress
text drawn from a source domain. However, applying the same tokenizer to a new
target domain often leads to inferior compression, more costly inference, and
reduced semantic alignment. To address this deficiency, we introduce Sparse
Sinkhorn Token Translation (S2T2). S2T2 trains a tailored tokenizer for the
target domain and learns to translate between target and source tokens,
enabling more effective reuse of the pre-trained next-source-token predictor.
In our experiments with finetuned English language models, S2T2 improves both
the perplexity and the compression of out-of-domain protein sequences,
outperforming direct finetuning with either the source or target tokenizer. In
addition, we find that token translations learned for smaller, less expensive
models can be directly transferred to larger, more powerful models to reap the
benefits of S2T2 at lower cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphReader: Building Graph-based Agent to Enhance Long-Context
  Abilities of Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, Wenbo Su, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-context capabilities are essential for large language models (LLMs) to
tackle complex and long-input tasks. Despite numerous efforts made to optimize
LLMs for long contexts, challenges persist in robustly processing long inputs.
In this paper, we introduce GraphReader, a graph-based agent system designed to
handle long texts by structuring them into a graph and employing an agent to
explore this graph autonomously. Upon receiving a question, the agent first
undertakes a step-by-step analysis and devises a rational plan. It then invokes
a set of predefined functions to read node content and neighbors, facilitating
a coarse-to-fine exploration of the graph. Throughout the exploration, the
agent continuously records new insights and reflects on current circumstances
to optimize the process until it has gathered sufficient information to
generate an answer. Experimental results on the LV-Eval dataset reveal that
GraphReader, using a 4k context window, consistently outperforms GPT-4-128k
across context lengths from 16k to 256k by a large margin. Additionally, our
approach demonstrates superior performance on four challenging single-hop and
multi-hop benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>[EMNLP 2024] The first four authors contributed equally, 29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallelizing Linear <span class="highlight-title">Transformer</span>s with the Delta Rule over Sequence
  Length <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06484v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06484v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers with linear attention (i.e., linear transformers) and
state-space models have recently been suggested as a viable linear-time
alternative to transformers with softmax attention. However, these models still
underperform transformers especially on tasks that require in-context
retrieval. While more expressive variants of linear transformers which replace
the additive update in linear transformers with the delta rule (DeltaNet) have
been found to be more effective at associative recall, existing algorithms for
training such models do not parallelize over sequence length and are thus
inefficient to train on modern hardware. This work describes a
hardware-efficient algorithm for training linear transformers with the delta
rule, which exploits a memory-efficient representation for computing products
of Householder matrices. This algorithm allows us to scale up DeltaNet to
standard language modeling settings. We train a 1.3B model for 100B tokens and
find that it outperforms recent linear-time baselines such as Mamba and GLA in
terms of perplexity and zero-shot performance on downstream tasks. We also
experiment with two hybrid models which combine DeltaNet layers with (1)
sliding-window attention layers every other layer or (2) two global attention
layers, and find that these hybrids outperform strong transformer baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RepLiQA: A Question-Answering <span class="highlight-title">Dataset</span> for Benchmarking LLMs on Unseen
  Reference Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11811v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11811v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Monteiro, Pierre-Andre Noel, Etienne Marcotte, Sai Rajeswar, Valentina Zantedeschi, David Vazquez, Nicolas Chapados, Christopher Pal, Perouz Taslakian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are trained on vast amounts of data, most of
which is automatically scraped from the internet. This data includes
encyclopedic documents that harbor a vast amount of general knowledge (e.g.,
Wikipedia) but also potentially overlap with benchmark datasets used for
evaluating LLMs. Consequently, evaluating models on test splits that might have
leaked into the training set is prone to misleading conclusions. To foster
sound evaluation of language models, we introduce a new test dataset named
RepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a
collection of five splits of test sets, four of which have not been released to
the internet or exposed to LLM APIs prior to this publication. Each sample in
RepLiQA comprises (1) a reference document crafted by a human annotator and
depicting an imaginary scenario (e.g., a news article) absent from the
internet; (2) a question about the document's topic; (3) a ground-truth answer
derived directly from the information in the document; and (4) the paragraph
extracted from the reference document containing the answer. As such, accurate
answers can only be generated if a model can find relevant content within the
provided document. We run a large-scale benchmark comprising several
state-of-the-art LLMs to uncover differences in performance across models of
various types and sizes in a context-conditional language modeling setting.
Released splits of RepLiQA can be found here:
https://huggingface.co/datasets/ServiceNow/repliqa.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language
  Models in Multi-Turn Dialogues <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.14762v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.14762v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has drastically enhanced dialogue
systems. However, comprehensively evaluating the dialogue abilities of LLMs
remains a challenge. Previous benchmarks have primarily focused on single-turn
dialogues or provided coarse-grained and incomplete assessments of multi-turn
dialogues, overlooking the complexity and fine-grained nuances of real-life
dialogues. To address this issue, we introduce MT-Bench-101, specifically
designed to evaluate the fine-grained abilities of LLMs in multi-turn
dialogues. By conducting a detailed analysis of real multi-turn dialogue data,
we construct a three-tier hierarchical ability taxonomy comprising 4208 turns
across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21
popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both
ability and task perspectives and observing differing trends in LLMs
performance across dialogue turns within various tasks. Further analysis
indicates that neither utilizing common alignment techniques nor chat-specific
designs has led to obvious enhancements in the multi-turn abilities of LLMs.
Extensive case studies suggest that our designed tasks accurately assess the
corresponding multi-turn abilities. The data and code are available at
\url{https://github.com/mtbench101/mt-bench-101}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>[ACL 2024] The first three authors contribute equally, 34 pages, repo
  at https://github.com/mtbench101/mt-bench-101</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Knowledge Pursuit for Faithful Visual Synthesis <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17898v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17898v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqi Luo, Kwan Ho Ryan Chan, Dimitris Dimos, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern text-to-vision generative models often hallucinate when the prompt
describing the scene to be generated is underspecified. In large language
models (LLMs), a prevalent strategy to reduce hallucinations is to retrieve
factual knowledge from an external database. While such retrieval augmentation
strategies have great potential to enhance text-to-vision generators, existing
static top-K retrieval methods explore the knowledge pool once, missing the
broader context necessary for high-quality generation. Furthermore, LLMs
internally possess rich world knowledge learned during large-scale training
(parametric knowledge) that could mitigate the need for external data
retrieval. This paper proposes Contextual Knowledge Pursuit (CKPT), a framework
that leverages the complementary strengths of external and parametric knowledge
to help generators produce reliable visual content. Instead of the one-time
retrieval of facts from an external database to improve a given prompt, CKPT
uses (1) an LLM to decide whether to seek external knowledge or to self-elicit
descriptions from LLM parametric knowledge, (2) a knowledge pursuit process to
contextually seek and sequentially gather most relevant facts, (3) a knowledge
aggregator for prompt enhancement with the gathered fact context, and (4) a
filtered fine-tuning objective to improve visual synthesis with richer prompts.
We evaluate CKPT across multiple text-driven generative tasks (image, 3D
rendering, and video) on datasets of rare objects and daily scenarios. Our
results show that CKPT is capable of generating faithful and semantically rich
content across diverse visual domains, offering a promising data source for
zero-shot synthesis and filtered fine-tuning of text-to-vision generative
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECCV 2024 SDCV Workshop. GitHub repository at
  https://github.com/peterljq/Contextual-Knowledge-Pursuit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficacy of Various Large Language Models in Generating Smart Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhartha Chatterjee, Bina Ramamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study analyzes the application of code-generating Large Language Models
in the creation of immutable Solidity smart contracts on the Ethereum
Blockchain. Other works have previously analyzed Artificial Intelligence code
generation abilities. This paper aims to expand this to a larger scope to
include programs where security and efficiency are of utmost priority such as
smart contracts. The hypothesis leading into the study was that LLMs in general
would have difficulty in rigorously implementing security details in the code,
which was shown through our results, but surprisingly generally succeeded in
many common types of contracts. We also discovered a novel way of generating
smart contracts through new prompting strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, accepted for presentation at 8th annual Future of
  Information and Communication Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kun: Answer Polishment for Chinese Self-Alignment with Instruction
  Back-Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06477v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06477v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Zheng, Shuyue Guo, Xingwei Qu, Jiawei Guo, Xinrun Du, Qi Jia, Chenghua Lin, Wenhao Huang, Jie Fu, Ge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Kun, a novel approach for creating high-quality
instruction-tuning datasets for large language models (LLMs) without relying on
manual annotations. Adapting a self-training algorithm based on instruction
back-translation and answer polishment, Kun leverages unlabelled data from
diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial
dataset of over a million Chinese instructional data points. This approach
significantly deviates from traditional methods by using a self-curation
process to refine and select the most effective instruction-output pairs. Our
experiments with the 6B-parameter Yi model across various benchmarks
demonstrate Kun's robustness and scalability. Our method's core contributions
lie in its algorithmic advancement, which enhances data retention and clarity,
and its innovative data generation approach that substantially reduces the
reliance on costly and time-consuming manual annotations. This methodology
presents a scalable and efficient solution for improving the
instruction-following capabilities of LLMs, with significant implications for
their application across diverse fields. The code and dataset can be found at
https://github.com/Zheng0428/COIG-Kun
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PaCE: Parsimonious Concept Engineering for Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are being used for a wide variety of tasks.
While they are capable of generating human-like responses, they can also
produce undesirable output including potentially harmful information, racist or
sexist language, and hallucinations. Alignment methods are designed to reduce
such undesirable outputs via techniques such as fine-tuning, prompt
engineering, and representation engineering. However, existing methods face
several challenges: some require costly fine-tuning for every alignment task;
some do not adequately remove undesirable concepts, failing alignment; some
remove benign concepts, lowering the linguistic capabilities of LLMs. To
address these issues, we propose Parsimonious Concept Engineering (PaCE), a
novel activation engineering framework for alignment. First, to sufficiently
model the concepts, we construct a large-scale concept dictionary in the
activation space, in which each atom corresponds to a semantic concept. Given
any alignment task, we instruct a concept partitioner to efficiently annotate
the concepts as benign or undesirable. Then, at inference time, we decompose
the LLM activations along the concept dictionary via sparse coding, to
accurately represent the activations as linear combinations of benign and
undesirable components. By removing the latter ones from the activations, we
reorient the behavior of the LLM towards the alignment goal. We conduct
experiments on tasks such as response detoxification, faithfulness enhancement,
and sentiment revising, and show that PaCE achieves state-of-the-art alignment
performance while maintaining linguistic capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024. GitHub repository at
  https://github.com/peterljq/Parsimonious-Concept-Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy Risks of Speculative Decoding in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01076v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01076v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiankun Wei, Abdulrahman Abdulrazzag, Tianchen Zhang, Adel Muursepp, Gururaj Saileshwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding in large language models (LLMs) accelerates token
generation by speculatively predicting multiple tokens cheaply and verifying
them in parallel, and has been widely deployed. In this paper, we provide the
first study demonstrating the privacy risks of speculative decoding. We observe
that input-dependent patterns of correct and incorrect predictions can be
leaked out to an adversary monitoring token generation times and packet sizes,
leading to privacy breaches. By observing the pattern of correctly and
incorrectly speculated tokens, we show that a malicious adversary can
fingerprint queries and learn private user inputs with more than $90\%$
accuracy across three different speculative decoding techniques - REST (almost
$100\%$ accuracy), LADE (up to $92\%$ accuracy), and BiLD (up to $95\%$
accuracy). We show that an adversary can also leak out confidential
intellectual property used to design these techniques, such as data from
data-stores used for prediction (in REST) at a rate of more than $25$ tokens
per second, or even hyper-parameters used for prediction (in LADE). We also
discuss mitigation strategies, such as aggregating tokens across multiple
iterations and padding packets with additional bytes, to avoid such privacy or
confidentiality breaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on
  Tasks where Thinking Makes Humans Worse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Liu, Jiayi Geng, Addison J. Wu, Ilia Sucholutsky, Tania Lombrozo, Thomas L. Griffiths
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) prompting has become a widely used strategy for
working with large language and multimodal models. While CoT has been shown to
improve performance across many tasks, determining the settings in which it is
effective remains an ongoing effort. In particular, it is still an open
question in what settings CoT systematically reduces model performance. In this
paper, we seek to identify the characteristics of tasks where CoT reduces
performance by drawing inspiration from cognitive psychology, looking at cases
where (i) verbal thinking or deliberation hurts performance in humans, and (ii)
the constraints governing human performance generalize to language models.
Three such cases are implicit statistical learning, visual recognition, and
classifying with patterns containing exceptions. In extensive experiments
across all three settings, we find that a diverse collection of
state-of-the-art models exhibit significant drop-offs in performance (e.g., up
to 36.3% absolute accuracy for OpenAI o1-preview compared to GPT-4o) when using
inference-time reasoning compared to zero-shot counterparts. We also identify
three tasks that satisfy condition (i) but not (ii), and find that while verbal
thinking reduces human performance in these tasks, CoT retains or increases
model performance. Overall, our results show that while there is not an exact
parallel between the cognitive processes of models and those of humans,
considering cases where thinking has negative consequences for human
performance can help us identify settings where it negatively impacts models.
By connecting the literature on human deliberation with evaluations of CoT, we
offer a new tool that can be used in understanding the impact of prompt choices
and inference-time reasoning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Programming Language Sandbox for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihan Dou, Jiazheng Zhang, Jianxiang Zang, Yunbo Tao, Weikang Zhou, Haoxiang Jia, Shichun Liu, Yuming Yang, Zhiheng Xi, Shenxi Wu, Shaoqing Zhang, Muling Wu, Changze Lv, Limao Xiong, Wenyu Zhan, Lin Zhang, Rongxiang Weng, Jingang Wang, Xunliang Cai, Yueming Wu, Ming Wen, Rui Zheng, Tao Ji, Yixin Cao, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MPLSandbox, an out-of-the-box multi-programming language sandbox
designed to provide unified and comprehensive feedback from compiler and
analysis tools for Large Language Models (LLMs). It can automatically identify
the programming language of the code, compiling and executing it within an
isolated sub-sandbox to ensure safety and stability. In addition, MPLSandbox
also integrates both traditional and LLM-based code analysis tools, providing a
comprehensive analysis of generated code. MPLSandbox can be effortlessly
integrated into the training and deployment of LLMs to improve the quality and
correctness of their generated code. It also helps researchers streamline their
workflows for various LLM-based code-related tasks, reducing the development
cost. To validate the effectiveness of MPLSandbox, we integrate it into
training and deployment approaches, and also employ it to optimize workflows
for a wide range of real-world code-related tasks. Our goal is to enhance
researcher productivity on LLM-based code-related tasks by simplifying and
automating workflows through delegation to MPLSandbox.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Natural Language Processing-Based Classification and Mode-Based
  Ranking of Musculoskeletal Disorder Risk Factors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11517v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11517v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Abrar Jahin, Subrata Talapatra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research delves into Musculoskeletal Disorder (MSD) risk factors, using
a blend of Natural Language Processing (NLP) and mode-based ranking. The aim is
to refine understanding, classification, and prioritization for focused
prevention and treatment. Eight NLP models are evaluated, combining pre-trained
transformers, cosine similarity, and distance metrics to categorize factors
into personal, biomechanical, workplace, psychological, and organizational
classes. BERT with cosine similarity achieves 28% accuracy; sentence
transformer with Euclidean, Bray-Curtis, and Minkowski distances scores 100%.
With 10-fold cross-validation, statistical tests ensure robust results. Survey
data and mode-based ranking determine severity hierarchy, aligning with the
literature. "Working posture" is the most severe, highlighting posture's role.
Survey insights emphasize "Job insecurity," "Effort reward imbalance," and
"Poor employee facility" as significant contributors. Rankings offer actionable
insights for MSD prevention. The study suggests targeted interventions,
workplace improvements, and future research directions. This integrated NLP and
ranking approach enhances MSD comprehension and informs occupational health
strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Text Steganography with Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxuan Wu, Zhengxian Wu, Yiming Xue, Juan Wen, Wanli Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have blurred the boundary of
high-quality text generation between humans and machines, which is favorable
for generative text steganography. While, current advanced steganographic
mapping is not suitable for LLMs since most users are restricted to accessing
only the black-box API or user interface of the LLMs, thereby lacking access to
the training vocabulary and its sampling probabilities. In this paper, we
explore a black-box generative text steganographic method based on the user
interfaces of large language models, which is called LLM-Stega. The main goal
of LLM-Stega is that the secure covert communication between Alice (sender) and
Bob (receiver) is conducted by using the user interfaces of LLMs. Specifically,
We first construct a keyword set and design a new encrypted steganographic
mapping to embed secret messages. Furthermore, to guarantee accurate extraction
of secret messages and rich semantics of generated stego texts, an optimization
mechanism based on reject sampling is proposed. Comprehensive experiments
demonstrate that the proposed LLM-Stega outperforms current state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, accepted at ACM Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Former: Lightning-fast Compressing Context for Large Language
  Model <span class="chip">EMNLP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangfeng Wang, Zaiyi Chen, Zheyong Xie, Tong Xu, Yongyi He, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rising popularity of Transformer-based large language models (LLMs),
reducing their high inference costs has become a significant research focus.
One effective approach is to compress the long input contexts. Existing methods
typically leverage the self-attention mechanism of the LLM itself for context
compression. While these methods have achieved notable results, the compression
process still involves quadratic time complexity, which limits their
applicability. To mitigate this limitation, we propose the In-Context Former
(IC-Former). Unlike previous methods, IC-Former does not depend on the target
LLMs. Instead, it leverages the cross-attention mechanism and a small number of
learnable digest tokens to directly condense information from the contextual
word embeddings. This approach significantly reduces inference time, which
achieves linear growth in time complexity within the compression range.
Experimental results indicate that our method requires only 1/32 of the
floating-point operations of the baseline during compression and improves
processing speed by 68 to 112 times while achieving over 90% of the baseline
performance on evaluation metrics. Overall, our model effectively reduces
compression costs and makes real-time compression scenarios feasible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP2024(Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezra Karger, Houtan Bastani, Chen Yueh-Han, Zachary Jacobs, Danny Halawi, Fred Zhang, Philip E. Tetlock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasts of future events are essential inputs into informed
decision-making. Machine learning (ML) systems have the potential to deliver
forecasts at scale, but there is no framework for evaluating the accuracy of ML
systems on a standardized set of forecasting questions. To address this gap, we
introduce ForecastBench: a dynamic benchmark that evaluates the accuracy of ML
systems on an automatically generated and regularly updated set of 1,000
forecasting questions. To avoid any possibility of data leakage, ForecastBench
is comprised solely of questions about future events that have no known answer
at the time of submission. We quantify the capabilities of current ML systems
by collecting forecasts from expert (human) forecasters, the general public,
and LLMs on a random subset of questions from the benchmark ($N=200$). While
LLMs have achieved super-human performance on many benchmarks, they perform
less well here: expert forecasters outperform the top-performing LLM (p-value
$=0.01$). We display system and human scores in a public leaderboard at
www.forecastbench.org.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence
  Embeddings for Automatic Dialog Flow Extraction <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergio Burdisso, Srikanth Madikeri, Petr Motlicek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently deriving structured workflows from unannotated dialogs remains an
underexplored and formidable challenge in computational linguistics. Automating
this process could significantly accelerate the manual design of workflows in
new domains and enable the grounding of large language models in
domain-specific flowcharts, enhancing transparency and controllability. In this
paper, we introduce Dialog2Flow (D2F) embeddings, which differ from
conventional sentence embeddings by mapping utterances to a latent space where
they are grouped according to their communicative and informative functions
(i.e., the actions they represent). D2F allows for modeling dialogs as
continuous trajectories in a latent space with distinct action-related regions.
By clustering D2F embeddings, the latent space is quantized, and dialogs can be
converted into sequences of region/action IDs, facilitating the extraction of
the underlying workflow. To pre-train D2F, we build a comprehensive dataset by
unifying twenty task-oriented dialog datasets with normalized per-turn action
annotations. We also introduce a novel soft contrastive loss that leverages the
semantic information of these actions to guide the representation learning
process, showing superior performance compared to standard supervised
contrastive loss. Evaluation against various sentence embeddings, including
dialog-specific ones, demonstrates that D2F yields superior qualitative and
quantitative results across diverse domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEM: Distribution Edited Model for Training with Mixed Data
  Distributions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15570v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15570v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhananjay Ram, Aditya Rawal, Momchil Hardalov, Nikolaos Pappas, Sheng Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training with mixed data distributions is a common and important part of
creating multi-task and instruction-following models. The diversity of the data
distributions and cost of joint training makes the optimization procedure
extremely challenging. Data mixing methods partially address this problem,
albeit having a sub-optimal performance across data sources and require
multiple expensive training runs. In this paper, we propose a simple and
efficient alternative for better optimization of the data sources by combining
models individually trained on each data source with the base model using basic
element-wise vector operations. The resulting model, namely Distribution Edited
Model (DEM), is 11x cheaper than standard data mixing and outperforms strong
baselines on a variety of benchmarks, yielding upto 6.2% improvement on MMLU,
11.5% on BBH, 16.1% on DROP, 6% on MathQA, and 9.3% on HELM with models of size
3B to 13B. Notably, DEM does not require full re-training when modifying a
single data-source, thus making it very flexible and scalable for training with
diverse data sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ocean-omni: To Understand the World with Omni-modality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08565v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08565v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, Song Chen, Xu Li, Da Pan, Shusen Zhang, Xin Wu, Zheng Liang, Jun Liu, Tao Zhang, Keer Lu, Yaqi Zhao, Yanjun Shen, Fan Yang, Kaicheng Yu, Tao Lin, Jianhua Xu, Zenan Zhou, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The salient multimodal capabilities and interactive experience of GPT-4o
highlight its critical role in practical applications, yet it lacks a
high-performing open-source counterpart. In this paper, we introduce
Ocean-omni, the first open-source 7B Multimodal Large Language Model (MLLM)
adept at concurrently processing and analyzing modalities of image, video,
audio, and text, while delivering an advanced multimodal interactive experience
and strong performance. We propose an effective multimodal training schema
starting with 7B model and proceeding through two stages of multimodal
alignment and multitask fine-tuning across audio, image, video, and text modal.
This approach equips the language model with the ability to handle visual and
audio data effectively. Demonstrating strong performance across various
omni-modal and multimodal benchmarks, we aim for this contribution to serve as
a competitive baseline for the open-source community in advancing multimodal
understanding and real-time interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies
  in Medical Question Answering <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Yang, Andrew M. Bean, Robert McCraith, Adam Mahdi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning Large Language Models (LLMs) incurs considerable training costs,
driving the need for data-efficient training with optimised data ordering.
Human-inspired strategies offer a solution by organising data based on human
learning practices. This study evaluates the fine-tuning efficiency of five
human-inspired strategies across four language models, three datasets, and both
human- and LLM-labelled data in the context of medical question answering.
These strategies achieve the best accuracy gain of 1.81% and an average gain of
1.02% across datasets, with interleaved strategies delivering the best average
results. However, the best strategy varies across model-dataset combinations,
limiting the generalisability of the effects of any single strategy.
Additionally, LLM-defined question difficulty outperforms human-defined labels
in curriculum-based learning, showing the potential of model-generated data as
a cost-effective alternative for optimising fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning:
  Principles and Scalability (FITML)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthetic SQL Column Descriptions and Their Impact on Text-to-SQL
  Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04691v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04691v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Wretblad, Oskar Holmström, Erik Larsson, Axel Wiksäter, Oscar Söderlund, Hjalmar Öhman, Ture Pontén, Martin Forsberg, Martin Sörme, Fredrik Heintz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relational databases often suffer from uninformative descriptors of table
contents, such as ambiguous columns and hard-to-interpret values, impacting
both human users and text-to-SQL models. In this paper, we explore the use of
large language models (LLMs) to automatically generate detailed natural
language descriptions for SQL database columns, aiming to improve text-to-SQL
performance and automate metadata creation. We create a dataset of gold column
descriptions based on the BIRD-Bench benchmark, manually refining its column
descriptions and creating a taxonomy for categorizing column difficulty. We
then evaluate several different LLMs in generating column descriptions across
the columns and different difficulties in the dataset, finding that models
unsurprisingly struggle with columns that exhibit inherent ambiguity,
highlighting the need for manual expert input. We also find that incorporating
such generated column descriptions consistently enhances text-to-SQL model
performance, particularly for larger models like GPT-4o, Qwen2 72B and Mixtral
22Bx8. Notably, Qwen2-generated descriptions, containing by annotators deemed
superfluous information, outperform manually curated gold descriptions,
suggesting that models benefit from more detailed metadata than humans expect.
Future work will investigate the specific features of these high-performing
descriptions and explore other types of metadata, such as numerical reasoning
and synonyms, to further improve text-to-SQL systems. The dataset, annotations
and code will all be made available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding <span class="highlight-title">Transformer</span>s via N-gram Statistics <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer based large-language models (LLMs) display extreme proficiency
with language yet a precise understanding of how they work remains elusive. One
way of demystifying transformer predictions would be to describe how they
depend on their context in terms of simple template functions. This paper takes
a first step in this direction by considering families of functions (i.e.
rules) formed out of simple N-gram based statistics of the training data. By
studying how well these rulesets approximate transformer predictions, we obtain
a variety of novel discoveries: a simple method to detect overfitting during
training without using a holdout set, a quantitative measure of how
transformers progress from learning simple to more complex statistical rules
over the course of training, a model-variance criterion governing when
transformer predictions tend to be described by N-gram rules, and insights into
how well transformers can be approximated by N-gram rulesets in the limit where
these rulesets become increasingly complex. In this latter direction, we find
that for 79% and 68% of LLM next-token distributions on TinyStories and
Wikipedia, respectively, their top-1 predictions agree with those provided by
our N-gram rulesets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024. Datasets and N-gram statistics open-sourced:
  https://github.com/google-deepmind/transformer_ngrams</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bias in the Mirror: Are LLMs opinions robust to their own adversarial
  attacks ? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Virgile Rennard, Christos Xypolopoulos, Michalis Vazirgiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) inherit biases from their training data and
alignment processes, influencing their responses in subtle ways. While many
studies have examined these biases, little work has explored their robustness
during interactions. In this paper, we introduce a novel approach where two
instances of an LLM engage in self-debate, arguing opposing viewpoints to
persuade a neutral version of the model. Through this, we evaluate how firmly
biases hold and whether models are susceptible to reinforcing misinformation or
shifting to harmful viewpoints. Our experiments span multiple LLMs of varying
sizes, origins, and languages, providing deeper insights into bias persistence
and flexibility across linguistic and cultural contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned
  Judge Model is not a General Substitute for <span class="highlight-title">GPT</span>-4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02839v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02839v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Huang, Yingqi Qu, Xingyuan Bu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, Tiejun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a growing trend of utilizing Large Language Model
(LLM) to evaluate the quality of other LLMs. Many studies have employed
proprietary close-sourced models, especially GPT-4, as the evaluator.
Alternatively, other works have fine-tuned judge models based on open-source
LLMs as the evaluator. While the fine-tuned judge models are claimed to achieve
comparable evaluation capability with GPT-4, in this work, we conduct an
empirical study of judge models. Our findings indicate that although the
fine-tuned judge models achieve high performance on in-domain test sets, even
surpassing GPT-4, they underperform GPT-4 across several dimensions, including
generalizability, fairness, aspect-specific evaluation, and scalability. We
also reveal that the fine-tuned judge model inherently operates as a
task-specific classifier, consequently imposing the limitations. Finally, we
introduce a integrated method, leveraging GPT-4 to compensate for the
limitations and improve the fine-tuned judges. Experiment results show our
method achieves accuracy on par with GPT-4 with only 50% of the API expense.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreaking Large Language Models with Symbolic Mathematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emet Bethany, Mazal Bethany, Juan Arturo Nolazco Flores, Sumit Kumar Jha, Peyman Najafirad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in AI safety have led to increased efforts in training
and red-teaming large language models (LLMs) to mitigate unsafe content
generation. However, these safety mechanisms may not be comprehensive, leaving
potential vulnerabilities unexplored. This paper introduces MathPrompt, a novel
jailbreaking technique that exploits LLMs' advanced capabilities in symbolic
mathematics to bypass their safety mechanisms. By encoding harmful natural
language prompts into mathematical problems, we demonstrate a critical
vulnerability in current AI safety measures. Our experiments across 13
state-of-the-art LLMs reveal an average attack success rate of 73.6\%,
highlighting the inability of existing safety training mechanisms to generalize
to mathematically encoded inputs. Analysis of embedding vectors shows a
substantial semantic shift between original and encoded prompts, helping
explain the attack's success. This work emphasizes the importance of a holistic
approach to AI safety, calling for expanded red-teaming efforts to develop
robust safeguards across all potential input types and their associated risks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancements and limitations of LLMs in replicating human color-word
  associations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color-word associations play a fundamental role in human cognition and design
applications. Large Language Models (LLMs) have become widely available and
demonstrated intelligent behaviors in various benchmarks with natural
conversation skills. However, their ability to replicate human color-word
associations remains understudied. We compared multiple generations of LLMs
(from GPT-3 to GPT-4o) against human color-word associations using data
collected from over 10,000 Japanese participants, involving 17 colors and words
from eight categories in Japanese. Our findings reveal a clear progression in
LLM performance across generations, with GPT-4o achieving the highest accuracy
in predicting the best voted word for each color and category. However, the
highest median performance was approximately 50% even for GPT-4o with visual
inputs (chance level is 10%), and the performance levels varied significantly
across word categories and colors, indicating a failure to fully replicate
human color-word associations. On the other hand, color discrimination ability
estimated from our color-word association data showed that LLMs demonstrated
high correlation with human color discrimination patterns, similarly to
previous studies. Our study highlights both the advancements in LLM
capabilities and their persistent limitations, suggesting differences in
semantic memory structures between humans and LLMs in representing color-word
associations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAPE: Data-Adaptive Positional Encoding for Length Extrapolation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14722v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14722v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Positional encoding plays a crucial role in transformers, significantly
impacting model performance and length generalization. Prior research has
introduced absolute positional encoding (APE) and relative positional encoding
(RPE) to distinguish token positions in given sequences. However, both APE and
RPE remain fixed after model training regardless of input data, limiting their
adaptability and flexibility. Hence, we expect that the desired positional
encoding should be data-adaptive and can be dynamically adjusted with the given
attention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)
method, which dynamically and semantically adjusts based on input context and
learned fixed priors. Experimental validation on real-world datasets (Arxiv,
Books3, and CHE) demonstrates that DAPE enhances model performances in terms of
trained length and length generalization, where the improvements are
statistically significant. The model visualization suggests that our model can
keep both local and anti-local information. Finally, we successfully train the
model on sequence length 128 and achieve better performance at evaluation
sequence length 8192, compared with other static positional encoding methods,
revealing the benefit of the adaptive positional encoding method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeTAR: Out-of-Distribution Detection with Selective Low-Rank
  Approximation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12629v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12629v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixia Li, Boya Xiong, Guanhua Chen, Yun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is crucial for the safe deployment of
neural networks. Existing CLIP-based approaches perform OOD detection by
devising novel scoring functions or sophisticated fine-tuning methods. In this
work, we propose SeTAR, a novel, training-free OOD detection method that
leverages selective low-rank approximation of weight matrices in
vision-language and vision-only models. SeTAR enhances OOD detection via
post-hoc modification of the model's weight matrices using a simple greedy
search algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning
extension optimizing model performance for OOD detection tasks. Extensive
evaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior
performance, reducing the relatively false positive rate by up to 18.95% and
36.80% compared to zero-shot and fine-tuning baselines. Ablation studies
further validate SeTAR's effectiveness, robustness, and generalizability across
different model backbones. Our work offers a scalable, efficient solution for
OOD detection, setting a new state-of-the-art in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. Project page is live at
  https://SeTAR-OOD.github.io. Code are available at
  https://github.com/X1AOX1A/SeTAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Limits of <span class="highlight-title">Transformer</span> Language Models on Learning to Compose Algorithms <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05785v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05785v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Thomm, Giacomo Camposampiero, Aleksandar Terzic, Michael Hersche, Bernhard Schölkopf, Abbas Rahimi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the capabilities of Transformer language models in learning
compositional discrete tasks. To this end, we evaluate training LLaMA models
and prompting GPT-4 and Gemini on four tasks demanding to learn a composition
of several discrete sub-tasks. In particular, we measure how well these models
can reuse primitives observable in the sub-tasks to learn the composition task.
Our results indicate that compositional learning in state-of-the-art
Transformer language models is highly sample inefficient: LLaMA requires more
data samples than relearning all sub-tasks from scratch to learn the
compositional task; in-context prompting with few samples is unreliable and
fails at executing the sub-tasks or correcting the errors in multi-round code
generation. Further, by leveraging complexity theory, we support these findings
with a theoretical analysis focused on the sample inefficiency of gradient
descent in memorizing feedforward models. We open source our code at
https://github.com/IBM/limitations-lm-algorithmic-compositional-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation and Improvement of Fault Detection for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14419v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14419v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Hu, Jin Wen, Maxime Cordy, Yuheng Huang, Wei Ma, Xiaofei Xie, Lei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently achieved significant success
across various application domains, garnering substantial attention from
different communities. Unfortunately, even for the best LLM, many
\textit{faults} still exist that LLM cannot properly predict. Such faults will
harm the usability of LLMs in general and could introduce safety issues in
reliability-critical systems such as autonomous driving systems. How to quickly
reveal these faults in real-world datasets that LLM could face is important,
but challenging. The major reason is that the ground truth is necessary but the
data labeling process is heavy considering the time and human effort. To handle
this problem, in the conventional deep learning testing field, test selection
methods have been proposed for efficiently evaluating deep learning models by
prioritizing faults. However, despite their importance, the usefulness of these
methods on LLMs is unclear, and lack of exploration. In this paper, we conduct
the first empirical study to investigate the effectiveness of existing fault
detection methods for LLMs. Experimental results on four different
tasks~(including both code tasks and natural language processing tasks) and
four LLMs~(e.g., LLaMA3 and GPT4) demonstrated that simple methods such as
Margin perform well on LLMs but there is still a big room for improvement.
Based on the study, we further propose \textbf{MuCS}, a prompt
\textbf{Mu}tation-based prediction \textbf{C}onfidence \textbf{S}moothing
framework to boost the fault detection capability of existing methods.
Concretely, multiple prompt mutation techniques have been proposed to help
collect more diverse outputs for confidence smoothing. The results show that
our proposed framework significantly enhances existing methods with the
improvement of test relative coverage by up to 70.53\%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Preference Alignment Remedies Degradation of Visual
  Instruction Tuning on Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10884v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10884v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengzhi Li, Rongyu Lin, Shichao Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal large language models (MLLMs) are expected to support multi-turn
queries of interchanging image and text modalities in production. However, the
current MLLMs trained with visual-question-answering (VQA) datasets could
suffer from degradation, as VQA datasets lack the diversity and complexity of
the original text instruction datasets with which the underlying language model
was trained. To address this degradation, we first collect a lightweight,
5k-sample VQA preference dataset where answers were annotated by Gemini for
five quality metrics in a granular fashion and investigate standard Supervised
Fine-tuning, rejection sampling, Direct Preference Optimization (DPO) and
SteerLM algorithms. Our findings indicate that with DPO, we can surpass the
instruction-following capabilities of the language model, achieving a 6.73
score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99. This enhancement
in textual instruction-following capability correlates with boosted visual
instruction performance (+4.9\% on MM-Vet, +6\% on LLaVA-Bench), with minimal
alignment tax on visual knowledge benchmarks compared to the previous RLHF
approach. In conclusion, we propose a distillation-based multi-modal alignment
model with fine-grained annotations on a small dataset that restores and boosts
MLLM's language capability after visual instruction tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project code, model and data: https://github.com/findalexli/mllm-dpo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Layer-wise Importance Matters: Less Memory for Better Performance in
  <span class="highlight-title">Parameter-efficient Fine-tuning</span> of Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11772v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11772v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yao, Penglei Gao, Lichun Li, Yuan Zhao, Xiaofeng Wang, Wei Wang, Jianke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant
popularity for adapting pre-trained Large Language Models (LLMs) to downstream
tasks, primarily due to their potential to significantly reduce memory and
computational overheads. However, a common limitation in most PEFT approaches
is their application of a uniform architectural design across all layers. This
uniformity involves identical trainable modules and ignores the varying
importance of each layer, leading to sub-optimal fine-tuning results. To
overcome the above limitation and obtain better performance, we develop a novel
approach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent
sparsity and select the most important subset of full layers with effective
layer-wise importance scoring. The proposed IST is a versatile and
plug-and-play technique compatible with various PEFT methods that operate on a
per-layer basis. By leveraging the estimated importance scores, IST dynamically
updates these selected layers in PEFT modules, leading to reduced memory
demands. We further provide theoretical proof of convergence and empirical
evidence of superior performance to demonstrate the advantages of IST over
uniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,
and downstream tasks substantiate the effectiveness of our proposed method,
showcasing IST's capacity to enhance existing layer-based PEFT methods. Our
code is available at https://github.com/Kaiseem/IST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning to Thousands of Preferences via System Message Generalization <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongyun Lee, Sue Hyun Park, Seungone Kim, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although humans inherently have diverse values, current large language model
(LLM) alignment methods often assume that aligning LLMs with the general
public's preferences is optimal. A major challenge in adopting a more
individualized approach to LLM alignment is its lack of scalability, as it
involves repeatedly acquiring preference data and training new reward models
and LLMs for each individual's preferences. To address these challenges, we
propose a new paradigm where users specify what they value most within the
system message, steering the LLM's generation behavior to better align with the
user's intentions. However, a naive application of such an approach is
non-trivial since LLMs are typically trained on a uniform system message (e.g.,
"You are a helpful assistant") which limits their ability to generalize to
diverse, unseen system messages. To improve this generalization, we create the
Multifaceted Collection, a preference dataset with 192k combinations of values
beyond generic helpfulness and harmlessness, spanning 65k user instructions.
Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts
from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)
by adding various unseen system messages that reflect user preferences. Janus
achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct
v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks
focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto
v0.1), Janus also outperforms LLaMA 3 8B Instruct by a +4.0%, +0.1%, +3.0%
margin, underscoring that training with a vast array of system messages could
also enhance alignment to the general public's preference as well. Our code,
dataset, benchmark, and models are available at
https://github.com/kaistAI/Janus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Birdie: Advancing State Space Models with Reward-Driven Objectives and
  Curricula <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Blouir, Jimmy T. H. Smith, Antonios Anastasopoulos, Amarda Shehu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient state space models (SSMs), such as linear recurrent neural networks
and linear attention variants, offer computational advantages over Transformers
but struggle with tasks requiring long-range in-context retrieval-like text
copying, associative recall, and question answering over long contexts.
Previous efforts to address these challenges have focused on architectural
modifications, often reintroducing computational inefficiencies. In this paper,
we propose a novel training procedure, Birdie, that significantly enhances the
in-context retrieval capabilities of SSMs without altering their architecture.
Our approach combines bidirectional input processing with dynamic mixtures of
specialized pre-training objectives, optimized via reinforcement learning. We
introduce a new bidirectional SSM architecture that seamlessly transitions from
bidirectional context processing to causal generation. Experimental evaluations
demonstrate that Birdie markedly improves performance on retrieval-intensive
tasks such as multi-number phone book lookup, long paragraph
question-answering, and infilling. This narrows the performance gap with
Transformers, while retaining computational efficiency. Our findings highlight
the importance of training procedures in leveraging the fixed-state capacity of
SSMs, offering a new direction to advance their capabilities. All code and
pre-trained models are available at https://www.github.com/samblouir/birdie,
with support for JAX and PyTorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Investigation of Warning Erroneous Chat Translations in Cross-lingual
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunmeng Li, Jun Suzuki, Makoto Morishita, Kaori Abe, Kentaro Inui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine translation models are still inappropriate for translating chats,
despite the popularity of translation software and plug-in applications. The
complexity of dialogues poses significant challenges and can hinder
crosslingual communication. Instead of pursuing a flawless translation system,
a more practical approach would be to issue warning messages about potential
mistranslations to reduce confusion. However, it is still unclear how
individuals perceive these warning messages and whether they benefit the crowd.
This paper tackles to investigate this question and demonstrates the warning
messages' contribution to making chat translation systems effective.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">49</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Ego-Body Pose from Doubly Sparse Egocentric Video Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seunggeun Chi, Pin-Hao Huang, Enna Sachdeva, Hengbo Ma, Karthik Ramani, Kwonjoon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of estimating the body movements of a camera wearer from
egocentric videos. Current methods for ego-body pose estimation rely on
temporally dense sensor data, such as IMU measurements from spatially sparse
body parts like the head and hands. However, we propose that even temporally
sparse observations, such as hand poses captured intermittently from egocentric
videos during natural or periodic hand movements, can effectively constrain
overall body motion. Naively applying diffusion models to generate full-body
pose from head pose and sparse hand pose leads to suboptimal results. To
overcome this, we develop a two-stage approach that decomposes the problem into
temporal completion and spatial completion. First, our method employs masked
autoencoders to impute hand trajectories by leveraging the spatiotemporal
correlations between the head pose sequence and intermittent hand poses,
providing uncertainty estimates. Subsequently, we employ conditional diffusion
models to generate plausible full-body motions based on these temporally dense
trajectories of the head and hands, guided by the uncertainty estimates from
the imputation. The effectiveness of our method was rigorously tested and
validated through comprehensive experiments conducted on various HMD setup with
AMASS and Ego-Exo4D datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object and Contact Point Tracking in Demonstrations Using 3D Gaussian
  Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Büttner, Jonathan Francis, Helge Rhodin, Andrew Melnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a method to enhance Interactive Imitation Learning
(IIL) by extracting touch interaction points and tracking object movement from
video demonstrations. The approach extends current IIL systems by providing
robots with detailed knowledge of both where and how to interact with objects,
particularly complex articulated ones like doors and drawers. By leveraging
cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for
tracking, this method allows robots to better understand and manipulate objects
in dynamic environments. The research lays the foundation for more effective
task learning and execution in autonomous robotic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoRL 2024, Workshop on Lifelong Learning for Home Robots, Munich,
  Germany</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Vision Language Model Unlearning via Fictitious Facial
  Identity <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingzi Ma, Jiongxiao Wang, Fei Wang, Siyuan Ma, Jiazhao Li, Xiujun Li, Furong Huang, Lichao Sun, Bo Li, Yejin Choi, Muhao Chen, Chaowei Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning has emerged as an effective strategy for forgetting
specific information in the training data. However, with the increasing
integration of visual data, privacy concerns in Vision Language Models (VLMs)
remain underexplored. To address this, we introduce Facial Identity Unlearning
Benchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly
evaluate the effectiveness of unlearning algorithms under the Right to be
Forgotten setting. Specifically, we formulate the VLM unlearning task via
constructing the Fictitious Facial Identity VQA dataset and apply a two-stage
evaluation pipeline that is designed to precisely control the sources of
information and their exposure levels. In terms of evaluation, since VLM
supports various forms of ways to ask questions with the same semantic meaning,
we also provide robust evaluation metrics including membership inference
attacks and carefully designed adversarial privacy attacks to evaluate the
performance of algorithms. Through the evaluation of four baseline VLM
unlearning algorithms within FIUBench, we find that all methods remain limited
in their unlearning performance, with significant trade-offs between model
utility and forget quality. Furthermore, our findings also highlight the
importance of privacy attacks for robust evaluations. We hope FIUBench will
drive progress in developing more effective VLM unlearning algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Weakly Supervised Semantic Segmentation for Fibrosis via
  Controllable Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiling Yue, Yingying Fang, Liutao Yang, Nikhil Baid, Simon Walsh, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fibrotic Lung Disease (FLD) is a severe condition marked by lung stiffening
and scarring, leading to respiratory decline. High-resolution computed
tomography (HRCT) is critical for diagnosing and monitoring FLD; however,
fibrosis appears as irregular, diffuse patterns with unclear boundaries,
leading to high inter-observer variability and time-intensive manual
annotation. To tackle this challenge, we propose DiffSeg, a novel weakly
supervised semantic segmentation (WSSS) method that uses image-level
annotations to generate pixel-level fibrosis segmentation, reducing the need
for fine-grained manual labeling. Additionally, our DiffSeg incorporates a
diffusion-based generative model to synthesize HRCT images with different
levels of fibrosis from healthy slices, enabling the generation of the
fibrosis-injected slices and their paired fibrosis location. Experiments
indicate that our method significantly improves the accuracy of pseudo masks
generated by existing WSSS methods, greatly reducing the complexity of manual
labeling and enhancing the consistency of the generated masks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Video Summarization by Multimodal Video Understanding <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Chen, Xiangyuan Zhao, Yingnan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video summarization techniques have been proven to improve the overall user
experience when it comes to accessing and comprehending video content. If the
user's preference is known, video summarization can identify significant
information or relevant content from an input video, aiding them in obtaining
the necessary information or determining their interest in watching the
original video. Adapting video summarization to various types of video and user
preferences requires significant training data and expensive human labeling. To
facilitate such research, we proposed a new benchmark for video summarization
that captures various user preferences. Also, we present a pipeline called
Video Summarization with Language (VSL) for user-preferred video summarization
that is based on pre-trained visual language models (VLMs) to avoid the need to
train a video summarization system on a large training dataset. The pipeline
takes both video and closed captioning as input and performs semantic analysis
at the scene level by converting video frames into text. Subsequently, the
user's genre preference was used as the basis for selecting the pertinent
textual scenes. The experimental results demonstrate that our proposed pipeline
outperforms current state-of-the-art unsupervised video summarization models.
We show that our method is more adaptable across different datasets compared to
supervised query-based video summarization models. In the end, the runtime
analysis demonstrates that our pipeline is more suitable for practical use when
scaling up the number of user preferences and videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of CIKM 2024 Applied Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Complete Shapes: A quantitative Evaluation of 3D Shape Matching
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktoria Ehm, Nafie El Amrani, Yizheng Xie, Lennart Bastian, Maolin Gao, Weikang Wang, Lu Sang, Dongliang Cao, Zorah Lähner, Daniel Cremers, Florian Bernard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding correspondences between 3D shapes is an important and long-standing
problem in computer vision, graphics and beyond. While approaches based on
machine learning dominate modern 3D shape matching, almost all existing
(learning-based) methods require that at least one of the involved shapes is
complete. In contrast, the most challenging and arguably most practically
relevant setting of matching partially observed shapes, is currently
underexplored. One important factor is that existing datasets contain only a
small number of shapes (typically below 100), which are unable to serve
data-hungry machine learning approaches, particularly in the unsupervised
regime. In addition, the type of partiality present in existing datasets is
often artificial and far from realistic. To address these limitations and to
encourage research on these relevant settings, we provide a generic and
flexible framework for the procedural generation of challenging partial shape
matching scenarios. Our framework allows for a virtually infinite generation of
partial shape matching instances from a finite set of shapes with complete
geometry. Further, we manually create cross-dataset correspondences between
seven existing (complete geometry) shape matching datasets, leading to a total
of 2543 shapes. Based on this, we propose several challenging partial benchmark
settings, for which we evaluate respective state-of-the-art methods as
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynthSet: Generative <span class="highlight-title">Diffusion</span> Model for Semantic Segmentation in
  Precision Agriculture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Heschl, Mauricio Murillo, Keyhan Najafian, Farhad Maleki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a methodology for generating synthetic annotated data
to address data scarcity in semantic segmentation tasks within the precision
agriculture domain. Utilizing Denoising Diffusion Probabilistic Models (DDPMs)
and Generative Adversarial Networks (GANs), we propose a dual diffusion model
architecture for synthesizing realistic annotated agricultural data, without
any human intervention. We employ super-resolution to enhance the phenotypic
characteristics of the synthesized images and their coherence with the
corresponding generated masks. We showcase the utility of the proposed method
for wheat head segmentation. The high quality of synthesized data underscores
the effectiveness of the proposed methodology in generating image-mask pairs.
Furthermore, models trained on our generated data exhibit promising performance
when tested on an external, diverse dataset of real wheat fields. The results
show the efficacy of the proposed methodology for addressing data scarcity for
semantic segmentation tasks. Moreover, the proposed approach can be readily
adapted for various segmentation tasks in precision agriculture and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Application-Agnostic Automatic Target Recognition System Using Vision
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Palladino, Dana Gajewski, Abigail Aronica, Patryk Deptula, Alexander Hamme, Seiyoung C. Lee, Jeff Muri, Todd Nelling, Michael A. Riley, Brian Wong, Margaret Duff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel Automatic Target Recognition (ATR) system using
open-vocabulary object detection and classification models. A primary advantage
of this approach is that target classes can be defined just before runtime by a
non-technical end user, using either a few natural language text descriptions
of the target, or a few image exemplars, or both. Nuances in the desired
targets can be expressed in natural language, which is useful for unique
targets with little or no training data. We also implemented a novel
combination of several techniques to improve performance, such as leveraging
the additional information in the sequence of overlapping frames to perform
tubelet identification (i.e., sequential bounding box matching), bounding box
re-scoring, and tubelet linking. Additionally, we developed a technique to
visualize the aggregate output of many overlapping frames as a mosaic of the
area scanned during the aerial surveillance or reconnaissance, and a kernel
density estimate (or heatmap) of the detected targets. We initially applied
this ATR system to the use case of detecting and clearing unexploded ordinance
on airfield runways and we are currently extending our research to other
real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Thirty-Seventh Annual Conference on Innovative
  Applications of Artificial Intelligence (IAAI-25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rainfall regression from C-band Synthetic Aperture Radar using
  Multi-Task Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aurélien Colin, Romain Husson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a data-driven approach to estimate precipitation rates
from Synthetic Aperture Radar (SAR) at a spatial resolution of 200 meters per
pixel. It addresses previous challenges related to the collocation of SAR and
weather radar data, specifically the misalignment in collocations and the
scarcity of rainfall examples under strong wind. To tackle these challenges,
the paper proposes a multi-objective formulation, introducing patch-level
components and an adversarial component. It exploits the full NEXRAD archive to
look for potential co-locations with Sentinel-1 data. With additional
enhancements to the training procedure and the incorporation of additional
inputs, the resulting model demonstrates improved accuracy in rainfall
estimates and the ability to extend its performance to scenarios up to 15 m/s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self Supervised Networks for Learning Latent Space Representations of
  Human Body Scans and Motions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanuel Hartman, Nicolas Charon, Martin Bauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces self-supervised neural network models to tackle several
fundamental problems in the field of 3D human body analysis and processing.
First, we propose VariShaPE (Varifold Shape Parameter Estimator), a novel
architecture for the retrieval of latent space representations of body shapes
and poses. This network offers a fast and robust method to estimate the
embedding of arbitrary unregistered meshes into the latent space. Second, we
complement the estimation of latent codes with MoGeN (Motion Geometry Network)
a framework that learns the geometry on the latent space itself. This is
achieved by lifting the body pose parameter space into a higher dimensional
Euclidean space in which body motion mini-sequences from a training set of 4D
data can be approximated by simple linear interpolation. Using the SMPL latent
space representation we illustrate how the combination of these network models,
once trained, can be used to perform a variety of tasks with very limited
computational cost. This includes operations such as motion interpolation,
extrapolation and transfer as well as random shape and pose generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TopoTxR: A topology-guided deep convolutional network for breast
  parenchyma learning on DCE-MRIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Wang, Zhilin Zou, Nicole Sakla, Luke Partyka, Nil Rawal, Gagandeep Singh, Wei Zhao, Haibin Ling, Chuan Huang, Prateek Prasanna, Chao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characterization of breast parenchyma in dynamic contrast-enhanced magnetic
resonance imaging (DCE-MRI) is a challenging task owing to the complexity of
underlying tissue structures. Existing quantitative approaches, like radiomics
and deep learning models, lack explicit quantification of intricate and subtle
parenchymal structures, including fibroglandular tissue. To address this, we
propose a novel topological approach that explicitly extracts multi-scale
topological structures to better approximate breast parenchymal structures, and
then incorporates these structures into a deep-learning-based prediction model
via an attention mechanism. Our topology-informed deep learning model,
\emph{TopoTxR}, leverages topology to provide enhanced insights into tissues
critical for disease pathophysiology and treatment response. We empirically
validate \emph{TopoTxR} using the VICTRE phantom breast dataset, showing that
the topological structures extracted by our model effectively approximate the
breast parenchymal structures. We further demonstrate \emph{TopoTxR}'s efficacy
in predicting response to neoadjuvant chemotherapy. Our qualitative and
quantitative analyses suggest differential topological behavior of breast
tissue in treatment-na\"ive imaging, in patients who respond favorably to
therapy as achieving pathological complete response (pCR) versus those who do
not. In a comparative analysis with several baselines on the publicly available
I-SPY 1 dataset (N=161, including 47 patients with pCR and 114 without) and the
Rutgers proprietary dataset (N=120, with 69 patients achieving pCR and 51 not),
\emph{TopoTxR} demonstrates a notable improvement, achieving a 2.6\% increase
in accuracy and a 4.6\% enhancement in AUC compared to the state-of-the-art
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 8 figures, 8 tables, accepted by Medical Image Analysis (
  https://www.sciencedirect.com/science/article/abs/pii/S1361841524002986 )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BOston Neonatal Brain Injury Data for Hypoxic Ischemic Encephalopathy
  (BONBID-HIE): II. 2-year Neurocognitive Outcome and NICU Outcome <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rina Bao, Yangming Ou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypoxic Ischemic Encephalopathy (HIE) affects approximately 1-5/1000 newborns
globally and leads to adverse neurocognitive outcomes in 30% to 50% of cases by
two years of age. Despite therapeutic advances with Therapeutic Hypothermia
(TH), prognosis remains challenging, highlighting the need for improved
biomarkers. This paper introduces the second release of the Boston Neonatal
Brain Injury Dataset for Hypoxic-Ischemic Encephalopathy (BONBID-HIE), an
open-source, comprehensive MRI and clinical dataset featuring 237 patients,
including NICU outcomes and 2-year neurocognitive outcomes from Massachusetts
General Hospital and Boston Children's Hospital.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data description for BONBID-HIE 2024 Challenge on MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Trojan Detection Competitions with Linear Weight Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Todd Huster, Peter Lin, Razvan Stefanescu, Emmanuel Ekwedike, Ritu Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks can conceal malicious Trojan backdoors that allow a trigger
to covertly change the model behavior. Detecting signs of these backdoors,
particularly without access to any triggered data, is the subject of ongoing
research and open challenges. In one common formulation of the problem, we are
given a set of clean and poisoned models and need to predict whether a given
test model is clean or poisoned. In this paper, we introduce a detector that
works remarkably well across many of the existing datasets and domains. It is
obtained by training a binary classifier on a large number of models' weights
after performing a few different pre-processing steps including feature
selection and standardization, reference model weights subtraction, and model
alignment prior to detection. We evaluate this algorithm on a diverse set of
Trojan detection benchmarks and domains and examine the cases where the
approach is most and least effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MME-Finance: A Multimodal Finance Benchmark for Expert-level
  Understanding and Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziliang Gan, Yu Lu, Dong Zhang, Haohan Li, Che Liu, Jian Liu, Ji Liu, Haipang Wu, Chaoyou Fu, Zenglin Xu, Rongjunchen Zhang, Yong Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, multimodal benchmarks for general domains have guided the
rapid development of multimodal models on general tasks. However, the financial
field has its peculiarities. It features unique graphical images (e.g.,
candlestick charts, technical indicator charts) and possesses a wealth of
specialized financial knowledge (e.g., futures, turnover rate). Therefore,
benchmarks from general fields often fail to measure the performance of
multimodal models in the financial domain, and thus cannot effectively guide
the rapid development of large financial models. To promote the development of
large financial multimodal models, we propose MME-Finance, an bilingual
open-ended and practical usage-oriented Visual Question Answering (VQA)
benchmark. The characteristics of our benchmark are finance and expertise,
which include constructing charts that reflect the actual usage needs of users
(e.g., computer screenshots and mobile photography), creating questions
according to the preferences in financial domain inquiries, and annotating
questions by experts with 10+ years of experience in the financial industry.
Additionally, we have developed a custom-designed financial evaluation system
in which visual information is first introduced in the multi-modal evaluation
process. Extensive experimental evaluations of 19 mainstream MLLMs are
conducted to test their perception, reasoning, and cognition capabilities. The
results indicate that models performing well on general benchmarks cannot do
well on MME-Finance; for instance, the top-performing open-source and
closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o),
respectively. Their performance is particularly poor in categories most
relevant to finance, such as candlestick charts and technical indicator charts.
In addition, we propose a Chinese version, which helps compare performance of
MLLMs under a Chinese context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://hithink-research.github.io/MME-Finance/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference Optimal VLMs Need Only One Visual Token but Larger Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Y. Li, Sachin Goyal, Joao D. Semedo, J. Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have demonstrated strong capabilities across
various visual understanding and reasoning tasks. However, their real-world
deployment is often constrained by high latency during inference due to
substantial compute required to process the large number of input tokens
(predominantly from the image) by the LLM. To reduce inference costs, one can
either downsize the LLM or reduce the number of input image-tokens, the latter
of which has been the focus of many recent works around token compression.
However, it is unclear what the optimal trade-off is, as both the factors
directly affect the VLM performance. We first characterize this optimal
trade-off between the number of visual tokens and LLM parameters by
establishing scaling laws that capture variations in performance with these two
factors. Our results reveal a surprising trend: for visual reasoning tasks, the
inference-optimal behavior in VLMs, i.e., minimum downstream error at any given
fixed inference compute, is achieved when using the largest LLM that fits
within the inference budget while minimizing visual token count - often to a
single token. While the token reduction literature has mainly focused on
maintaining base model performance by modestly reducing the token count (e.g.,
$5-10\times$), our results indicate that the compute-optimal inference regime
requires operating under even higher token compression ratios. Based on these
insights, we take some initial steps towards building approaches tailored for
high token compression settings. Code is available at
https://github.com/locuslab/llava-token-compression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Spatial and Verbal Losses for 3D Visual Grounding <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sombit Dey, Ozan Unal, Christos Sakaridis, Luc Van Gool
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding consists of identifying the instance in a 3D scene which
is referred by an accompanying language description. While several
architectures have been proposed within the commonly employed
grounding-by-selection framework, the utilized losses are comparatively
under-explored. In particular, most methods rely on a basic supervised
cross-entropy loss on the predicted distribution over candidate instances,
which fails to model both spatial relations between instances and the internal
fine-grained word-level structure of the verbal referral. Sparse attempts to
additionally supervise verbal embeddings globally by learning the class of the
referred instance from the description or employing verbo-visual contrast to
better separate instance embeddings do not fundamentally lift the
aforementioned limitations. Responding to these shortcomings, we introduce two
novel losses for 3D visual grounding: a visual-level offset loss on regressed
vector offsets from each instance to the ground-truth referred instance and a
language-related span loss on predictions for the word-level span of the
referred instance in the description. In addition, we equip the verbo-visual
fusion module of our new 3D visual grounding architecture AsphaltNet with a
top-down bidirectional attentive fusion block, which enables the supervisory
signals from our two losses to propagate to the respective converse branches of
the network and thus aid the latter to learn context-aware instance embeddings
and grounding-aware verbal embeddings. AsphaltNet proposes novel auxiliary
losses to aid 3D visual grounding with competitive results compared to the
state-of-the-art on the ReferIt3D benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Maritime Situational Awareness through End-to-End Onboard Raw
  Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Del Prete, Manuel Salvoldi, Domenico Barretta, Nicolas Longépé, Gabriele Meoni, Arnon Karnieli, Maria Daniela Graziano, Alfredo Renga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Satellite-based onboard data processing is crucial for time-sensitive
applications requiring timely and efficient rapid response. Advances in edge
artificial intelligence are shifting computational power from ground-based
centers to on-orbit platforms, transforming the
"sensing-communication-decision-feedback" cycle and reducing latency from
acquisition to delivery. The current research presents a framework addressing
the strict bandwidth, energy, and latency constraints of small satellites,
focusing on maritime monitoring. The study contributes three main innovations.
Firstly, it investigates the application of deep learning techniques for direct
ship detection and classification from raw satellite imagery. By simplifying
the onboard processing chain, our approach facilitates direct analyses without
requiring computationally intensive steps such as calibration and
ortho-rectification. Secondly, to address the scarcity of raw satellite data,
we introduce two novel datasets, VDS2Raw and VDV2Raw, which are derived from
raw data from Sentinel-2 and Vegetation and Environment Monitoring New Micro
Satellite (VENuS) missions, respectively, and enriched with Automatic
Identification System (AIS) records. Thirdly, we characterize the tasks'
optimal single and multiple spectral band combinations through statistical and
feature-based analyses validated on both datasets. In sum, we demonstrate the
feasibility of the proposed method through a proof-of-concept on CubeSat-like
hardware, confirming the models' potential for operational satellite-based
maritime monitoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiT4Edit: <span class="highlight-title">Diffusion</span> <span class="highlight-title">Transformer</span> for Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, Zeyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in UNet-based image editing, methods for shape-aware
object editing in high-resolution images are still lacking. Compared to UNet,
Diffusion Transformers (DiT) demonstrate superior capabilities to effectively
capture the long-range dependencies among patches, leading to higher-quality
image generation. In this paper, we propose DiT4Edit, the first Diffusion
Transformer-based image editing framework. Specifically, DiT4Edit uses the
DPM-Solver inversion algorithm to obtain the inverted latents, reducing the
number of steps compared to the DDIM inversion algorithm commonly used in
UNet-based frameworks. Additionally, we design unified attention control and
patches merging, tailored for transformer computation streams. This integration
allows our framework to generate higher-quality edited images faster. Our
design leverages the advantages of DiT, enabling it to surpass UNet structures
in image editing, especially in high-resolution and arbitrary-size images.
Extensive experiments demonstrate the strong performance of DiT4Edit across
various editing scenarios, highlighting the potential of Diffusion Transformers
in supporting image editing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ShadowMamba: State-Space Model with Boundary-Region Selective Scan for
  Shadow Removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiujin Zhu, Chee-Onn Chow, Joon Huang Chuah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image shadow removal is a typical low-level vision problem, where the
presence of shadows leads to abrupt changes in brightness in certain regions,
affecting the accuracy of upstream tasks. Current shadow removal methods still
face challenges such as residual boundary artifacts, and capturing feature
information at shadow boundaries is crucial for removing shadows and
eliminating residual boundary artifacts. Recently, Mamba has achieved
remarkable success in computer vision by globally modeling long-sequence
information with linear complexity. However, when applied to image shadow
removal, the original Mamba scanning method overlooks the semantic continuity
of shadow boundaries as well as the continuity of semantics within the same
region. Based on the unique characteristics of shadow images, this paper
proposes a novel selective scanning method called boundary-region selective
scanning. This method scans boundary regions, shadow regions, and non-shadow
regions independently, bringing pixels of the same region type closer together
in the long sequence, especially focusing on the local information at the
boundaries, which is crucial for shadow removal. This method combines with
global scanning and channel scanning to jointly accomplish the shadow removal.
We name our model ShadowMamba, the first Mamba-based model for shadow removal.
Extensive experimental results show that our method outperforms current
state-of-the-art models across most metrics on multiple datasets. The code for
ShadowMamba is available at (Code will be released upon acceptance).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupling Fine Detail and Global Geometry for Compressed Depth Map
  Super-Resolution <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Zheng, Wencheng Han, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering high-quality depth maps from compressed sources has gained
significant attention due to the limitations of consumer-grade depth cameras
and the bandwidth restrictions during data transmission. However, current
methods still suffer from two challenges. First, bit-depth compression produces
a uniform depth representation in regions with subtle variations, hindering the
recovery of detailed information. Second, densely distributed random noise
reduces the accuracy of estimating the global geometric structure of the scene.
To address these challenges, we propose a novel framework, termed
geometry-decoupled network (GDNet), for compressed depth map super-resolution
that decouples the high-quality depth map reconstruction process by handling
global and detailed geometric features separately. To be specific, we propose
the fine geometry detail encoder (FGDE), which is designed to aggregate fine
geometry details in high-resolution low-level image features while
simultaneously enriching them with complementary information from
low-resolution context-level image features. In addition, we develop the global
geometry encoder (GGE) that aims at suppressing noise and extracting global
geometric information effectively via constructing compact feature
representation in a low-rank space. We conduct experiments on multiple
benchmark datasets, demonstrating that our GDNet significantly outperforms
current methods in terms of geometric consistency and detail recovery. In the
ECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st
place award. Our codes will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 1st solution for the ECCV 2024 AIM Compressed Depth Upsampling
  Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topograph: An efficient Graph-Based Framework for Strictly Topology
  Preserving Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurin Lux, Alexander H. Berger, Alexander Weers, Nico Stucki, Daniel Rueckert, Ulrich Bauer, Johannes C. Paetzold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological correctness plays a critical role in many image segmentation
tasks, yet most networks are trained using pixel-wise loss functions, such as
Dice, neglecting topological accuracy. Existing topology-aware methods often
lack robust topological guarantees, are limited to specific use cases, or
impose high computational costs. In this work, we propose a novel, graph-based
framework for topologically accurate image segmentation that is both
computationally efficient and generally applicable. Our method constructs a
component graph that fully encodes the topological information of both the
prediction and ground truth, allowing us to efficiently identify topologically
critical regions and aggregate a loss based on local neighborhood information.
Furthermore, we introduce a strict topological metric capturing the homotopy
equivalence between the union and intersection of prediction-label pairs. We
formally prove the topological guarantees of our approach and empirically
validate its effectiveness on binary and multi-class datasets. Our loss
demonstrates state-of-the-art performance with up to fivefold faster loss
computation compared to persistent homology methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel Orthogonality does not necessarily imply a Decrease in Feature
  Map Redundancy in CNNs: Convolutional Similarity Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zakariae Belmekki, Jun Li, Patrick Reuter, David Antonio Gómez Jáuregui, Karl Jenkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) have been heavily used in Deep Learning
due to their success in various tasks. Nonetheless, it has been observed that
CNNs suffer from redundancy in feature maps, leading to inefficient capacity
utilization. Efforts to mitigate and solve this problem led to the emergence of
multiple methods, amongst which is kernel orthogonality through variant means.
In this work, we challenge the common belief that kernel orthogonality leads to
a decrease in feature map redundancy, which is, supposedly, the ultimate
objective behind kernel orthogonality. We prove, theoretically and empirically,
that kernel orthogonality has an unpredictable effect on feature map similarity
and does not necessarily decrease it. Based on our theoretical result, we
propose an effective method to reduce feature map similarity independently of
the input of the CNN. This is done by minimizing a novel loss function we call
Convolutional Similarity. Empirical results show that minimizing the
Convolutional Similarity increases the performance of classification models and
can accelerate their convergence. Furthermore, using our proposed method pushes
towards a more efficient use of the capacity of models, allowing the use of
significantly smaller models to achieve the same levels of performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities
  of Neurosymbolic AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruwan Wickramarachchi, Cory Henson, Amit Sheth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of Generative AI, Neurosymbolic AI is emerging as a powerful
approach for tasks spanning from perception to cognition. The use of
Neurosymbolic AI has been shown to achieve enhanced capabilities, including
improved grounding, alignment, explainability, and reliability. However, due to
its nascent stage, there is a lack of widely available real-world benchmark
datasets tailored to Neurosymbolic AI tasks. To address this gap and support
the evaluation of current and future methods, we introduce DSceneKG -- a suite
of knowledge graphs of driving scenes built from real-world, high-quality
scenes from multiple open autonomous driving datasets. In this article, we
detail the construction process of DSceneKG and highlight its application in
seven different tasks. DSceneKG is publicly accessible at:
https://github.com/ruwantw/DSceneKG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Improved Conditioning Mechanisms and Pre-training Strategies for
  <span class="highlight-title">Diffusion</span> Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tariq Berrada Ifriqi, Pietro Astolfi, Melissa Hall, Reyhane Askari-Hemmat, Yohann Benchetrit, Marton Havasi, Matthew Muckley, Karteek Alahari, Adriana Romero-Soriano, Jakob Verbeek, Michal Drozdzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale training of latent diffusion models (LDMs) has enabled
unprecedented quality in image generation. However, the key components of the
best performing LDM training recipes are oftentimes not available to the
research community, preventing apple-to-apple comparisons and hindering the
validation of progress in the field. In this work, we perform an in-depth study
of LDM training recipes focusing on the performance of models and their
training efficiency. To ensure apple-to-apple comparisons, we re-implement five
previously published models with their corresponding recipes. Through our
study, we explore the effects of (i)~the mechanisms used to condition the
generative model on semantic information (e.g., text prompt) and control
metadata (e.g., crop size, random flip flag, etc.) on the model performance,
and (ii)~the transfer of the representations learned on smaller and
lower-resolution datasets to larger ones on the training efficiency and model
performance. We then propose a novel conditioning mechanism that disentangles
semantic and control metadata conditionings and sets a new state-of-the-art in
class-conditional generation on the ImageNet-1k dataset -- with FID
improvements of 7% on 256 and 8% on 512 resolutions -- as well as text-to-image
generation on the CC12M dataset -- with FID improvements of 8% on 256 and 23%
on 512 resolution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper (poster) for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pre-trained Visual Dynamics Representations for Efficient Policy
  Learning <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Luo, Bohan Zhou, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training for Reinforcement Learning (RL) with purely video data is a
valuable yet challenging problem. Although in-the-wild videos are readily
available and inhere a vast amount of prior world knowledge, the absence of
action annotations and the common domain gap with downstream tasks hinder
utilizing videos for RL pre-training. To address the challenge of pre-training
with videos, we propose Pre-trained Visual Dynamics Representations (PVDR) to
bridge the domain gap between videos and downstream tasks for efficient policy
learning. By adopting video prediction as a pre-training task, we use a
Transformer-based Conditional Variational Autoencoder (CVAE) to learn visual
dynamics representations. The pre-trained visual dynamics representations
capture the visual dynamics prior knowledge in the videos. This abstract prior
knowledge can be readily adapted to downstream tasks and aligned with
executable actions through online adaptation. We conduct experiments on a
series of robotics visual control tasks and verify that PVDR is an effective
form for pre-training with videos to promote policy learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MA^2: A Self-Supervised and Motion Augmenting Autoencoder for Gait-Based
  Automatic Disease Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Liu, Ke Zhang, Yin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ground reaction force (GRF) is the force exerted by the ground on a body in
contact with it. GRF-based automatic disease detection (ADD) has become an
emerging medical diagnosis method, which aims to learn and identify disease
patterns corresponding to different gait pressures based on deep learning
methods. Although existing ADD methods can save doctors time in making
diagnoses, training deep models still struggles with the cost caused by the
labeling engineering for a large number of gait diagnostic data for subjects.
On the other hand, the accuracy of the deep model under the unified benchmark
GRF dataset and the generalization ability on scalable gait datasets need to be
further improved. To address these issues, we propose MA2, a GRF-based
self-supervised and motion augmenting auto-encoder, which models the ADD task
as an encoder-decoder paradigm. In the encoder, we introduce an embedding block
including the 3-layer 1D convolution for extracting the token and a mask
generator to randomly mask out the sequence of tokens to maximize the model's
potential to capture high-level, discriminative, intrinsic representations.
whereafter, the decoder utilizes this information to reconstruct the pixel
sequence of the origin input and calculate the reconstruction loss to optimize
the network. Moreover, the backbone of an auto-encoder is multi-head
self-attention that can consider the global information of the token from the
input, not just the local neighborhood. This allows the model to capture
generalized contextual information. Extensive experiments demonstrate MA2 has
SOTA performance of 90.91% accuracy on 1% limited pathological GRF samples with
labels, and good generalization ability of 78.57% accuracy on scalable
Parkinson disease dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LADDER: Language Driven Slice Discovery and Error Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07832v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07832v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Shyam Visweswaran, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error slice discovery associates structured patterns with model errors.
Existing methods discover error slices by clustering the error-prone samples
with similar patterns or assigning discrete attributes to each sample for
post-hoc analysis. While these methods aim for interpretability and easier
mitigation through reweighting or rebalancing, they may not capture the full
complexity of error patterns due to incomplete or missing attributes. Contrary
to the existing approach, this paper utilizes the reasoning capabilities of the
Large Language Model (LLM) to analyze complex error patterns and generate
testable hypotheses. This paper proposes LADDER: Language Driven slice
Discovery and Error Rectification. It first projects the model's representation
into a language-aligned feature space (eg CLIP) to preserve semantics in the
original model feature space. This ensures the accurate retrieval of sentences
that highlight the model's errors. Next, the LLM utilizes the sentences and
generates hypotheses to discover error slices. Finally, we mitigate the error
by fine-tuning the classification head by creating a group-balanced dataset
using the hypotheses. Our entire method does not require any attribute
annotation, either explicitly or through external tagging models. We validate
our method with \textbf{five} image classification datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form
  Egocentric Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05269v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05269v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Wang, Yanlai Yang, Mengye Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce LifelongMemory, a new framework for accessing
long-form egocentric videographic memory through natural language question
answering and retrieval. LifelongMemory generates concise video activity
descriptions of the camera wearer and leverages the zero-shot capabilities of
pretrained large language models to perform reasoning over long-form video
context. Furthermore, LifelongMemory uses a confidence and explanation module
to produce confident, high-quality, and interpretable answers. Our approach
achieves state-of-the-art performance on the EgoSchema benchmark for question
answering and is highly competitive on the natural language query (NLQ)
challenge of Ego4D. Code is available at
https://github.com/agentic-learning-ai-lab/lifelong-memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ D3: Data Diversity Design for Systematic Generalization in Visual
  Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Rahimi, Vanessa D'Amario, Moyuru Yamada, Kentaro Takemoto, Tomotake Sasaki, Xavier Boix
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systematic generalization is a crucial aspect of intelligence, which refers
to the ability to generalize to novel tasks by combining known subtasks and
concepts. One critical factor that has been shown to influence systematic
generalization is the diversity of training data. However, diversity can be
defined in various ways, as data have many factors of variation. A more
granular understanding of how different aspects of data diversity affect
systematic generalization is lacking. We present new evidence in the problem of
Visual Question Answering (VQA) that reveals that the diversity of simple tasks
(i.e. tasks formed by a few subtasks and concepts) plays a key role in
achieving systematic generalization. This implies that it may not be essential
to gather a large and varied number of complex tasks, which could be costly to
obtain. We demonstrate that this result is independent of the similarity
between the training and testing data and applies to well-known families of
neural network architectures for VQA (i.e. monolithic architectures and neural
module networks). Additionally, we observe that neural module networks leverage
all forms of data diversity we evaluated, while monolithic architectures
require more extensive amounts of data to do so. These findings provide a first
step towards understanding the interactions between data diversity design,
neural network architectures, and systematic generalization capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMLR (https://openreview.net/forum?id=ZAin13msOp)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Lightweight <span class="highlight-title">Transformer</span> via Unrolling of Learned Graph
  Smoothness Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tam Thuc Do, Parham Eftekhar, Seyed Alireza Hosseini, Gene Cheung, Philip Chou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We build interpretable and lightweight transformer-like neural networks by
unrolling iterative optimization algorithms that minimize graph smoothness
priors -- the quadratic graph Laplacian regularizer (GLR) and the $\ell_1$-norm
graph total variation (GTV) -- subject to an interpolation constraint. The
crucial insight is that a normalized signal-dependent graph learning module
amounts to a variant of the basic self-attention mechanism in conventional
transformers. Unlike "black-box" transformers that require learning of large
key, query and value matrices to compute scaled dot products as affinities and
subsequent output embeddings, resulting in huge parameter sets, our unrolled
networks employ shallow CNNs to learn low-dimensional features per node to
establish pairwise Mahalanobis distances and construct sparse similarity
graphs. At each layer, given a learned graph, the target interpolated signal is
simply a low-pass filtered output derived from the minimization of an assumed
graph smoothness prior, leading to a dramatic reduction in parameter count.
Experiments for two image interpolation applications verify the restoration
performance, parameter efficiency and robustness to covariate shift of our
graph-based unrolled networks compared to conventional transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GD doesn't make the cut: Three ways that non-differentiability affects
  neural network training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08426v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08426v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Krishna Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the distinctions between gradient methods applied to
non-differentiable functions (NGDMs) and classical gradient descents (GDs)
designed for differentiable functions. First, we demonstrate significant
differences in the convergence properties of NGDMs compared to GDs, challenging
the applicability of the extensive neural network convergence literature based
on $L-smoothness$ to non-smooth neural networks. Next, we demonstrate the
paradoxical nature of NGDM solutions for $L_{1}$-regularized problems, showing
that increasing the regularization penalty leads to an increase in the $L_{1}$
norm of optimal solutions in NGDMs. Consequently, we show that widely adopted
$L_{1}$ penalization-based techniques for network pruning do not yield expected
results. Additionally, we dispel the common belief that optimization algorithms
like Adam and RMSProp perform similarly in non-differentiable contexts.
Finally, we explore the Edge of Stability phenomenon, indicating its
inapplicability even to Lipschitz continuous convex differentiable functions,
leaving its relevance to non-convex non-differentiable neural networks
inconclusive. Our analysis exposes misguided interpretations of NGDMs in widely
referenced papers and texts due to an overreliance on strong smoothness
assumptions, emphasizing the necessity for a nuanced understanding of
foundational assumptions in the analysis of these systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anatomical Foundation Models for Brain MRIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Alberto Barbano, Matteo Brunello, Benoit Dufumier, Marco Grangetto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) in neuroimaging has become increasingly relevant for
detecting neurological conditions and neurodegenerative disorders. One of the
most predominant biomarkers in neuroimaging is represented by brain age, which
has been shown to be a good indicator for different conditions, such as
Alzheimer's Disease. Using brain age for pretraining DL models in transfer
learning settings has also recently shown promising results, especially when
dealing with data scarcity of different conditions. On the other hand,
anatomical information of brain MRIs (e.g. cortical thickness) can provide
important information for learning good representations that can be transferred
to many downstream tasks. In this work, we propose AnatCL, an anatomical
foundation model for brain MRIs that i.) leverages anatomical information with
a weakly contrastive learning approach and ii.) achieves state-of-the-art
performances in many different downstream tasks. To validate our approach we
consider 12 different downstream tasks for diagnosis classification, and
prediction of 10 different clinical assessment scores. Pretrained models can be
found at https://github.com/EIDOSLAB/AnatCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages; added source url</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage
  Training <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihong Yin, Vladimir Yugay, Yue Li, Sezer Karaoglu, Theo Gevers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of novel view synthesis from images has seen rapid advancements
with the introduction of Neural Radiance Fields (NeRF) and more recently with
3D Gaussian Splatting. Gaussian Splatting became widely adopted due to its
efficiency and ability to render novel views accurately. While Gaussian
Splatting performs well when a sufficient amount of training images are
available, its unstructured explicit representation tends to overfit in
scenarios with sparse input images, resulting in poor rendering performance. To
address this, we present a 3D Gaussian-based novel view synthesis method using
sparse input images that can accurately render the scene from the viewpoints
not covered by the training images. We propose a multi-stage training scheme
with matching-based consistency constraints imposed on the novel views without
relying on pre-trained depth estimation or diffusion models. This is achieved
by using the matches of the available training images to supervise the
generation of the novel views sampled between the training frames with color,
geometry, and semantic losses. In addition, we introduce a locality preserving
regularization for 3D Gaussians which removes rendering artifacts by preserving
the local color structure of the scene. Evaluation on synthetic and real-world
datasets demonstrates competitive or superior performance of our method in
few-shot novel view synthesis compared to existing state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAAL: Density-Aware Adaptive Line Margin Loss for Multi-Modal Deep
  Metric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadush Hailu Gebrerufael, Anil Kumar Tiwari, Gaurav Neupane, Goitom Ybrah Hailu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal deep metric learning is crucial for effectively capturing diverse
representations in tasks such as face verification, fine-grained object
recognition, and product search. Traditional approaches to metric learning,
whether based on distance or margin metrics, primarily emphasize class
separation, often overlooking the intra-class distribution essential for
multi-modal feature learning. In this context, we propose a novel loss function
called Density-Aware Adaptive Margin Loss(DAAL), which preserves the density
distribution of embeddings while encouraging the formation of adaptive
sub-clusters within each class. By employing an adaptive line strategy, DAAL
not only enhances intra-class variance but also ensures robust inter-class
separation, facilitating effective multi-modal representation. Comprehensive
experiments on benchmark fine-grained datasets demonstrate the superior
performance of DAAL, underscoring its potential in advancing retrieval
applications and multi-modal deep metric learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 fugues, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLVideo: A Sign Language Video Moment Retrieval Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonçalo Vinagre Martins, João Magalhães, Afonso Quinaz, Carla Viegas, Sofia Cavaco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLVideo is a video moment retrieval system for Sign Language videos that
incorporates facial expressions, addressing this gap in existing technology.
The system extracts embedding representations for the hand and face signs from
video frames to capture the signs in their entirety, enabling users to search
for a specific sign language video segment with text queries. A collection of
eight hours of annotated Portuguese Sign Language videos is used as the
dataset, and a CLIP model is used to generate the embeddings. The initial
results are promising in a zero-shot setting. In addition, SLVideo incorporates
a thesaurus that enables users to search for similar signs to those retrieved,
using the video segment embeddings, and also supports the edition and creation
of video sign language annotations. Project web page:
https://novasearch.github.io/SLVideo/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DC-Gaussian: Improving 3D Gaussian Splatting for Reflective Dash Cam
  Videos <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17705v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17705v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhan Wang, Kai Cheng, Shuo Lei, Shengkun Wang, Wei Yin, Chenyang Lei, Xiaoxiao Long, Chang-Tien Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DC-Gaussian, a new method for generating novel views from
in-vehicle dash cam videos. While neural rendering techniques have made
significant strides in driving scenarios, existing methods are primarily
designed for videos collected by autonomous vehicles. However, these videos are
limited in both quantity and diversity compared to dash cam videos, which are
more widely used across various types of vehicles and capture a broader range
of scenarios. Dash cam videos often suffer from severe obstructions such as
reflections and occlusions on the windshields, which significantly impede the
application of neural rendering techniques. To address this challenge, we
develop DC-Gaussian based on the recent real-time neural rendering technique 3D
Gaussian Splatting (3DGS). Our approach includes an adaptive image
decomposition module to model reflections and occlusions in a unified manner.
Additionally, we introduce illumination-aware obstruction modeling to manage
reflections and occlusions under varying lighting conditions. Lastly, we employ
a geometry-guided Gaussian enhancement strategy to improve rendering details by
incorporating additional geometry priors. Experiments on self-captured and
public dash cam videos show that our method not only achieves state-of-the-art
performance in novel view synthesis, but also accurately reconstructing
captured scenes getting rid of obstructions. See the project page for code,
data: https://linhanwang.github.io/dcgaussian/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,7 figures;project page:
  https://linhanwang.github.io/dcgaussian/; Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cognitive Planning for Object Goal Navigation using Generative AI Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun P S, Andrew Melnik, Gora Chand Nandi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Generative AI, particularly in Large Language Models
(LLMs) and Large Vision-Language Models (LVLMs), offer new possibilities for
integrating cognitive planning into robotic systems. In this work, we present a
novel framework for solving the object goal navigation problem that generates
efficient exploration strategies. Our approach enables a robot to navigate
unfamiliar environments by leveraging LLMs and LVLMs to understand the semantic
structure of the scene. To address the challenge of representing complex
environments without overwhelming the system, we propose a 3D modular scene
representation, enriched with semantic descriptions. This representation is
dynamically pruned using an LLM-based mechanism, which filters irrelevant
information and focuses on task-specific data. By combining these elements, our
system generates high-level sub-goals that guide the exploration of the robot
toward the target object. We validate our approach in simulated environments,
demonstrating its ability to enhance object search efficiency while maintaining
scalability in complex settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Pseudo-labeling for Semi-Supervised Monocular 3D Object
  Detection <span class="chip">CVPR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17387v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17387v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zhang, Jiaming Li, Xiangru Lin, Wei Zhang, Xiao Tan, Junyu Han, Errui Ding, Jingdong Wang, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We delve into pseudo-labeling for semi-supervised monocular 3D object
detection (SSM3OD) and discover two primary issues: a misalignment between the
prediction quality of 3D and 2D attributes and the tendency of depth
supervision derived from pseudo-labels to be noisy, leading to significant
optimization conflicts with other reliable forms of supervision. We introduce a
novel decoupled pseudo-labeling (DPL) approach for SSM3OD. Our approach
features a Decoupled Pseudo-label Generation (DPG) module, designed to
efficiently generate pseudo-labels by separately processing 2D and 3D
attributes. This module incorporates a unique homography-based method for
identifying dependable pseudo-labels in BEV space, specifically for 3D
attributes. Additionally, we present a DepthGradient Projection (DGP) module to
mitigate optimization conflicts caused by noisy depth supervision of
pseudo-labels, effectively decoupling the depth gradient and removing
conflicting gradients. This dual decoupling strategy-at both the pseudo-label
generation and gradient levels-significantly improves the utilization of
pseudo-labels in SSM3OD. Our comprehensive experiments on the KITTI benchmark
demonstrate the superiority of our method over existing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ODGEN: Domain-specific Object Detection Data Generation with <span class="highlight-title">Diffusion</span>
  Models <span class="chip">NeurIPS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15199v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15199v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyuan Zhu, Shiyu Li, Yuxuan Liu, Ping Huang, Jiulong Shan, Huimin Ma, Jian Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern diffusion-based image generative models have made significant progress
and become promising to enrich training data for the object detection task.
However, the generation quality and the controllability for complex scenes
containing multi-class objects and dense objects with occlusions remain
limited. This paper presents ODGEN, a novel method to generate high-quality
images conditioned on bounding boxes, thereby facilitating data synthesis for
object detection. Given a domain-specific object detection dataset, we first
fine-tune a pre-trained diffusion model on both cropped foreground objects and
entire images to fit target distributions. Then we propose to control the
diffusion model using synthesized visual prompts with spatial constraints and
object-wise textual descriptions. ODGEN exhibits robustness in handling complex
scenes and specific domains. Further, we design a dataset synthesis pipeline to
evaluate ODGEN on 7 domain-specific benchmarks to demonstrate its
effectiveness. Adding training data generated by ODGEN improves up to 25.3%
mAP@.50:.95 with object detectors like YOLOv5 and YOLOv7, outperforming prior
controllable generative methods. In addition, we design an evaluation protocol
based on COCO-2014 to validate ODGEN in general domains and observe an
advantage up to 5.6% in mAP@.50:.95 against existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Knowledge Pursuit for Faithful Visual Synthesis <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17898v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17898v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqi Luo, Kwan Ho Ryan Chan, Dimitris Dimos, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern text-to-vision generative models often hallucinate when the prompt
describing the scene to be generated is underspecified. In large language
models (LLMs), a prevalent strategy to reduce hallucinations is to retrieve
factual knowledge from an external database. While such retrieval augmentation
strategies have great potential to enhance text-to-vision generators, existing
static top-K retrieval methods explore the knowledge pool once, missing the
broader context necessary for high-quality generation. Furthermore, LLMs
internally possess rich world knowledge learned during large-scale training
(parametric knowledge) that could mitigate the need for external data
retrieval. This paper proposes Contextual Knowledge Pursuit (CKPT), a framework
that leverages the complementary strengths of external and parametric knowledge
to help generators produce reliable visual content. Instead of the one-time
retrieval of facts from an external database to improve a given prompt, CKPT
uses (1) an LLM to decide whether to seek external knowledge or to self-elicit
descriptions from LLM parametric knowledge, (2) a knowledge pursuit process to
contextually seek and sequentially gather most relevant facts, (3) a knowledge
aggregator for prompt enhancement with the gathered fact context, and (4) a
filtered fine-tuning objective to improve visual synthesis with richer prompts.
We evaluate CKPT across multiple text-driven generative tasks (image, 3D
rendering, and video) on datasets of rare objects and daily scenarios. Our
results show that CKPT is capable of generating faithful and semantically rich
content across diverse visual domains, offering a promising data source for
zero-shot synthesis and filtered fine-tuning of text-to-vision generative
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECCV 2024 SDCV Workshop. GitHub repository at
  https://github.com/peterljq/Contextual-Knowledge-Pursuit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeBaRA: Denoising-Based 3D Room Arrangement Generation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Léopold Maillard, Nicolas Sereyjol-Garros, Tom Durand, Maks Ovsjanikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating realistic and diverse layouts of furnished indoor 3D scenes
unlocks multiple interactive applications impacting a wide range of industries.
The inherent complexity of object interactions, the limited amount of available
data and the requirement to fulfill spatial constraints all make generative
modeling for 3D scene synthesis and arrangement challenging. Current methods
address these challenges autoregressively or by using off-the-shelf diffusion
objectives by simultaneously predicting all attributes without 3D reasoning
considerations. In this paper, we introduce DeBaRA, a score-based model
specifically tailored for precise, controllable and flexible arrangement
generation in a bounded environment. We argue that the most critical component
of a scene synthesis system is to accurately establish the size and position of
various objects within a restricted area. Based on this insight, we propose a
lightweight conditional score-based model designed with 3D spatial awareness at
its core. We demonstrate that by focusing on spatial attributes of objects, a
single trained DeBaRA model can be leveraged at test time to perform several
downstream applications such as scene synthesis, completion and re-arrangement.
Further, we introduce a novel Self Score Evaluation procedure so it can be
optimally employed alongside external LLM models. We evaluate our approach
through extensive experiments and demonstrate significant improvement upon
state-of-the-art approaches in a range of scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seeing Eye to AI: Comparing Human Gaze and Model Attention in Video
  Memorability <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16484v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16484v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajneya Kumar, Eshika Khandelwal, Makarand Tapaswi, Vishnu Sreekumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding what makes a video memorable has important applications in
advertising or education technology. Towards this goal, we investigate
spatio-temporal attention mechanisms underlying video memorability. Different
from previous works that fuse multiple features, we adopt a simple
CNN+Transformer architecture that enables analysis of spatio-temporal attention
while matching state-of-the-art (SoTA) performance on video memorability
prediction. We compare model attention against human gaze fixations collected
through a small-scale eye-tracking study where humans perform the video memory
task. We uncover the following insights: (i) Quantitative saliency metrics show
that our model, trained only to predict a memorability score, exhibits similar
spatial attention patterns to human gaze, especially for more memorable videos.
(ii) The model assigns greater importance to initial frames in a video,
mimicking human attention patterns. (iii) Panoptic segmentation reveals that
both (model and humans) assign a greater share of attention to things and less
attention to stuff as compared to their occurrence probability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re-assembling the past: The RePAIR <span class="highlight-title">dataset</span> and benchmark for real world
  2D and 3D puzzle solving <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24010v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24010v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodore Tsesmelis, Luca Palmieri, Marina Khoroshiltseva, Adeela Islam, Gur Elkin, Ofir Itzhak Shahar, Gianluca Scarpellini, Stefano Fiorini, Yaniv Ohayon, Nadav Alali, Sinem Aslan, Pietro Morerio, Sebastiano Vascon, Elena Gravina, Maria Cristina Napolitano, Giuseppe Scarpati, Gabriel Zuchtriegel, Alexandra Spühler, Michel E. Fuchs, Stuart James, Ohad Ben-Shahar, Marcello Pelillo, Alessio Del Bue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes the RePAIR dataset that represents a challenging
benchmark to test modern computational and data driven methods for
puzzle-solving and reassembly tasks. Our dataset has unique properties that are
uncommon to current benchmarks for 2D and 3D puzzle solving. The fragments and
fractures are realistic, caused by a collapse of a fresco during a World War II
bombing at the Pompeii archaeological park. The fragments are also eroded and
have missing pieces with irregular shapes and different dimensions, challenging
further the reassembly algorithms. The dataset is multi-modal providing high
resolution images with characteristic pictorial elements, detailed 3D scans of
the fragments and meta-data annotated by the archaeologists. Ground truth has
been generated through several years of unceasing fieldwork, including the
excavation and cleaning of each fragment, followed by manual puzzle solving by
archaeologists of a subset of approx. 1000 pieces among the 16000 available.
After digitizing all the fragments in 3D, a benchmark was prepared to challenge
current reassembly and puzzle-solving methods that often solve more simplistic
synthetic scenarios. The tested baselines show that there clearly exists a gap
to fill in solving this computationally complex problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, Track Datasets and Benchmarks, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention-based Class-Conditioned Alignment for Multi-Source Domain
  Adaptation of Object Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09918v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09918v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atif Belal, Akhil Meethal, Francisco Perdigon Romero, Marco Pedersoli, Eric Granger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptation methods for object detection (OD) strive to mitigate the
impact of distribution shifts by promoting feature alignment across source and
target domains. Multi-source domain adaptation (MSDA) allows leveraging
multiple annotated source datasets and unlabeled target data to improve the
accuracy and robustness of the detection model. Most state-of-the-art MSDA
methods for OD perform feature alignment in a class-agnostic manner. This is
challenging since the objects have unique modal information due to variations
in object appearance across domains. A recent prototype-based approach proposed
a class-wise alignment, yet it suffers from error accumulation due to noisy
pseudo-labels that can negatively affect adaptation with imbalanced data. To
overcome these limitations, we propose an attention-based class-conditioned
alignment method for MSDA that aligns instances of each object category across
domains. In particular, an attention module coupled with an adversarial domain
classifier allows learning domain-invariant and class-specific instance
representations. Experimental results on multiple benchmarking MSDA datasets
indicate that our method outperforms the state-of-the-art methods and is robust
to class imbalance using a conceptually simple class-conditioning method. Our
code is available at https://github.com/imatif17/ACIA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2309.14950</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GlobalDoc: A Cross-Modal Vision-Language Framework for Real-World
  Document Image Retrieval and Classification <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05756v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05756v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Souhail Bakkali, Sanket Biswas, Zuheng Ming, Mickaël Coustaty, Marçal Rusiñol, Oriol Ramos Terrades, Josep Lladós
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual document understanding (VDU) has rapidly advanced with the development
of powerful multi-modal language models. However, these models typically
require extensive document pre-training data to learn intermediate
representations and often suffer a significant performance drop in real-world
online industrial settings. A primary issue is their heavy reliance on OCR
engines to extract local positional information within document pages, which
limits the models' ability to capture global information and hinders their
generalizability, flexibility, and robustness. In this paper, we introduce
GlobalDoc, a cross-modal transformer-based architecture pre-trained in a
self-supervised manner using three novel pretext objective tasks. GlobalDoc
improves the learning of richer semantic concepts by unifying language and
visual representations, resulting in more transferable models. For proper
evaluation, we also propose two novel document-level downstream VDU tasks,
Few-Shot Document Image Classification (DIC) and Content-based Document Image
Retrieval (DIR), designed to simulate industrial scenarios more closely.
Extensive experimentation has been conducted to demonstrate GlobalDoc's
effectiveness in practical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynCo: Synthetic Hard Negatives in Contrastive Learning for Better
  Unsupervised Visual Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02401v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02401v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Giakoumoglou, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has become a dominant approach in self-supervised visual
representation learning. Hard negatives - samples closely resembling the anchor
- are key to enhancing learned representations' discriminative power. However,
efficiently leveraging hard negatives remains challenging. We introduce SynCo
(Synthetic Negatives in Contrastive learning), a novel approach that improves
model performance by generating synthetic hard negatives on the representation
space. Building on the MoCo framework, SynCo introduces six strategies for
creating diverse synthetic hard negatives on-the-fly with minimal computational
overhead. SynCo achieves faster training and better representation learning,
reaching 67.9% top-1 accuracy on ImageNet ILSVRC-2012 linear evaluation after
200 pretraining epochs, surpassing MoCo's 67.5% using the same ResNet-50
encoder. It also transfers more effectively to detection tasks: on PASCAL VOC,
it outperforms both the supervised baseline and MoCo with 82.5% AP; on COCO, it
sets new benchmarks with 40.9% AP for bounding box detection and 35.5% AP for
instance segmentation. Our synthetic hard negative generation approach
significantly enhances visual representations learned through self-supervised
contrastive learning. Code is available at
https://github.com/giakoumoglou/synco.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and
  Image-to-3D Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, Lifu Wang, Zhuo Chen, Sicong Liu, Yuhong Liu, Yong Yang, Di Wang, Jie Jiang, Chunchao Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While 3D generative models have greatly improved artists' workflows, the
existing diffusion models for 3D generation suffer from slow generation and
poor generalization. To address this issue, we propose a two-stage approach
named Hunyuan3D-1.0 including a lite version and a standard version, that both
support text- and image-conditioned generation. In the first stage, we employ a
multi-view diffusion model that efficiently generates multi-view RGB in
approximately 4 seconds. These multi-view images capture rich details of the 3D
asset from different viewpoints, relaxing the tasks from single-view to
multi-view reconstruction. In the second stage, we introduce a feed-forward
reconstruction model that rapidly and faithfully reconstructs the 3D asset
given the generated multi-view images in approximately 7 seconds. The
reconstruction network learns to handle noises and in-consistency introduced by
the multi-view diffusion and leverages the available information from the
condition image to efficiently recover the 3D structure. Our framework involves
the text-to-image model, i.e., Hunyuan-DiT, making it a unified framework to
support both text- and image-conditioned 3D generation. Our standard version
has 3x more parameters than our lite and other existing model. Our
Hunyuan3D-1.0 achieves an impressive balance between speed and quality,
significantly reducing generation time while maintaining the quality and
diversity of the produced assets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report; 3D Generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Real-Time Volcano-Seismic Event Recognition Based on
  Multi-Station Seismograms and Semantic Segmentation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20595v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20595v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camilo Espinosa-Curilem, Millaray Curilem, Daniel Basualto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In volcano monitoring, effective recognition of seismic events is essential
for understanding volcanic activity and raising timely warning alerts.
Traditional methods rely on manual analysis, which can be subjective and
labor-intensive. Furthermore, current automatic approaches often tackle
detection and classification separately, mostly rely on single station
information and generally require tailored preprocessing and representations to
perform predictions. These limitations often hinder their application to
real-time monitoring and utilization across different volcano conditions. This
study introduces a novel approach that utilizes Semantic Segmentation models to
automate seismic event recognition by applying a straight forward
transformation of multi-channel 1D signals into 2D representations, enabling
their use as images. Our framework employs a data-driven, end-to-end design
that integrates multi-station seismic data with minimal preprocessing,
performing both detection and classification simultaneously for five seismic
event classes. We evaluated four state-of-the-art segmentation models (UNet,
UNet++, DeepLabV3+ and SwinUNet) on approximately 25.000 seismic events
recorded at four different Chilean volcanoes: Nevados del Chill\'an Volcanic
Complex, Laguna del Maule, Villarrica and Puyehue-Cord\'on Caulle. Among these
models, the UNet architecture was identified as the most effective model,
achieving mean F1 and Intersection over Union (IoU) scores of up to 0.91 and
0.88, respectively, and demonstrating superior noise robustness and model
flexibility to unseen volcano datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures. This is a pre-print, it is currently under
  review for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better, Not Just More: Data-Centric Machine Learning for Earth
  Observation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05327v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05327v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ribana Roscher, Marc Rußwurm, Caroline Gevaert, Michael Kampffmeyer, Jefersson A. dos Santos, Maria Vakalopoulou, Ronny Hänsch, Stine Hansen, Keiller Nogueira, Jonathan Prexl, Devis Tuia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments and research in modern machine learning have led to
substantial improvements in the geospatial field. Although numerous deep
learning architectures and models have been proposed, the majority of them have
been solely developed on benchmark datasets that lack strong real-world
relevance. Furthermore, the performance of many methods has already saturated
on these datasets. We argue that a shift from a model-centric view to a
complementary data-centric perspective is necessary for further improvements in
accuracy, generalization ability, and real impact on end-user applications.
Furthermore, considering the entire machine learning cycle-from problem
definition to model deployment with feedback-is crucial for enhancing machine
learning models that can be reliable in unforeseen situations. This work
presents a definition as well as a precise categorization and overview of
automated data-centric learning approaches for geospatial data. It highlights
the complementary role of data-centric learning with respect to model-centric
in the larger machine learning deployment cycle. We review papers across the
entire geospatial field and categorize them into different groups. A set of
representative experiments shows concrete implementation examples. These
examples provide concrete steps to act on geospatial data with data-centric
machine learning approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Geoscience and Remote Sensing Magazine</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">24</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated, LLM enabled extraction of synthesis details for reticular
  materials from scientific literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viviane Torres da Silva, Alexandre Rademaker, Krystelle Lionti, Ronaldo Giro, Geisa Lima, Sandro Fiorini, Marcelo Archanjo, Breno W. Carvalho, Rodrigo Neumann, Anaximandro Souza, João Pedro Souza, Gabriela de Valnisio, Carmen Nilda Paz, Renato Cerqueira, Mathias Steiner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated knowledge extraction from scientific literature can potentially
accelerate materials discovery. We have investigated an approach for extracting
synthesis protocols for reticular materials from scientific literature using
large language models (LLMs). To that end, we introduce a Knowledge Extraction
Pipeline (KEP) that automatizes LLM-assisted paragraph classification and
information extraction. By applying prompt engineering with in-context learning
(ICL) to a set of open-source LLMs, we demonstrate that LLMs can retrieve
chemical information from PDF documents, without the need for fine-tuning or
training and at a reduced risk of hallucination. By comparing the performance
of five open-source families of LLMs in both paragraph classification and
information extraction tasks, we observe excellent model performance even if
only few example paragraphs are included in the ICL prompts. The results show
the potential of the KEP approach for reducing human annotations and data
curation efforts in automated scientific knowledge extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-supervised Hierarchical Representation for Medication
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Liang, Yuting Liu, Yizhou Dang, Enneng Yang, Guibing Guo, Wei Cai, Jianzhe Zhao, Xingwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medication recommender is to suggest appropriate medication combinations
based on a patient's health history, e.g., diagnoses and procedures. Existing
works represent different diagnoses/procedures well separated by one-hot
encodings. However, they ignore the latent hierarchical structures of these
medical terms, undermining the generalization performance of the model. For
example, "Respiratory Diseases", "Chronic Respiratory Diseases" and "Chronic
Bronchiti" have a hierarchical relationship, progressing from general to
specific. To address this issue, we propose a novel hierarchical encoder named
HIER to hierarchically represent diagnoses and procedures, which is based on
standard medical codes and compatible with any existing methods. Specifically,
the proposed method learns relation embedding with a self-supervised objective
for incorporating the neighbor hierarchical structure. Additionally, we develop
the position encoding to explicitly introduce global hierarchical position.
Extensive experiments demonstrate significant and consistent improvements in
recommendation accuracy across four baselines and two real-world clinical
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Effective Adaptation of Multimodal Foundation Models in
  Sequential Recommendation <span class="chip">SIGIR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junchen Fu, Xuri Ge, Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, Kaiwen Zheng, Yongxin Ni, Joemon M. Jose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal foundation models (MFMs) have revolutionized sequential
recommender systems through advanced representation learning. While
Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt these models,
studies often prioritize parameter efficiency, neglecting GPU memory and
training speed. To address this, we introduced the IISAN framework,
significantly enhancing efficiency. However, IISAN was limited to symmetrical
MFMs and identical text and image encoders, preventing the use of
state-of-the-art Large Language Models. To overcome this, we developed
IISAN-Versa, a versatile plug-and-play architecture compatible with both
symmetrical and asymmetrical MFMs. IISAN-Versa employs a Decoupled PEFT
structure and utilizes both intra- and inter-modal adaptation. It effectively
handles asymmetry through a simple yet effective combination of group
layer-dropping and dimension transformation alignment. Our research
demonstrates that IISAN-Versa effectively adapts large text encoders, and we
further identify a scaling effect where larger encoders generally perform
better. IISAN-Versa also demonstrates strong versatility in our defined
multimodal scenarios, which include raw titles and captions generated from
images and videos. Additionally, IISAN-Versa achieved state-of-the-art
performance on the Microlens public benchmark. We will release our code and
datasets to support future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The extension of IISAN in SIGIR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge
  in RAG Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has been shown to improve knowledge
capabilities and alleviate the hallucination problem of LLMs. The Web is a
major source of external knowledge used in RAG systems, and many commercial
systems such as ChatGPT and Perplexity have used Web search engines as their
major retrieval systems. Typically, such RAG systems retrieve search results,
download HTML sources of the results, and then extract plain texts from the
HTML sources. Plain text documents or chunks are fed into the LLMs to augment
the generation. However, much of the structural and semantic information
inherent in HTML, such as headings and table structures, is lost during this
plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG,
which uses HTML instead of plain text as the format of retrieved knowledge in
RAG. We believe HTML is better than plain text in modeling knowledge in
external documents, and most LLMs possess robust capacities to understand HTML.
However, utilizing HTML presents new challenges. HTML contains additional
content such as tags, JavaScript, and CSS specifications, which bring extra
input tokens and noise to the RAG system. To address this issue, we propose
HTML cleaning, compression, and pruning strategies, to shorten the HTML while
minimizing the loss of information. Specifically, we design a two-step
block-tree-based pruning method that prunes useless HTML blocks and keeps only
the relevant part of the HTML. Experiments on six QA datasets confirm the
superiority of using HTML in RAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document
  Relation Extraction with Graph-of-Thoughts Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Zhang, Ning Yan, Masood Mortazavi, Hoang H. Nguyen, Zhongfen Deng, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) pre-trained on massive corpora have demonstrated
impressive few-shot learning capability on many NLP tasks. Recasting an NLP
task into a text-to-text generation task is a common practice so that
generative LLMs can be prompted to resolve it. However, performing
document-level relation extraction (DocRE) tasks with generative LLM models is
still challenging due to the structured output format of DocRE, which
complicates the conversion to plain text. Limited information available in
few-shot samples and prompt instructions induce further difficulties and
challenges in relation extraction for mentioned entities in a document. In this
paper, we represent the structured output as a graph-style triplet rather than
natural language expressions and leverage generative LLMs for the DocRE task.
Our approach, the Graph-DPEP framework is grounded in the reasoning behind
triplet explanation thoughts presented in natural language. In this framework,
we first introduce a ``decomposed-plug" method for performing the generation
from LLMs over prompts with type-space decomposition to alleviate the burden of
distinguishing all relation types. Second, we employ a verifier for calibrating
the generation and identifying overlooked query entity pairs. Third, we develop
"ensemble-play", reapplying generation on the entire type list by leveraging
the reasoning thoughts embedded in a sub-graph associated with the missing
query pair to address the missingness issue. Through extensive comparisons with
existing prompt techniques and alternative Language Models (LLMs), our
framework demonstrates superior performance on publicly available benchmarks in
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DM4Steal: <span class="highlight-title">Diffusion</span> Model For Link Stealing Attack On Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyin Chen, Haonan Ma, Haibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph has become increasingly integral to the advancement of recommendation
systems, particularly with the fast development of graph neural network(GNN).
By exploring the virtue of rich node features and link information, GNN is
designed to provide personalized and accurate suggestions. Meanwhile, the
privacy leakage of GNN in such contexts has also captured special attention.
Prior work has revealed that a malicious user can utilize auxiliary knowledge
to extract sensitive link data of the target graph, integral to recommendation
systems, via the decision made by the target GNN model. This poses a
significant risk to the integrity and confidentiality of data used in
recommendation system. Though important, previous works on GNN's privacy
leakage are still challenged in three aspects, i.e., limited stealing attack
scenarios, sub-optimal attack performance, and adaptation against defense. To
address these issues, we propose a diffusion model based link stealing attack,
named DM4Steal. It differs previous work from three critical aspects. (i)
Generality: aiming at six attack scenarios with limited auxiliary knowledge, we
propose a novel training strategy for diffusion models so that DM4Steal is
transferable to diverse attack scenarios. (ii) Effectiveness: benefiting from
the retention of semantic structure in the diffusion model during the training
process, DM4Steal is capable to learn the precise topology of the target graph
through the GNN decision process. (iii) Adaptation: when GNN is defensive
(e.g., DP, Dropout), DM4Steal relies on the stability that comes from sampling
the score model multiple times to keep performance degradation to a minimum,
thus DM4Steal implements successful adaptive attack on defensive GNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual
  Visual Answer Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibin Wen, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Multilingual Visual Answer Localization (MVAL) is to locate a
video segment that answers a given multilingual question. Existing methods
either focus solely on visual modality or integrate visual and subtitle
modalities. However, these methods neglect the audio modality in videos,
consequently leading to incomplete input information and poor performance in
the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span
Localization (AVTSL) method that incorporates audio modality to augment both
visual and textual representations for the MVAL task. Specifically, we
integrate features from three modalities and develop three predictors, each
tailored to the unique contributions of the fused modalities: an audio-visual
predictor, a visual predictor, and a textual predictor. Each predictor
generates predictions based on its respective modality. To maintain consistency
across the predicted results, we introduce an Audio-Visual-Textual Consistency
module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing
each modality's predictor to dynamically learn from the others. This
collaborative learning ensures that the model generates consistent and
comprehensive answers. Extensive experiments show that our proposed method
outperforms several state-of-the-art (SOTA) methods, which demonstrates the
effectiveness of the audio modality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WASHtsApp -- A RAG-powered WhatsApp Chatbot for supporting rural African
  clean water access, sanitation and hygiene 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Kloker, Alex Cedric Luyima, Matthew Bazanya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces WASHtsApp, a WhatsApp-based chatbot designed to educate
rural African communities on clean water access, sanitation, and hygiene (WASH)
principles. WASHtsApp leverages a Retrieval-Augmented Generation (RAG) approach
to address the limitations of previous approaches with limited reach or missing
contextualization. The paper details the development process, employing Design
Science Research Methodology. The evaluation consisted of two phases: content
validation by four WASH experts and community validation by potential users.
Content validation confirmed WASHtsApp's ability to provide accurate and
relevant WASH-related information. Community validation indicated high user
acceptance and perceived usefulness of the chatbot. The paper concludes by
discussing the potential for further development, including incorporating local
languages and user data analysis for targeted interventions. It also proposes
future research cycles focused on wider deployment and leveraging user data for
educational purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing EmoBot: An In-Depth Analysis of User Satisfaction and Faults
  in an Emotion-Aware Chatbot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taseen Mubassira, Mehedi Hasan, A. B. M. Alim Al Iislam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research community has traditionally shown a keen interest in emotion
modeling, with a notable emphasis on the detection aspect. In contrast, the
exploration of emotion generation has received less attention.This study delves
into an existing state-of-the-art emotional chatbot, EmoBot, designed for
generating emotions in general-purpose conversations. This research involves a
comprehensive examination, including a survey to evaluate EmoBot's proficiency
in key dimensions like usability, accuracy, and overall user satisfaction, with
a specific focus on fault tolerance. By closely examining the chatbot's
operations, we identified some noteworthy shortcomings in the existing model.
We propose some solutions designed to address and overcome the identified
issues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, extended abstract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Vision-Language Models for Manufacturing Feature Recognition
  in CAD Designs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Tayyab Khan, Lequn Chen, Ye Han Ng, Wenhe Feng, Nicholas Yew Jin Tan, Seung Ki Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic feature recognition (AFR) is essential for transforming design
knowledge into actionable manufacturing information. Traditional AFR methods,
which rely on predefined geometric rules and large datasets, are often
time-consuming and lack generalizability across various manufacturing features.
To address these challenges, this study investigates vision-language models
(VLMs) for automating the recognition of a wide range of manufacturing features
in CAD designs without the need for extensive training datasets or predefined
rules. Instead, prompt engineering techniques, such as multi-view query images,
few-shot learning, sequential reasoning, and chain-of-thought, are applied to
enable recognition. The approach is evaluated on a newly developed CAD dataset
containing designs of varying complexity relevant to machining, additive
manufacturing, sheet metal forming, molding, and casting. Five VLMs, including
three closed-source models (GPT-4o, Claude-3.5-Sonnet, and Claude-3.0-Opus) and
two open-source models (LLava and MiniCPM), are evaluated on this dataset with
ground truth features labelled by experts. Key metrics include feature quantity
accuracy, feature name matching accuracy, hallucination rate, and mean absolute
error (MAE). Results show that Claude-3.5-Sonnet achieves the highest feature
quantity accuracy (74%) and name-matching accuracy (75%) with the lowest MAE
(3.2), while GPT-4o records the lowest hallucination rate (8%). In contrast,
open-source models have higher hallucination rates (>30%) and lower accuracies
(<40%). This study demonstrates the potential of VLMs to automate feature
recognition in CAD designs within diverse manufacturing scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper has been submitted to The ASME Journal of Computing and
  Information Science in Engineering (JCISE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models and Cycle Consistency for Self-Reflective Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianqiao Wangni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel framework that leverages large language models
(LLMs) for machine translation (MT). We start with one conjecture: an ideal
translation should contain complete and accurate information for a strong
enough LLM to recover the original sentence. We generate multiple translation
candidates from a source language A to a target language B, and subsequently
translate these candidates back to the original language A. By evaluating the
cycle consistency between the original and back-translated sentences using
metrics such as token-level precision and accuracy, we implicitly estimate the
translation quality in language B, without knowing its ground-truth. This also
helps to evaluate the LLM translation capability, only with monolingual
corpora. For each source sentence, we identify the translation candidate with
optimal cycle consistency with the original sentence as the final answer. Our
experiments demonstrate that larger LLMs, or the same LLM with more forward
passes during inference, exhibit increased cycle consistency, aligning with the
LLM model size scaling law and test-time computation scaling law. This work
provide methods for, 1) to implicitly evaluate translation quality of a
sentence in the target language, 2), to evaluate capability of LLM for
any-to-any-language translation, and 3), how to generate a better translation
for a specific LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory Augmented Cross-encoders for Controllable Personalized Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheshera Mysore, Garima Dhanania, Kishor Patil, Surya Kallumadi, Andrew McCallum, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized search represents a problem where retrieval models condition on
historical user interaction data in order to improve retrieval results.
However, personalization is commonly perceived as opaque and not amenable to
control by users. Further, personalization necessarily limits the space of
items that users are exposed to. Therefore, prior work notes a tension between
personalization and users' ability for discovering novel items. While discovery
of novel items in personalization setups may be resolved through search result
diversification, these approaches do little to allow user control over
personalization. Therefore, in this paper, we introduce an approach for
controllable personalized search. Our model, CtrlCE presents a novel
cross-encoder model augmented with an editable memory constructed from users
historical items. Our proposed memory augmentation allows cross-encoder models
to condition on large amounts of historical user data and supports interaction
from users permitting control over personalization. Further, controllable
personalization for search must account for queries which don't require
personalization, and in turn user control. For this, we introduce a calibrated
mixing model which determines when personalization is necessary. This allows
system designers using CtrlCE to only obtain user input for control when
necessary. In multiple datasets of personalized search, we show CtrlCE to
result in effective personalization as well as fulfill various key goals for
controllable personalized search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JEL: Applying End-to-End Neural Entity Linking in JPMorgan Chase 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanying Ding, Vinay K. Chaudhri, Naren Chittar, Krishna Konakanchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs have emerged as a compelling abstraction for capturing key
relationship among the entities of interest to enterprises and for integrating
data from heterogeneous sources. JPMorgan Chase (JPMC) is leading this trend by
leveraging knowledge graphs across the organization for multiple mission
critical applications such as risk assessment, fraud detection, investment
advice, etc. A core problem in leveraging a knowledge graph is to link mentions
(e.g., company names) that are encountered in textual sources to entities in
the knowledge graph. Although several techniques exist for entity linking, they
are tuned for entities that exist in Wikipedia, and fail to generalize for the
entities that are of interest to an enterprise. In this paper, we propose a
novel end-to-end neural entity linking model (JEL) that uses minimal context
information and a margin loss to generate entity embeddings, and a Wide & Deep
Learning model to match character and semantic information respectively. We
show that JEL achieves the state-of-the-art performance to link mentions of
company names in financial news with entities in our knowledge graph. We report
on our efforts to deploy this model in the company-wide system to generate
alerts in response to financial news. The methodology used for JEL is directly
applicable and usable by other enterprises who need entity linking solutions
for data that are unique to their respective situations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, IAAI-21</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JPEC: A Novel Graph Neural Network for Competitor Retrieval in Financial
  Knowledge Graphs <span class="chip">SIGIR'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanying Ding, Manoj Cherukumalli, Santosh Chikoti, Vinay K. Chaudhri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graphs have gained popularity for their ability to organize and
analyze complex data effectively. When combined with graph embedding
techniques, such as graph neural networks (GNNs), knowledge graphs become a
potent tool in providing valuable insights. This study explores the application
of graph embedding in identifying competitors from a financial knowledge graph.
Existing state-of-the-art(SOTA) models face challenges due to the unique
attributes of our knowledge graph, including directed and undirected
relationships, attributed nodes, and minimal annotated competitor connections.
To address these challenges, we propose a novel graph embedding model,
JPEC(JPMorgan Proximity Embedding for Competitor Detection), which utilizes
graph neural network to learn from both first-order and second-order node
proximity together with vital features for competitor retrieval. JPEC had
outperformed most existing models in extensive experiments, showcasing its
effectiveness in competitor retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, accepted by SIGIR'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PaCE: Parsimonious Concept Engineering for Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are being used for a wide variety of tasks.
While they are capable of generating human-like responses, they can also
produce undesirable output including potentially harmful information, racist or
sexist language, and hallucinations. Alignment methods are designed to reduce
such undesirable outputs via techniques such as fine-tuning, prompt
engineering, and representation engineering. However, existing methods face
several challenges: some require costly fine-tuning for every alignment task;
some do not adequately remove undesirable concepts, failing alignment; some
remove benign concepts, lowering the linguistic capabilities of LLMs. To
address these issues, we propose Parsimonious Concept Engineering (PaCE), a
novel activation engineering framework for alignment. First, to sufficiently
model the concepts, we construct a large-scale concept dictionary in the
activation space, in which each atom corresponds to a semantic concept. Given
any alignment task, we instruct a concept partitioner to efficiently annotate
the concepts as benign or undesirable. Then, at inference time, we decompose
the LLM activations along the concept dictionary via sparse coding, to
accurately represent the activations as linear combinations of benign and
undesirable components. By removing the latter ones from the activations, we
reorient the behavior of the LLM towards the alignment goal. We conduct
experiments on tasks such as response detoxification, faithfulness enhancement,
and sentiment revising, and show that PaCE achieves state-of-the-art alignment
performance while maintaining linguistic capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024. GitHub repository at
  https://github.com/peterljq/Parsimonious-Concept-Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R^3AG: First Workshop on Refined and Reliable Retrieval Augmented
  Generation <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20598v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20598v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Wang, Xuri Ge, Joemon M. Jose, Haitao Yu, Weizhi Ma, Zhaochun Ren, Xin Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has gained wide attention as the key
component to improve generative models with external knowledge augmentation
from information retrieval. It has shown great prominence in enhancing the
functionality and performance of large language model (LLM)-based applications.
However, with the comprehensive application of RAG, more and more problems and
limitations have been identified, thus urgently requiring further fundamental
exploration to improve current RAG frameworks. This workshop aims to explore in
depth how to conduct refined and reliable RAG for downstream AI tasks.
  To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024
to call for participants to re-examine and formulate the basic principles and
practical implementation of refined and reliable RAG. The workshop serves as a
platform for both academia and industry researchers to conduct discussions,
share insights, and foster research to build the next generation of RAG
systems. Participants will engage in discussions and presentations focusing on
fundamental challenges, cutting-edge research, and potential pathways to
improve RAG. At the end of the workshop, we aim to have a clearer understanding
of how to improve the reliability and applicability of RAG with more robust
information retrieval and language generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>R^3AG workshop overview at SIGIR-AP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Facilitating Interdisciplinary Knowledge Transfer with Research Paper
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eoghan Cunningham, Derek Greene, Barry Smyth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the extensive recommender systems literature, novelty and diversity have
been identified as key properties of useful recommendations. However, these
properties have received limited attention in the specific sub-field of
research paper recommender systems. In this work, we argue for the importance
of offering novel and diverse research paper recommendations to scientists.
This approach aims to reduce siloed reading, break down filter bubbles, and
promote interdisciplinary research. We propose a novel framework for evaluating
the novelty and diversity of research paper recommendations that leverages
methods from network analysis and natural language processing. Using this
framework, we show that the choice of representational method within a larger
research paper recommendation system can have a measurable impact on the nature
of downstream recommendations, specifically on their novelty and diversity. We
highlight a novel paper embedding method, which we demonstrate offers more
innovative and diverse recommendations without sacrificing precision, compared
to other state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review at QSS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DSFNet: Learning Disentangled Scenario Factorization for Multi-Scenario
  Route Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Yu, Yihai Duan, Longfei Xu, Chao Chen, Shuliang Liu, Kaikui Liu, Fan Yang, Xiangxiang Chu, Ning Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-scenario route ranking (MSRR) is crucial in many industrial mapping
systems. However, the industrial community mainly adopts interactive interfaces
to encourage users to select pre-defined scenarios, which may hinder the
downstream ranking performance. In addition, in the academic community, the
multi-scenario ranking works only come from other fields, and there are no
works specifically focusing on route data due to lacking a publicly available
MSRR dataset. Moreover, all the existing multi-scenario works still fail to
address the three specific challenges of MSRR simultaneously, i.e. explosion of
scenario number, high entanglement, and high-capacity demand. Different from
the prior, to address MSRR, our key idea is to factorize the complicated
scenario in route ranking into several disentangled factor scenario patterns.
Accordingly, we propose a novel method, Disentangled Scenario Factorization
Network (DSFNet), which flexibly composes scenario-dependent parameters based
on a high-capacity multi-factor-scenario-branch structure. Then, a novel
regularization is proposed to induce the disentanglement of factor scenarios.
Furthermore, two extra novel techniques, i.e. scenario-aware batch
normalization and scenario-aware feature filtering, are developed to improve
the network awareness of scenario representation. Additionally, to facilitate
MSRR research in the academic community, we propose MSDR, the first large-scale
publicly available annotated industrial Multi-Scenario Driving Route dataset.
Comprehensive experimental results demonstrate the superiority of our DSFNet,
which has been successfully deployed in AMap to serve the major online traffic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Music Foundation Model as Generic Booster for Music Downstream Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        WeiHsiang Liao, Yuhta Takida, Yukara Ikemiya, Zhi Zhong, Chieh-Hsin Lai, Giorgio Fabbro, Kazuki Shimada, Keisuke Toyama, Kinwai Cheuk, Marco A. Martínez-Ramírez, Shusuke Takahashi, Stefan Uhlich, Taketo Akama, Woosung Choi, Yuichiro Koyama, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate the efficacy of using intermediate representations from a
single foundation model to enhance various music downstream tasks. We introduce
SoniDo, a music foundation model (MFM) designed to extract hierarchical
features from target music samples. By leveraging hierarchical intermediate
features, SoniDo constrains the information granularity, leading to improved
performance across various downstream tasks including both understanding and
generative tasks. We specifically evaluated this approach on representative
tasks such as music tagging, music transcription, music source separation, and
music mixing. Our results reveal that the features extracted from foundation
models provide valuable enhancements in training downstream task models. This
highlights the capability of using features extracted from music foundation
models as a booster for downstream tasks. Our approach not only benefits
existing task-specific models but also supports music downstream tasks
constrained by data scarcity. This paves the way for more effective and
accessible music processing solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages with 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding Matters: Addressing Amplification Bias and Homogeneity Issue
  for LLM-based Recommendation <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14900v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14900v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keqin Bao, Jizhi Zhang, Yang Zhang, Xinyue Huo, Chong Chen, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting Large Language Models (LLMs) for recommendation requires careful
consideration of the decoding process, given the inherent differences between
generating items and natural language. Existing approaches often directly apply
LLMs' original decoding methods. However, we find these methods encounter
significant challenges: 1) amplification bias -- where standard length
normalization inflates scores for items containing tokens with generation
probabilities close to 1 (termed ghost tokens), and 2) homogeneity issue --
generating multiple similar or repetitive items for a user. To tackle these
challenges, we introduce a new decoding approach named Debiasing-Diversifying
Decoding (D3). D3 disables length normalization for ghost tokens to alleviate
amplification bias, and it incorporates a text-free assistant model to
encourage tokens less frequently generated by LLMs for counteracting
recommendation homogeneity. Extensive experiments on real-world datasets
demonstrate the method's effectiveness in enhancing accuracy and diversity. The
code is available at https://github.com/SAI990323/DecodingMatters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Green Recommender Systems: Optimizing <span class="highlight-title">Dataset</span> Size for Energy-Efficient
  Algorithm Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ardalan Arabzadeh, Tobias Vente, Joeran Beel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As recommender systems become increasingly prevalent, the environmental
impact and energy efficiency of training large-scale models have come under
scrutiny. This paper investigates the potential for energy-efficient algorithm
performance by optimizing dataset sizes through downsampling techniques in the
context of Green Recommender Systems. We conducted experiments on the MovieLens
100K, 1M, 10M, and Amazon Toys and Games datasets, analyzing the performance of
various recommender algorithms under different portions of dataset size. Our
results indicate that while more training data generally leads to higher
algorithm performance, certain algorithms, such as FunkSVD and BiasedMF,
particularly with unbalanced and sparse datasets like Amazon Toys and Games,
maintain high-quality recommendations with up to a 50% reduction in training
data, achieving nDCG@10 scores within approximately 13% of full dataset
performance. These findings suggest that strategic dataset reduction can
decrease computational and environmental costs without substantially
compromising recommendation quality. This study advances sustainable and green
recommender systems by providing insights for reducing energy consumption while
maintaining effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pearl: Personalizing Large Language Model Writing Assistants with
  Generation-Calibrated Retrievers <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Bahareh Sarrafzadeh, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, Tara Safavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Powerful large language models have facilitated the development of writing
assistants that promise to significantly improve the quality and efficiency of
composition and communication. However, a barrier to effective assistance is
the lack of personalization in LLM outputs to the author's communication style,
specialized knowledge, and values. In this paper, we address this challenge by
proposing Pearl, a LLM writing assistant personalized with a retriever that is
trained to be generation-calibrated for personalization. Generation calibration
ensures that our retriever selects historic user authored documents to augment
an LLM prompt such that they are likely to help an LLM generation better adhere
to a users' preferences. We propose two key novelties for training such a
retriever: (1) A training data selection method that identifies user requests
likely to benefit from personalization and documents that provide that benefit;
and (2) A scale-calibrating KL-divergence objective that ensures that our
retriever scores remain proportional to the downstream generation quality from
using the document for personalized generation. In a series of holistic
evaluations, we demonstrate the effectiveness of Pearl in generating long-form
texts on multiple social media datasets. Finally, we demonstrate how a
generation-calibrated retriever can double as a performance predictor --
detecting low quality retrieval, and improving potentially under-performing
outputs via revision with LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Workshop on Customizable NLP at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Harnessing Multimodal Large Language Models for Multimodal Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09698v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09698v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Ye, Zhi Zheng, Yishan Shen, Tianshu Wang, Hengruo Zhang, Pei<span class="highlight-author">jun Zhu</span>, Runlong Yu, Kai Zhang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have demonstrated significant
potential in the field of Recommendation Systems (RSs). Most existing studies
have focused on converting user behavior logs into textual prompts and
leveraging techniques such as prompt tuning to enable LLMs for recommendation
tasks. Meanwhile, research interest has recently grown in multimodal
recommendation systems that integrate data from images, text, and other sources
using modality fusion techniques. This introduces new challenges to the
existing LLM-based recommendation paradigm which relies solely on text modality
information. Moreover, although Multimodal Large Language Models (MLLMs)
capable of processing multi-modal inputs have emerged, how to equip MLLMs with
multi-modal recommendation capabilities remains largely unexplored. To this
end, in this paper, we propose the Multimodal Large Language Model-enhanced
Multimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic
user preference, we design a two-stage user preference summarization method.
Specifically, we first utilize an MLLM-based item-summarizer to extract image
feature given an item and convert the image into text. Then, we employ a
recurrent user preference summarization generation paradigm to capture the
dynamic changes in user preferences based on an LLM-based user-summarizer.
Finally, to enable the MLLM for multi-modal recommendation task, we propose to
fine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)
techniques. Extensive evaluations across various datasets validate the
effectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt
to the evolving dynamics of user preferences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Graph <span class="highlight-title">Diffusion</span> with Applications in Personalized
  PageRanks <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00077v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00077v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhe Wei, Eli Chien, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph diffusion, which iteratively propagates real-valued substances among
the graph, is used in numerous graph/network-involved applications. However,
releasing diffusion vectors may reveal sensitive linking information in the
data such as transaction information in financial network data. However,
protecting the privacy of graph data is challenging due to its interconnected
nature. This work proposes a novel graph diffusion framework with edge-level
differential privacy guarantees by using noisy diffusion iterates. The
algorithm injects Laplace noise per diffusion iteration and adopts a
degree-based thresholding function to mitigate the high sensitivity induced by
low-degree nodes. Our privacy loss analysis is based on Privacy Amplification
by Iteration (PABI), which to our best knowledge, is the first effort that
analyzes PABI with Laplace noise and provides relevant applications. We also
introduce a novel Infinity-Wasserstein distance tracking method, which tightens
the analysis of privacy leakage and makes PABI more applicable in practice. We
evaluate this framework by applying it to Personalized Pagerank computation for
ranking tasks. Experiments on real-world network data demonstrate the
superiority of our method under stringent privacy conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appear in NeurIPS 2024. In this version, we provide a more rigorous
  analysis of graph distortion by establishing a tight bound, then update our
  corresponding experimental results, which are better than the previous
  version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ pTSE-T: Presentation Target Speaker Extraction using Unaligned Text Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Jiang, Xinquan Qian, Jiahe Lei, Zexu Pan, Wei Xue, Xu-cheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TSE aims to extract the clean speech of the target speaker in an audio
mixture, thus eliminating irrelevant background noise and speech. While prior
work has explored various auxiliary cues including pre-recorded speech, visual
information (e.g., lip motions and gestures), and spatial information, the
acquisition and selection of such strong cues are infeasible in many practical
scenarios. Unlike all existing work, in this paper, we condition the TSE
algorithm on semantic cues extracted from limited and unaligned text content,
such as condensed points from a presentation slide. This method is particularly
useful in scenarios like meetings, poster sessions, or lecture presentations,
where acquiring other cues in real-time is challenging. To this end, we design
two different networks. Specifically, our proposed TPE fuses audio features
with content-based semantic cues to facilitate time-frequency mask generation
to filter out extraneous noise, while another proposal, namely TSR, employs the
contrastive learning technique to associate blindly separated speech signals
with semantic cues. The experimental results show the efficacy in accurately
identifying the target speaker by utilizing semantic cues derived from limited
and unaligned text, resulting in SI-SDRi of 12.16 dB, SDRi of 12.66 dB, PESQi
of 0.830 and STOIi of 0.150, respectively. Dataset and source code will be
publicly available. Project demo page: https://slideTSE.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speech Separation with Pretrained Frontend to Minimize Domain Mismatch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wupeng Wang, Zexu Pan, Xinke Li, Shuai Wang, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech separation seeks to separate individual speech signals from a speech
mixture. Typically, most separation models are trained on synthetic data due to
the unavailability of target reference in real-world cocktail party scenarios.
As a result, there exists a domain gap between real and synthetic data when
deploying speech separation models in real-world applications. In this paper,
we propose a self-supervised domain-invariant pretrained (DIP) frontend that is
exposed to mixture data without the need for target reference speech. The DIP
frontend utilizes a Siamese network with two innovative pretext tasks, mixture
predictive coding (MPC) and mixture invariant coding (MIC), to capture shared
contextual cues between real and synthetic unlabeled mixtures. Subsequently, we
freeze the DIP frontend as a feature extractor when training the downstream
speech separation models on synthetic data. By pretraining the DIP frontend
with the contextual cues, we expect that the speech separation skills learned
from synthetic data can be effectively transferred to real data. To benefit
from the DIP frontend, we introduce a novel separation pipeline to align the
feature resolution of the separation models. We evaluate the speech separation
quality on standard benchmarks and real-world datasets. The results confirm the
superiority of our DIP frontend over existing speech separation models. This
study underscores the potential of large-scale pretraining to enhance the
quality and intelligibility of speech separation in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/ACM Transactions on Audio, Speech, and Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HumanVLM: Foundation for Human-Scene Vision-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Dai, Xu Long, Li Yutang, Zhang Yuanhui, Shuyin Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-scene vision-language tasks are increasingly prevalent in diverse
social applications, yet recent advancements predominantly rely on models
specifically tailored to individual tasks. Emerging research indicates that
large vision-language models (VLMs) can enhance performance across various
downstream vision-language understanding tasks. However, general-domain models
often underperform in specialized fields. This study introduces a
domain-specific Large Vision-Language Model, Human-Scene Vision-Language Model
(HumanVLM), designed to provide a foundation for human-scene Vision-Language
tasks. Specifically, (1) we create a large-scale human-scene multimodal
image-text dataset (HumanCaption-10M) sourced from the Internet to facilitate
domain-specific alignment; (2) develop a captioning approach for human-centered
images, capturing human faces, bodies, and backgrounds, and construct a
high-quality Human-Scene image-text dataset (HumanCaptionHQ, about 311k pairs)
that contain as much detailed information as possible about human; (3) Using
HumanCaption-10M and HumanCaptionHQ, we train a HumanVLM. In the experiments,
we then evaluate our HumanVLM across varous downstream tasks, where it
demonstrates superior overall performance among multimodal models of comparable
scale, particularly excelling in human-related tasks and significantly
outperforming similar models, including Qwen2VL and ChatGPT-4o. HumanVLM,
alongside the data introduced, will stimulate the research in human-around
fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages,11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-based Lossless Event Data Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmadreza Sezavar, Catarina Brites, Joao Ascenso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging event cameras acquire visual information by detecting time domain
brightness changes asynchronously at the pixel level and, unlike conventional
cameras, are able to provide high temporal resolution, very high dynamic range,
low latency, and low power consumption. Considering the huge amount of data
involved, efficient compression solutions are very much needed. In this
context, this paper presents a novel deep-learning-based lossless event data
compression scheme based on octree partitioning and a learned hyperprior model.
The proposed method arranges the event stream as a 3D volume and employs an
octree structure for adaptive partitioning. A deep neural network-based entropy
model, using a hyperprior, is then applied. Experimental results demonstrate
that the proposed method outperforms traditional lossless data compression
techniques in terms of compression ratio and bits per event.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Audio-Visual Sound Separation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiguo Pian, Yiyang Nan, Shijian Deng, Shentong Mo, Yunhui Guo, Yapeng Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel continual audio-visual sound separation
task, aiming to continuously separate sound sources for new classes while
preserving performance on previously learned classes, with the aid of visual
guidance. This problem is crucial for practical visually guided auditory
perception as it can significantly enhance the adaptability and robustness of
audio-visual sound separation models, making them more applicable for
real-world scenarios where encountering new sound sources is commonplace. The
task is inherently challenging as our models must not only effectively utilize
information from both modalities in current tasks but also preserve their
cross-modal association in old tasks to mitigate catastrophic forgetting during
audio-visual continual learning. To address these challenges, we propose a
novel approach named ContAV-Sep (\textbf{Cont}inual
\textbf{A}udio-\textbf{V}isual Sound \textbf{Sep}aration). ContAV-Sep presents
a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to uphold the
cross-modal semantic similarity through incremental tasks and retain previously
acquired knowledge of semantic similarity in old models, mitigating the risk of
catastrophic forgetting. The CrossSDC can seamlessly integrate into the
training process of different audio-visual sound separation frameworks.
Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic
forgetting and achieve significantly better performance compared to other
continual learning baselines for audio-visual sound separation. Code is
available at: \url{https://github.com/weiguoPian/ContAV-Sep_NeurIPS2024}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual
  Visual Answer Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibin Wen, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Multilingual Visual Answer Localization (MVAL) is to locate a
video segment that answers a given multilingual question. Existing methods
either focus solely on visual modality or integrate visual and subtitle
modalities. However, these methods neglect the audio modality in videos,
consequently leading to incomplete input information and poor performance in
the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span
Localization (AVTSL) method that incorporates audio modality to augment both
visual and textual representations for the MVAL task. Specifically, we
integrate features from three modalities and develop three predictors, each
tailored to the unique contributions of the fused modalities: an audio-visual
predictor, a visual predictor, and a textual predictor. Each predictor
generates predictions based on its respective modality. To maintain consistency
across the predicted results, we introduce an Audio-Visual-Textual Consistency
module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing
each modality's predictor to dynamically learn from the others. This
collaborative learning ensures that the model generates consistent and
comprehensive answers. Extensive experiments show that our proposed method
outperforms several state-of-the-art (SOTA) methods, which demonstrates the
effectiveness of the audio modality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FASTER: A Font-Agnostic Scene Text Editing and Rendering Framework <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02905v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02905v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alloy Das, Sanket Biswas, Prasun Roy, Subhankar Ghosh, Umapada Pal, Michael Blumenstein, Josep Lladós, Saumik Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene Text Editing (STE) is a challenging research problem, that primarily
aims towards modifying existing texts in an image while preserving the
background and the font style of the original text. Despite its utility in
numerous real-world applications, existing style-transfer-based approaches have
shown sub-par editing performance due to (1) complex image backgrounds, (2)
diverse font attributes, and (3) varying word lengths within the text. To
address such limitations, in this paper, we propose a novel font-agnostic scene
text editing and rendering framework, named FASTER, for simultaneously
generating text in arbitrary styles and locations while preserving a natural
and realistic appearance and structure. A combined fusion of target mask
generation and style transfer units, with a cascaded self-attention mechanism
has been proposed to focus on multi-level text region edits to handle varying
word lengths. Extensive evaluation on a real-world database with further
subjective human evaluation study indicates the superiority of FASTER in both
scene text editing and rendering tasks, in terms of model performance and
efficiency. Our code will be released upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POINTS: Improving Your Vision-language Model with Affordable Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04828v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04828v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Liu, Zhongyin Zhao, Ziyuan Zhuang, Le Tian, Xiao Zhou, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, vision-language models have made significant strides,
excelling in tasks like optical character recognition and geometric
problem-solving. However, several critical issues remain: 1) Proprietary models
often lack transparency about their architectures, while open-source models
need more detailed ablations of their training strategies. 2) Pre-training data
in open-source works is under-explored, with datasets added empirically, making
the process cumbersome. 3) Fine-tuning often focuses on adding datasets,
leading to diminishing returns. To address these issues, we propose the
following contributions: 1) We trained a robust baseline model using the latest
advancements in vision-language models, introducing effective improvements and
conducting comprehensive ablation and validation for each technique. 2)
Inspired by recent work on large language models, we filtered pre-training data
using perplexity, selecting the lowest perplexity data for training. This
approach allowed us to train on a curated 1M dataset, achieving competitive
performance. 3) During visual instruction tuning, we used model soup on
different datasets when adding more datasets yielded marginal improvements.
These innovations resulted in a 9B parameter model that performs competitively
with state-of-the-art models. Our strategies are efficient and lightweight,
making them easily adoptable by the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">98</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Orchestrating Structured Reasoning Achieve Kaggle
  Grandmaster Level 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Grosnit, Alexandre Maraval, James Doran, Giuseppe Paolo, Albert Thomas, Refinath Shahul Hameed Nabeezath Beevi, Jonas Gonzalez, Khyati Khandelwal, Ignacio Iacobacci, Abdelhakim Benechehab, Hamza Cherkaoui, Youssef Attia El-Hili, Kun Shao, Jianye Hao, Jun Yao, Balazs Kegl, Haitham Bou-Ammar, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Agent K v1.0, an end-to-end autonomous data science agent
designed to automate, optimise, and generalise across diverse data science
tasks. Fully automated, Agent K v1.0 manages the entire data science life cycle
by learning from experience. It leverages a highly flexible structured
reasoning framework to enable it to dynamically process memory in a nested
structure, effectively learning from accumulated experience stored to handle
complex reasoning tasks. It optimises long- and short-term memory by
selectively storing and retrieving key information, guiding future decisions
based on environmental rewards. This iterative approach allows it to refine
decisions without fine-tuning or backpropagation, achieving continuous
improvement through experiential learning. We evaluate our agent's apabilities
using Kaggle competitions as a case study. Following a fully automated
protocol, Agent K v1.0 systematically addresses complex and multimodal data
science tasks, employing Bayesian optimisation for hyperparameter tuning and
feature engineering. Our new evaluation framework rigorously assesses Agent K
v1.0's end-to-end capabilities to generate and send submissions starting from a
Kaggle competition URL. Results demonstrate that Agent K v1.0 achieves a 92.5\%
success rate across tasks, spanning tabular, computer vision, NLP, and
multimodal domains. When benchmarking against 5,856 human Kaggle competitors by
calculating Elo-MMR scores for each, Agent K v1.0 ranks in the top 38\%,
demonstrating an overall skill level comparable to Expert-level users. Notably,
its Elo-MMR score falls between the first and third quartiles of scores
achieved by human Grandmasters. Furthermore, our results indicate that Agent K
v1.0 has reached a performance level equivalent to Kaggle Grandmaster, with a
record of 6 gold, 3 silver, and 7 bronze medals, as defined by Kaggle's
progression system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Mice Grok? Glimpses of Hidden Progress During Overtraining in Sensory
  Cortex 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanishq Kumar, Blake Bordelon, Cengiz Pehlevan, Venkatesh N. Murthy, Samuel J. Gershman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Does learning of task-relevant representations stop when behavior stops
changing? Motivated by recent theoretical advances in machine learning and the
intuitive observation that human experts continue to learn from practice even
after mastery, we hypothesize that task-specific representation learning can
continue, even when behavior plateaus. In a novel reanalysis of recently
published neural data, we find evidence for such learning in posterior piriform
cortex of mice following continued training on a task, long after behavior
saturates at near-ceiling performance ("overtraining"). This learning is marked
by an increase in decoding accuracy from piriform neural populations and
improved performance on held-out generalization tests. We demonstrate that
class representations in cortex continue to separate during overtraining, so
that examples that were incorrectly classified at the beginning of overtraining
can abruptly be correctly classified later on, despite no changes in behavior
during that time. We hypothesize this hidden yet rich learning takes the form
of approximate margin maximization; we validate this and other predictions in
the neural data, as well as build and interpret a simple synthetic model that
recapitulates these phenomena. We conclude by showing how this model of
late-time feature learning implies an explanation for the empirical puzzle of
overtraining reversal in animal learning, where task-specific representations
are more robust to particular task changes because the learned features can be
reused.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long Context RAG Performance of Large Language Models <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, Michael Carbin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) has emerged as a crucial technique for
enhancing the accuracy of Large Language Models (LLMs) by incorporating
external information. With the advent of LLMs that support increasingly longer
context lengths, there is a growing interest in understanding how these models
perform in RAG scenarios. Can these new long context models improve RAG
performance? This paper presents a comprehensive study of the impact of
increased context length on RAG performance across 20 popular open source and
commercial LLMs. We ran RAG workflows while varying the total context length
from 2,000 to 128,000 tokens (and 2 million tokens when possible) on three
domain-specific datasets, and report key insights on the benefits and
limitations of long context in RAG applications. Our findings reveal that while
retrieving more documents can improve performance, only a handful of the most
recent state of the art LLMs can maintain consistent accuracy at long context
above 64k tokens. We also identify distinct failure modes in long context
scenarios, suggesting areas for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 NeurIPS workshop on Adaptive Foundation Models: Evolving AI for
  Personalized and Efficient Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-Stage Pretraining for Molecular Property Prediction in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Tirta Wijaya, Minghao Guo, Michael Sun, Hans-Peter Seidel, Wojciech Matusik, Vahid Babaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate property prediction is crucial for accelerating the discovery of new
molecules. Although deep learning models have achieved remarkable success,
their performance often relies on large amounts of labeled data that are
expensive and time-consuming to obtain. Thus, there is a growing need for
models that can perform well with limited experimentally-validated data. In
this work, we introduce MoleVers, a versatile pretrained model designed for
various types of molecular property prediction in the wild, i.e., where
experimentally-validated molecular property labels are scarce. MoleVers adopts
a two-stage pretraining strategy. In the first stage, the model learns
molecular representations from large unlabeled datasets via masked atom
prediction and dynamic denoising, a novel task enabled by a new branching
encoder architecture. In the second stage, MoleVers is further pretrained using
auxiliary labels obtained with inexpensive computational methods, enabling
supervised learning without the need for costly experimental data. This
two-stage framework allows MoleVers to learn representations that generalize
effectively across various downstream datasets. We evaluate MoleVers on a new
benchmark comprising 22 molecular datasets with diverse types of properties,
the majority of which contain 50 or fewer training labels reflecting real-world
conditions. MoleVers achieves state-of-the-art results on 20 out of the 22
datasets, and ranks second among the remaining two, highlighting its ability to
bridge the gap between data-hungry models and real-world conditions where
practically-useful labels are scarce.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Differentiable Feasibility Pump 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Cacciola, Alexandre Forel, Antonio Frangioni, Andrea Lodi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although nearly 20 years have passed since its conception, the feasibility
pump algorithm remains a widely used heuristic to find feasible primal
solutions to mixed-integer linear problems. Many extensions of the initial
algorithm have been proposed. Yet, its core algorithm remains centered around
two key steps: solving the linear relaxation of the original problem to obtain
a solution that respects the constraints, and rounding it to obtain an integer
solution. This paper shows that the traditional feasibility pump and many of
its follow-ups can be seen as gradient-descent algorithms with specific
parameters. A central aspect of this reinterpretation is observing that the
traditional algorithm differentiates the solution of the linear relaxation with
respect to its cost. This reinterpretation opens many opportunities for
improving the performance of the original algorithm. We study how to modify the
gradient-update step as well as extending its loss function. We perform
extensive experiments on MIPLIB instances and show that these modifications can
substantially reduce the number of iterations needed to find a solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PACE: Pacing Operator Learning to Accurate Optical Field Simulation for
  Complicated Photonic Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Zhu, Wenyan Cong, Guojin Chen, Shupeng Ning, Ray T. Chen, Jiaqi Gu, David Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electromagnetic field simulation is central to designing, optimizing, and
validating photonic devices and circuits. However, costly computation
associated with numerical simulation poses a significant bottleneck, hindering
scalability and turnaround time in the photonic circuit design process. Neural
operators offer a promising alternative, but existing SOTA approaches,
NeurOLight, struggle with predicting high-fidelity fields for real-world
complicated photonic devices, with the best reported 0.38 normalized mean
absolute error in NeurOLight. The inter-plays of highly complex light-matter
interaction, e.g., scattering and resonance, sensitivity to local structure
details, non-uniform learning complexity for full-domain simulation, and rich
frequency information, contribute to the failure of existing neural PDE
solvers. In this work, we boost the prediction fidelity to an unprecedented
level for simulating complex photonic devices with a novel operator design
driven by the above challenges. We propose a novel cross-axis factorized PACE
operator with a strong long-distance modeling capacity to connect the
full-domain complex field pattern with local device structures. Inspired by
human learning, we further divide and conquer the simulation task for extremely
hard cases into two progressively easy tasks, with a first-stage model learning
an initial solution refined by a second model. On various complicated photonic
device benchmarks, we demonstrate one sole PACE model is capable of achieving
73% lower error with 50% fewer parameters compared with various recent ML for
PDE solvers. The two-stage setup further advances high-fidelity simulation for
even more intricate cases. In terms of runtime, PACE demonstrates 154-577x and
11.8-12x simulation speedup over numerical solver using scipy or
highly-optimized pardiso solver, respectively. We open sourced the code and
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepeted by Neurips 2024, 21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Potentials and Challenges of Using Large Language Models
  for the Analysis of Transcriptional Regulation of Long Non-coding RNAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wang, Zhichao Hou, Xiaorui Liu, Xinxia Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on long non-coding RNAs (lncRNAs) has garnered significant attention
due to their critical roles in gene regulation and disease mechanisms. However,
the complexity and diversity of lncRNA sequences, along with the limited
knowledge of their functional mechanisms and the regulation of their
expressions, pose significant challenges to lncRNA studies. Given the
tremendous success of large language models (LLMs) in capturing complex
dependencies in sequential data, this study aims to systematically explore the
potential and limitations of LLMs in the sequence analysis related to the
transcriptional regulation of lncRNA genes. Our extensive experiments
demonstrated promising performance of fine-tuned genome foundation models on
progressively complex tasks. Furthermore, we conducted an insightful analysis
of the critical impact of task complexity, model selection, data quality, and
biological interpretability for the studies of the regulation of lncRNA gene
expression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecasting Outside the Box: Application-Driven Optimal Pointwise
  Forecasts for Stochastic Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tito Homem-de-Mello, Juan Valencia, Felipe Lagos, Guido Lagos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth in data availability in recent years has led to new
formulations of data-driven optimization problems. One such formulation is that
of stochastic optimization problems with contextual information, where the goal
is to optimize the expected value of a certain function given some contextual
information (also called features) that accompany the main data of interest.
The contextual information then allows for a better estimation of the quantity
of interest via machine learning methods, thereby leading to better solutions.
Oftentimes, however, machine learning methods yield just a pointwise estimate
instead of an entire distribution. In this paper we show that, when the problem
to be solved is a class of two-stage stochastic programs (namely, those with
fixed recourse matrix and fixed costs), under mild assumptions the problem can
be solved with just one scenario. While such a scenario - which does not have
be unique - is usually unknown, we present an integrated learning and
optimization procedure that yields the best approximation of that scenario
within the modeler's pre-specified set of parameterized forecast functions.
Numerical results conducted with inventory problems from the literature (with
synthetic data) as well as a bike-sharing problem with real data demonstrate
that the proposed approach performs well when compared to benchmark methods
from the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Metropolis: Scaling Large Language Model-based Multi-Agent Simulation
  with Out-of-order Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Xie, Hao Kang, Ying Sheng, Tushar Krishna, Kayvon Fatahalian, Christos Kozyrakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With more advanced natural language understanding and reasoning capabilities,
large language model (LLM)-powered agents are increasingly developed in
simulated environments to perform complex tasks, interact with other agents,
and exhibit emergent behaviors relevant to social science and gaming. However,
current multi-agent simulations frequently suffer from inefficiencies due to
the limited parallelism caused by false dependencies, resulting in performance
bottlenecks. In this paper, we introduce AI Metropolis, a simulation engine
that improves the efficiency of LLM agent simulations by incorporating
out-of-order execution scheduling. By dynamically tracking real dependencies
between agents, AI Metropolis minimizes false dependencies, enhancing
parallelism and enabling efficient hardware utilization. Our evaluations
demonstrate that AI Metropolis achieves speedups from 1.3x to 4.15x over
standard parallel simulation with global synchronization, approaching optimal
performance as the number of agents increases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Contrastive Learning via Gaussian Mixture Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parikshit Bansal, Ali Kavis, Sujay Sanghavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning attempts to learn representations from un-labeled data;
it does so via a loss function that encourages the embedding of a point to be
close to that of its augmentations, and far from the embeddings of random other
points. This simple idea performs remarkably well, yet it is not precisely
theoretically understood why this is the case. In this paper we analyze
contrastive learning (specifically, the InfoNCE loss) in a natural context:
dimensionality reduction in Gaussian Mixture Models. Crucially, we define an
augmentation of a data point as being another independent draw from the same
underlying mixture component. We show that vanilla InfoNCE is able to find the
optimal lower-dimensional subspace even when the Gaussians are not isotropic --
something that vanilla spectral techniques cannot do. We further extend our
analyses to multi-modal contrastive learning algorithms (e.g., CLIP). In this
setting we show that contrastive learning learns the subset of fisher-optimal
subspace, effectively filtering out all the noise from the learnt
representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Change Is the Only Constant: Dynamic LLM Slicing based on Layer
  Redundancy <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razvan-Gabriel Dumitru, Paul-Ioan Clotan, Vikas Yadav, Darius Peteleaza, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel model compression approach through dynamic
layer-specific pruning in Large Language Models (LLMs), enhancing the
traditional methodology established by SliceGPT. By transitioning from constant
to dynamic slicing, our method leverages the newly proposed Layer Redundancy
(LR) score, which assesses how much change each layer changes its input by
measuring the cosine similarity of the input to the output of the layer. We use
this score to prune parts of individual layers based on redundancy in such a
way that the average pruned percentage for all layers is a fixed value. We
conducted extensive experiments using models like Llama3-8B and Mistral-7B on
multiple datasets, evaluating different slicing bases and percentages to
determine optimal configurations that balance efficiency and performance. Our
findings show that our dynamic slicing approach not only maintains but, in many
cases, enhances model performance compared to the baseline established by
constant slicing methods. For instance, in several settings, we see performance
improvements of up to 5% over the SliceGPT baseline. Additionally, a perplexity
decrease by as much as 7% was observed across multiple benchmarks, validating
the effectiveness of our method. The code, model weights, and datasets are
open-sourced at https://github.com/RazvanDu/DynamicSlicing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Open-source Sim2Real Approach for Sensor-independent Robot Navigation
  in a Grid <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murad Mehrab Abrar, Souryadeep Mondal, Michelle Hickner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a Sim2Real (Simulation to Reality) approach to bridge the
gap between a trained agent in a simulated environment and its real-world
implementation in navigating a robot in a similar setting. Specifically, we
focus on navigating a quadruped robot in a real-world grid-like environment
inspired by the Gymnasium Frozen Lake -- a highly user-friendly and free
Application Programming Interface (API) to develop and test Reinforcement
Learning (RL) algorithms. We detail the development of a pipeline to transfer
motion policies learned in the Frozen Lake simulation to a physical quadruped
robot, thus enabling autonomous navigation and obstacle avoidance in a grid
without relying on expensive localization and mapping sensors. The work
involves training an RL agent in the Frozen Lake environment and utilizing the
resulting Q-table to control a 12 Degrees-of-Freedom (DOF) quadruped robot. In
addition to detailing the RL implementation, inverse kinematics-based quadruped
gaits, and the transfer policy pipeline, we open-source the project on GitHub
and include a demonstration video of our Sim2Real transfer approach. This work
provides an accessible, straightforward, and low-cost framework for
researchers, students, and hobbyists to explore and implement RL-based robot
navigation in real-world grid environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 9th IEEE International Conference on
  Robotics and Automation Engineering (IEEE ICRAE 2024), Singapore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LASER: Attention with Exponential Transformation <span class="chip">ICLR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Surya Duvvuri, Inderjit S. Dhillon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have had tremendous impact for several sequence related tasks,
largely due to their ability to retrieve from any part of the sequence via
softmax based dot-product attention. This mechanism plays a crucial role in
Transformer's performance. We analyze the gradients backpropagated through the
softmax operation in the attention mechanism and observe that these gradients
can often be small. This poor gradient signal backpropagation can lead to
inefficient learning of parameters preceeding the attention operations. To this
end, we introduce a new attention mechanism called LASER, which we analytically
show to admit a larger gradient signal. We show that LASER Attention can be
implemented by making small modifications to existing attention
implementations. We conduct experiments on autoregressive large language models
(LLMs) with upto 2.2 billion parameters where we show upto 3.38% and an average
of ~1% improvement over standard attention on downstream evaluations. Using
LASER gives the following relative improvements in generalization performance
across a variety of tasks (vision, text and speech): 4.67% accuracy in Vision
Transformer (ViT) on Imagenet, 2.25% error rate in Conformer on the Librispeech
speech-to-text and 0.93% fraction of incorrect predictions in BERT with 2.2
billion parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, under review in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pathway-Guided Optimization of Deep Generative Molecular Design Models
  for Cancer Therapy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alif Bin Abdul Qayyum, Susan D. Mertins, Amanda K. Paulson, Nathan M. Urban, Byung-Jun Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The data-driven drug design problem can be formulated as an optimization task
of a potentially expensive black-box objective function over a huge
high-dimensional and structured molecular space. The junction tree variational
autoencoder (JTVAE) has been shown to be an efficient generative model that can
be used for suggesting legitimate novel drug-like small molecules with improved
properties. While the performance of the generative molecular design (GMD)
scheme strongly depends on the initial training data, one can improve its
sampling efficiency for suggesting better molecules with enhanced properties by
optimizing the latent space. In this work, we propose how mechanistic models -
such as pathway models described by differential equations - can be used for
effective latent space optimization(LSO) of JTVAEs and other similar models for
GMD. To demonstrate the potential of our proposed approach, we show how a
pharmacodynamic model, assessing the therapeutic efficacy of a drug-like small
molecule by predicting how it modulates a cancer pathway, can be incorporated
for effective LSO of data-driven models for GMD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier Analysis of Variational Quantum Circuits for Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Wiedmann, Maniraman Periyasamy, Daniel D. Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  VQC can be understood through the lens of Fourier analysis. It is already
well-known that the function space represented by any circuit architecture can
be described through a truncated Fourier sum. We show that the spectrum
available to that truncated Fourier sum is not entirely determined by the
encoding gates of the circuit, since the variational part of the circuit can
constrain certain coefficients to zero, effectively removing that frequency
from the spectrum. To the best of our knowledge, we give the first description
of the functional dependence of the Fourier coefficients on the variational
parameters as trigonometric polynomials. This allows us to provide an algorithm
which computes the exact spectrum of any given circuit and the corresponding
Fourier coefficients. Finally, we demonstrate that by comparing the Fourier
transform of the dataset to the available spectra, it is possible to predict
which \gls{VQC} out of a given list of choices will be able to best fit the
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Trojan Detection Competitions with Linear Weight Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Todd Huster, Peter Lin, Razvan Stefanescu, Emmanuel Ekwedike, Ritu Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks can conceal malicious Trojan backdoors that allow a trigger
to covertly change the model behavior. Detecting signs of these backdoors,
particularly without access to any triggered data, is the subject of ongoing
research and open challenges. In one common formulation of the problem, we are
given a set of clean and poisoned models and need to predict whether a given
test model is clean or poisoned. In this paper, we introduce a detector that
works remarkably well across many of the existing datasets and domains. It is
obtained by training a binary classifier on a large number of models' weights
after performing a few different pre-processing steps including feature
selection and standardization, reference model weights subtraction, and model
alignment prior to detection. We evaluate this algorithm on a diverse set of
Trojan detection benchmarks and domains and examine the cases where the
approach is most and least effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference Optimal VLMs Need Only One Visual Token but Larger Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Y. Li, Sachin Goyal, Joao D. Semedo, J. Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have demonstrated strong capabilities across
various visual understanding and reasoning tasks. However, their real-world
deployment is often constrained by high latency during inference due to
substantial compute required to process the large number of input tokens
(predominantly from the image) by the LLM. To reduce inference costs, one can
either downsize the LLM or reduce the number of input image-tokens, the latter
of which has been the focus of many recent works around token compression.
However, it is unclear what the optimal trade-off is, as both the factors
directly affect the VLM performance. We first characterize this optimal
trade-off between the number of visual tokens and LLM parameters by
establishing scaling laws that capture variations in performance with these two
factors. Our results reveal a surprising trend: for visual reasoning tasks, the
inference-optimal behavior in VLMs, i.e., minimum downstream error at any given
fixed inference compute, is achieved when using the largest LLM that fits
within the inference budget while minimizing visual token count - often to a
single token. While the token reduction literature has mainly focused on
maintaining base model performance by modestly reducing the token count (e.g.,
$5-10\times$), our results indicate that the compute-optimal inference regime
requires operating under even higher token compression ratios. Based on these
insights, we take some initial steps towards building approaches tailored for
high token compression settings. Code is available at
https://github.com/locuslab/llava-token-compression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Climate AI for Corporate Decarbonization Metrics Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Dave, Mengchen Zhu, Dapeng Hu, Sachin Tiwari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Corporate Greenhouse Gas (GHG) emission targets are important metrics in
sustainable investing [12, 16]. To provide a comprehensive view of company
emission objectives, we propose an approach to source these metrics from
company public disclosures. Without automation, curating these metrics manually
is a labor-intensive process that requires combing through lengthy corporate
sustainability disclosures that often do not follow a standard format.
Furthermore, the resulting dataset needs to be validated thoroughly by Subject
Matter Experts (SMEs), further lengthening the time-to-market. We introduce the
Climate Artificial Intelligence for Corporate Decarbonization Metrics
Extraction (CAI) model and pipeline, a novel approach utilizing Large Language
Models (LLMs) to extract and validate linked metrics from corporate
disclosures. We demonstrate that the process improves data collection
efficiency and accuracy by automating data curation, validation, and metric
scoring from public corporate disclosures. We further show that our results are
agnostic to the choice of LLMs. This framework can be applied broadly to
information extraction from textual data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying Aleatoric Uncertainty of the Treatment Effect: A Novel
  Orthogonal Learner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentyn Melnychuk, Stefan Feuerriegel, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating causal quantities from observational data is crucial for
understanding the safety and effectiveness of medical treatments. However, to
make reliable inferences, medical practitioners require not only estimating
averaged causal quantities, such as the conditional average treatment effect,
but also understanding the randomness of the treatment effect as a random
variable. This randomness is referred to as aleatoric uncertainty and is
necessary for understanding the probability of benefit from treatment or
quantiles of the treatment effect. Yet, the aleatoric uncertainty of the
treatment effect has received surprisingly little attention in the causal
machine learning community. To fill this gap, we aim to quantify the aleatoric
uncertainty of the treatment effect at the covariate-conditional level, namely,
the conditional distribution of the treatment effect (CDTE). Unlike average
causal quantities, the CDTE is not point identifiable without strong additional
assumptions. As a remedy, we employ partial identification to obtain sharp
bounds on the CDTE and thereby quantify the aleatoric uncertainty of the
treatment effect. We then develop a novel, orthogonal learner for the bounds on
the CDTE, which we call AU-learner. We further show that our AU-learner has
several strengths in that it satisfies Neyman-orthogonality and is doubly
robust. Finally, we propose a fully-parametric deep learning instantiation of
our AU-learner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving stochastic partial differential equations using neural networks
  in the Wiener chaos expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Neufeld, Philipp Schmocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we solve stochastic partial differential equations (SPDEs)
numerically by using (possibly random) neural networks in the truncated Wiener
chaos expansion of their corresponding solution. Moreover, we provide some
approximation rates for learning the solution of SPDEs with additive and/or
multiplicative noise. Finally, we apply our results in numerical examples to
approximate the solution of three SPDEs: the stochastic heat equation, the
Heath-Jarrow-Morton equation, and the Zakai equation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Oblivious Defense in ML Models: Backdoor Removal without Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03279v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03279v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shafi Goldwasser, Jonathan Shafer, Neekon Vafa, Vinod Vaikuntanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As society grows more reliant on machine learning, ensuring the security of
machine learning systems against sophisticated attacks becomes a pressing
concern. A recent result of Goldwasser, Kim, Vaikuntanathan, and Zamir (2022)
shows that an adversary can plant undetectable backdoors in machine learning
models, allowing the adversary to covertly control the model's behavior.
Backdoors can be planted in such a way that the backdoored machine learning
model is computationally indistinguishable from an honest model without
backdoors.
  In this paper, we present strategies for defending against backdoors in ML
models, even if they are undetectable. The key observation is that it is
sometimes possible to provably mitigate or even remove backdoors without
needing to detect them, using techniques inspired by the notion of random
self-reducibility. This depends on properties of the ground-truth labels
(chosen by nature), and not of the proposed ML model (which may be chosen by an
attacker).
  We give formal definitions for secure backdoor mitigation, and proceed to
show two types of results. First, we show a "global mitigation" technique,
which removes all backdoors from a machine learning model under the assumption
that the ground-truth labels are close to a Fourier-heavy function. Second, we
consider distributions where the ground-truth labels are close to a linear or
polynomial function in $\mathbb{R}^n$. Here, we show "local mitigation"
techniques, which remove backdoors with high probability for every inputs of
interest, and are computationally cheaper than global mitigation. All of our
constructions are black-box, so our techniques work without needing access to
the model's representation (i.e., its code or parameters). Along the way we
prove a simple result for robust mean estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Based Semi-Supervised Segregated Lipschitz Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farid Bozorgnia, Yassine Belkheiri, Abderrahim Elmoataz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an approach to semi-supervised learning for the
classification of data using the Lipschitz Learning on graphs. We develop a
graph-based semi-supervised learning framework that leverages the properties of
the infinity Laplacian to propagate labels in a dataset where only a few
samples are labeled. By extending the theory of spatial segregation from the
Laplace operator to the infinity Laplace operator, both in continuum and
discrete settings, our approach provides a robust method for dealing with class
imbalance, a common challenge in machine learning. Experimental validation on
several benchmark datasets demonstrates that our method not only improves
classification accuracy compared to existing methods but also ensures efficient
label propagation in scenarios with limited labeled data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Matching with Ties: Approximation Ratios and Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyun Lin, Simon Mauras, Nadav Merlis, Vianney Perchet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of matching markets with ties, where one side of the
market does not necessarily have strict preferences over members at its other
side. For example, workers do not always have strict preferences over jobs,
students can give the same ranking for different schools and more. In
particular, assume w.l.o.g. that workers' preferences are determined by their
utility from being matched to each job, which might admit ties. Notably, in
contrast to classical two-sided markets with strict preferences, there is no
longer a single stable matching that simultaneously maximizes the utility for
all workers.
  We aim to guarantee each worker the largest possible share from the utility
in her best possible stable matching. We call the ratio between the worker's
best possible stable utility and its assigned utility the \emph{Optimal Stable
Share} (OSS)-ratio. We first prove that distributions over stable matchings
cannot guarantee an OSS-ratio that is sublinear in the number of workers.
Instead, randomizing over possibly non-stable matchings, we show how to achieve
a tight logarithmic OSS-ratio. Then, we analyze the case where the real utility
is not necessarily known and can only be approximated. In particular, we
provide an algorithm that guarantees a similar fraction of the utility compared
to the best possible utility. Finally, we move to a bandit setting, where we
select a matching at each round and only observe the utilities for matches we
perform. We show how to utilize our results for approximate utilities to
gracefully interpolate between problems without ties and problems with
statistical ties (small suboptimality gaps).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proxy-informed Bayesian transfer learning with unknown sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabina J. Sloman, Julien Martinelli, Samuel Kaski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalization outside the scope of one's training data requires leveraging
prior knowledge about the effects that transfer, and the effects that don't,
between different data sources. Bayesian transfer learning is a principled
paradigm for specifying this knowledge, and refining it on the basis of data
from the source (training) and target (prediction) tasks. We address the
challenging transfer learning setting where the learner (i) cannot fine-tune in
the target task, and (ii) does not know which source data points correspond to
the same task (i.e., the data sources are unknown). We propose a proxy-informed
robust method for probabilistic transfer learning (PROMPT), which provides a
posterior predictive estimate tailored to the structure of the target task,
without requiring the learner have access to any outcome information from the
target task. Instead, PROMPT relies on the availability of proxy information.
PROMPT uses the same proxy information for two purposes: (i) estimation of
effects specific to the target task, and (ii) construction of a robust
reweighting of the source data for estimation of effects that transfer between
tasks. We provide theoretical results on the effect of this reweighting on the
risk of negative transfer, and demonstrate application of PROMPT in two
synthetic settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discovering Data Structures: Nearest Neighbor Search and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Salemohamed, Laurent Charlin, Shivam Garg, Vatsal Sharan, Gregory Valiant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a general framework for end-to-end learning of data structures.
Our framework adapts to the underlying data distribution and provides
fine-grained control over query and space complexity. Crucially, the data
structure is learned from scratch, and does not require careful initialization
or seeding with candidate data structures/algorithms. We first apply this
framework to the problem of nearest neighbor search. In several settings, we
are able to reverse-engineer the learned data structures and query algorithms.
For 1D nearest neighbor search, the model discovers optimal distribution
(in)dependent algorithms such as binary search and variants of interpolation
search. In higher dimensions, the model learns solutions that resemble k-d
trees in some regimes, while in others, they have elements of
locality-sensitive hashing. The model can also learn useful representations of
high-dimensional data and exploit them to design effective data structures. We
also adapt our framework to the problem of estimating frequencies over a data
stream, and believe it could also be a powerful discovery tool for new
problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffLM: Controllable Synthetic Data Generation via <span class="highlight-title">Diffusion</span> Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Zhou, Xinyao Wang, Yulei Niu, Yaojie Shen, Lexin Tang, Fan Chen, Ben He, Le Sun, Longyin Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have significantly
enhanced their knowledge and generative capabilities, leading to a surge of
interest in leveraging LLMs for high-quality data synthesis. However, synthetic
data generation via prompting LLMs remains challenging due to LLMs' limited
understanding of target data distributions and the complexity of prompt
engineering, especially for structured formatted data. To address these issues,
we introduce DiffLM, a controllable data synthesis framework based on
variational autoencoder (VAE), which further (1) leverages diffusion models to
reserve more information of original distribution and format structure in the
learned latent distribution and (2) decouples the learning of target
distribution knowledge from the LLM's generative objectives via a plug-and-play
latent feature injection module. As we observed significant discrepancies
between the VAE's latent representations and the real data distribution, the
latent diffusion module is introduced into our framework to learn a fully
expressive latent distribution. Evaluations on seven real-world datasets with
structured formatted data (i.e., Tabular, Code and Tool data) demonstrate that
DiffLM generates high-quality data, with performance on downstream tasks
surpassing that of real data by 2-7 percent in certain cases. The data and code
will be publicly available upon completion of internal review.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Detection of Non-Cooperative RISs: Scan B-Testing via Deep
  Support Vector Data Description 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Stamatelis, Panagiotis Gavriilidis, Aymen Fakhreddine, George C. Alexandropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of promptly detecting the presence of
non-cooperative activity from one or more Reconfigurable Intelligent Surfaces
(RISs) with unknown characteristics lying in the vicinity of a Multiple-Input
Multiple-Output (MIMO) communication system using Orthogonal Frequency-Division
Multiplexing (OFDM) transmissions. We first present a novel wideband channel
model incorporating RISs as well as non-reconfigurable stationary surfaces,
which captures both the effect of the RIS actuation time on the channel in the
frequency domain as well as the difference between changing phase
configurations during or among transmissions. Considering that RISs may operate
under the coordination of a third-party system, and thus, may negatively impact
the communication of the intended MIMO OFDM system, we present a novel RIS
activity detection framework that is unaware of the distribution of the phase
configuration of any of the non-cooperative RISs. In particular, capitalizing
on the knowledge of the data distribution at the multi-antenna receiver, we
design a novel online change point detection statistic that combines a deep
support vector data description model with the scan $B$-test. The presented
numerical investigations demonstrate the improved detection accuracy as well as
decreased computational complexity of the proposed RIS detection approach over
existing change point detection schemes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, submitted to an IEEE conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing <span class="highlight-title">Transformer</span> Training Efficiency with Dynamic Dropout 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanrui Yan, Dan Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Dynamic Dropout, a novel regularization technique designed to
enhance the training efficiency of Transformer models by dynamically adjusting
the dropout rate based on training epochs or validation loss improvements. This
approach addresses the challenge of balancing regularization and model
capacity, which is crucial for achieving fast convergence and high performance.
Our method involves modifying the GPT model to accept a variable dropout rate
and updating dropout layers during training using schedules such as linear
decay, exponential decay, and validation loss-based adjustments. Extensive
experiments on the Shakespeare\_char dataset demonstrate that Dynamic Dropout
significantly accelerates training and improves inference efficiency compared
to a baseline model with a fixed dropout rate. The validation loss-based
adjustment schedule provided the best overall performance, highlighting the
potential of Dynamic Dropout as a valuable technique for training large-scale
Transformer models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topograph: An efficient Graph-Based Framework for Strictly Topology
  Preserving Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laurin Lux, Alexander H. Berger, Alexander Weers, Nico Stucki, Daniel Rueckert, Ulrich Bauer, Johannes C. Paetzold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological correctness plays a critical role in many image segmentation
tasks, yet most networks are trained using pixel-wise loss functions, such as
Dice, neglecting topological accuracy. Existing topology-aware methods often
lack robust topological guarantees, are limited to specific use cases, or
impose high computational costs. In this work, we propose a novel, graph-based
framework for topologically accurate image segmentation that is both
computationally efficient and generally applicable. Our method constructs a
component graph that fully encodes the topological information of both the
prediction and ground truth, allowing us to efficiently identify topologically
critical regions and aggregate a loss based on local neighborhood information.
Furthermore, we introduce a strict topological metric capturing the homotopy
equivalence between the union and intersection of prediction-label pairs. We
formally prove the topological guarantees of our approach and empirically
validate its effectiveness on binary and multi-class datasets. Our loss
demonstrates state-of-the-art performance with up to fivefold faster loss
computation compared to persistent homology methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel Orthogonality does not necessarily imply a Decrease in Feature
  Map Redundancy in CNNs: Convolutional Similarity Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zakariae Belmekki, Jun Li, Patrick Reuter, David Antonio Gómez Jáuregui, Karl Jenkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) have been heavily used in Deep Learning
due to their success in various tasks. Nonetheless, it has been observed that
CNNs suffer from redundancy in feature maps, leading to inefficient capacity
utilization. Efforts to mitigate and solve this problem led to the emergence of
multiple methods, amongst which is kernel orthogonality through variant means.
In this work, we challenge the common belief that kernel orthogonality leads to
a decrease in feature map redundancy, which is, supposedly, the ultimate
objective behind kernel orthogonality. We prove, theoretically and empirically,
that kernel orthogonality has an unpredictable effect on feature map similarity
and does not necessarily decrease it. Based on our theoretical result, we
propose an effective method to reduce feature map similarity independently of
the input of the CNN. This is done by minimizing a novel loss function we call
Convolutional Similarity. Empirical results show that minimizing the
Convolutional Similarity increases the performance of classification models and
can accelerate their convergence. Furthermore, using our proposed method pushes
towards a more efficient use of the capacity of models, allowing the use of
significantly smaller models to achieve the same levels of performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel Approximation using Analog In-Memory Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Büchel, Giacomo Camposampiero, Athanasios Vasilopoulos, Corey Lammie, Manuel Le Gallo, Abbas Rahimi, Abu Sebastian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kernel functions are vital ingredients of several machine learning
algorithms, but often incur significant memory and computational costs. We
introduce an approach to kernel approximation in machine learning algorithms
suitable for mixed-signal Analog In-Memory Computing (AIMC) architectures.
Analog In-Memory Kernel Approximation addresses the performance bottlenecks of
conventional kernel-based methods by executing most operations in approximate
kernel methods directly in memory. The IBM HERMES Project Chip, a
state-of-the-art phase-change memory based AIMC chip, is utilized for the
hardware demonstration of kernel approximation. Experimental results show that
our method maintains high accuracy, with less than a 1% drop in kernel-based
ridge classification benchmarks and within 1% accuracy on the Long Range Arena
benchmark for kernelized attention in Transformer neural networks. Compared to
traditional digital accelerators, our approach is estimated to deliver superior
energy efficiency and lower power consumption. These findings highlight the
potential of heterogeneous AIMC architectures to enhance the efficiency and
scalability of machine learning applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Predictive Models for Healthcare via Rational Logistic
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thiti Suttaket, L Vivek Harsha Vardhan, Stanley Kok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The healthcare sector has experienced a rapid accumulation of digital data
recently, especially in the form of electronic health records (EHRs). EHRs
constitute a precious resource that IS researchers could utilize for clinical
applications (e.g., morbidity prediction). Deep learning seems like the obvious
choice to exploit this surfeit of data. However, numerous studies have shown
that deep learning does not enjoy the same kind of success on EHR data as it
has in other domains; simple models like logistic regression are frequently as
good as sophisticated deep learning ones. Inspired by this observation, we
develop a novel model called rational logistic regression (RLR) that has
standard logistic regression (LR) as its special case (and thus inherits LR's
inductive bias that aligns with EHR data). RLR has rational series as its
theoretical underpinnings, works on longitudinal time-series data, and learns
interpretable patterns. Empirical comparisons on real-world clinical tasks
demonstrate RLR's efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICIS 2021 Proceedings ( see
  https://aisel.aisnet.org/icis2021/is_health/is_health/18 )</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Personal data Value at Risk Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Enriquez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What if the main data protection vulnerability is risk management? Data
Protection merges three disciplines: data protection law, information security,
and risk management. Nonetheless, very little research has been made on the
field of data protection risk management, where subjectivity and superficiality
are the dominant state of the art. Since the GDPR tells you what to do, but not
how to do it, the solution for approaching GDPR compliance is still a gray
zone, where the trend is using the rule of thumb. Considering that the most
important goal of risk management is to reduce uncertainty in order to take
informed decisions, risk management for the protection of the rights and
freedoms of the data subjects cannot be disconnected from the impact
materialization that data controllers and processors need to assess. This paper
proposes a quantitative approach to data protection risk-based compliance from
a data controllers perspective, with the aim of proposing a mindset change,
where data protection impact assessments can be improved by using data
protection analytics, quantitative risk analysis, and calibrating expert
opinions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Data Collection for Efficient Semiparametric Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Gupta, Zachary C. Lipton, David Childers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While many works have studied statistical data fusion, they typically assume
that the various datasets are given in advance. However, in practice,
estimation requires difficult data collection decisions like determining the
available data sources, their costs, and how many samples to collect from each
source. Moreover, this process is often sequential because the data collected
at a given time can improve collection decisions in the future. In our setup,
given access to multiple data sources and budget constraints, the agent must
sequentially decide which data source to query to efficiently estimate a target
parameter. We formalize this task using Online Moment Selection, a
semiparametric framework that applies to any parameter identified by a set of
moment conditions. Interestingly, the optimal budget allocation depends on the
(unknown) true parameters. We present two online data collection policies,
Explore-then-Commit and Explore-then-Greedy, that use the parameter estimates
at a given time to optimally allocate the remaining budget in the future steps.
We prove that both policies achieve zero regret (assessed by asymptotic MSE)
relative to an oracle policy. We empirically validate our methods on both
synthetic and real-world causal effect estimation tasks, demonstrating that the
online data collection policies outperform their fixed counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Insights into Lunar Mineralogy: An Unsupervised Approach for Clustering
  of the Moon Mineral Mapper (M3) spectral data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Freja Thoresen, Igor Drozdovskiy, Aidan Cowley, Magdelena Laban, Sebastien Besse, Sylvain Blunier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel method for mapping spectral features of the Moon
using machine learning-based clustering of hyperspectral data from the Moon
Mineral Mapper (M3) imaging spectrometer. The method uses a convolutional
variational autoencoder to reduce the dimensionality of the spectral data and
extract features of the spectra. Then, a k-means algorithm is applied to
cluster the latent variables into five distinct groups, corresponding to
dominant spectral features, which are related to the mineral composition of the
Moon's surface. The resulting global spectral cluster map shows the
distribution of the five clusters on the Moon, which consist of a mixture of,
among others, plagioclase, pyroxene, olivine, and Fe-bearing minerals across
the Moon's surface. The clusters are compared to the mineral maps from the
Kaguya mission, which showed that the locations of the clusters overlap with
the locations of high wt% of minerals such as plagioclase, clinopyroxene, and
olivine. The paper demonstrates the usefulness of unbiased unsupervised
learning for lunar mineral exploration and provides a comprehensive analysis of
lunar mineralogy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blind Estimation of Sub-band Acoustic Parameters from Ambisonics
  Recordings using Spectro-Spatial Covariance Features <span class="chip">ICASSP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyu Meng, Jeroen Breebaart, Jeremy Stoddard, Vidhyasaharan Sethu, Eliathamby Ambikairajah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating frequency-varying acoustic parameters is essential for enhancing
immersive perception in realistic spatial audio creation. In this paper, we
propose a unified framework that blindly estimates reverberation time (T60),
direct-to-reverberant ratio (DRR), and clarity (C50) across 10 frequency bands
using first-order Ambisonics (FOA) speech recordings as inputs. The proposed
framework utilizes a novel feature named Spectro-Spatial Covariance Vector
(SSCV), efficiently representing temporal, spectral as well as spatial
information of the FOA signal. Our models significantly outperform existing
single-channel methods with only spectral information, reducing estimation
errors by more than half for all three acoustic parameters. Additionally, we
introduce FOA-Conv3D, a novel back-end network for effectively utilising the
SSCV feature with a 3D convolutional encoder. FOA-Conv3D outperforms the
convolutional neural network (CNN) and recurrent convolutional neural network
(CRNN) backends, achieving lower estimation errors and accounting for a higher
proportion of variance (PoV) for all 3 acoustic parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pre-trained Visual Dynamics Representations for Efficient Policy
  Learning <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Luo, Bohan Zhou, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training for Reinforcement Learning (RL) with purely video data is a
valuable yet challenging problem. Although in-the-wild videos are readily
available and inhere a vast amount of prior world knowledge, the absence of
action annotations and the common domain gap with downstream tasks hinder
utilizing videos for RL pre-training. To address the challenge of pre-training
with videos, we propose Pre-trained Visual Dynamics Representations (PVDR) to
bridge the domain gap between videos and downstream tasks for efficient policy
learning. By adopting video prediction as a pre-training task, we use a
Transformer-based Conditional Variational Autoencoder (CVAE) to learn visual
dynamics representations. The pre-trained visual dynamics representations
capture the visual dynamics prior knowledge in the videos. This abstract prior
knowledge can be readily adapted to downstream tasks and aligned with
executable actions through online adaptation. We conduct experiments on a
series of robotics visual control tasks and verify that PVDR is an effective
form for pre-training with videos to promote policy learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Hamiltonian, structure and trace distance learning of Gaussian
  states 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Fanizza, Cambyse Rouzé, Daniel Stilck França
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we initiate the study of Hamiltonian learning for positive
temperature bosonic Gaussian states, the quantum generalization of the widely
studied problem of learning Gaussian graphical models. We obtain efficient
protocols, both in sample and computational complexity, for the task of
inferring the parameters of their underlying quadratic Hamiltonian under the
assumption of bounded temperature, squeezing, displacement and maximal degree
of the interaction graph. Our protocol only requires heterodyne measurements,
which are often experimentally feasible, and has a sample complexity that
scales logarithmically with the number of modes. Furthermore, we show that it
is possible to learn the underlying interaction graph in a similar setting and
sample complexity. Taken together, our results put the status of the quantum
Hamiltonian learning problem for continuous variable systems in a much more
advanced state when compared to spins, where state-of-the-art results are
either unavailable or quantitatively inferior to ours. In addition, we use our
techniques to obtain the first results on learning Gaussian states in trace
distance with a quadratic scaling in precision and polynomial in the number of
modes, albeit imposing certain restrictions on the Gaussian states. Our main
technical innovations are several continuity bounds for the covariance and
Hamiltonian matrix of a Gaussian state, which are of independent interest,
combined with what we call the local inversion technique. In essence, the local
inversion technique allows us to reliably infer the Hamiltonian of a Gaussian
state by only estimating in parallel submatrices of the covariance matrix whose
size scales with the desired precision, but not the number of modes. This way
we bypass the need to obtain precise global estimates of the covariance matrix,
controlling the sample complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Machine Learning Approach for the Efficient Estimation of Ground-Level
  Air Temperature in Urban Areas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iñigo Delgado-Enales, Joshua Lizundia-Loiola, Patricia Molina-Costa, Javier Del Ser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasingly populated cities of the 21st Century face the challenge of
being sustainable and resilient spaces for their inhabitants. However, climate
change, among other problems, makes these objectives difficult to achieve. The
Urban Heat Island (UHI) phenomenon that occurs in cities, increasing their
thermal stress, is one of the stumbling blocks to achieve a more sustainable
city. The ability to estimate temperatures with a high degree of accuracy
allows for the identification of the highest priority areas in cities where
urban improvements need to be made to reduce thermal discomfort. In this work
we explore the usefulness of image-to-image deep neural networks (DNNs) for
correlating spatial and meteorological variables of a urban area with
street-level air temperature. The air temperature at street-level is estimated
both spatially and temporally for a specific use case, and compared with
existing, well-established numerical models. Based on the obtained results,
deep neural networks are confirmed to be faster and less computationally
expensive alternative for ground-level air temperature compared to numerical
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 8 figures, 2 tables, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing the power of novel conditional generative approaches for new
  materials discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lev Novitskiy, Vladimir Lazarev, Mikhail Tiutiulnikov, Nikita Vakhrameev, Roman Eremin, Innokentiy Humonen, Andrey Kuznetsov, Denis Dimitrov, Semen Budennyy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For a very long time, computational approaches to the design of new materials
have relied on an iterative process of finding a candidate material and
modeling its properties. AI has played a crucial role in this regard, helping
to accelerate the discovery and optimization of crystal properties and
structures through advanced computational methodologies and data-driven
approaches. To address the problem of new materials design and fasten the
process of new materials search, we have applied latest generative approaches
to the problem of crystal structure design, trying to solve the inverse
problem: by given properties generate a structure that satisfies them without
utilizing supercomputer powers. In our work we propose two approaches: 1)
conditional structure modification: optimization of the stability of an
arbitrary atomic configuration, using the energy difference between the most
energetically favorable structure and all its less stable polymorphs and 2)
conditional structure generation. We used a representation for materials that
includes the following information: lattice, atom coordinates, atom types,
chemical features, space group and formation energy of the structure. The loss
function was optimized to take into account the periodic boundary conditions of
crystal structures. We have applied Diffusion models approach, Flow matching,
usual Autoencoder (AE) and compared the results of the models and approaches.
As a metric for the study, physical PyMatGen matcher was employed: we compare
target structure with generated one using default tolerances. So far, our
modifier and generator produce structures with needed properties with accuracy
41% and 82% respectively. To prove the offered methodology efficiency,
inference have been carried out, resulting in several potentially new
structures with formation energy below the AFLOW-derived convex hulls.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User Centric Semantic Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunze Liu, Yifei Sun, Zhaorui Wang, Lizhao You, Haoyuan Pan, Fangxin Wang, Shuguang Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current studies on semantic communications mainly focus on efficiently
extracting semantic information to reduce bandwidth usage between a transmitter
and a user. Although significant process has been made in the semantic
communications, a fundamental design problem is that the semantic information
is extracted based on certain criteria at the transmitter side along, without
considering the user's actual requirements. As a result, critical information
that is of primary concern to the user may be lost. In such cases, the semantic
transmission becomes meaningless to the user, as all received information is
irrelevant to the user's interests. To solve this problem, this paper presents
a user centric semantic communication system, where the user sends its request
for the desired semantic information to the transmitter at the start of each
transmission. Then, the transmitter extracts the required semantic information
accordingly. A key challenge is how the transmitter can understand the user's
requests for semantic information and extract the required semantic information
in a reasonable and robust manner. We solve this challenge by designing a
well-structured framework and leveraging off-the-shelf products, such as GPT-4,
along with several specialized tools for detection and estimation. Evaluation
results demonstrate the feasibility and effectiveness of the proposed user
centric semantic communication system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Density Ratio Estimation-based Bayesian Optimization with
  Semi-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15612v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15612v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungtaek Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimization has attracted huge attention from diverse research
areas in science and engineering, since it is capable of efficiently finding a
global optimum of an expensive-to-evaluate black-box function. In general, a
probabilistic regression model is widely used as a surrogate function to model
an explicit distribution over function evaluations given an input to estimate
and a training dataset. Beyond the probabilistic regression-based methods,
density ratio estimation-based Bayesian optimization has been suggested in
order to estimate a density ratio of the groups relatively close and relatively
far to a global optimum. Developing this line of research further, supervised
classifiers are employed to estimate a class probability for the two groups
instead of a density ratio. However, the supervised classifiers used in this
strategy are prone to be overconfident for known knowledge on global solution
candidates. Supposing that we have access to unlabeled points, e.g., predefined
fixed-size pools, we propose density ratio estimation-based Bayesian
optimization with semi-supervised learning to solve this challenge. Finally, we
show the empirical results of our methods and several baseline methods in two
distinct scenarios with unlabeled point sampling and a fixed-size pool and
analyze the validity of our proposed methods in diverse experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Reconstruction Using Counterfactual Explanations: A Perspective
  From Polytope Theory <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pasan Dissanayake, Sanghamitra Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual explanations provide ways of achieving a favorable model
outcome with minimum input perturbation. However, counterfactual explanations
can also be leveraged to reconstruct the model by strategically training a
surrogate model to give similar predictions as the original (target) model. In
this work, we analyze how model reconstruction using counterfactuals can be
improved by further leveraging the fact that the counterfactuals also lie quite
close to the decision boundary. Our main contribution is to derive novel
theoretical relationships between the error in model reconstruction and the
number of counterfactual queries required using polytope theory. Our
theoretical analysis leads us to propose a strategy for model reconstruction
that we call Counterfactual Clamping Attack (CCA) which trains a surrogate
model using a unique loss function that treats counterfactuals differently than
ordinary instances. Our approach also alleviates the related problem of
decision boundary shift that arises in existing model reconstruction approaches
when counterfactuals are treated as ordinary instances. Experimental results
demonstrate that our strategy improves fidelity between the target and
surrogate model predictions on several datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward Machines for Deep RL in Noisy and Uncertain Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00120v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00120v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew C. Li, Zizhao Chen, Toryn Q. Klassen, Pashootan Vaezipoor, Rodrigo Toro Icarte, Sheila A. McIlraith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward Machines provide an automaton-inspired structure for specifying
instructions, safety constraints, and other temporally extended reward-worthy
behaviour. By exposing the underlying structure of a reward function, they
enable the decomposition of an RL task, leading to impressive gains in sample
efficiency. Although Reward Machines and similar formal specifications have a
rich history of application towards sequential decision-making problems, they
critically rely on a ground-truth interpretation of the domain-specific
vocabulary that forms the building blocks of the reward function--such
ground-truth interpretations are elusive in the real world due in part to
partial observability and noisy sensing. In this work, we explore the use of
Reward Machines for Deep RL in noisy and uncertain environments. We
characterize this problem as a POMDP and propose a suite of RL algorithms that
exploit task structure under uncertain interpretation of the domain-specific
vocabulary. Through theory and experiments, we expose pitfalls in naive
approaches to this problem while simultaneously demonstrating how task
structure can be successfully leveraged under noisy interpretations of the
vocabulary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting critical treatment effect bias in small subgroups <span class="chip">UAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piersilvio De Bartolomeis, Javier Abad, Konstantin Donhauser, Fanny Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Randomized trials are considered the gold standard for making informed
decisions in medicine, yet they often lack generalizability to the patient
populations in clinical practice. Observational studies, on the other hand,
cover a broader patient population but are prone to various biases. Thus,
before using an observational study for decision-making, it is crucial to
benchmark its treatment effect estimates against those derived from a
randomized trial. We propose a novel strategy to benchmark observational
studies beyond the average treatment effect. First, we design a statistical
test for the null hypothesis that the treatment effects estimated from the two
studies, conditioned on a set of relevant features, differ up to some
tolerance. We then estimate an asymptotically valid lower bound on the maximum
bias strength for any subgroup in the observational study. Finally, we validate
our benchmarking strategy in a real-world setting and show that it leads to
conclusions that align with established medical knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the Conference on Uncertainty in
  Artificial Intelligence (UAI) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Weakly Convex Optimization Beyond Lipschitz Continuity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13971v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13971v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhi Gao, Qi Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers stochastic weakly convex optimization without the
standard Lipschitz continuity assumption. Based on new adaptive regularization
(stepsize) strategies, we show that a wide class of stochastic algorithms,
including the stochastic subgradient method, preserve the $\mathcal{O} ( 1 /
\sqrt{K})$ convergence rate with constant failure rate. Our analyses rest on
rather weak assumptions: the Lipschitz parameter can be either bounded by a
general growth function of $\|x\|$ or locally estimated through independent
random samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Content Moderation with Culturally-Aware Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex J. Chan, José Luis Redondo García, Fabrizio Silvestri, Colm O'Donnell, Konstantina Palla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content moderation on a global scale must navigate a complex array of local
cultural distinctions, which can hinder effective enforcement. While global
policies aim for consistency and broad applicability, they often miss the
subtleties of regional language interpretation, cultural beliefs, and local
legislation. This work introduces a flexible framework that enhances foundation
language models with cultural knowledge. Our approach involves fine-tuning
encoder-decoder models on media-diet data to capture cultural nuances, and
applies a continued training regime to effectively integrate these models into
a content moderation pipeline. We evaluate this framework in a case study of an
online podcast platform with content spanning various regions. The results show
that our culturally adapted models improve the accuracy of local violation
detection and offer explanations that align more closely with regional cultural
norms. Our findings reinforce the need for an adaptable content moderation
approach that remains flexible in response to the diverse cultural landscapes
it operates in and represents a step towards a more equitable and culturally
sensitive framework for content moderation, demonstrating what is achievable in
this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 Figures. Supplementary material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LifelongMemory: Leveraging LLMs for Answering Queries in Long-form
  Egocentric Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05269v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05269v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Wang, Yanlai Yang, Mengye Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce LifelongMemory, a new framework for accessing
long-form egocentric videographic memory through natural language question
answering and retrieval. LifelongMemory generates concise video activity
descriptions of the camera wearer and leverages the zero-shot capabilities of
pretrained large language models to perform reasoning over long-form video
context. Furthermore, LifelongMemory uses a confidence and explanation module
to produce confident, high-quality, and interpretable answers. Our approach
achieves state-of-the-art performance on the EgoSchema benchmark for question
answering and is highly competitive on the natural language query (NLQ)
challenge of Ego4D. Code is available at
https://github.com/agentic-learning-ai-lab/lifelong-memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effects of Multi-Task Learning on ReLU Neural Network Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Nakhleh, Joseph Shenouda, Robert D. Nowak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the properties of solutions to multi-task shallow ReLU
neural network learning problems, wherein the network is trained to fit a
dataset with minimal sum of squared weights. Remarkably, the solutions learned
for each individual task resemble those obtained by solving a kernel method,
revealing a novel connection between neural networks and kernel methods. It is
known that single-task neural network training problems are equivalent to
minimum norm interpolation problem in a non-Hilbertian Banach space, and that
the solutions of such problems are generally non-unique. In contrast, we prove
that the solutions to univariate-input, multi-task neural network interpolation
problems are almost always unique, and coincide with the solution to a
minimum-norm interpolation problem in a Sobolev (Reproducing Kernel) Hilbert
Space. We also demonstrate a similar phenomenon in the multivariate-input case;
specifically, we show that neural network learning problems with large numbers
of diverse tasks are approximately equivalent to an $\ell^2$ (Hilbert space)
minimization problem over a fixed kernel determined by the optimal neurons.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonlinear Distributionally Robust Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03202v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03202v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Rayyan Sheriff, Peyman Mohajerin Esfahani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article focuses on a class of distributionally robust optimization (DRO)
problems where, unlike the growing body of the literature, the objective
function is potentially nonlinear in the distribution. Existing methods to
optimize nonlinear functions in probability space use the Frechet derivatives,
which present theoretical and computational challenges. Motivated by this, we
propose an alternative notion for the derivative and corresponding smoothness
based on Gateaux (G)-derivative for generic risk measures. These concepts are
explained via three running risk measure examples of variance, entropic risk,
and risk on finite support sets. We then propose a G-derivative-based
Frank-Wolfe (FW) algorithm for generic nonlinear optimization problems in
probability spaces and establish its convergence under the proposed notion of
smoothness in a completely norm-independent manner. We use the set-up of the FW
algorithm to devise a methodology to compute a saddle point of the nonlinear
DRO problem. Finally, we validate our theoretical results on two cases of the
$entropic$ and $variance$ risk measures in the context of portfolio selection
problems. In particular, we analyze their regularity conditions and "sufficient
statistic", compute the respective FW-oracle in various settings, and confirm
the theoretical outcomes through numerical validation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pre-trained Mixed Integer Optimization through Multi-variable
  Cardinality Branching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanguang Chen, Wenzhi Gao, Dongdong Ge, Yinyu Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a Pre-trained Mixed Integer Optimization framework
(PreMIO) that accelerates online mixed integer program (MIP) solving with
offline datasets and machine learning models. Our method is based on a
data-driven multi-variable cardinality branching procedure that splits the MIP
feasible region using hyperplanes chosen by the concentration inequalities.
Unlike most previous ML+MIP approaches that either require complicated
implementation or suffer from a lack of theoretical justification, our method
is simple, flexible, provable, and explainable. Numerical experiments on both
classical OR benchmark datasets and real-life instances validate the efficiency
of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Adaptation for Offline Reinforcement Learning with Limited
  Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiqin Chen, Sandipan Mishra, Santiago Paternain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) learns effective policies from a static
target dataset. Despite state-of-the-art (SOTA) offline RL algorithms being
promising, they highly rely on the quality of the target dataset. The
performance of SOTA algorithms can degrade in scenarios with limited samples in
the target dataset, which is often the case in real-world applications. To
address this issue, domain adaptation that leverages auxiliary samples from
related source datasets (such as simulators) can be beneficial. In this
context, determining the optimal way to trade off the source and target
datasets remains a critical challenge in offline RL. To the best of our
knowledge, this paper proposes the first framework that theoretically and
experimentally explores how the weight assigned to each dataset affects the
performance of offline RL. We establish the performance bounds and convergence
neighborhood of our framework, both of which depend on the selection of the
weight. Furthermore, we identify the existence of an optimal weight for
balancing the two datasets. All theoretical guarantees and optimal weight
depend on the quality of the source dataset and the size of the target dataset.
Our empirical results on the well-known Procgen Benchmark substantiate our
theoretical contributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Numerical Wave Propagation Enhanced By An End-to-End Deep
  Learning Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02304v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02304v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Kaiser, Richard Tsai, Christian Klingenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a variety of scientific and engineering domains, the need for
high-fidelity and efficient solutions for high-frequency wave propagation holds
great significance. Recent advances in wave modeling use sufficiently accurate
fine solver outputs to train a neural network that enhances the accuracy of a
fast but inaccurate coarse solver. In this paper we build upon the work of
Nguyen and Tsai (2023) and present a novel unified system that integrates a
numerical solver with a deep learning component into an end-to-end framework.
In the proposed setting, we investigate refinements to the network architecture
and data generation algorithm. A stable and fast solver further allows the use
of Parareal, a parallel-in-time algorithm to correct high-frequency wave
components. Our results show that the cohesive structure improves performance
without sacrificing speed, and demonstrate the importance of temporal dynamics,
as well as Parareal, for accurate wave propagation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the proceedings of ENUMATH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Improved Empirical Fisher Approximation for Natural Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaodong Wu, Wenyi Yu, Chao Zhang, Philip Woodland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate Natural Gradient Descent (NGD) methods are an important family of
optimisers for deep learning models, which use approximate Fisher information
matrices to pre-condition gradients during training. The empirical Fisher (EF)
method approximates the Fisher information matrix empirically by reusing the
per-sample gradients collected during back-propagation. Despite its ease of
implementation, the EF approximation has its theoretical and practical
limitations. This paper investigates the inversely-scaled projection issue of
EF, which is shown to be a major cause of its poor empirical approximation
quality. An improved empirical Fisher (iEF) method is proposed to address this
issue, which is motivated as a generalised NGD method from a loss reduction
perspective, meanwhile retaining the practical convenience of EF. The exact iEF
and EF methods are experimentally evaluated using practical deep learning
setups. Optimisation experiments show that applying exact iEF directly as an
optimiser provides strong convergence and generalisation. Additionally, under a
novel empirical evaluation framework, the proposed iEF method shows
consistently better approximation quality to exact Natural Gradient updates
than both the EF and the more expensive sampled Fisher methods, meanwhile
demonstrating the superior property of being robust to the choice of damping
across tasks and training stages. Improving existing approximate NGD optimisers
with iEF is expected to lead to better convergence and robustness. Furthermore,
the iEF method also serves as a better approximation method to the Fisher
information matrix itself, which enables the improvement of a variety of
Fisher-based methods, not limited to the scope of optimisation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 15 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data needs and challenges for quantum dot devices automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14322v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14322v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justyna P. Zwolak, Jacob M. Taylor, Reed W. Andrews, Jared Benson, Garnett W. Bryant, Donovan Buterakos, Anasua Chatterjee, Sankar Das Sarma, Mark A. Eriksson, Eliška Greplová, Michael J. Gullans, Fabian Hader, Tyler J. Kovach, Pranav S. Mundada, Mick Ramsey, Torbjørn Rasmussen, Brandon Severin, Anthony Sigillito, Brennan Undseth, Brian Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gate-defined quantum dots are a promising candidate system for realizing
scalable, coupled qubit systems and serving as a fundamental building block for
quantum computers. However, present-day quantum dot devices suffer from
imperfections that must be accounted for, which hinders the characterization,
tuning, and operation process. Moreover, with an increasing number of quantum
dot qubits, the relevant parameter space grows sufficiently to make heuristic
control infeasible. Thus, it is imperative that reliable and scalable
autonomous tuning approaches are developed. This meeting report outlines
current challenges in automating quantum dot device tuning and operation with a
particular focus on datasets, benchmarking, and standardization. We also
present insights and ideas put forward by the quantum dot community on how to
overcome them. We aim to provide guidance and inspiration to researchers
invested in automation efforts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A meeting report from a workshop held at the National Institute of
  Standards and Technology, Gaithersburg, MD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compute-Update Federated Learning: A Lattice Coding Approach
  Over-the-Air 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Mohammad Azimi-Abarghouyi, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a federated learning framework that enables
over-the-air computation via digital communications, using a new joint
source-channel coding scheme. Without relying on channel state information at
devices, this scheme employs lattice codes to both quantize model parameters
and exploit interference from the devices. We propose a novel receiver
structure at the server, designed to reliably decode an integer combination of
the quantized model parameters as a lattice point for the purpose of
aggregation. We present a mathematical approach to derive a convergence bound
for the proposed scheme and offer design remarks. In this context, we suggest
an aggregation metric and a corresponding algorithm to determine effective
integer coefficients for the aggregation in each communication round. Our
results illustrate that, regardless of channel dynamics and data heterogeneity,
our scheme consistently delivers superior learning accuracy across various
parameters and markedly surpasses other over-the-air methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the preprint available at arXiv:2403.01023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient Methods with Online Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhi Gao, Ya-Chi Chu, Yinyu Ye, Madeleine Udell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a framework to accelerate the convergence of gradient-based
methods with online learning. The framework learns to scale the gradient at
each iteration through an online learning algorithm and provably accelerates
gradient-based methods asymptotically. In contrast with previous literature,
where convergence is established based on worst-case analysis, our framework
provides a strong convergence guarantee with respect to the optimal scaling
matrix for the iteration trajectory. For smooth strongly convex optimization,
our results provide an $O(\kappa^\star \log(1/\varepsilon)$) complexity result,
where $\kappa^\star$ is the condition number achievable by the optimal
preconditioner, improving on the previous $O(\sqrt{n}\kappa^\star
\log(1/\varepsilon))$ result. In particular, a variant of our method achieves
superlinear convergence on convex quadratics. For smooth convex optimization,
we show for the first time that the widely-used hypergradient descent heuristic
improves on the convergence of gradient descent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tree-Averaging Algorithms for Ensemble-Based Unsupervised Discontinuous
  Constituency Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behzad Shayegh, Yuqiao Wen, Lili Mou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address unsupervised discontinuous constituency parsing, where we observe
a high variance in the performance of the only previous model in the
literature. We propose to build an ensemble of different runs of the existing
discontinuous parser by averaging the predicted trees, to stabilize and boost
performance. To begin with, we provide comprehensive computational complexity
analysis (in terms of P and NP-complete) for tree averaging under different
setups of binarity and continuity. We then develop an efficient exact algorithm
to tackle the task, which runs in a reasonable time for all samples in our
experiments. Results on three datasets show our method outperforms all
baselines in all metrics; we also provide in-depth analyses of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enriching Tabular Data with Contextual LLM Embeddings: A Comprehensive
  Ablation Study for Ensemble Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gjergji Kasneci, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature engineering is crucial for optimizing machine learning model
performance, particularly in tabular data classification tasks. Leveraging
advancements in natural language processing, this study presents a systematic
approach to enrich tabular datasets with features derived from large language
model embeddings. Through a comprehensive ablation study on diverse datasets,
we assess the impact of RoBERTa and GPT-2 embeddings on ensemble classifiers,
including Random Forest, XGBoost, and CatBoost. Results indicate that
integrating embeddings with traditional numerical and categorical features
often enhances predictive performance, especially on datasets with class
imbalance or limited features and samples, such as UCI Adult, Heart Disease,
Titanic, and Pima Indian Diabetes, with improvements particularly notable in
XGBoost and CatBoost classifiers. Additionally, feature importance analysis
reveals that LLM-derived features frequently rank among the most impactful for
the predictions. This study provides a structured approach to embedding-based
feature enrichment and illustrates its benefits in ensemble learning for
tabular data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Lightweight <span class="highlight-title">Transformer</span> via Unrolling of Learned Graph
  Smoothness Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tam Thuc Do, Parham Eftekhar, Seyed Alireza Hosseini, Gene Cheung, Philip Chou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We build interpretable and lightweight transformer-like neural networks by
unrolling iterative optimization algorithms that minimize graph smoothness
priors -- the quadratic graph Laplacian regularizer (GLR) and the $\ell_1$-norm
graph total variation (GTV) -- subject to an interpolation constraint. The
crucial insight is that a normalized signal-dependent graph learning module
amounts to a variant of the basic self-attention mechanism in conventional
transformers. Unlike "black-box" transformers that require learning of large
key, query and value matrices to compute scaled dot products as affinities and
subsequent output embeddings, resulting in huge parameter sets, our unrolled
networks employ shallow CNNs to learn low-dimensional features per node to
establish pairwise Mahalanobis distances and construct sparse similarity
graphs. At each layer, given a learned graph, the target interpolated signal is
simply a low-pass filtered output derived from the minimization of an assumed
graph smoothness prior, leading to a dramatic reduction in parameter count.
Experiments for two image interpolation applications verify the restoration
performance, parameter efficiency and robustness to covariate shift of our
graph-based unrolled networks compared to conventional transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GD doesn't make the cut: Three ways that non-differentiability affects
  neural network training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08426v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08426v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Krishna Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the distinctions between gradient methods applied to
non-differentiable functions (NGDMs) and classical gradient descents (GDs)
designed for differentiable functions. First, we demonstrate significant
differences in the convergence properties of NGDMs compared to GDs, challenging
the applicability of the extensive neural network convergence literature based
on $L-smoothness$ to non-smooth neural networks. Next, we demonstrate the
paradoxical nature of NGDM solutions for $L_{1}$-regularized problems, showing
that increasing the regularization penalty leads to an increase in the $L_{1}$
norm of optimal solutions in NGDMs. Consequently, we show that widely adopted
$L_{1}$ penalization-based techniques for network pruning do not yield expected
results. Additionally, we dispel the common belief that optimization algorithms
like Adam and RMSProp perform similarly in non-differentiable contexts.
Finally, we explore the Edge of Stability phenomenon, indicating its
inapplicability even to Lipschitz continuous convex differentiable functions,
leaving its relevance to non-convex non-differentiable neural networks
inconclusive. Our analysis exposes misguided interpretations of NGDMs in widely
referenced papers and texts due to an overreliance on strong smoothness
assumptions, emphasizing the necessity for a nuanced understanding of
foundational assumptions in the analysis of these systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explain Like I'm Five: Using LLMs to Improve PDE Surrogate Models with
  Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01137v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01137v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cooper Lorsung, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving Partial Differential Equations (PDEs) is ubiquitous in science and
engineering. Computational complexity and difficulty in writing numerical
solvers has motivated the development of machine learning techniques to
generate solutions quickly. Many existing methods are purely data driven,
relying solely on numerical solution fields, rather than known system
information such as boundary conditions and governing equations. However, the
recent rise in popularity of Large Language Models (LLMs) has enabled easy
integration of text in multimodal machine learning models. In this work, we use
pretrained LLMs to integrate various amounts known system information into PDE
learning. Our multimodal approach significantly outperforms our baseline model,
FactFormer, in both next-step prediction and autoregressive rollout performance
on the 2D Heat, Burgers, Navier-Stokes, and Shallow Water equations. Further
analysis shows that pretrained LLMs provide highly structured latent space that
is consistent with the amount of system information provided through text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 15 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anatomical Foundation Models for Brain MRIs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Alberto Barbano, Matteo Brunello, Benoit Dufumier, Marco Grangetto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) in neuroimaging has become increasingly relevant for
detecting neurological conditions and neurodegenerative disorders. One of the
most predominant biomarkers in neuroimaging is represented by brain age, which
has been shown to be a good indicator for different conditions, such as
Alzheimer's Disease. Using brain age for pretraining DL models in transfer
learning settings has also recently shown promising results, especially when
dealing with data scarcity of different conditions. On the other hand,
anatomical information of brain MRIs (e.g. cortical thickness) can provide
important information for learning good representations that can be transferred
to many downstream tasks. In this work, we propose AnatCL, an anatomical
foundation model for brain MRIs that i.) leverages anatomical information with
a weakly contrastive learning approach and ii.) achieves state-of-the-art
performances in many different downstream tasks. To validate our approach we
consider 12 different downstream tasks for diagnosis classification, and
prediction of 10 different clinical assessment scores. Pretrained models can be
found at https://github.com/EIDOSLAB/AnatCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages; added source url</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonparametric Evaluation of Noisy ICA Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.08468v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.08468v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syamantak Kumar, Purnamrita Sarkar, Peter Bickel, Derek Bean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Independent Component Analysis (ICA) was introduced in the 1980's as a model
for Blind Source Separation (BSS), which refers to the process of recovering
the sources underlying a mixture of signals, with little knowledge about the
source signals or the mixing process. While there are many sophisticated
algorithms for estimation, different methods have different shortcomings. In
this paper, we develop a nonparametric score to adaptively pick the right
algorithm for ICA with arbitrary Gaussian noise. The novelty of this score
stems from the fact that it just assumes a finite second moment of the data and
uses the characteristic function to evaluate the quality of the estimated
mixing matrix without any knowledge of the parameters of the noise
distribution. In addition, we propose some new contrast functions and
algorithms that enjoy the same fast computability as existing algorithms like
FASTICA and JADE but work in domains where the former may fail. While these
also may have weaknesses, our proposed diagnostic, as shown by our simulations,
can remedy them. Finally, we propose a theoretical framework to analyze the
local and global convergence properties of our algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vulnerable Road User Detection and Safety Enhancement: A Comprehensive
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19202v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19202v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renato M. Silva, Gregório F. Azevedo, Matheus V. V. Berto, Jean R. Rocha, Eduardo C. Fidelis, Matheus V. Nogueira, Pedro H. Lisboa, Tiago A. Almeida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic incidents involving vulnerable road users (VRUs) constitute a
significant proportion of global road accidents. Advances in traffic
communication ecosystems, coupled with sophisticated signal processing and
machine learning techniques, have facilitated the utilization of data from
diverse sensors. Despite these advancements and the availability of extensive
datasets, substantial progress is required to mitigate traffic casualties. This
paper provides a comprehensive survey of state-of-the-art technologies and
methodologies to enhance the safety of VRUs. The study delves into the
communication networks between vehicles and VRUs, emphasizing the integration
of advanced sensors and the availability of relevant datasets. It explores
preprocessing techniques and data fusion methods to enhance sensor data
quality. Furthermore, our study assesses critical simulation environments
essential for developing and testing VRU safety systems. Our research also
highlights recent advances in VRU detection and classification algorithms,
addressing challenges such as variable environmental conditions. Additionally,
we cover cutting-edge research in predicting VRU intentions and behaviors,
which is crucial for proactive collision avoidance strategies. Through this
survey, we aim to provide a comprehensive understanding of the current
landscape of VRU safety technologies, identifying areas of progress and areas
needing further research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages, 18 tables, 8 figures, citing 339 (up-to-date) papers,
  preprint submitted to Expert Systems with Applications (Elsevier)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Oja's Algorithm for Streaming Sparse PCA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07240v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07240v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syamantak Kumar, Purnamrita Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Oja's algorithm for Streaming Principal Component Analysis (PCA) for $n$
data-points in a $d$ dimensional space achieves the same sin-squared error
$O(r_{\mathsf{eff}}/n)$ as the offline algorithm in $O(d)$ space and $O(nd)$
time and a single pass through the datapoints. Here $r_{\mathsf{eff}}$ is the
effective rank (ratio of the trace and the principal eigenvalue of the
population covariance matrix $\Sigma$). Under this computational budget, we
consider the problem of sparse PCA, where the principal eigenvector of $\Sigma$
is $s$-sparse, and $r_{\mathsf{eff}}$ can be large. In this setting, to our
knowledge, \textit{there are no known single-pass algorithms} that achieve the
minimax error bound in $O(d)$ space and $O(nd)$ time without either requiring
strong initialization conditions or assuming further structure (e.g., spiked)
of the covariance matrix. We show that a simple single-pass procedure that
thresholds the output of Oja's algorithm (the Oja vector) can achieve the
minimax error bound under some regularity conditions in $O(d)$ space and
$O(nd)$ time. We present a nontrivial and novel analysis of the entries of the
unnormalized Oja vector, which involves the projection of a product of
independent random matrices on a random initial vector. This is completely
different from previous analyses of Oja's algorithm and matrix products, which
have been done when the $r_{\mathsf{eff}}$ is bounded.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parameter uncertainties for imperfect surrogate models in the low-noise
  regime 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01810v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01810v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas D Swinburne, Danny Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian regression determines model parameters by minimizing the expected
loss, an upper bound to the true generalization error. However, the loss
ignores misspecification, where models are imperfect. Parameter uncertainties
from Bayesian regression are thus significantly underestimated and vanish in
the large data limit. This is particularly problematic when building models of
low-noise, or near-deterministic, calculations, as the main source of
uncertainty is neglected. We analyze the generalization error of misspecified,
near-deterministic surrogate models, a regime of broad relevance in science and
engineering. We show posterior distributions must cover every training point to
avoid a divergent generalization error and design an ansatz that respects this
constraint, which for linear models incurs minimal overhead. This is
demonstrated on model problems before application to thousand dimensional
datasets in atomistic machine learning. Our efficient misspecification-aware
scheme gives accurate prediction and bounding of test errors where existing
schemes fail, allowing this important source of uncertainty to be incorporated
in computational workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning Based Amharic Chatbot for FAQs in Universities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01720v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01720v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Goitom Ybrah Hailu, Hadush Hailu, Shishay Welay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  University students often spend a considerable amount of time seeking answers
to common questions from administrators or teachers. This can become tedious
for both parties, leading to a need for a solution. In response, this paper
proposes a chatbot model that utilizes natural language processing and deep
learning techniques to answer frequently asked questions (FAQs) in the Amharic
language. Chatbots are computer programs that simulate human conversation
through the use of artificial intelligence (AI), acting as a virtual assistant
to handle questions and other tasks. The proposed chatbot program employs
tokenization, normalization, stop word removal, and stemming to analyze and
categorize Amharic input sentences. Three machine learning model algorithms
were used to classify tokens and retrieve appropriate responses: Support Vector
Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented
through TensorFlow, Keras, and NLTK. The deep learning model achieved the best
results with 91.55% accuracy and a validation loss of 0.3548 using an Adam
optimizer and SoftMax activation function. The chatbot model was integrated
with Facebook Messenger and deployed on a Heroku server for 24-hour
accessibility. The experimental results demonstrate that the chatbot framework
achieved its objectives and effectively addressed challenges such as Amharic
Fidel variation, morphological variation, and lexical gaps. Future research
could explore the integration of Amharic WordNet to narrow the lexical gap and
support more complex questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DAAL: Density-Aware Adaptive Line Margin Loss for Multi-Modal Deep
  Metric Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadush Hailu Gebrerufael, Anil Kumar Tiwari, Gaurav Neupane, Goitom Ybrah Hailu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal deep metric learning is crucial for effectively capturing diverse
representations in tasks such as face verification, fine-grained object
recognition, and product search. Traditional approaches to metric learning,
whether based on distance or margin metrics, primarily emphasize class
separation, often overlooking the intra-class distribution essential for
multi-modal feature learning. In this context, we propose a novel loss function
called Density-Aware Adaptive Margin Loss(DAAL), which preserves the density
distribution of embeddings while encouraging the formation of adaptive
sub-clusters within each class. By employing an adaptive line strategy, DAAL
not only enhances intra-class variance but also ensures robust inter-class
separation, facilitating effective multi-modal representation. Comprehensive
experiments on benchmark fine-grained datasets demonstrate the superior
performance of DAAL, underscoring its potential in advancing retrieval
applications and multi-modal deep metric learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 fugues, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Language Models via Token Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhili Feng, Tanya Marwah, Nicolo Fusi, David Alvarez-Melis, Lester Mackey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern large language models use a fixed tokenizer to effectively compress
text drawn from a source domain. However, applying the same tokenizer to a new
target domain often leads to inferior compression, more costly inference, and
reduced semantic alignment. To address this deficiency, we introduce Sparse
Sinkhorn Token Translation (S2T2). S2T2 trains a tailored tokenizer for the
target domain and learns to translate between target and source tokens,
enabling more effective reuse of the pre-trained next-source-token predictor.
In our experiments with finetuned English language models, S2T2 improves both
the perplexity and the compression of out-of-domain protein sequences,
outperforming direct finetuning with either the source or target tokenizer. In
addition, we find that token translations learned for smaller, less expensive
models can be directly transferred to larger, more powerful models to reap the
benefits of S2T2 at lower cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span>s as Transducers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02040v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02040v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lena Strobl, Dana Angluin, David Chiang, Jonathan Rawski, Ashish Sabharwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the sequence-to-sequence mapping capacity of transformers by
relating them to finite transducers, and find that they can express
surprisingly large classes of transductions. We do so using variants of RASP, a
programming language designed to help people "think like transformers," as an
intermediate representation. We extend the existing Boolean variant B-RASP to
sequence-to-sequence functions and show that it computes exactly the
first-order rational functions (such as string rotation). Then, we introduce
two new extensions. B-RASP[pos] enables calculations on positions (such as
copying the first half of a string) and contains all first-order regular
functions. S-RASP adds prefix sum, which enables additional arithmetic
operations (such as squaring a string) and contains all first-order polyregular
functions. Finally, we show that masked average-hard attention transformers can
simulate S-RASP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Transactions of the Association for Computational
  Linguistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Initialisation and Network Effects in Decentralised Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15855v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15855v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arash Badie-Modiri, Chiara Boldrini, Lorenzo Valerio, János Kertész, Márton Karsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fully decentralised federated learning enables collaborative training of
individual machine learning models on a distributed network of communicating
devices while keeping the training data localised on each node. This approach
avoids central coordination, enhances data privacy and eliminates the risk of a
single point of failure. Our research highlights that the effectiveness of
decentralised federated learning is significantly influenced by the network
topology of connected devices and the learning models' initial conditions. We
propose a strategy for uncoordinated initialisation of the artificial neural
networks based on the distribution of eigenvector centralities of the
underlying communication network, leading to a radically improved training
efficiency. Additionally, our study explores the scaling behaviour and the
choice of environmental parameters under our proposed initialisation strategy.
This work paves the way for more efficient and scalable artificial neural
network training in a distributed and uncoordinated environment, offering a
deeper understanding of the intertwining roles of network structure and
learning dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Statistical Properties of Deep Neural Networks with Dependent Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chad Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper establishes statistical properties of deep neural network (DNN)
estimators under dependent data. Two general results for nonparametric sieve
estimators directly applicable to DNN estimators are given. The first
establishes rates for convergence in probability under nonstationary data. The
second provides non-asymptotic probability bounds on $\mathcal{L}^{2}$-errors
under stationary $\beta$-mixing data. I apply these results to DNN estimators
in both regression and classification contexts imposing only a standard
H\"older smoothness assumption. The DNN architectures considered are common in
applications, featuring fully connected feedforward networks with any
continuous piecewise linear activation function, unbounded weights, and a width
and depth that grows with sample size. The framework provided also offers
potential for research into other DNN architectures and time-series
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>85 pages, 2 figures, removed partially linear model section and
  uploaded as a separate paper (arXiv:2410.22574v1)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Scalable Reference-Free Evaluation of Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azim Ospanov, Jingwei Zhang, Mohammad Jalali, Xuenan Cao, Andrej Bogdanov, Farzan Farnia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While standard evaluation scores for generative models are mostly
reference-based, a reference-dependent assessment of generative models could be
generally difficult due to the unavailability of applicable reference datasets.
Recently, the reference-free entropy scores, VENDI and RKE, have been proposed
to evaluate the diversity of generated data. However, estimating these scores
from data leads to significant computational costs for large-scale generative
models. In this work, we leverage the random Fourier features framework to
reduce the computational price and propose the Fourier-based Kernel Entropy
Approximation (FKEA) method. We utilize FKEA's approximated eigenspectrum of
the kernel matrix to efficiently estimate the mentioned entropy scores.
Furthermore, we show the application of FKEA's proxy eigenvectors to reveal the
method's identified modes in evaluating the diversity of produced samples. We
provide a stochastic implementation of the FKEA assessment algorithm with a
complexity $O(n)$ linearly growing with sample size $n$. We extensively
evaluate FKEA's numerical performance in application to standard image, text,
and video datasets. Our empirical results indicate the method's scalability and
interpretability applied to large-scale generative models. The codebase is
available at https://github.com/aziksh-ospanov/FKEA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Introducing Perturb-ability Score (PS) to Enhance Robustness Against
  Evasion Adversarial Attacks on ML-NIDS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed elShehaby, Ashraf Matrawy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As network security threats continue to evolve, safeguarding Machine Learning
(ML)-based Network Intrusion Detection Systems (NIDS) from adversarial attacks
is crucial. This paper introduces the notion of feature perturb-ability and
presents a novel Perturb-ability Score (PS) metric that identifies NIDS
features susceptible to manipulation in the problem-space by an attacker. By
quantifying a feature's susceptibility to perturbations within the
problem-space, the PS facilitates the selection of features that are inherently
more robust against evasion adversarial attacks on ML-NIDS during the feature
selection phase. These features exhibit natural resilience to perturbations, as
they are heavily constrained by the problem-space limitations and correlations
of the NIDS domain. Furthermore, manipulating these features may either disrupt
the malicious function of evasion adversarial attacks on NIDS or render the
network traffic invalid for processing (or both). This proposed novel approach
employs a fresh angle by leveraging network domain constraints as a defense
mechanism against problem-space evasion adversarial attacks targeting ML-NIDS.
We demonstrate the effectiveness of our PS-guided feature selection defense in
enhancing NIDS robustness. Experimental results across various ML-based NIDS
models and public datasets show that selecting only robust features (low-PS
features) can maintain solid detection performance while significantly reducing
vulnerability to evasion adversarial attacks. Additionally, our findings verify
that the PS effectively identifies NIDS features highly vulnerable to
problem-space perturbations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of Active Feature Acquisition Methods for Time-varying
  Feature Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01530v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01530v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henrik von Kleist, Alireza Zamanian, Ilya Shpitser, Narges Ahmidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning methods often assume that input features are available at no
cost. However, in domains like healthcare, where acquiring features could be
expensive or harmful, it is necessary to balance a feature's acquisition cost
against its predictive value. The task of training an AI agent to decide which
features to acquire is called active feature acquisition (AFA). By deploying an
AFA agent, we effectively alter the acquisition strategy and trigger a
distribution shift. To safely deploy AFA agents under this distribution shift,
we present the problem of active feature acquisition performance evaluation
(AFAPE). We examine AFAPE under i) a no direct effect (NDE) assumption, stating
that acquisitions do not affect the underlying feature values; and ii) a no
unobserved confounding (NUC) assumption, stating that retrospective feature
acquisition decisions were only based on observed features. We show that one
can apply missing data methods under the NDE assumption and offline
reinforcement learning under the NUC assumption. When NUC and NDE hold, we
propose a novel semi-offline reinforcement learning framework. This framework
requires a weaker positivity assumption and introduces three new estimators: A
direct method (DM), an inverse probability weighting (IPW), and a double
reinforcement learning (DRL) estimator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>61 pages, 4 tables, 11 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallelizing Linear <span class="highlight-title">Transformer</span>s with the Delta Rule over Sequence
  Length <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06484v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06484v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers with linear attention (i.e., linear transformers) and
state-space models have recently been suggested as a viable linear-time
alternative to transformers with softmax attention. However, these models still
underperform transformers especially on tasks that require in-context
retrieval. While more expressive variants of linear transformers which replace
the additive update in linear transformers with the delta rule (DeltaNet) have
been found to be more effective at associative recall, existing algorithms for
training such models do not parallelize over sequence length and are thus
inefficient to train on modern hardware. This work describes a
hardware-efficient algorithm for training linear transformers with the delta
rule, which exploits a memory-efficient representation for computing products
of Householder matrices. This algorithm allows us to scale up DeltaNet to
standard language modeling settings. We train a 1.3B model for 100B tokens and
find that it outperforms recent linear-time baselines such as Mamba and GLA in
terms of perplexity and zero-shot performance on downstream tasks. We also
experiment with two hybrid models which combine DeltaNet layers with (1)
sliding-window attention layers every other layer or (2) two global attention
layers, and find that these hybrids outperform strong transformer baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Your AIs Deceive You: Challenges of Partial Observability in
  Reinforcement Learning from Human Feedback <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17747v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17747v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Lang, Davis Foote, Stuart Russell, Anca Dragan, Erik Jenner, Scott Emmons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Past analyses of reinforcement learning from human feedback (RLHF) assume
that the human evaluators fully observe the environment. What happens when
human feedback is based only on partial observations? We formally define two
failure cases: deceptive inflation and overjustification. Modeling the human as
Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under
which RLHF is guaranteed to result in policies that deceptively inflate their
performance, overjustify their behavior to make an impression, or both. Under
the new assumption that the human's partial observability is known and
accounted for, we then analyze how much information the feedback process
provides about the return function. We show that sometimes, the human's
feedback determines the return function uniquely up to an additive constant,
but in other realistic cases, there is irreducible ambiguity. We propose
exploratory research directions to help tackle these challenges, experimentally
validate both the theoretical concerns and potential mitigations, and caution
against blindly applying RLHF in partially observable settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Advances in Neural Information Processing Systems 37 (NeurIPS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GemNet: Menu-Based, Strategy-Proof Multi-Bidder Auctions Through Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07428v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07428v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tonghan Wang, Yanchen Jiang, David C. Parkes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated mechanism design (AMD) uses computational methods for mechanism
design. Differentiable economics is a form of AMD that uses deep learning to
learn mechanism designs and has enabled strong progress in AMD in recent years.
Nevertheless, a major open problem has been to learn multi-bidder, general, and
fully strategy-proof (SP) auctions. We introduce GEneral Menu-based NETwork
(GemNet), which significantly extends the menu-based approach of the
single-bidder RochetNet (D\"utting et al., 2024) to the multi-bidder setting.
The challenge in achieving SP is to learn bidder-independent menus that are
feasible, so that the optimal menu choices for each bidder do not over-allocate
items when taken together (we call this menu compatibility). GemNet penalizes
the failure of menu compatibility during training, and transforms learned menus
after training through price changes, by considering a set of discretized
bidder values and reasoning about Lipschitz smoothness to guarantee menu
compatibility on the entire value space. This approach is general, leaving
trained menus that already satisfy menu compatibility undisturbed and reducing
to RochetNet for a single bidder. Mixed-integer linear programs are used for
menu transforms, and through a number of optimizations enabled by deep
learning, including adaptive grids and methods to skip menu elements, we scale
to large auction design problems. GemNet learns auctions with better revenue
than affine maximization methods, achieves exact SP whereas previous general
multi-bidder methods are approximately SP, and offers greatly enhanced
interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper received the Exemplary Paper Award for the AI track at the
  Twenty-Fifth ACM Conference on Economics and Computation (ACM EC '24), where
  it appeared as an extended abstract; The first two authors contributed
  equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Matroid Optimization through Fast Imprecise Oracles <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franziska Eberle, Felix Hommelsheim, Alexander Lindermayr, Zhenwei Liu, Nicole Megow, Jens Schlöter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Querying complex models for precise information (e.g. traffic models,
database systems, large ML models) often entails intense computations and
results in long response times. Thus, weaker models which give imprecise
results quickly can be advantageous, provided inaccuracies can be resolved
using few queries to a stronger model. In the fundamental problem of computing
a maximum-weight basis of a matroid, a well-known generalization of many
combinatorial optimization problems, algorithms have access to a clean oracle
to query matroid information. We additionally equip algorithms with a fast but
dirty oracle modelling an unknown, potentially different matroid. We design and
analyze practical algorithms which only use few clean queries w.r.t. the
quality of the dirty oracle, while maintaining robustness against arbitrarily
poor dirty matroids, approaching the performance of classic algorithms for the
given problem. Notably, we prove that our algorithms are, in many respects,
best-possible. Further, we outline extensions to other matroid oracle types,
non-free dirty oracles and other matroid problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Poseidon: Efficient Foundation Models for PDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Herde, Bogdan Raonić, Tobias Rohner, Roger Käppeli, Roberto Molinaro, Emmanuel de Bézenac, Siddhartha Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Poseidon, a foundation model for learning the solution operators
of PDEs. It is based on a multiscale operator transformer, with
time-conditioned layer norms that enable continuous-in-time evaluations. A
novel training strategy leveraging the semi-group property of time-dependent
PDEs to allow for significant scaling-up of the training data is also proposed.
Poseidon is pretrained on a diverse, large scale dataset for the governing
equations of fluid dynamics. It is then evaluated on a suite of 15 challenging
downstream tasks that include a wide variety of PDE types and operators. We
show that Poseidon exhibits excellent performance across the board by
outperforming baselines significantly, both in terms of sample efficiency and
accuracy. Poseidon also generalizes very well to new physics that is not seen
during pretraining. Moreover, Poseidon scales with respect to model and data
size, both for pretraining and for downstream tasks. Taken together, our
results showcase the surprising ability of Poseidon to learn effective
representations from a very small set of PDEs during pretraining in order to
generalize well to unseen and unrelated PDEs downstream, demonstrating its
potential as an effective, general purpose PDE foundation model. Finally, the
Poseidon model as well as underlying pretraining and downstream datasets are
open sourced, with code being available at
https://github.com/camlab-ethz/poseidon and pretrained models and datasets at
https://huggingface.co/camlab-ethz.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Knowledge Pursuit for Faithful Visual Synthesis <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17898v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17898v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqi Luo, Kwan Ho Ryan Chan, Dimitris Dimos, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern text-to-vision generative models often hallucinate when the prompt
describing the scene to be generated is underspecified. In large language
models (LLMs), a prevalent strategy to reduce hallucinations is to retrieve
factual knowledge from an external database. While such retrieval augmentation
strategies have great potential to enhance text-to-vision generators, existing
static top-K retrieval methods explore the knowledge pool once, missing the
broader context necessary for high-quality generation. Furthermore, LLMs
internally possess rich world knowledge learned during large-scale training
(parametric knowledge) that could mitigate the need for external data
retrieval. This paper proposes Contextual Knowledge Pursuit (CKPT), a framework
that leverages the complementary strengths of external and parametric knowledge
to help generators produce reliable visual content. Instead of the one-time
retrieval of facts from an external database to improve a given prompt, CKPT
uses (1) an LLM to decide whether to seek external knowledge or to self-elicit
descriptions from LLM parametric knowledge, (2) a knowledge pursuit process to
contextually seek and sequentially gather most relevant facts, (3) a knowledge
aggregator for prompt enhancement with the gathered fact context, and (4) a
filtered fine-tuning objective to improve visual synthesis with richer prompts.
We evaluate CKPT across multiple text-driven generative tasks (image, 3D
rendering, and video) on datasets of rare objects and daily scenarios. Our
results show that CKPT is capable of generating faithful and semantically rich
content across diverse visual domains, offering a promising data source for
zero-shot synthesis and filtered fine-tuning of text-to-vision generative
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ECCV 2024 SDCV Workshop. GitHub repository at
  https://github.com/peterljq/Contextual-Knowledge-Pursuit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Piecewise deterministic generative models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19448v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19448v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Bertazzi, Dario Shariatian, Umut Simsekli, Eric Moulines, Alain Durmus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel class of generative models based on piecewise
deterministic Markov processes (PDMPs), a family of non-diffusive stochastic
processes consisting of deterministic motion and random jumps at random times.
Similarly to diffusions, such Markov processes admit time reversals that turn
out to be PDMPs as well. We apply this observation to three PDMPs considered in
the literature: the Zig-Zag process, Bouncy Particle Sampler, and Randomised
Hamiltonian Monte Carlo. For these three particular instances, we show that the
jump rates and kernels of the corresponding time reversals admit explicit
expressions depending on some conditional densities of the PDMP under
consideration before and after a jump. Based on these results, we propose
efficient training procedures to learn these characteristics and consider
methods to approximately simulate the reverse process. Finally, we provide
bounds in the total variation distance between the data distribution and the
resulting distribution of our model in the case where the base distribution is
the standard $d$-dimensional Gaussian distribution. Promising numerical
simulations support further investigations into this class of models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Analytic Exemplar-Free Continual Learning with Large Models for
  Imbalanced Autonomous Driving Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17779v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17779v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiping Zhuang, Di Fang, Kai Tong, Yuchen Liu, Ziqian Zeng, Xu Zhou, Cen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, even a meticulously trained model can encounter
failures when facing unfamiliar scenarios. One of these scenarios can be
formulated as an online continual learning (OCL) problem. That is, data come in
an online fashion, and models are updated according to these streaming data.
Two major OCL challenges are catastrophic forgetting and data imbalance. To
address these challenges, in this paper, we propose an Analytic Exemplar-Free
Online Continual Learning algorithm (AEF-OCL). The AEF-OCL leverages analytic
continual learning principles and employs ridge regression as a classifier for
features extracted by a large backbone network. It solves the OCL problem by
recursively calculating the analytical solution, ensuring an equalization
between the continual learning and its joint-learning counterpart, and works
without the need to save any used samples (i.e., exemplar-free). Additionally,
we introduce a Pseudo-Features Generator (PFG) module that recursively
estimates the mean and the variance of real features for each class. It
over-samples offset pseudo-features from the same normal distribution as the
real features, thereby addressing the data imbalance issue. Experimental
results demonstrate that despite being an exemplar-free strategy, our method
outperforms various methods on the autonomous driving SODA10M dataset. Source
code is available at https://github.com/ZHUANGHP/Analytic-continual-learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is to be published in IEEE Transactions on Vehicular
  Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PaCE: Parsimonious Concept Engineering for Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are being used for a wide variety of tasks.
While they are capable of generating human-like responses, they can also
produce undesirable output including potentially harmful information, racist or
sexist language, and hallucinations. Alignment methods are designed to reduce
such undesirable outputs via techniques such as fine-tuning, prompt
engineering, and representation engineering. However, existing methods face
several challenges: some require costly fine-tuning for every alignment task;
some do not adequately remove undesirable concepts, failing alignment; some
remove benign concepts, lowering the linguistic capabilities of LLMs. To
address these issues, we propose Parsimonious Concept Engineering (PaCE), a
novel activation engineering framework for alignment. First, to sufficiently
model the concepts, we construct a large-scale concept dictionary in the
activation space, in which each atom corresponds to a semantic concept. Given
any alignment task, we instruct a concept partitioner to efficiently annotate
the concepts as benign or undesirable. Then, at inference time, we decompose
the LLM activations along the concept dictionary via sparse coding, to
accurately represent the activations as linear combinations of benign and
undesirable components. By removing the latter ones from the activations, we
reorient the behavior of the LLM towards the alignment goal. We conduct
experiments on tasks such as response detoxification, faithfulness enhancement,
and sentiment revising, and show that PaCE achieves state-of-the-art alignment
performance while maintaining linguistic capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2024. GitHub repository at
  https://github.com/peterljq/Parsimonious-Concept-Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention-based Class-Conditioned Alignment for Multi-Source Domain
  Adaptation of Object Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09918v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09918v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atif Belal, Akhil Meethal, Francisco Perdigon Romero, Marco Pedersoli, Eric Granger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptation methods for object detection (OD) strive to mitigate the
impact of distribution shifts by promoting feature alignment across source and
target domains. Multi-source domain adaptation (MSDA) allows leveraging
multiple annotated source datasets and unlabeled target data to improve the
accuracy and robustness of the detection model. Most state-of-the-art MSDA
methods for OD perform feature alignment in a class-agnostic manner. This is
challenging since the objects have unique modal information due to variations
in object appearance across domains. A recent prototype-based approach proposed
a class-wise alignment, yet it suffers from error accumulation due to noisy
pseudo-labels that can negatively affect adaptation with imbalanced data. To
overcome these limitations, we propose an attention-based class-conditioned
alignment method for MSDA that aligns instances of each object category across
domains. In particular, an attention module coupled with an adversarial domain
classifier allows learning domain-invariant and class-specific instance
representations. Experimental results on multiple benchmarking MSDA datasets
indicate that our method outperforms the state-of-the-art methods and is robust
to class imbalance using a conceptually simple class-conditioning method. Our
code is available at https://github.com/imatif17/ACIA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2309.14950</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FUSE: Fast Unified Simulation and Estimation for PDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Levi E. Lingsch, Dana Grund, Siddhartha Mishra, Georgios Kissas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The joint prediction of continuous fields and statistical estimation of the
underlying discrete parameters is a common problem for many physical systems,
governed by PDEs. Hitherto, it has been separately addressed by employing
operator learning surrogates for field prediction while using simulation-based
inference (and its variants) for statistical parameter determination. Here, we
argue that solving both problems within the same framework can lead to
consistent gains in accuracy and robustness. To this end, We propose a novel
and flexible formulation of the operator learning problem that allows jointly
predicting continuous quantities and inferring distributions of discrete
parameters, and thus amortizing the cost of both the inverse and the surrogate
models to a joint pre-training step. We present the capabilities of the
proposed methodology for predicting continuous and discrete biomarkers in
full-body haemodynamics simulations under different levels of missing
information. We also consider a test case for atmospheric large-eddy simulation
of a two-dimensional dry cold bubble, where we infer both continuous
time-series and information about the systems conditions. We present
comparisons against different baselines to showcase significantly increased
accuracy in both the inverse and the surrogate tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-Driven approach for sustainable extraction of earth's subsurface
  renewable energy while minimizing seismic activity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Gutierrez-Oribio, Alexandros Stathas, Ioannis Stefanou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Geothermal Energy, Carbon Capture and Storage, and Hydrogen Storage hold
considerable promise for meeting the energy sector's large-scale requirements
and reducing CO$_2$ emissions. However, the injection of fluids into the
Earth's crust, essential for these activities, can induce or trigger
earthquakes. In this paper, we highlight a new approach based on Reinforcement
Learning for the control of human-induced seismicity in the highly complex
environment of an underground reservoir. This complex system poses significant
challenges in the control design due to parameter uncertainties and unmodeled
dynamics. We show that the reinforcement learning algorithm can interact
efficiently with a robust controller, by choosing the controller parameters in
real-time, reducing human-induced seismicity and allowing the consideration of
further production objectives, \textit{e.g.}, minimal control power.
Simulations are presented for a simplified underground reservoir under various
energy demand scenarios, demonstrating the reliability and effectiveness of the
proposed control-reinforcement learning approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Smoothness Regularisers for Neural Link Predictors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09045v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09045v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Dileo, Pasquale Minervini, Matteo Zignani, Sabrina Gaito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most algorithms for representation learning and link prediction on relational
data are designed for static data. However, the data to which they are applied
typically evolves over time, including online social networks or interactions
between users and items in recommender systems. This is also the case for
graph-structured knowledge bases -- knowledge graphs -- which contain facts
that are valid only for specific points in time. In such contexts, it becomes
crucial to correctly identify missing links at a precise time point, i.e. the
temporal prediction link task. Recently, Lacroix et al. and Sadeghian et al.
proposed a solution to the problem of link prediction for knowledge graphs
under temporal constraints inspired by the canonical decomposition of 4-order
tensors, where they regularise the representations of time steps by enforcing
temporal smoothing, i.e. by learning similar transformation for adjacent
timestamps. However, the impact of the choice of temporal regularisation terms
is still poorly understood. In this work, we systematically analyse several
choices of temporal smoothing regularisers using linear functions and recurrent
architectures. In our experiments, we show that by carefully selecting the
temporal smoothing regulariser and regularisation weight, a simple method like
TNTComplEx can produce significantly more accurate results than
state-of-the-art methods on three widely used temporal link prediction
datasets. Furthermore, we evaluate the impact of a wide range of temporal
smoothing regularisers on two state-of-the-art temporal link prediction models.
Our work shows that simple tensor factorisation models can produce new
state-of-the-art results using newly proposed temporal regularisers,
highlighting a promising avenue for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dimension-free deterministic equivalents and scaling laws for random
  feature regression <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15699v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15699v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Defilippis, Bruno Loureiro, Theodor Misiakiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we investigate the generalization performance of random feature
ridge regression (RFRR). Our main contribution is a general deterministic
equivalent for the test error of RFRR. Specifically, under a certain
concentration property, we show that the test error is well approximated by a
closed-form expression that only depends on the feature map eigenvalues.
Notably, our approximation guarantee is non-asymptotic, multiplicative, and
independent of the feature map dimension -- allowing for infinite-dimensional
features. We expect this deterministic equivalent to hold broadly beyond our
theoretical analysis, and we empirically validate its predictions on various
real and synthetic datasets. As an application, we derive sharp excess error
rates under standard power-law assumptions of the spectrum and target decay. In
particular, we provide a tight result for the smallest number of features
achieving optimal minimax error rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MicroAdam: Accurate Adaptive Optimization with Low Space Overhead and
  Provable Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ionut-Vlad Modoranu, Mher Safaryan, Grigory Malinovsky, Eldar Kurtic, Thomas Robert, Peter Richtarik, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new variant of the Adam optimizer called MicroAdam that
specifically minimizes memory overheads, while maintaining theoretical
convergence guarantees. We achieve this by compressing the gradient information
before it is fed into the optimizer state, thereby reducing its memory
footprint significantly. We control the resulting compression error via a novel
instance of the classical \emph{error feedback} mechanism from distributed
optimization in which *the error correction information is itself compressed*
to allow for practical memory gains. We prove that the resulting approach
maintains theoretical convergence guarantees competitive to those of AMSGrad,
while providing good practical performance. Specifically, we show that
MicroAdam can be implemented efficiently on GPUs: on both million-scale (BERT)
and billion-scale (LLaMA) models, MicroAdam provides practical convergence
competitive to that of the uncompressed Adam baseline, with lower memory usage
and similar running time. Our code is available at
https://github.com/IST-DASLab/MicroAdam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Agent Coordination via Multi-Level Communication <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziluo Ding, Zeyuan Liu, Zhirui Fang, Kefan Su, Liwen Zhu, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The partial observability and stochasticity in multi-agent settings can be
mitigated by accessing more information about others via communication.
However, the coordination problem still exists since agents cannot communicate
actual actions with each other at the same time due to the circular
dependencies. In this paper, we propose a novel multi-level communication
scheme, Sequential Communication (SeqComm). SeqComm treats agents
asynchronously (the upper-level agents make decisions before the lower-level
ones) and has two communication phases. In the negotiation phase, agents
determine the priority of decision-making by communicating hidden states of
observations and comparing the value of intention, obtained by modeling the
environment dynamics. In the launching phase, the upper-level agents take the
lead in making decisions and then communicate their actions with the
lower-level agents. Theoretically, we prove the policies learned by SeqComm are
guaranteed to improve monotonically and converge. Empirically, we show that
SeqComm outperforms existing methods in various cooperative multi-agent tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy Risks of Speculative Decoding in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01076v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01076v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiankun Wei, Abdulrahman Abdulrazzag, Tianchen Zhang, Adel Muursepp, Gururaj Saileshwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding in large language models (LLMs) accelerates token
generation by speculatively predicting multiple tokens cheaply and verifying
them in parallel, and has been widely deployed. In this paper, we provide the
first study demonstrating the privacy risks of speculative decoding. We observe
that input-dependent patterns of correct and incorrect predictions can be
leaked out to an adversary monitoring token generation times and packet sizes,
leading to privacy breaches. By observing the pattern of correctly and
incorrectly speculated tokens, we show that a malicious adversary can
fingerprint queries and learn private user inputs with more than $90\%$
accuracy across three different speculative decoding techniques - REST (almost
$100\%$ accuracy), LADE (up to $92\%$ accuracy), and BiLD (up to $95\%$
accuracy). We show that an adversary can also leak out confidential
intellectual property used to design these techniques, such as data from
data-stores used for prediction (in REST) at a rate of more than $25$ tokens
per second, or even hyper-parameters used for prediction (in LADE). We also
discuss mitigation strategies, such as aggregating tokens across multiple
iterations and padding packets with additional bytes, to avoid such privacy or
confidentiality breaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy of the last iterate in cyclically-sampled DP-SGD on nonconvex
  composite losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiwei Kong, Mónica Ribero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially-private stochastic gradient descent (DP-SGD) is a family of
iterative machine learning training algorithms that privatize gradients to
generate a sequence of differentially-private (DP) model parameters. It is also
the standard tool used to train DP models in practice, even though most users
are only interested in protecting the privacy of the final model. Tight DP
accounting for the last iterate would minimize the amount of noise required
while maintaining the same privacy guarantee and potentially increasing model
utility. However, last-iterate accounting is challenging, and existing works
require strong assumptions not satisfied by most implementations. These include
assuming (i) the global sensitivity constant is known - to avoid gradient
clipping; (ii) the loss function is Lipschitz or convex; and (iii) input
batches are sampled randomly.
  In this work, we forego any unrealistic assumptions and provide privacy
bounds for the most commonly used variant of DP-SGD, in which data is traversed
cyclically, gradients are clipped, and only the last model is released. More
specifically, we establish new Renyi differential privacy (RDP) upper bounds
for the last iterate under realistic assumptions of small stepsize and
Lipschitz smoothness of the loss function. Our general bounds also recover the
special-case convex bounds when the weak-convexity parameter of the objective
function approaches zero and no clipping is performed. The approach itself
leverages optimal transport techniques for last iterate bounds, which is a
nontrivial task when the data is traversed cyclically and the loss function is
nonconvex.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arithmetical Binary Decision Tree Traversals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04825v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04825v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinxiong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a series of methodes for traversing binary decision
trees using arithmetic operations. We present a suite of binary tree traversal
algorithms that leverage novel representation matrices to flatten the full
binary tree structure and embed the aggregated internal node Boolean tests into
a single bitvector. Our approach, grounded in maximum inner product search,
offers new insights into decision tree.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Global Convergence of Risk-Averse Policy Gradient Methods with
  Expected Conditional Risk Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10932v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10932v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Yu, Lei Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Risk-sensitive reinforcement learning (RL) has become a popular tool for
controlling the risk of uncertain outcomes and ensuring reliable performance in
highly stochastic sequential decision-making problems. While Policy Gradient
(PG) methods have been developed for risk-sensitive RL, it remains unclear if
these methods enjoy the same global convergence guarantees as in the
risk-neutral case
\citep{mei2020global,agarwal2021theory,cen2022fast,bhandari2024global}. In this
paper, we consider a class of dynamic time-consistent risk measures, named
Expected Conditional Risk Measures (ECRMs), and derive PG and Natural Policy
Gradient (NPG) updates for ECRMs-based RL problems. We provide global
optimality {and iteration complexities} of the proposed algorithms under the
following four settings: (i) PG with constrained direct parameterization, (ii)
PG with softmax parameterization and log barrier regularization, (iii) NPG with
softmax parameterization and entropy regularization, and (iv) approximate NPG
with inexact policy evaluation. Furthermore, we test a risk-averse REINFORCE
algorithm \citep{williams1992simple} and a risk-averse NPG algorithm
\citep{kakade2001natural} on a stochastic Cliffwalk environment to demonstrate
the efficacy of our methods and the importance of risk control.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning with Lie Group Orientations for Robotics <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11935v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11935v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Schuck, Jan Brüdigam, Sandra Hirche, Angela Schoellig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handling orientations of robots and objects is a crucial aspect of many
applications. Yet, ever so often, there is a lack of mathematical correctness
when dealing with orientations, especially in learning pipelines involving, for
example, artificial neural networks. In this paper, we investigate
reinforcement learning with orientations and propose a simple modification of
the network's input and output that adheres to the Lie group structure of
orientations. As a result, we obtain an easy and efficient implementation that
is directly usable with existing learning libraries and achieves significantly
better performance than other common orientation representations. We briefly
introduce Lie theory specifically for orientations in robotics to motivate and
outline our approach. Subsequently, a thorough empirical evaluation of
different combinations of orientation representations for states and actions
demonstrates the superior performance of our proposed approach in different
scenarios, including: direct orientation control, end effector orientation
control, and pick-and-place tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taming the Long Tail in Human Mobility Prediction <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14970v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14970v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohang Xu, Renhe Jiang, Chuang Yang, Zipei Fan, Kaoru Sezaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the popularity of location-based services, human mobility prediction
plays a key role in enhancing personalized navigation, optimizing
recommendation systems, and facilitating urban mobility and planning. This
involves predicting a user's next POI (point-of-interest) visit using their
past visit history. However, the uneven distribution of visitations over time
and space, namely the long-tail problem in spatial distribution, makes it
difficult for AI models to predict those POIs that are less visited by humans.
In light of this issue, we propose the Long-Tail Adjusted Next POI Prediction
(LoTNext) framework for mobility prediction, combining a Long-Tailed Graph
Adjustment module to reduce the impact of the long-tailed nodes in the user-POI
interaction graph and a novel Long-Tailed Loss Adjustment module to adjust loss
by logit score and sample weight adjustment strategy. Also, we employ the
auxiliary prediction task to enhance generalization and accuracy. Our
experiments with two real-world trajectory datasets demonstrate that LoTNext
significantly surpasses existing state-of-the-art works. Our code is available
at https://github.com/Yukayo/LoTNext.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-04T00:00:00Z">2024-11-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Context-Aware Adaptation in Extended Reality: A Design Space for
  XR Interfaces and an Adaptive Placement Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shakiba Davari, Doug A. Bowman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By converting the entire 3D space around the user into a screen, Extended
Reality (XR) can ameliorate traditional displays' space limitations and
facilitate the consumption of multiple pieces of information at a time.
However, if designed inappropriately, these XR interfaces can overwhelm the
user and complicate information access. In this work, we explored the design
dimensions that can be adapted to enable suitable presentation and interaction
within an XR interface. To investigate a specific use case of context-aware
adaptations within our proposed design space, we concentrated on the spatial
layout of the XR content and investigated non-adaptive and adaptive placement
strategies. In this paper, we (1) present a comprehensive design space for XR
interfaces, (2) propose Environment-referenced, an adaptive placement strategy
that uses a relevant intermediary from the environment within a Hybrid Frame of
Reference (FoR) for each XR object, and (3) evaluate the effectiveness of this
adaptive placement strategy and a non-adaptive Body-Fixed placement strategy in
four contextual scenarios varying in terms of social setting and user mobility
in the environment. The performance of these placement strategies from our
within-subjects user study emphasized the importance of intermediaries'
relevance to the user's focus. These findings underscore the importance of
context-aware interfaces, indicating that the appropriate use of an adaptive
content placement strategy in a context can significantly improve task
efficiency, accuracy, and usability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng-Chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, Wei Ping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art retrieval models typically address a straightforward search
scenario, where retrieval tasks are fixed (e.g., finding a passage to answer a
specific question) and only a single modality is supported for both queries and
retrieved results. This paper introduces techniques for advancing information
retrieval with multimodal large language models (MLLMs), enabling a broader
search scenario, termed universal multimodal retrieval, where multiple
modalities and diverse retrieval tasks are accommodated. To this end, we first
study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16
retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever
is capable of understanding challenging queries, composed of both text and
image, but underperforms a smaller CLIP retriever in cross-modal retrieval
tasks due to modality bias from MLLMs. To address the issue, we propose
modality-aware hard negative mining to mitigate the modality bias exhibited by
MLLM retrievers. Second, we propose to continually fine-tune the universal
multimodal retriever to enhance its text retrieval capability while maintaining
multimodal retrieval capability. As a result, our model, MM-Embed, achieves
state-of-the-art performance on the multimodal retrieval benchmark M-BEIR,
which spans multiple domains and tasks, while also surpassing the
state-of-the-art text retrieval model, NV-Embed-v1, on MTEB retrieval
benchmark. Finally, we explore to prompt the off-the-shelf MLLMs as the
zero-shot rerankers to refine the ranking of the candidates from the multimodal
retriever. We find that through prompt-and-reranking, MLLMs can further improve
multimodal retrieval when the user queries (e.g., text-image composed queries)
are more complex and challenging to understand. These findings also pave the
way to advance universal multimodal retrieval in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We release the model weights at:
  https://huggingface.co/nvidia/MM-Embed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INQUIRE: A Natural World Text-to-Image Retrieval Benchmark <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara Beery, Grant Van Horn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce INQUIRE, a text-to-image retrieval benchmark designed to
challenge multimodal vision-language models on expert-level queries. INQUIRE
includes iNaturalist 2024 (iNat24), a new dataset of five million natural world
images, along with 250 expert-level retrieval queries. These queries are paired
with all relevant images comprehensively labeled within iNat24, comprising
33,000 total matches. Queries span categories such as species identification,
context, behavior, and appearance, emphasizing tasks that require nuanced image
understanding and domain expertise. Our benchmark evaluates two core retrieval
tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)
INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed
evaluation of a range of recent multimodal models demonstrates that INQUIRE
poses a significant challenge, with the best models failing to achieve an
mAP@50 above 50%. In addition, we show that reranking with more powerful
multimodal models can enhance retrieval performance, yet there remains a
significant margin for improvement. By focusing on scientifically-motivated
ecological challenges, INQUIRE aims to bridge the gap between AI capabilities
and the needs of real-world scientific inquiry, encouraging the development of
retrieval systems that can assist with accelerating ecological and biodiversity
research. Our dataset and code are available at
https://inquire-benchmark.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NeurIPS 2024, Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training on the Test Model: Contamination in Ranking Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishakha Suresh Kalal, Andrew Parry, Sean MacAvaney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural approaches to ranking based on pre-trained language models are highly
effective in ad-hoc search. However, the computational expense of these models
can limit their application. As such, a process known as knowledge distillation
is frequently applied to allow a smaller, efficient model to learn from an
effective but expensive model. A key example of this is the distillation of
expensive API-based commercial Large Language Models into smaller
production-ready models. However, due to the opacity of training data and
processes of most commercial models, one cannot ensure that a chosen test
collection has not been observed previously, creating the potential for
inadvertent data contamination. We, therefore, investigate the effect of a
contaminated teacher model in a distillation setting. We evaluate several
distillation techniques to assess the degree to which contamination occurs
during distillation. By simulating a ``worst-case'' setting where the degree of
contamination is known, we find that contamination occurs even when the test
data represents a small fraction of the teacher's training samples. We,
therefore, encourage caution when training using black-box teacher models where
data provenance is ambiguous.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing ID-based Recommendation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Chen, Chen Gao, Xiaoyi Du, Hengliang Luo, Depeng Jin, Yong Li, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently garnered significant attention in
various domains, including recommendation systems. Recent research leverages
the capabilities of LLMs to improve the performance and user modeling aspects
of recommender systems. These studies primarily focus on utilizing LLMs to
interpret textual data in recommendation tasks. However, it's worth noting that
in ID-based recommendations, textual data is absent, and only ID data is
available. The untapped potential of LLMs for ID data within the ID-based
recommendation paradigm remains relatively unexplored. To this end, we
introduce a pioneering approach called "LLM for ID-based Recommendation"
(LLM4IDRec). This innovative approach integrates the capabilities of LLMs while
exclusively relying on ID data, thus diverging from the previous reliance on
textual data. The basic idea of LLM4IDRec is that by employing LLM to augment
ID data, if augmented ID data can improve recommendation performance, it
demonstrates the ability of LLM to interpret ID data effectively, exploring an
innovative way for the integration of LLM in ID-based recommendation. We
evaluate the effectiveness of our LLM4IDRec approach using three widely-used
datasets. Our results demonstrate a notable improvement in recommendation
performance, with our approach consistently outperforming existing methods in
ID-based recommendation by solely augmenting input data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dissertation: On the Theoretical Foundation of Model Comparison and
  Evaluation for Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become increasingly important with the rise of the
web as a medium for electronic and business transactions. One of the key
drivers of this technology is the ease with which users can provide feedback
about their likes and dislikes through simple clicks of a mouse. This feedback
is commonly collected in the form of ratings, but can also be inferred from a
user's browsing and purchasing history. Recommender systems utilize users'
historical data to infer customer interests and provide personalized
recommendations. The basic principle of recommendations is that significant
dependencies exist between user- and item-centric activity, which can be
learned in a data-driven manner to make accurate predictions. Collaborative
filtering is one family of recommendation algorithms that uses ratings from
multiple users to predict missing ratings or uses binary click information to
predict potential clicks. However, recommender systems can be more complex and
incorporate auxiliary data such as content-based attributes, user interactions,
and contextual information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.08517</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transferable Sequential Recommendation via Vector Quantized Meta
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenrui Yue, Huimin Zeng, Yang Zhang, Julian McAuley, Dong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While sequential recommendation achieves significant progress on capturing
user-item transition patterns, transferring such large-scale recommender
systems remains challenging due to the disjoint user and item groups across
domains. In this paper, we propose a vector quantized meta learning for
transferable sequential recommenders (MetaRec). Without requiring additional
modalities or shared information across domains, our approach leverages
user-item interactions from multiple source domains to improve the target
domain performance. To solve the input heterogeneity issue, we adopt vector
quantization that maps item embeddings from heterogeneous input spaces to a
shared feature space. Moreover, our meta transfer paradigm exploits limited
target data to guide the transfer of source domain knowledge to the target
domain (i.e., learn to transfer). In addition, MetaRec adaptively transfers
from multiple source tasks by rescaling meta gradients based on the
source-target domain similarity, enabling selective learning to improve
recommendation performance. To validate the effectiveness of our approach, we
perform extensive experiments on benchmark datasets, where MetaRec consistently
outperforms baseline methods by a considerable margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to BigData 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Retrieval: End-to-End Information Retrieval with One Large Language
  Model <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00801v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00801v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoyu Tang, Jiawei Chen, Zhuoqun Li, Bowen Yu, Yaojie Lu, Cheng Fu, Haiyang Yu, Hongyu Lin, Fei Huang, Ben He, Xianpei Han, Le Sun, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of large language models (LLMs) has significantly transformed both
the construction and application of information retrieval (IR) systems.
However, current interactions between IR systems and LLMs remain limited, with
LLMs merely serving as part of components within IR systems, and IR systems
being constructed independently of LLMs. This separated architecture restricts
knowledge sharing and deep collaboration between them. In this paper, we
introduce Self-Retrieval, a novel end-to-end LLM-driven information retrieval
architecture. Self-Retrieval unifies all essential IR functions within a single
LLM, leveraging the inherent capabilities of LLMs throughout the IR process.
Specifically, Self-Retrieval internalizes the retrieval corpus through
self-supervised learning, transforms the retrieval process into sequential
passage generation, and performs relevance assessment for reranking.
Experimental results demonstrate that Self-Retrieval not only outperforms
existing retrieval approaches by a significant margin, but also substantially
enhances the performance of LLM-driven downstream applications like
retrieval-augmented generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Camera-ready Version. Code:
  https://github.com/icip-cas/SelfRetrieval</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Context-Aware Adaptation in Extended Reality: A Design Space for
  XR Interfaces and an Adaptive Placement Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shakiba Davari, Doug A. Bowman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By converting the entire 3D space around the user into a screen, Extended
Reality (XR) can ameliorate traditional displays' space limitations and
facilitate the consumption of multiple pieces of information at a time.
However, if designed inappropriately, these XR interfaces can overwhelm the
user and complicate information access. In this work, we explored the design
dimensions that can be adapted to enable suitable presentation and interaction
within an XR interface. To investigate a specific use case of context-aware
adaptations within our proposed design space, we concentrated on the spatial
layout of the XR content and investigated non-adaptive and adaptive placement
strategies. In this paper, we (1) present a comprehensive design space for XR
interfaces, (2) propose Environment-referenced, an adaptive placement strategy
that uses a relevant intermediary from the environment within a Hybrid Frame of
Reference (FoR) for each XR object, and (3) evaluate the effectiveness of this
adaptive placement strategy and a non-adaptive Body-Fixed placement strategy in
four contextual scenarios varying in terms of social setting and user mobility
in the environment. The performance of these placement strategies from our
within-subjects user study emphasized the importance of intermediaries'
relevance to the user's focus. These findings underscore the importance of
context-aware interfaces, indicating that the appropriate use of an adaptive
content placement strategy in a context can significantly improve task
efficiency, accuracy, and usability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIAST: A Multimodal Piano <span class="highlight-title">Dataset</span> with Audio, Symbolic and Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hayeon Bang, Eunjin Choi, Megan Finch, Seungheon Doh, Seolhee Lee, Gyeong-Hoon Lee, Juan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While piano music has become a significant area of study in Music Information
Retrieval (MIR), there is a notable lack of datasets for piano solo music with
text labels. To address this gap, we present PIAST (PIano dataset with Audio,
Symbolic, and Text), a piano music dataset. Utilizing a piano-specific taxonomy
of semantic tags, we collected 9,673 tracks from YouTube and added human
annotations for 2,023 tracks by music experts, resulting in two subsets:
PIAST-YT and PIAST-AT. Both include audio, text, tag annotations, and
transcribed MIDI utilizing state-of-the-art piano transcription and beat
tracking models. Among many possible tasks with the multi-modal dataset, we
conduct music tagging and retrieval using both audio and MIDI data and report
baseline performances to demonstrate its potential as a valuable resource for
MIR research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 3rd Workshop on NLP for Music and
  Audio (NLP4MusA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Diffusion</span>-based Generative Multicasting with Intent-aware Semantic
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinkai Liu, Mahdi Boloursaz Mashhadi, Li Qiao, Yi Ma, Rahim Tafazolli, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative diffusion models (GDMs) have recently shown great success in
synthesizing multimedia signals with high perceptual quality enabling highly
efficient semantic communications in future wireless networks. In this paper,
we develop an intent-aware generative semantic multicasting framework utilizing
pre-trained diffusion models. In the proposed framework, the transmitter
decomposes the source signal to multiple semantic classes based on the
multi-user intent, i.e. each user is assumed to be interested in details of
only a subset of the semantic classes. The transmitter then sends to each user
only its intended classes, and multicasts a highly compressed semantic map to
all users over shared wireless resources that allows them to locally synthesize
the other classes, i.e. non-intended classes, utilizing pre-trained diffusion
models. The signal retrieved at each user is thereby partially reconstructed
and partially synthesized utilizing the received semantic map. This improves
utilization of the wireless resources, with better preserving privacy of the
non-intended classes. We design a communication/computation-aware scheme for
per-class adaptation of the communication parameters, such as the transmission
power and compression rate to minimize the total latency of retrieving signals
at multiple receivers, tailored to the prevailing channel conditions as well as
the users reconstruction/synthesis distortion/perception requirements. The
simulation results demonstrate significantly reduced per-user latency compared
with non-generative and intent-unaware multicasting benchmarks while
maintaining high perceptual quality of the signals retrieved at the users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Audio-Visual Segmentation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Sokolov, Swapnil Bhosale, Xiatian Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing the sounding objects in scenes is a longstanding objective in
embodied AI, with diverse applications in robotics and AR/VR/MR. To that end,
Audio-Visual Segmentation (AVS), taking as condition an audio signal to
identify the masks of the target sounding objects in an input image with
synchronous camera and microphone sensors, has been recently advanced. However,
this paradigm is still insufficient for real-world operation, as the mapping
from 2D images to 3D scenes is missing. To address this fundamental limitation,
we introduce a novel research problem, 3D Audio-Visual Segmentation, extending
the existing AVS to the 3D output space. This problem poses more challenges due
to variations in camera extrinsics, audio scattering, occlusions, and diverse
acoustics across sounding object categories. To facilitate this research, we
create the very first simulation based benchmark, 3DAVS-S34-O7, providing
photorealistic 3D scene environments with grounded spatial audio under
single-instance and multi-instance settings, across 34 scenes and 7 object
categories. This is made possible by re-purposing the Habitat simulator to
generate comprehensive annotations of sounding object locations and
corresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet,
characterized by integrating the ready-to-use knowledge from pretrained 2D
audio-visual foundation models synergistically with 3D visual scene
representation through spatial audio-aware mask alignment and refinement.
Extensive experiments demonstrate that EchoSegnet can effectively segment
sounding objects in 3D space on our new benchmark, representing a significant
advancement in the field of embodied AI. Project page:
https://surrey-uplab.github.io/research/3d-audio-visual-segmentation/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the NeurIPS 2024 Workshop on Audio Imagination</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoMu-<span class="highlight-title">Diffusion</span>: On Learning Long-Term Motion-Music Synchronization and
  Correspondence <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuming You, Minghui Fang, Li Tang, Rongjie Huang, Yongqi Wang, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion-to-music and music-to-motion have been studied separately, each
attracting substantial research interest within their respective domains. The
interaction between human motion and music is a reflection of advanced human
intelligence, and establishing a unified relationship between them is
particularly important. However, to date, there has been no work that considers
them jointly to explore the modality alignment within. To bridge this gap, we
propose a novel framework, termed MoMu-Diffusion, for long-term and synchronous
motion-music generation. Firstly, to mitigate the huge computational costs
raised by long sequences, we propose a novel Bidirectional Contrastive Rhythmic
Variational Auto-Encoder (BiCoR-VAE) that extracts the modality-aligned latent
representations for both motion and music inputs. Subsequently, leveraging the
aligned latent spaces, we introduce a multi-modal Transformer-based diffusion
model and a cross-guidance sampling strategy to enable various generation
tasks, including cross-modal, multi-modal, and variable-length generation.
Extensive experiments demonstrate that MoMu-Diffusion surpasses recent
state-of-the-art methods both qualitatively and quantitatively, and can
synthesize realistic, diverse, long-term, and beat-matched music or motion
sequences. The generated samples and codes are available at
https://momu-diffusion.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OneDiff: A Generalist Model for Image Difference Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05645v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05645v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erdong Hu, Longteng Guo, Tongtian Yue, Zijia Zhao, Shuning Xue, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computer vision, Image Difference Captioning (IDC) is crucial for
accurately describing variations between closely related images. Traditional
IDC methods often rely on specialist models, which restrict their applicability
across varied contexts. This paper introduces the OneDiff model, a novel
generalist approach that utilizes a robust vision-language model architecture,
integrating a siamese image encoder with a Visual Delta Module. This innovative
configuration allows for the precise detection and articulation of fine-grained
differences between image pairs. OneDiff is trained through a dual-phase
strategy, encompassing Coupled Sample Training and multi-task learning across a
diverse array of data types, supported by our newly developed DiffCap Dataset.
This dataset merges real-world and synthetic data, enhancing the training
process and bolstering the model's robustness. Extensive testing on diverse IDC
benchmarks, such as Spot-the-Diff, Image-Editing-Request, and Birds-to-Words,
shows that OneDiff consistently outperforms existing state-of-the-art models in
accuracy and adaptability, achieving improvements of up to 97% CIDEr points in
average. By setting a new benchmark in IDC, OneDiff paves the way for more
versatile and effective applications in detecting and describing visual
differences. The code, models, and data will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReactFace: Online Multiple Appropriate Facial Reaction Generation in
  Dyadic Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Luo, Siyang Song, Weicheng Xie, Micol Spitale, Zongyuan Ge, Linlin Shen, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dyadic interaction, predicting the listener's facial reactions is
challenging as different reactions could be appropriate in response to the same
speaker's behaviour. Previous approaches predominantly treated this task as an
interpolation or fitting problem, emphasizing deterministic outcomes but
ignoring the diversity and uncertainty of human facial reactions. Furthermore,
these methods often failed to model short-range and long-range dependencies
within the interaction context, leading to issues in the synchrony and
appropriateness of the generated facial reactions. To address these
limitations, this paper reformulates the task as an extrapolation or prediction
problem, and proposes an novel framework (called ReactFace) to generate
multiple different but appropriate facial reactions from a speaker behaviour
rather than merely replicating the corresponding listener facial behaviours.
Our ReactFace generates multiple different but appropriate photo-realistic
human facial reactions by: (i) learning an appropriate facial reaction
distribution representing multiple different but appropriate facial reactions;
and (ii) synchronizing the generated facial reactions with the speaker verbal
and non-verbal behaviours at each time stamp, resulting in realistic 2D facial
reaction sequences. Experimental results demonstrate the effectiveness of our
approach in generating multiple diverse, synchronized, and appropriate facial
reactions from each speaker's behaviour. The quality of the generated facial
reactions is intimately tied to the speaker's speech and facial expressions,
achieved through our novel speaker-listener interaction modules. Our code is
made publicly available at \url{https://github.com/lingjivoo/ReactFace}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Visualization and Computer Graphics
  (TVCG), 18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-03T00:00:00Z">2024-11-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-clustering for Federated Recommender System <span class="chip">WWW '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinrui He, Shuo Liu, Jackey Keung, Jingrui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As data privacy and security attract increasing attention, Federated
Recommender System (FRS) offers a solution that strikes a balance between
providing high-quality recommendations and preserving user privacy. However,
the presence of statistical heterogeneity in FRS, commonly observed due to
personalized decision-making patterns, can pose challenges. To address this
issue and maximize the benefit of collaborative filtering (CF) in FRS, it is
intuitive to consider clustering clients (users) as well as items into
different groups and learning group-specific models. Existing methods either
resort to client clustering via user representations-risking privacy leakage,
or employ classical clustering strategies on item embeddings or gradients,
which we found are plagued by the curse of dimensionality. In this paper, we
delve into the inefficiencies of the K-Means method in client grouping,
attributing failures due to the high dimensionality as well as data sparsity
occurring in FRS, and propose CoFedRec, a novel Co-clustering Federated
Recommendation mechanism, to address clients heterogeneity and enhance the
collaborative filtering within the federated framework. Specifically, the
server initially formulates an item membership from the client-provided item
networks. Subsequently, clients are grouped regarding a specific item category
picked from the item membership during each communication round, resulting in
an intelligently aggregated group model. Meanwhile, to comprehensively capture
the global inter-relationships among items, we incorporate an additional
supervised contrastive learning term based on the server-side generated item
membership into the local training phase for each client. Extensive experiments
on four datasets are provided, which verify the effectiveness of the proposed
CoFedRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WWW '24: Proceedings of the ACM Web Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-based Confidence Calibration for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Li, Sijia Wang, Lifu Huang, Li-Ping Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One important approach to improving the reliability of large language models
(LLMs) is to provide accurate confidence estimations regarding the correctness
of their answers. However, developing a well-calibrated confidence estimation
model is challenging, as mistakes made by LLMs can be difficult to detect. We
propose a novel method combining the LLM's self-consistency with labeled data
and training an auxiliary model to estimate the correctness of its responses to
questions. This auxiliary model predicts the correctness of responses based
solely on their consistent information. To set up the learning problem, we use
a weighted graph to represent the consistency among the LLM's multiple
responses to a question. Correctness labels are assigned to these responses
based on their similarity to the correct answer. We then train a graph neural
network to estimate the probability of correct responses. Experiments
demonstrate that the proposed approach substantially outperforms several of the
most recent methods in confidence calibration across multiple widely adopted
benchmark datasets. Furthermore, the proposed approach significantly improves
the generalization capability of confidence calibration on out-of-domain (OOD)
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Communication Avoidance for Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lutfi Eren Erdogan, Vijay Anand Raghava Kanakagiri, Kurt Keutzer, Zhen Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the major bottlenecks for efficient deployment of neural network based
recommendation systems is the memory footprint of their embedding tables.
Although many neural network based recommendation systems could benefit from
the faster on-chip memory access and increased computational power of hardware
accelerators, the large embedding tables in these models often cannot fit on
the constrained memory of accelerators. Despite the pervasiveness of these
models, prior methods in memory optimization and parallelism fail to address
the memory and communication costs of large embedding tables on accelerators.
As a result, the majority of models are trained on CPUs, while current
implementations of accelerators are hindered by issues such as bottlenecks in
inter-device communication and main memory lookups. In this paper, we propose a
theoretical framework that analyses the communication costs of arbitrary
distributed systems that use lookup tables. We use this framework to propose
algorithms that maximize throughput subject to memory, computation, and
communication constraints. Furthermore, we demonstrate that our method achieves
strong theoretical performance across dataset distributions and memory
constraints, applicable to a wide range of use cases from mobile federated
learning to warehouse-scale computation. We implement our framework and
algorithms in PyTorch and achieve up to 6x increases in training throughput on
GPU systems over baselines, on the Criteo Terabytes dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Graph Neural Network for Recommendation with Dynamic
  De-redundancy and Modality-Guided Feature De-noisy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Mo, Lin Xiao, Qiya Song, Xieping Gao, Eryao Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have become crucial in multimodal recommendation
tasks because of their powerful ability to capture complex relationships
between neighboring nodes. However, increasing the number of propagation layers
in GNNs can lead to feature redundancy, which may negatively impact the overall
recommendation performance. In addition, the existing recommendation task
method directly maps the preprocessed multimodal features to the
low-dimensional space, which will bring the noise unrelated to user preference,
thus affecting the representation ability of the model. To tackle the
aforementioned challenges, we propose Multimodal Graph Neural Network for
Recommendation (MGNM) with Dynamic De-redundancy and Modality-Guided Feature
De-noisy, which is divided into local and global interaction. Initially, in the
local interaction process,we integrate a dynamic de-redundancy (DDR) loss
function which is achieved by utilizing the product of the feature coefficient
matrix and the feature matrix as a penalization factor. It reduces the feature
redundancy effects of multimodal and behavioral features caused by the stacking
of multiple GNN layers. Subsequently, in the global interaction process, we
developed modality-guided global feature purifiers for each modality to
alleviate the impact of modality noise. It is a two-fold guiding mechanism
eliminating modality features that are irrelevant to user preferences and
captures complex relationships within the modality. Experimental results
demonstrate that MGNM achieves superior performance on multimodal information
denoising and removal of redundant information compared to the state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Robust Regularized Federated Recommendation <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Langming Liu, Wanyu Wang, Xiangyu Zhao, Zijian Zhang, Chunxu Zhang, Shanru Lin, Yiqi Wang, Lixin Zou, Zitao Liu, Xuetao Wei, Hongzhi Yin, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems play a pivotal role across practical scenarios,
showcasing remarkable capabilities in user preference modeling. However, the
centralized learning paradigm predominantly used raises serious privacy
concerns. The federated recommender system (FedRS) addresses this by updating
models on clients, while a central server orchestrates training without
accessing private data. Existing FedRS approaches, however, face unresolved
challenges, including non-convex optimization, vulnerability, potential privacy
leakage risk, and communication inefficiency. This paper addresses these
challenges by reformulating the federated recommendation problem as a convex
optimization issue, ensuring convergence to the global optimum. Based on this,
we devise a novel method, RFRec, to tackle this optimization problem
efficiently. In addition, we propose RFRecF, a highly efficient version that
incorporates non-uniform stochastic gradient descent to improve communication
efficiency. In user preference modeling, both methods learn local and global
models, collaboratively learning users' common and personalized interests under
the federated learning setting. Moreover, both methods significantly enhance
communication efficiency, robustness, and privacy protection, with theoretical
support. Comprehensive evaluations on four benchmark datasets demonstrate RFRec
and RFRecF's superior performance compared to diverse baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LinRec: Linear Attention Mechanism for Long-term Sequential Recommender
  Systems <span class="chip">SIGIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Langming Liu, Xiangyu Zhao, Chi Zhang, Jingtong Gao, Wanyu Wang, Wenqi Fan, Yiqi Wang, Ming He, Zitao Liu, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer models have achieved remarkable success in sequential recommender
systems (SRSs). However, computing the attention matrix in traditional
dot-product attention mechanisms results in a quadratic complexity with
sequence lengths, leading to high computational costs for long-term sequential
recommendation. Motivated by the above observation, we propose a novel
L2-Normalized Linear Attention for the Transformer-based Sequential Recommender
Systems (LinRec), which theoretically improves efficiency while preserving the
learning capabilities of the traditional dot-product attention. Specifically,
by thoroughly examining the equivalence conditions of efficient attention
mechanisms, we show that LinRec possesses linear complexity while preserving
the property of attention mechanisms. In addition, we reveal its latent
efficiency properties by interpreting the proposed LinRec mechanism through a
statistical lens. Extensive experiments are conducted based on two public
benchmark datasets, demonstrating that the combination of LinRec and
Transformer models achieves comparable or even superior performance than
state-of-the-art Transformer-based SRS models while significantly improving
time and memory efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-performance automated abstract screening with large language model
  ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Sanghera, Arun James Thirunavukarasu, Marc El Khoury, Jessica O'Logbon, Yuqing Chen, Archie Watt, Mustafa Mahmood, Hamid Butt, George Nishimura, Andrew Soltan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel in tasks requiring processing and
interpretation of input text. Abstract screening is a labour-intensive
component of systematic review involving repetitive application of inclusion
and exclusion criteria on a large volume of studies identified by a literature
search. Here, LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Llama 3 70B, Gemini 1.5
Pro, and Claude Sonnet 3.5) were trialled on systematic reviews in a full issue
of the Cochrane Library to evaluate their accuracy in zero-shot binary
classification for abstract screening. Trials over a subset of 800 records
identified optimal prompting strategies and demonstrated superior performance
of LLMs to human researchers in terms of sensitivity (LLMmax = 1.000, humanmax
= 0.775), precision (LLMmax = 0.927, humanmax = 0.911), and balanced accuracy
(LLMmax = 0.904, humanmax = 0.865). The best performing LLM-prompt combinations
were trialled across every replicated search result (n = 119,691), and
exhibited consistent sensitivity (range 0.756-1.000) but diminished precision
(range 0.004-0.096). 66 LLM-human and LLM-LLM ensembles exhibited perfect
sensitivity with a maximal precision of 0.458, with less observed performance
drop in larger trials. Significant variation in performance was observed
between reviews, highlighting the importance of domain-specific validation
before deployment. LLMs may reduce the human labour cost of systematic review
with maintained or improved accuracy and sensitivity. Systematic review is the
foundation of evidence-based medicine, and LLMs can contribute to increasing
the efficiency and quality of this mode of research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RS and AJT are joint-first authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential
  Recommendation <span class="chip">WSDM'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Liu, Sixiao Zhang, Cheng Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation (SR) systems excel at capturing users' dynamic
preferences by leveraging their interaction histories. Most existing SR systems
assign a single embedding vector to each item to represent its features, and
various types of models are adopted to combine these item embeddings into a
sequence representation vector to capture the user intent. However, we argue
that this representation alone is insufficient to capture an item's
multi-faceted nature (e.g., movie genres, starring actors). Besides, users
often exhibit complex and varied preferences within these facets (e.g., liking
both action and musical films in the facet of genre), which are challenging to
fully represent. To address the issues above, we propose a novel structure
called Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential
Recommendation (FAME). We leverage sub-embeddings from each head in the last
multi-head attention layer to predict the next item separately. This approach
captures the potential multi-faceted nature of items without increasing model
complexity. A gating mechanism integrates recommendations from each head and
dynamically determines their importance. Furthermore, we introduce a
Mixture-of-Experts (MoE) network in each attention head to disentangle various
user preferences within each facet. Each expert within the MoE focuses on a
specific preference. A learnable router network is adopted to compute the
importance weight for each expert and aggregate them. We conduct extensive
experiments on four public sequential recommendation datasets and the results
demonstrate the effectiveness of our method over existing baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by WSDM'25. The final camera-ready
  version will be available soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding and Scaling Collaborative Filtering Optimization from the
  Perspective of Matrix Rank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donald Loveland, Xinyi Wu, Tong Zhao, Danai Koutra, Neil Shah, Mingxuan Ju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative Filtering (CF) methods dominate real-world recommender systems
given their ability to learn high-quality, sparse ID-embedding tables that
effectively capture user preferences. These tables scale linearly with the
number of users and items, and are trained to ensure high similarity between
embeddings of interacted user-item pairs, while maintaining low similarity for
non-interacted pairs. Despite their high performance, encouraging dispersion
for non-interacted pairs necessitates expensive regularization (e.g., negative
sampling), hurting runtime and scalability. Existing research tends to address
these challenges by simplifying the learning process, either by reducing model
complexity or sampling data, trading performance for runtime. In this work, we
move beyond model-level modifications and study the properties of the embedding
tables under different learning strategies. Through theoretical analysis, we
find that the singular values of the embedding tables are intrinsically linked
to different CF loss functions. These findings are empirically validated on
real-world datasets, demonstrating the practical benefits of higher stable
rank, a continuous version of matrix rank which encodes the distribution of
singular values. Based on these insights, we propose an efficient warm-start
strategy that regularizes the stable rank of the user and item embeddings. We
show that stable rank regularization during early training phases can promote
higher-quality embeddings, resulting in training speed improvements of up to
66%. Additionally, stable rank regularization can act as a proxy for negative
sampling, allowing for performance gains of up to 21% over loss functions with
small negative sampling ratios. Overall, our analysis unifies current CF
methods under a new perspective, their optimization of stable rank, motivating
a flexible regularization method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Little Giants: Synthesizing High-Quality Embedding Data at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data generation has become an increasingly popular way of training
models without the need for large, manually labeled datasets. For tasks like
text embedding, synthetic data offers diverse and scalable training examples,
significantly reducing the cost of human annotation. However, most current
approaches rely heavily on proprietary models like GPT-4, which are expensive
and inefficient for generating large-scale embedding data. In this paper, we
introduce SPEED, a framework that aligns open-source small models (8B) to
efficiently generate large-scale synthetic embedding data. Through supervised
fine-tuning, preference optimization, and self-improvement, SPEED enables small
open-source models to produce high-quality data. Remarkably, SPEED uses only
less than 1/10 of the GPT API calls, outperforming the state-of-the-art
embedding model E5_mistral when both are trained solely on their synthetic
data. Using this efficient generator, we conduct a comprehensive study on how
various factors within the alignment pipeline impact data quality and reveal
the scaling law for synthetic embedding data.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Graph Neural Network for Recommendation with Dynamic
  De-redundancy and Modality-Guided Feature De-noisy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Mo, Lin Xiao, Qiya Song, Xieping Gao, Eryao Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have become crucial in multimodal recommendation
tasks because of their powerful ability to capture complex relationships
between neighboring nodes. However, increasing the number of propagation layers
in GNNs can lead to feature redundancy, which may negatively impact the overall
recommendation performance. In addition, the existing recommendation task
method directly maps the preprocessed multimodal features to the
low-dimensional space, which will bring the noise unrelated to user preference,
thus affecting the representation ability of the model. To tackle the
aforementioned challenges, we propose Multimodal Graph Neural Network for
Recommendation (MGNM) with Dynamic De-redundancy and Modality-Guided Feature
De-noisy, which is divided into local and global interaction. Initially, in the
local interaction process,we integrate a dynamic de-redundancy (DDR) loss
function which is achieved by utilizing the product of the feature coefficient
matrix and the feature matrix as a penalization factor. It reduces the feature
redundancy effects of multimodal and behavioral features caused by the stacking
of multiple GNN layers. Subsequently, in the global interaction process, we
developed modality-guided global feature purifiers for each modality to
alleviate the impact of modality noise. It is a two-fold guiding mechanism
eliminating modality features that are irrelevant to user preferences and
captures complex relationships within the modality. Experimental results
demonstrate that MGNM achieves superior performance on multimodal information
denoising and removal of redundant information compared to the state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14468v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14468v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Ku, Cong Wei, Weiming Ren, Harry Yang, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the dynamic field of digital content creation using generative models,
state-of-the-art video editing models still do not offer the level of quality
and control that users desire. Previous works on video editing either extended
from image-based generative models in a zero-shot manner or necessitated
extensive fine-tuning, which can hinder the production of fluid video edits.
Furthermore, these methods frequently rely on textual input as the editing
guidance, leading to ambiguities and limiting the types of edits they can
perform. Recognizing these challenges, we introduce AnyV2V, a novel tuning-free
paradigm designed to simplify video editing into two primary steps: (1)
employing an off-the-shelf image editing model to modify the first frame, (2)
utilizing an existing image-to-video generation model to generate the edited
video through temporal feature injection. AnyV2V can leverage any existing
image editing tools to support an extensive array of video editing tasks,
including prompt-based editing, reference-based style transfer, subject-driven
editing, and identity manipulation, which were unattainable by previous
methods. AnyV2V can also support any video length. Our evaluation shows that
AnyV2V achieved CLIP-scores comparable to other baseline methods. Furthermore,
AnyV2V significantly outperformed these baselines in human evaluations,
demonstrating notable improvements in visual consistency with the source video
while producing high-quality edits across all editing tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR 2024)
  (11/2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13743v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13743v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, Deva Ramanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While text-to-visual models now produce photo-realistic images and videos,
they struggle with compositional text prompts involving attributes,
relationships, and higher-order reasoning such as logic and comparison. In this
work, we conduct an extensive human study on GenAI-Bench to evaluate the
performance of leading image and video generation models in various aspects of
compositional text-to-visual generation. We also compare automated evaluation
metrics against our collected human ratings and find that VQAScore -- a metric
measuring the likelihood that a VQA model views an image as accurately
depicting the prompt -- significantly outperforms previous metrics such as
CLIPScore. In addition, VQAScore can improve generation in a black-box manner
(without finetuning) via simply ranking a few (3 to 9) candidate images.
Ranking by VQAScore is 2x to 3x more effective than other scoring methods like
PickScore, HPSv2, and ImageReward at improving human alignment ratings for
DALL-E 3 and Stable Diffusion, especially on compositional prompts that require
advanced visio-linguistic reasoning. We release a new GenAI-Rank benchmark with
over 40,000 human ratings to evaluate scoring metrics on ranking images
generated from the same prompt. Lastly, we discuss promising areas for
improvement in VQAScore, such as addressing fine-grained visual details. We
will release all human ratings (over 80,000) to facilitate scientific
benchmarking of both generative models and automated metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We open-source our dataset, model, and code at:
  https://linzhiqiu.github.io/papers/genai_bench ; Project page:
  https://linzhiqiu.github.io/papers/genai_bench ; GenAI-Bench was first
  introduced in arxiv:2404.01291. This article extends it with an additional
  GenAI-Rank benchmark</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-02T00:00:00Z">2024-11-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Channel Hypergraph Contrastive Learning for Matrix Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Changsheng Shui, Yanwei Yu, Chao Huang, Zhongying Zhao, Junyu Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rating is a typical user explicit feedback that visually reflects how much a
user likes a related item. The (rating) matrix completion is essentially a
rating prediction process, which is also a significant problem in recommender
systems. Recently, graph neural networks (GNNs) have been widely used in matrix
completion, which captures users' preferences over items by formulating a
rating matrix as a bipartite graph. However, existing methods are susceptible
due to data sparsity and long-tail distribution in real-world scenarios.
Moreover, the messaging mechanism of GNNs makes it difficult to capture
high-order correlations and constraints between nodes, which are essentially
useful in recommendation tasks. To tackle these challenges, we propose a
Multi-Channel Hypergraph Contrastive Learning framework for matrix completion,
named MHCL. Specifically, MHCL adaptively learns hypergraph structures to
capture high-order correlations between nodes and jointly captures local and
global collaborative relationships through attention-based cross-view
aggregation. Additionally, to consider the magnitude and order information of
ratings, we treat different rating subgraphs as different channels, encourage
alignment between adjacent ratings, and further achieve the mutual enhancement
between different ratings through multi-channel cross-rating contrastive
learning. Extensive experiments on five public datasets demonstrate that the
proposed method significantly outperforms the current state-of-the-art
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Financial Data and News Articles for Stock Price Movement
  Prediction Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Elahi, Fatemeh Taghvaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting financial markets and stock price movements requires analyzing a
company's performance, historic price movements, industry-specific events
alongside the influence of human factors such as social media and press
coverage. We assume that financial reports (such as income statements, balance
sheets, and cash flow statements), historical price data, and recent news
articles can collectively represent aforementioned factors. We combine
financial data in tabular format with textual news articles and employ
pre-trained Large Language Models (LLMs) to predict market movements. Recent
research in LLMs has demonstrated that they are able to perform both tabular
and text classification tasks, making them our primary model to classify the
multi-modal data. We utilize retrieval augmentation techniques to retrieve and
attach relevant chunks of news articles to financial metrics related to a
company and prompt the LLMs in zero, two, and four-shot settings. Our dataset
contains news articles collected from different sources, historic stock price,
and financial report data for 20 companies with the highest trading volume
across different industries in the stock market. We utilized recently released
language models for our LLM-based classifier, including GPT- 3 and 4, and
LLaMA- 2 and 3 models. We introduce an LLM-based classifier capable of
performing classification tasks using combination of tabular (structured) and
textual (unstructured) data. By using this model, we predicted the movement of
a given stock's price in our dataset with a weighted F1-score of 58.5% and
59.1% and Matthews Correlation Coefficient of 0.175 for both 3-month and
6-month periods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online and Offline Evaluations of Collaborative Filtering and Content
  Based Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Elahi, Armin Zirak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are widely used AI applications designed to help users
efficiently discover relevant items. The effectiveness of such systems is tied
to the satisfaction of both users and providers. However, user satisfaction is
complex and cannot be easily framed mathematically using information retrieval
and accuracy metrics. While many studies evaluate accuracy through offline
tests, a growing number of researchers argue that online evaluation methods
such as A/B testing are better suited for this purpose. We have employed a
variety of algorithms on different types of datasets divergent in size and
subject, producing recommendations in various platforms, including media
streaming services, digital publishing websites, e-commerce systems, and news
broadcasting networks. Notably, our target websites and datasets are in Persian
(Farsi) language.
  This study provides a comparative analysis of a large-scale recommender
system that has been operating for the past year across about 70 websites in
Iran, processing roughly 300 requests per second collectively. The system
employs user-based and item-based recommendations using content-based,
collaborative filtering, trend-based methods, and hybrid approaches. Through
both offline and online evaluations, we aim to identify where these algorithms
perform most efficiently and determine the best method for our specific needs,
considering the dataset and system scale. Our methods of evaluation include
manual evaluation, offline tests including accuracy and ranking metrics like
hit-rate@k and nDCG, and online tests consisting of click-through rate (CTR).
Additionally we analyzed and proposed methods to address cold-start and
popularity bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Knowledge Graph for Teaching Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eleni Ilkou, Ernesto Jiménez-Ruiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This poster paper describes the ongoing research project for the creation of
a use-case-driven Knowledge Graph resource tailored to the needs of teaching
education in Knowledge Graphs (KGs). We gather resources related to KG courses
from lectures offered by the Semantic Web community, with the help of the COST
Action Distributed Knowledge Graphs and the interest group on KGs at The Alan
Turing Institute. Our goal is to create a resource-focused KG with multiple
interconnected semantic layers that interlink topics, courses, and materials
with each lecturer. Our approach formulates a domain KG in teaching and relates
it with multiple Personal KGs created for the lecturers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TODO: Enhancing LLM Alignment with Ternary Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Guo, Lu Yin, Bo Jiang, Jiaqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human intent is critical for
enhancing their performance across a variety of tasks. Standard alignment
techniques, such as Direct Preference Optimization (DPO), often rely on the
binary Bradley-Terry (BT) model, which can struggle to capture the complexities
of human preferences -- particularly in the presence of noisy or inconsistent
labels and frequent ties. To address these limitations, we introduce the
Tie-rank Oriented Bradley-Terry model (TOBT), an extension of the BT model that
explicitly incorporates ties, enabling more nuanced preference representation.
Building on this, we propose Tie-rank Oriented Direct Preference Optimization
(TODO), a novel alignment algorithm that leverages TOBT's ternary ranking
system to improve preference alignment. In evaluations on Mistral-7B and Llama
3-8B models, TODO consistently outperforms DPO in modeling preferences across
both in-distribution and out-of-distribution datasets. Additional assessments
using MT Bench and benchmarks such as Piqa, ARC-c, and MMLU further demonstrate
TODO's superior alignment performance. Notably, TODO also shows strong results
in binary preference alignment, highlighting its versatility and potential for
broader integration into LLM alignment. The implementation details can be found
in https://github.com/XXares/TODO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Cross-Correlated Network for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Chen, Yuanchen Bei, Wenbing Huang, Shengyuan Chen, Feiran Huang, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering (CF) models have demonstrated remarkable performance
in recommender systems, which represent users and items as embedding vectors.
Recently, due to the powerful modeling capability of graph neural networks for
user-item interaction graphs, graph-based CF models have gained increasing
attention. They encode each user/item and its subgraph into a single super
vector by combining graph embeddings after each graph convolution. However,
each hop of the neighbor in the user-item subgraphs carries a specific semantic
meaning. Encoding all subgraph information into single vectors and inferring
user-item relations with dot products can weaken the semantic information
between user and item subgraphs, thus leaving untapped potential. Exploiting
this untapped potential provides insight into improving performance for
existing recommendation models. To this end, we propose the Graph
Cross-correlated Network for Recommendation (GCR), which serves as a general
recommendation paradigm that explicitly considers correlations between
user/item subgraphs. GCR first introduces the Plain Graph Representation (PGR)
to extract information directly from each hop of neighbors into corresponding
PGR vectors. Then, GCR develops Cross-Correlated Aggregation (CCA) to construct
possible cross-correlated terms between PGR vectors of user/item subgraphs.
Finally, GCR comprehensively incorporates the cross-correlated terms for
recommendations. Experimental results show that GCR outperforms
state-of-the-art models on both interaction prediction and click-through rate
prediction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, accepted by TKDE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM4PR: Improving Post-Ranking in Search Engine with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yan, Yihao Wang, Chi Zhang, Wenyuan Hou, Kang Pan, Xingkai Ren, Zelun Wu, Zhixin Zhai, Enyun Yu, Wenwu Ou, Yang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alongside the rapid development of Large Language Models (LLMs), there has
been a notable increase in efforts to integrate LLM techniques in information
retrieval (IR) and search engines (SE). Recently, an additional post-ranking
stage is suggested in SE to enhance user satisfaction in practical
applications. Nevertheless, research dedicated to enhancing the post-ranking
stage through LLMs remains largely unexplored. In this study, we introduce a
novel paradigm named Large Language Models for Post-Ranking in search engine
(LLM4PR), which leverages the capabilities of LLMs to accomplish the
post-ranking task in SE. Concretely, a Query-Instructed Adapter (QIA) module is
designed to derive the user/item representation vectors by incorporating their
heterogeneous features. A feature adaptation step is further introduced to
align the semantics of user/item representations with the LLM. Finally, the
LLM4PR integrates a learning to post-rank step, leveraging both a main task and
an auxiliary task to fine-tune the model to adapt the post-ranking task.
Experiment studies demonstrate that the proposed framework leads to significant
improvements and exhibits state-of-the-art performance compared with other
alternatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting the Geolocation of Tweets Using <span class="highlight-title">transformer</span> models on
  Customized Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07865v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07865v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kateryna Lutsai, Christoph H. Lampert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research is aimed to solve the tweet/user geolocation prediction task
and provide a flexible methodology for the geotagging of textual big data. The
suggested approach implements neural networks for natural language processing
(NLP) to estimate the location as coordinate pairs (longitude, latitude) and
two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models
has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder
Representations from Transformers (BERT) as base models. Performance metrics
show a median error of fewer than 30 km on a worldwide-level, and fewer than 15
km on the US-level datasets for the models trained and evaluated on text
features of tweets' content and metadata context. Our source code and data are
available at https://github.com/K4TEL/geo-twitter.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 5 tables, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding and Improving Adversarial Collaborative Filtering for
  Robust Recommendation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaike Zhang, Qi Cao, Yunfan Wu, Fei Sun, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial Collaborative Filtering (ACF), which typically applies
adversarial perturbations at user and item embeddings through adversarial
training, is widely recognized as an effective strategy for enhancing the
robustness of Collaborative Filtering (CF) recommender systems against
poisoning attacks. Besides, numerous studies have empirically shown that ACF
can also improve recommendation performance compared to traditional CF. Despite
these empirical successes, the theoretical understanding of ACF's effectiveness
in terms of both performance and robustness remains unclear. To bridge this
gap, in this paper, we first theoretically show that ACF can achieve a lower
recommendation error compared to traditional CF with the same training epochs
in both clean and poisoned data contexts. Furthermore, by establishing bounds
for reductions in recommendation error during ACF's optimization process, we
find that applying personalized magnitudes of perturbation for different users
based on their embedding scales can further improve ACF's effectiveness.
Building on these theoretical understandings, we propose Personalized Magnitude
Adversarial Collaborative Filtering (PamaCF). Extensive experiments demonstrate
that PamaCF effectively defends against various types of poisoning attacks
while significantly enhancing recommendation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two-stage Conformal Risk Control with Application to Ranked Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Xu, Mufang Ying, Wenge Guo, Zhi Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many practical machine learning systems, such as ranking and recommendation
systems, consist of two concatenated stages: retrieval and ranking. These
systems present significant challenges in accurately assessing and managing the
uncertainty inherent in their predictions. To address these challenges, we
extend the recently developed framework of conformal risk control, originally
designed for single-stage problems, to accommodate the more complex two-stage
setup. We first demonstrate that a straightforward application of conformal
risk control, treating each stage independently, may fail to maintain risk at
their pre-specified levels. Therefore, we propose an integrated approach that
considers both stages simultaneously, devising algorithms to control the risk
of each stage by jointly identifying thresholds for both stages. Our algorithm
further optimizes for a weighted combination of prediction set sizes across all
feasible thresholds, resulting in more effective prediction sets. Finally, we
apply the proposed method to the critical task of two-stage ranked retrieval.
We validate the efficacy of our method through extensive experiments on two
large-scale public datasets, MSLR-WEB and MS MARCO, commonly used for ranked
retrieval tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 figures; 5 supplementary pages, 3 supplementary figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparative Analysis of Modality Fusion Approaches for Audio-Visual
  Person Identification and Verification <span class="chip">SP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aref Farhadipour, Masoumeh Chapariniya, Teodora Vukovic, Volker Dellwo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning involves integrating information from various modalities
to enhance learning and comprehension. We compare three modality fusion
strategies in person identification and verification by processing two
modalities: voice and face. In this paper, a one-dimensional convolutional
neural network is employed for x-vector extraction from voice, while the
pre-trained VGGFace2 network and transfer learning are utilized for face
modality. In addition, gammatonegram is used as speech representation in
engagement with the Darknet19 pre-trained network. The proposed systems are
evaluated using the K-fold cross-validation technique on the 118 speakers of
the test set of the VoxCeleb2 dataset. The comparative evaluations are done for
single-modality and three proposed multimodal strategies in equal situations.
Results demonstrate that the feature fusion strategy of gammatonegram and
facial features achieves the highest performance, with an accuracy of 98.37% in
the person identification task. However, concatenating facial features with the
x-vector reaches 0.62% for EER in verification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted at the ICNLSP2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-modal Speech Emotion Recognition via Feature Distribution
  Adaptation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22023v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22023v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaokai Li, Yixuan Ji, Peng Song, Haoqin Sun, Wenming Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel deep inductive transfer learning framework,
named feature distribution adaptation network, to tackle the challenging
multi-modal speech emotion recognition problem. Our method aims to use deep
transfer learning strategies to align visual and audio feature distributions to
obtain consistent representation of emotion, thereby improving the performance
of speech emotion recognition. In our model, the pre-trained ResNet-34 is
utilized for feature extraction for facial expression images and acoustic Mel
spectrograms, respectively. Then, the cross-attention mechanism is introduced
to model the intrinsic similarity relationships of multi-modal features.
Finally, the multi-modal feature distribution adaptation is performed
efficiently with feed-forward network, which is extended using the local
maximum mean discrepancy loss. Experiments are carried out on two benchmark
datasets, and the results demonstrate that our model can achieve excellent
performance compared with existing ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Visual Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18709v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18709v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruohao Guo, Xianghua Ying, Yaru Chen, Dantong Niu, Guangyao Li, Liao Qu, Yanyu Qi, Jinxing Zhou, Bowei Xing, Wenzhen Yue, Ji Shi, Qixun Wang, Peiliang Zhang, Buwen Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new multi-modal task, termed audio-visual
instance segmentation (AVIS), which aims to simultaneously identify, segment
and track individual sounding object instances in audible videos. To facilitate
this research, we introduce a high-quality benchmark named AVISeg, containing
over 90K instance masks from 26 semantic categories in 926 long videos.
Additionally, we propose a strong baseline model for this task. Our model first
localizes sound source within each frame, and condenses object-specific
contexts into concise tokens. Then it builds long-range audio-visual
dependencies between these tokens using window-based attention, and tracks
sounding objects among the entire video sequences. Extensive experiments reveal
that our method performs best on AVISeg, surpassing the existing methods from
related tasks. We further conduct the evaluation on several multi-modal large
models; however, they exhibits subpar performance on instance-level sound
source localization and temporal perception. We expect that AVIS will inspire
the community towards a more comprehensive multi-modal understanding. The
dataset and code will soon be released on https://github.com/ruohaoguo/avis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/ruohaoguo/avis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with
  Instruction Tuning <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zebang Cheng, Zhi-Qi Cheng, Jun-Yan He, Jingdong Sun, Kai Wang, Yuxiang Lin, Zheng Lian, Xiaojiang Peng, Alexander Hauptmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate emotion perception is crucial for various applications, including
human-computer interaction, education, and counseling. However, traditional
single-modality approaches often fail to capture the complexity of real-world
emotional expressions, which are inherently multimodal. Moreover, existing
Multimodal Large Language Models (MLLMs) face challenges in integrating audio
and recognizing subtle facial micro-expressions. To address this, we introduce
the MERR dataset, containing 28,618 coarse-grained and 4,487 fine-grained
annotated samples across diverse emotional categories. This dataset enables
models to learn from varied scenarios and generalize to real-world
applications. Furthermore, we propose Emotion-LLaMA, a model that seamlessly
integrates audio, visual, and textual inputs through emotion-specific encoders.
By aligning features into a shared space and employing a modified LLaMA model
with instruction tuning, Emotion-LLaMA significantly enhances both emotional
recognition and reasoning capabilities. Extensive evaluations show
Emotion-LLaMA outperforms other MLLMs, achieving top scores in Clue Overlap
(7.83) and Label Overlap (6.25) on EMER, an F1 score of 0.9036 on MER2023-SEMI
challenge, and the highest UAR (45.59) and WAR (59.37) in zero-shot evaluations
on DFEW dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2024. 49 pages, 13 figures, Project:
  https://github.com/ZebangCheng/Emotion-LLaMA, Demo:
  https://huggingface.co/spaces/ZebangCheng/Emotion-LLaMA</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-11-01T00:00:00Z">2024-11-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Question Answering Precision with Optimized Vector Retrieval
  and Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lixiao Yang, Mengyang Xu, Weimao Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question-answering (QA) is an important application of Information Retrieval
(IR) and language models, and the latest trend is toward pre-trained large
neural networks with embedding parameters. Augmenting QA performances with
these LLMs requires intensive computational resources for fine-tuning. We
propose an innovative approach to improve QA task performances by integrating
optimized vector retrievals and instruction methodologies. Based on retrieval
augmentation, the process involves document embedding, vector retrieval, and
context construction for optimal QA results. We experiment with different
combinations of text segmentation techniques and similarity functions, and
analyze their impacts on QA performances. Results show that the model with a
small chunk size of 100 without any overlap of the chunks achieves the best
result and outperforms the models based on semantic segmentation using
sentences. We discuss related QA examples and offer insight into how model
performances are improved within the two-stage framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORAG: A Cost-Constrained Retrieval Optimization System for
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziting Wang, Haitao Yuan, Wei Dong, Gao Cong, Feifei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable generation
capabilities but often struggle to access up-to-date information, which can
lead to hallucinations. Retrieval-Augmented Generation (RAG) addresses this
issue by incorporating knowledge from external databases, enabling more
accurate and relevant responses. Due to the context window constraints of LLMs,
it is impractical to input the entire external database context directly into
the model. Instead, only the most relevant information, referred to as chunks,
is selectively retrieved. However, current RAG research faces three key
challenges. First, existing solutions often select each chunk independently,
overlooking potential correlations among them. Second, in practice the utility
of chunks is non-monotonic, meaning that adding more chunks can decrease
overall utility. Traditional methods emphasize maximizing the number of
included chunks, which can inadvertently compromise performance. Third, each
type of user query possesses unique characteristics that require tailored
handling, an aspect that current approaches do not fully consider. To overcome
these challenges, we propose a cost constrained retrieval optimization system
CORAG for retrieval-augmented generation. We employ a Monte Carlo Tree Search
(MCTS) based policy framework to find optimal chunk combinations sequentially,
allowing for a comprehensive consideration of correlations among chunks.
Additionally, rather than viewing budget exhaustion as a termination condition,
we integrate budget constraints into the optimization of chunk combinations,
effectively addressing the non-monotonicity of chunk utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A graph-based approach to extracting narrative signals from public
  discourse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Pournaki, Tom Willaert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narratives are key interpretative devices by which humans make sense of
political reality. As the significance of narratives for understanding current
societal issues such as polarization and misinformation becomes increasingly
evident, there is a growing demand for methods that support their empirical
analysis. To this end, we propose a graph-based formalism and machine-guided
method for extracting, representing, and analyzing selected narrative signals
from digital textual corpora, based on Abstract Meaning Representation (AMR).
The formalism and method introduced here specifically cater to the study of
political narratives that figure in texts from digital media such as archived
political speeches, social media posts, political manifestos and transcripts of
parliamentary debates. We conceptualize these political narratives as a type of
ontological narratives: stories by which actors position themselves as
political beings, and which are akin to political worldviews in which actors
present their normative vision of the world, or aspects thereof. We approach
the study of such political narratives as a problem of information retrieval:
starting from a textual corpus, we first extract a graph-like representation of
the meaning of each sentence in the corpus using AMR. Drawing on transferable
concepts from narratology, we then apply a set of heuristics to filter these
graphs for representations of 1) actors, 2) the events in which these actors
figure, and 3) traces of the perspectivization of these events. We approach
these references to actors, events, and instances of perspectivization as core
narrative signals that initiate a further analysis by alluding to larger
political narratives. By means of a case study of State of the European Union
addresses, we demonstrate how the formalism can be used to inductively surface
signals of political narratives from public discourse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making Sense of Metadata Mess: Alignment & Risk Assessment for Diatom
  Data Use Case 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kio Polson, Marina Potapova, Uttam Meena, Chad Peiper, Joshua Brown, Joshua Agar, Jane Greenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biologists study Diatoms, a fundamental algae, to assess the health of
aquatic systems. Diatom specimens have traditionally been preserved on analog
slides, where a single slide can contain thousands of these microscopic
organisms. Digitization of these collections presents both metadata challenges
and opportunities. This paper reports on metadata research aimed at providing
access to a digital portion of the Academy of Natural Sciences' Diatom
Herbarium, Drexel University. We report results of a 3-part study covering 1) a
review of relevant metadata standards and a microscopy metadata framework
shared by Hammer et al., 2) a baseline metadata alignment mapping current
diatom metadata properties to standard metadata types, and 3) a metadata risk
analysis associated with the course of standard data curation practices. This
research is part of an effort involving the transfer of these digital slides to
an new system, DataFed, to support global accessible. The final section of this
paper includes a conclusion and discusses next steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures, 1 table, to be published in MTSR 2024 conference
  proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Semantic Interoperability Across Materials Science With
  HIVE4MAT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jane Greenberg, Kio Polson, Scott McClellan, Xintong Zhao, Alex Kalinowski, Yuan An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  HIVE4MAT is a linked data interactive application for navigating ontologies
of value to materials science. HIVE enables automatic indexing of textual
resources with standardized terminology. This article presents the motivation
underlying HIVE4MAT, explains the system architecture, reports on two
evaluations, and discusses future plans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figures, 3 tables, to be published in SeMatS 2024
  workshop proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-KT: A Versatile Framework for Knowledge Transfer from Large Language
  Models to Collaborative Filtering <span class="chip">ICDM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Severin, Aleksei Ziablitsev, Yulia Savelyeva, Valeriy Tashchilin, Ivan Bulychev, Mikhail Yushkov, Artem Kushneruk, Amaliya Zaryvnykh, Dmitrii Kiselev, Andrey Savchenko, Ilya Makarov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LLM-KT, a flexible framework designed to enhance collaborative
filtering (CF) models by seamlessly integrating LLM (Large Language
Model)-generated features. Unlike existing methods that rely on passing
LLM-generated features as direct inputs, our framework injects these features
into an intermediate layer of any CF model, allowing the model to reconstruct
and leverage the embeddings internally. This model-agnostic approach works with
a wide range of CF models without requiring architectural changes, making it
adaptable to various recommendation scenarios. Our framework is built for easy
integration and modification, providing researchers and developers with a
powerful tool for extending CF model capabilities through efficient knowledge
transfer. We demonstrate its effectiveness through experiments on the MovieLens
and Amazon datasets, where it consistently improves baseline CF models.
Experimental studies showed that LLM-KT is competitive with the
state-of-the-art methods in context-aware settings but can be applied to a
broader range of CF models than current approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ICDM 2024 (demo track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIRFLEX: Music Information Retrieval Feature Library for Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuradha Chopra, Abhinaba Roy, Dorien Herremans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an extendable modular system that compiles a range of
music feature extraction models to aid music information retrieval research.
The features include musical elements like key, downbeats, and genre, as well
as audio characteristics like instrument recognition, vocals/instrumental
classification, and vocals gender detection. The integrated models are
state-of-the-art or latest open-source. The features can be extracted as latent
or post-processed labels, enabling integration into music applications such as
generative music, recommendation, and playlist generation. The modular design
allows easy integration of newly developed systems, making it a good
benchmarking and comparison tool. This versatile toolkit supports the research
community in developing innovative solutions by providing concrete musical
features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages, 4 tables, submitted to Extended Abstracts for the
  Late-Breaking Demo Session of the 25th Int. Society for Music Information
  Retrieval Conf., San Francisco, United States, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Few-Shot Cross-Domain Named Entity Recognition by Instruction
  Tuning a Word-Embedding based Retrieval Augmented Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhadip Nandi, Neeraj Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-Shot Cross-Domain NER is the process of leveraging knowledge from
data-rich source domains to perform entity recognition on data scarce target
domains. Most previous state-of-the-art (SOTA) approaches use pre-trained
language models (PLMs) for cross-domain NER. However, these models are often
domain specific. To successfully use these models for new target domains, we
need to modify either the model architecture or perform model finetuning using
data from the new domains. Both of these result in the creation of entirely new
NER models for each target domain which is infeasible for practical scenarios.
Recently,several works have attempted to use LLMs to solve Few-Shot
Cross-Domain NER. However, most of these are either too expensive for practical
purposes or struggle to follow LLM prompt instructions. In this paper, we
propose IF-WRANER (Instruction Finetuned Word-embedding based Retrieval
Augmented large language model for Named Entity Recognition), a retrieval
augmented LLM, finetuned for the NER task. By virtue of the regularization
techniques used during LLM finetuning and the adoption of word-level embedding
over sentence-level embedding during the retrieval of in-prompt examples,
IF-WRANER is able to outperform previous SOTA Few-Shot Cross-Domain NER
approaches. We have demonstrated the effectiveness of our model by benchmarking
its performance on the open source CrossNER dataset, on which it shows more
than 2% F1 score improvement over the previous SOTA model. We have deployed the
model for multiple customer care domains of an enterprise. Accurate entity
prediction through IF-WRANER helps direct customers to automated workflows for
the domains, thereby reducing escalations to human agents by almost 15% and
leading to millions of dollars in yearly savings for the company.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DivNet: Diversity-Aware Self-Correcting Sequential Recommendation
  Networks <span class="chip">CIKM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Xiao, Zaifan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the last stage of a typical \textit{recommendation system},
\textit{collective recommendation} aims to give the final touches to the
recommended items and their layout so as to optimize overall objectives such as
diversity and whole-page relevance. In practice, however, the interaction
dynamics among the recommended items, their visual appearances and meta-data
such as specifications are often too complex to be captured by experts'
heuristics or simple models. To address this issue, we propose a
\textit{\underline{div}ersity-aware self-correcting sequential recommendation
\underline{net}works} (\textit{DivNet}) that is able to estimate utility by
capturing the complex interactions among sequential items and diversify
recommendations simultaneously. Experiments on both offline and online settings
demonstrate that \textit{DivNet} can achieve better results compared to
baselines with or without collective recommendations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at CIKM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Bundle Recommendation: Methods, Applications, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Sun, Lin Li, Ming Li, Xiaohui Tao, Dong Zhang, Peipei Wang, Jimmy Xiangji Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, bundle recommendation systems have gained significant
attention in both academia and industry due to their ability to enhance user
experience and increase sales by recommending a set of items as a bundle rather
than individual items. This survey provides a comprehensive review on bundle
recommendation, beginning by a taxonomy for exploring product bundling. We
classify it into two categories based on bundling strategy from various
application domains, i.e., discriminative and generative bundle recommendation.
Then we formulate the corresponding tasks of the two categories and
systematically review their methods: 1) representation learning from bundle and
item levels and interaction modeling for discriminative bundle recommendation;
2) representation learning from item level and bundle generation for generative
bundle recommendation. Subsequently, we survey the resources of bundle
recommendation including datasets and evaluation metrics, and conduct
reproducibility experiments on mainstream models. Lastly, we discuss the main
challenges and highlight the promising future directions in the field of bundle
recommendation, aiming to serve as a useful resource for researchers and
practitioners. Our code and datasets are publicly available at
https://github.com/WUT-IDEA/bundle-recommendation-survey.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Utility: Evaluating LLM as Recommender 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chumeng Jiang, Jiayin Wang, Weizhi Ma, Charles L. A. Clarke, Shuai Wang, Chuhan Wu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of Large Language Models (LLMs), recent studies
employed LLMs as recommenders to provide personalized information services for
distinct users. Despite efforts to improve the accuracy of LLM-based
recommendation models, relatively little attention is paid to beyond-utility
dimensions. Moreover, there are unique evaluation aspects of LLM-based
recommendation models, which have been largely ignored. To bridge this gap, we
explore four new evaluation dimensions and propose a multidimensional
evaluation framework. The new evaluation dimensions include: 1) history length
sensitivity, 2) candidate position bias, 3) generation-involved performance,
and 4) hallucinations. All four dimensions have the potential to impact
performance, but are largely unnecessary for consideration in traditional
systems. Using this multidimensional evaluation framework, along with
traditional aspects, we evaluate the performance of seven LLM-based
recommenders, with three prompting strategies, comparing them with six
traditional models on both ranking and re-ranking tasks on four datasets. We
find that LLMs excel at handling tasks with prior knowledge and shorter input
histories in the ranking setting, and perform better in the re-ranking setting,
beating traditional models across multiple dimensions. However, LLMs exhibit
substantial candidate position bias issues, and some models hallucinate
non-existent items much more often than others. We intend our evaluation
framework and observations to benefit future research on the use of LLMs as
recommenders. The code and data are available at
https://github.com/JiangDeccc/EvaLLMasRecommender.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Musical Instrument Classification with Advanced Machine
  Learning Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanikij Chulev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Musical instrument classification, a key area in Music Information Retrieval,
has gained considerable interest due to its applications in education, digital
music production, and consumer media. Recent advances in machine learning,
specifically deep learning, have enhanced the capability to identify and
classify musical instruments from audio signals. This study applies various
machine learning methods, including Naive Bayes, Support Vector Machines,
Random Forests, Boosting techniques like AdaBoost and XGBoost, as well as deep
learning models such as Convolutional Neural Networks and Artificial Neural
Networks. The effectiveness of these methods is evaluated on the NSynth
dataset, a large repository of annotated musical sounds. By comparing these
approaches, the analysis aims to showcase the advantages and limitations of
each method, providing guidance for developing more accurate and efficient
classification systems. Additionally, hybrid model testing and discussion are
included. This research aims to support further studies in instrument
classification by proposing new approaches and future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 35 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Confidence Evaluation Measures in Zero-Shot CSS Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Farr, Iain Cruickshank, Nico Manzonelli, Nicholas Clark, Kate Starbird, Jevin West
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing classification confidence is critical for leveraging large language
models (LLMs) in automated labeling tasks, especially in the sensitive domains
presented by Computational Social Science (CSS) tasks. In this paper, we make
three key contributions: (1) we propose an uncertainty quantification (UQ)
performance measure tailored for data annotation tasks, (2) we compare, for the
first time, five different UQ strategies across three distinct LLMs and CSS
data annotation tasks, (3) we introduce a novel UQ aggregation strategy that
effectively identifies low-confidence LLM annotations and disproportionately
uncovers data incorrectly labeled by the LLMs. Our results demonstrate that our
proposed UQ aggregation strategy improves upon existing methods andcan be used
to significantly improve human-in-the-loop data annotation processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\texttt{MixGR}$: Enhancing Retriever Generalization for Scientific
  Domain through Complementary Granularity <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengyu Cai, Xinran Zhao, Tong Chen, Sihao Chen, Hongming Zhang, Iryna Gurevych, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show the growing significance of document retrieval in the
generation of LLMs, i.e., RAG, within the scientific domain by bridging their
knowledge gap. However, dense retrievers often struggle with domain-specific
retrieval and complex query-document relationships, particularly when query
segments correspond to various parts of a document. To alleviate such prevalent
challenges, this paper introduces $\texttt{MixGR}$, which improves dense
retrievers' awareness of query-document matching across various levels of
granularity in queries and documents using a zero-shot approach.
$\texttt{MixGR}$ fuses various metrics based on these granularities to a united
score that reflects a comprehensive query-document similarity. Our experiments
demonstrate that $\texttt{MixGR}$ outperforms previous document retrieval by
24.7%, 9.8%, and 6.9% on nDCG@5 with unsupervised, supervised, and LLM-based
retrievers, respectively, averaged on queries containing multiple subqueries
from five scientific retrieval datasets. Moreover, the efficacy of two
downstream scientific question-answering tasks highlights the advantage of
$\texttt{MixGR}$ to boost the application of LLMs in the scientific domain. The
code and experimental datasets are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Customizing Language Models with Instance-wise <span class="highlight-title">LoRA</span> for Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10159v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10159v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Kong, Jiancan Wu, An Zhang, Leheng Sheng, Hui Lin, Xiang Wang, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation systems predict the next interaction item based on
users' past interactions, aligning recommendations with individual preferences.
Leveraging the strengths of Large Language Models (LLMs) in knowledge
comprehension and reasoning, recent approaches are eager to apply LLMs to
sequential recommendation. A common paradigm is converting user behavior
sequences into instruction data, and fine-tuning the LLM with
parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).
However, the uniform application of LoRA across diverse user behaviors is
insufficient to capture individual variability, resulting in negative transfer
between disparate sequences. To address these challenges, we propose
Instance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation
task as a form of multi-task learning, integrating LoRA with the Mixture of
Experts (MoE) framework. This approach encourages different experts to capture
various aspects of user behavior. Additionally, we introduce a sequence
representation guided gate function that generates customized expert
participation weights for each user sequence, which allows dynamic parameter
adjustment for instance-wise recommendations. In sequential recommendation,
iLoRA achieves an average relative improvement of 11.4\% over basic LoRA in the
hit ratio metric, with less than a 1\% relative increase in trainable
parameters. Extensive experiments on three benchmark datasets demonstrate the
effectiveness of iLoRA, highlighting its superior performance compared to
existing methods in mitigating negative transfer and improving recommendation
accuracy. Our data and code are available at
https://github.com/AkaliKong/iLoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-ESR: Large Language Models Enhancement for Long-tailed Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Liu, Xian Wu, Yejing Wang, Zijian Zhang, Feng Tian, Yefeng Zheng, Xiangyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems (SRS) aim to predict users' subsequent choices
based on their historical interactions and have found applications in diverse
fields such as e-commerce and social media. However, in real-world systems,
most users interact with only a handful of items, while the majority of items
are seldom consumed. These two issues, known as the long-tail user and
long-tail item challenges, often pose difficulties for existing SRS. These
challenges can adversely affect user experience and seller benefits, making
them crucial to address. Though a few works have addressed the challenges, they
still struggle with the seesaw or noisy issues due to the intrinsic scarcity of
interactions. The advancements in large language models (LLMs) present a
promising solution to these problems from a semantic perspective. As one of the
pioneers in this field, we propose the Large Language Models Enhancement
framework for Sequential Recommendation (LLM-ESR). This framework utilizes
semantic embeddings derived from LLMs to enhance SRS without adding extra
inference load from LLMs. To address the long-tail item challenge, we design a
dual-view modeling framework that combines semantics from LLMs and
collaborative signals from conventional SRS. For the long-tail user challenge,
we propose a retrieval augmented self-distillation method to enhance user
preference representation using more informative interactions from similar
users. To verify the effectiveness and versatility of our proposed enhancement
framework, we conduct extensive experiments on three real-world datasets using
three popular SRS models. The results show that our method surpasses existing
baselines consistently, and benefits long-tail users and items especially. The
implementation code is available at
https://github.com/Applied-Machine-Learning-Lab/LLM-ESR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by NeruIPS'24 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MACRec: a Multi-Agent Collaboration Framework for Recommendation <span class="chip">SIGIR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15235v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15235v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhefan Wang, Yuanqing Yu, Wendi Zheng, Weizhi Ma, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-based agents have gained considerable attention for their decision-making
skills and ability to handle complex tasks. Recognizing the current gap in
leveraging agent capabilities for multi-agent collaboration in recommendation
systems, we introduce MACRec, a novel framework designed to enhance
recommendation systems through multi-agent collaboration. Unlike existing work
on using agents for user/item simulation, we aim to deploy multi-agents to
tackle recommendation tasks directly. In our framework, recommendation tasks
are addressed through the collaborative efforts of various specialized agents,
including Manager, User/Item Analyst, Reflector, Searcher, and Task
Interpreter, with different working flows. Furthermore, we provide application
examples of how developers can easily use MACRec on various recommendation
tasks, including rating prediction, sequential recommendation, conversational
recommendation, and explanation generation of recommendation results. The
framework and demonstration video are publicly available at
https://github.com/wzf2000/MACRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGIR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling User Satisfaction and Creator Productivity Trade-Offs in
  Recommendation Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yao, Yiming Liao, Jingzhou Liu, Shaoliang Nie, Qifan Wang, Haifeng Xu, Hongning Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On User-Generated Content (UGC) platforms, recommendation algorithms
significantly impact creators' motivation to produce content as they compete
for algorithmically allocated user traffic. This phenomenon subtly shapes the
volume and diversity of the content pool, which is crucial for the platform's
sustainability. In this work, we demonstrate, both theoretically and
empirically, that a purely relevance-driven policy with low exploration
strength boosts short-term user satisfaction but undermines the long-term
richness of the content pool. In contrast, a more aggressive exploration policy
may slightly compromise user satisfaction but promote higher content creation
volume. Our findings reveal a fundamental trade-off between immediate user
satisfaction and overall content production on UGC platforms. Building on this
finding, we propose an efficient optimization method to identify the optimal
exploration strength, balancing user and creator engagement. Our model can
serve as a pre-deployment audit tool for recommendation algorithms on UGC
platforms, helping to align their immediate objectives with sustainable,
long-term goals.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Generative and Discriminative Training for Multi-modal Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chow, Juncheng Li, Qifan Yu, Kaihang Pan, Hao Fei, Zhiqi Ge, Shuai Yang, Siliang Tang, Hanwang Zhang, Qianru Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, Vision-Language Models (VLMs) have been trained under two
predominant paradigms. Generative training has enabled Multimodal Large
Language Models (MLLMs) to tackle various complex tasks, yet issues such as
hallucinations and weak object discrimination persist. Discriminative training,
exemplified by models like CLIP, excels in zero-shot image-text classification
and retrieval, yet struggles with complex scenarios requiring fine-grained
semantic differentiation. This paper addresses these challenges by proposing a
unified approach that integrates the strengths of both paradigms. Considering
interleaved image-text sequences as the general format of input samples, we
introduce a structure-induced training strategy that imposes semantic
relationships between input samples and the MLLM's hidden state. This approach
enhances the MLLM's ability to capture global semantics and distinguish
fine-grained semantics. By leveraging dynamic sequence alignment within the
Dynamic Time Warping framework and integrating a novel kernel for fine-grained
semantic differentiation, our method effectively balances generative and
discriminative tasks. Extensive experiments demonstrate the effectiveness of
our approach, achieving state-of-the-art results in multiple generative tasks,
especially those requiring cognitive and discrimination abilities.
Additionally, our method surpasses discriminative benchmarks in interleaved and
fine-grained retrieval tasks. By employing a retrieval-augmented generation
strategy, our approach further enhances performance in some generative tasks
within one model, offering a promising direction for future research in
vision-language modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust Multimodal Sentiment Analysis with Incomplete Data <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhang, Wenbin Wang, Tianshu Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an
emerging direction seeking to tackle the issue of data incompleteness.
Recognizing that the language modality typically contains dense sentiment
information, we consider it as the dominant modality and present an innovative
Language-dominated Noise-resistant Learning Network (LNLN) to achieve robust
MSA. The proposed LNLN features a dominant modality correction (DMC) module and
dominant modality based multimodal learning (DMML) module, which enhances the
model's robustness across various noise scenarios by ensuring the quality of
dominant modality representations. Aside from the methodical design, we perform
comprehensive experiments under random data missing scenarios, utilizing
diverse and meaningful settings on several popular datasets (\textit{e.g.,}
MOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and
fairness compared to existing evaluations in the literature. Empirically, LNLN
consistently outperforms existing baselines, demonstrating superior performance
across these challenging and extensive evaluation metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-11-09T05:28:58.314474193Z">
            2024-11-09 05:28:58 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
